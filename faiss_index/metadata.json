[
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**Image:** [No caption returned] ##### **Foundations of Machine Learning** **second edition** **Adaptive Computation and Machine Learning** Francis Bach, Editor A complete list of books published in The Adaptive Computations and Machine Learning series appears at the back of this book. ##### **Foundations of Machine Learning** **second edition** Mehryar Mohri Afshin Rostamizadeh Ameet Talwalkar The MIT Press Cambridge, Massachusetts London, England _\u20dd_ c 2018 Massachusetts Institute of Technology This work is subject to a Creative Commons CC BY-NC-ND license. Subject to All rights reserved. No part of this book may be reproduced in any form by any ~~such~~ ~~electronic~~ ~~license~~ ~~or~~, ~~mechanical~~ ~~all~~ ~~rights~~ ~~are~~ ~~means~~ ~~reserved~~ ~~(including~~ . ~~photocopying~~, ~~recording~~, ~~or~~ ~~information~~ storage and retrieval) without permission in writing from the publisher. This book was set in L [A] TEX by the authors. Printed and bound in the United States **Image:** [No caption returned] of America. Library of Congress Cataloging-in-Publication Data Names: Mohri, Mehryar, author. _|_ Rostamizadeh, Afshin, author. _|_ Talwalkar, Ameet, author. Title: Foundations of machine learning / Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Description: Second edition. _|_ Cambridge, MA : The MIT Press, [2018] _|_ Series: Adaptive computation and machine learning series _|_ Includes bibliographical references and index. Identifiers: LCCN 2018022812 _|_ ISBN 9780262039406 (hardcover : alk. paper) Subjects: LCSH: Machine learning. _|_ Computer algorithms. Classification: LCC Q325.5 .M64 2018 _|_ DDC 006.3/1--dc23 LC record available at https://lccn.loc.gov/2018022812 10 9 8 7 6 5 4 3 2 **Contents** Preface xiii **1** **Introduction** **1** 1.1 What is machine learning? 1 1.2 What kind of problems can be tackled using machine learning? 2 1.3 Some standard learning tasks 3 1.4 Learning stages 4 1.5 Learning scenarios 6 1.6 Generalization 7 **2** **The PAC Learning Framework** **9** 2.1 The PAC learning model 9 2.2 Guarantees for finite hypothesis sets \u2014 consistent case 15 2.3 Guarantees for finite hypothesis sets \u2014 inconsistent case 19 2.4 Generalities 21 2.4.1 Deterministic versus stochastic scenarios 21 2.4.2 Bayes error and noise 22 2.5 Chapter notes 23 2.6 Exercises 23 **3** **Rademacher Complexity and VC-Dimension** **29** 3.1 Rademacher complexity 30 3.2 Growth function 34 3.3 VC-dimension 36 3.4 Lower bounds 43 3.5 Chapter notes 48 3.6 Exercises 50 **4** **Model Selection** **61** 4.1 Estimation and approximation errors 61 4.2 Empirical risk minimization (ERM) 62 4.3 Structural risk minimization (SRM) 64 **vi** **Contents** 4.4 Cross-validation 68 4.5 _n_ -Fold cross-validation 71 4.6 Regularization-based algorithms 72 4.7 Convex surrogate losses 73 4.8 Chapter notes 77 4.9 Exercises 78 **5** **Support Vector Machines** **79** 5.1 Linear classification 79 5.2 Separable case 80 5.2.1 Primal optimization problem 81 5.2.2 Support vectors 83 5.2.3 Dual optimization problem 83 5.2.4 Leave-one-out analysis 85 5.3 Non-separable case 87 5.3.1 Primal optimization problem 88 5.3.2 Support vectors 89 5.3.3 Dual optimization problem 90 5.4 Margin theory 91 5.5 Chapter notes 100 5.6 Exercises 100 **6** **Kernel Methods** **105** 6.1 Introduction 105 6.2 Positive definite symmetric kernels 108 6.2.1 Definitions 108 6.2.2 Reproducing kernel Hilbert space 110 6.2.3 Properties 112 6.3 Kernel-based algorithms 116 6.3.1 SVMs with PDS kernels 116 6.3.2 Representer theorem 117 6.3.3 Learning",
    "chunk_id": "foundations_machine_learning_0"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "guarantees 117 6.4 Negative definite symmetric kernels 119 6.5 Sequence kernels 121 6.5.1 Weighted transducers 122 6.5.2 Rational kernels 126 6.6 Approximate kernel feature maps 130 6.7 Chapter notes 135 6.8 Exercises 137 **7** **Boosting** **145** 7.1 Introduction 145 7.2 AdaBoost 146 7.2.1 Bound on the empirical error 149 7.2.2 Relationship with coordinate descent 150 7.2.3 Practical use 154 **Contents** **vii** 7.3 Theoretical results 154 7.3.1 VC-dimension-based analysis 154 7.3.2 _L_ 1 -geometric margin 155 7.3.3 Margin-based analysis 157 7.3.4 Margin maximization 161 7.3.5 Game-theoretic interpretation 162 7.4 _L_ 1 -regularization 165 7.5 Discussion 167 7.6 Chapter notes 168 7.7 Exercises 170 **8** **On-Line Learning** **177** 8.1 Introduction 178 8.2 Prediction with expert advice 178 8.2.1 Mistake bounds and Halving algorithm 179 8.2.2 Weighted majority algorithm 181 8.2.3 Randomized weighted majority algorithm 183 8.2.4 Exponential weighted average algorithm 186 8.3 Linear classification 190 8.3.1 Perceptron algorithm 190 8.3.2 Winnow algorithm 198 8.4 On-line to batch conversion 201 8.5 Game-theoretic connection 204 8.6 Chapter notes 205 8.7 Exercises 206 **9** **Multi-Class Classification** **213** 9.1 Multi-class classification problem 213 9.2 Generalization bounds 215 9.3 Uncombined multi-class algorithms 221 9.3.1 Multi-class SVMs 221 9.3.2 Multi-class boosting algorithms 222 9.3.3 Decision trees 224 9.4 Aggregated multi-class algorithms 228 9.4.1 One-versus-all 229 9.4.2 One-versus-one 229 9.4.3 Error-correcting output codes 231 9.5 Structured prediction algorithms 233 9.6 Chapter notes 235 9.7 Exercises 237 **10** **Ranking** **239** 10.1 The problem of ranking 240 10.2 Generalization bound 241 10.3 Ranking with SVMs 243 **viii** **Contents** 10.4 RankBoost 244 10.4.1 Bound on the empirical error 246 10.4.2 Relationship with coordinate descent 248 10.4.3 Margin bound for ensemble methods in ranking 250 10.5 Bipartite ranking 251 10.5.1 Boosting in bipartite ranking 252 10.5.2 Area under the ROC curve 255 10.6 Preference-based setting 257 10.6.1 Second-stage ranking problem 257 10.6.2 Deterministic algorithm 259 10.6.3 Randomized algorithm 260 10.6.4 Extension to other loss functions 262 10.7 Other ranking criteria 262 10.8 Chapter notes 263 10.9 Exercises 264 **11** **Regression** **267** 11.1 The problem of regression 267 11.2 Generalization bounds 268 11.2.1 Finite hypothesis sets 268 11.2.2 Rademacher complexity bounds 269 11.2.3 Pseudo-dimension bounds 271 11.3 Regression algorithms 275 11.3.1 Linear regression 275 11.3.2 Kernel ridge regression 276 11.3.3 Support vector regression 281 11.3.4 Lasso 285 11.3.5 Group norm regression algorithms 289 11.3.6 On-line regression algorithms 289 11.4 Chapter notes 290 11.5 Exercises 292 **12** **Maximum Entropy Models** **295** 12.1 Density estimation problem 295 12.1.1 Maximum Likelihood (ML) solution 296 12.1.2 Maximum a Posteriori (MAP) solution 297 12.2 Density estimation problem augmented with features 297 12.3 Maxent principle 298 12.4 Maxent models 299 12.5 Dual problem 299 12.6 Generalization bound 303 12.7 Coordinate descent algorithm 304 12.8 Extensions 306 12.9 _L_ 2 -regularization 308 **Contents** **ix** 12.10 Chapter notes 312 12.11 Exercises 313 **13** **Conditional Maximum Entropy Models** **315** 13.1 Learning problem 315 13.2 Conditional Maxent principle 316 13.3 Conditional Maxent models 316 13.4 Dual problem 317 13.5 Properties 319 13.5.1 Optimization problem 320 13.5.2 Feature vectors 320 13.5.3 Prediction 321 13.6 Generalization bounds 321 13.7 Logistic regression 325 13.7.1 Optimization problem 325 13.7.2 Logistic",
    "chunk_id": "foundations_machine_learning_1"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "model 325 13.8 _L_ 2 -regularization 326 13.9 Proof of the duality theorem 328 13.10 Chapter notes 330 13.11 Exercises 331 **14** **Algorithmic Stability** **333** 14.1 Definitions 333 14.2 Stability-based generalization guarantee 334 14.3 Stability of kernel-based regularization algorithms 336 14.3.1 Application to regression algorithms: SVR and KRR 339 14.3.2 Application to classification algorithms: SVMs 341 14.3.3 Discussion 342 14.4 Chapter notes 342 14.5 Exercises 343 **15** **Dimensionality Reduction** **347** 15.1 Principal component analysis 348 15.2 Kernel principal component analysis (KPCA) 349 15.3 KPCA and manifold learning 351 15.3.1 Isomap 351 15.3.2 Laplacian eigenmaps 352 15.3.3 Locally linear embedding (LLE) 353 15.4 Johnson-Lindenstrauss lemma 354 15.5 Chapter notes 356 15.6 Exercises 356 **16** **Learning Automata and Languages** **359** 16.1 Introduction 359 **x** **Contents** 16.2 Finite automata 360 16.3 Efficient exact learning 361 16.3.1 Passive learning 362 16.3.2 Learning with queries 363 16.3.3 Learning automata with queries 364 16.4 Identification in the limit 369 16.4.1 Learning reversible automata 370 16.5 Chapter notes 375 16.6 Exercises 376 **17** **Reinforcement Learning** **379** 17.1 Learning scenario 379 17.2 Markov decision process model 380 17.3 Policy 381 17.3.1 Definition 381 17.3.2 Policy value 382 17.3.3 Optimal policies 382 17.3.4 Policy evaluation 385 17.4 Planning algorithms 387 17.4.1 Value iteration 387 17.4.2 Policy iteration 390 17.4.3 Linear programming 392 17.5 Learning algorithms 393 17.5.1 Stochastic approximation 394 17.5.2 TD(0) algorithm 397 17.5.3 Q-learning algorithm 398 17.5.4 SARSA 402 17.5.5 TD( _\u03bb_ ) algorithm 402 17.5.6 Large state space 403 17.6 Chapter notes 405 **Conclusion** **407** **A** **Linear Algebra Review** **409** A.1 Vectors and norms 409 A.1.1 Norms 409 A.1.2 Dual norms 410 A.1.3 Relationship between norms 411 A.2 Matrices 411 A.2.1 Matrix norms 411 A.2.2 Singular value decomposition 412 A.2.3 Symmetric positive semidefinite (SPSD) matrices 412 **Contents** **xi** **B** **Convex Optimization** **415** B.1 Differentiation and unconstrained optimization 415 B.2 Convexity 415 B.3 Constrained optimization 419 B.4 Fenchel duality 422 B.4.1 Subgradients 422 B.4.2 Core 423 B.4.3 Conjugate functions 423 B.5 Chapter notes 426 B.6 Exercises 427 **C** **Probability Review** **429** C.1 Probability 429 C.2 Random variables 429 C.3 Conditional probability and independence 431 C.4 Expectation and Markov\u2019s inequality 431 C.5 Variance and Chebyshev\u2019s inequality 432 C.6 Moment-generating functions 434 C.7 Exercises 435 **D** **Concentration Inequalities** **437** D.1 Hoeffding\u2019s inequality 437 D.2 Sanov\u2019s theorem 438 D.3 Multiplicative Chernoff bounds 439 D.4 Binomial distribution tails: Upper bounds 440 D.5 Binomial distribution tails: Lower bound 440 D.6 Azuma\u2019s inequality 441 D.7 McDiarmid\u2019s inequality 442 D.8 Normal distribution tails: Lower bound 443 D.9 Khintchine-Kahane inequality 443 D.10 Maximal inequality 444 D.11 Chapter notes 445 D.12 Exercises 445 **E** **Notions of Information Theory** **449** E.1 Entropy 449 E.2 Relative entropy 450 E.3 Mutual information 453 E.4 Bregman divergences 453 E.5 Chapter notes 456 E.6 Exercises 457 **xii** **Contents** **F** **Notation** **459** **Bibliography** **461** **Index** **475** **Preface** This book is a general introduction to machine learning that can serve as a reference book for researchers and a textbook for students. It covers fundamental modern topics in machine learning while providing the theoretical basis and conceptual tools needed for the discussion and justification of algorithms.",
    "chunk_id": "foundations_machine_learning_2"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "It also describes several key aspects of the application of these algorithms. We have aimed to present the most novel theoretical tools and concepts while giving concise proofs, even for relatively advanced results. In general, whenever possible, we have chosen to favor succinctness. Nevertheless, we discuss some crucial complex topics arising in machine learning and highlight several open research questions. Certain topics often merged with others or treated with insufficient attention are discussed separately here and with more emphasis: for example, a different chapter is reserved for multi-class classification, ranking, and regression. Although we cover a very wide variety of important topics in machine learning, we have chosen to omit a few important ones, including graphical models and neural networks, both for the sake of brevity and because of the current lack of solid theoretical guarantees for some methods. The book is intended for students and researchers in machine learning, statistics and other related areas. It can be used as a textbook for both graduate and advanced undergraduate classes in machine learning or as a reference text for a research seminar. The first three or four chapters of the book lay the theoretical foundation for the subsequent material. Other chapters are mostly self-contained, with the exception of chapter 6 which introduces some concepts that are extensively used in later ones and chapter 13, which is closely related to chapter 12. Each chapter concludes with a series of exercises, with full solutions presented separately. The reader is assumed to be familiar with basic concepts in linear algebra, probability, and analysis of algorithms. However, to further help, we have included an extensive appendix presenting a concise review of linear algebra, an introduction to convex optimization, a brief probability review, a collection of concentration **xiv** **Preface** inequalities useful to the analyses and discussions in this book, and a short introduction to information theory. Our goal has been to give a unified presentation of multiple topics and areas, as opposed to a more specialized presentation adopted by some books which favor a particular viewpoint, such as for example a Bayesian view, or a particular topic, such as for example kernel methods. The theoretical foundation of this book and its deliberate emphasis on proofs and analysis make it also very distinct from many other presentations. In this second edition, we have updated the entire book. The changes include a different writing style in most chapters, new figures and illustrations, many simplifications, some additions to existing chapters, in particular chapter 6 and chapter 17, and several new chapters. We have added a full chapter on model selection (chapter 4), which is an important topic that was only briefly discussed in the previous edition. We have also added a new chapter on Maximum Entropy models (chapter 12) and a new chapter on Conditional Maximum Entropy models (chapter 13) which are both essential topics in machine learning. We have also significantly changed the appendix. In particular, we have added a full section on Fenchel duality to appendix B on convex optimization, made a number of changes and additions to appendix",
    "chunk_id": "foundations_machine_learning_3"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "D dealing with concentration inequalities, added appendix E on information theory, and updated most of the material. Additionally, we have included a number of new exercises and their solutions for existing and new chapters. Most of the material presented here takes its origins in a machine learning graduate course ( _Foundations of Machine Learning_ ) taught by the first author at the Courant Institute of Mathematical Sciences in New York University over the last fourteen years. This book has considerably benefited from the comments and suggestions from students in these classes, along with those of many friends, colleagues and researchers to whom we are deeply indebted. We are particularly grateful to Corinna Cortes and Yishay Mansour who made a number of key suggestions for the design and organization of the material presented in the first edition, with detailed comments that we have fully taken into account and that have greatly improved the presentation. We are also grateful to Yishay Mansour for using a preliminary version of the first edition of the book for teaching, and for reporting his feedback to us. We also thank for discussions, suggested improvement, and contributions of many kinds the following colleagues and friends from academic and corporate research laboratories: Jacob Abernethy, Cyril Allauzen, Kareem Amin, Stephen Boyd, Aldo Corbisiero, Giulia DeSalvo, Claudio Gentile, Spencer Greenberg, Lisa Hellerstein, Sanjiv Kumar, Vitaly Kuznetsov, Ryan McDonald, Andr`es Mu\u02dcnoz Medina, Tyler Neylon, Peter Norvig, Fernando Pereira, Maria Pershina, Borja de Balle Pigem, **Preface** **xv** Ashish Rastogi, Michael Riley, Dmitry Storcheus, Ananda Theertha Suresh, Umar Syed, Csaba Szepesv\u00b4ari, Toshiyuki Tanaka, Eugene Weinstein, Jason Weston, Scott Yang, and Ningshan Zhang. Finally, we thank the MIT Press publication team for their help and support in the development of this text. # 1 Introduction This chapter presents a preliminary introduction to machine learning, including an overview of some key learning tasks and applications, basic definitions and terminology, and the discussion of some general scenarios. **1.1** **What is machine learning?** Machine learning can be broadly defined as computational methods using experience to improve performance or to make accurate predictions. Here, _experience_ refers to the past information available to the learner, which typically takes the form of electronic data collected and made available for analysis. This data could be in the form of digitized human-labeled training sets, or other types of information obtained via interaction with the environment. In all cases, its quality and size are crucial to the success of the predictions made by the learner. An example of a learning problem is how to use a finite sample of randomly selected documents, each labeled with a topic, to accurately predict the topic of unseen documents. Clearly, the larger is the sample, the easier is the task. But the difficulty of the task also depends on the quality of the labels assigned to the documents in the sample, since the labels may not be all correct, and on the number of possible topics. Machine learning consists of designing efficient and accurate prediction _algo-_ _rithms_ . As in other areas of computer science, some critical measures of",
    "chunk_id": "foundations_machine_learning_4"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the quality of these algorithms are their time and space complexity. But, in machine learning, we will need additionally a notion of _sample complexity_ to evaluate the sample size required for the algorithm to learn a family of concepts. More generally, theoretical learning guarantees for an algorithm depend on the complexity of the concept classes considered and the size of the training sample. Since the success of a learning algorithm depends on the data used, machine learning is inherently related to data analysis and statistics. More generally, learning **2** **Chapter 1** **Introduction** techniques are data-driven methods combining fundamental concepts in computer science with ideas from statistics, probability and optimization. **1.2** **What kind of problems can be tackled using machine learning?** Predicting the label of a document, also known as document classification, is by no means the only learning task. Machine learning admits a very broad set of practical applications, which include the following: _\u2022_ Text or document classification. This includes problems such as assigning a topic to a text or a document, or determining automatically if the content of a web page is inappropriate or too explicit; it also includes spam detection. _\u2022_ Natural language processing (NLP). Most tasks in this field, including part-ofspeech tagging, named-entity recognition, context-free parsing, or dependency parsing, are cast as learning problems. In these problems, predictions admit some structure. For example, in part-of-speech tagging, the prediction for a sentence is a sequence of part-of-speech tags labeling each word. In context-free parsing the prediction is a tree. These are instances of richer learning problems known as _structured prediction problems_ . _\u2022_ Speech processing applications. This includes speech recognition, speech synthesis, speaker verification, speaker identification, as well as sub-problems such as language modeling and acoustic modeling. _\u2022_ Computer vision applications. This includes object recognition, object identification, face detection, Optical character recognition (OCR), content-based image retrieval, or pose estimation. _\u2022_ Computational biology applications. This includes protein function prediction, identification of key sites, or the analysis of gene and protein networks. _\u2022_ Many other problems such as fraud detection for credit card, telephone or insurance companies, network intrusion, learning to play games such as chess, backgammon, or Go, unassisted control of vehicles such as robots or cars, medical diagnosis, the design of recommendation systems, search engines, or information extraction systems, are tackled using machine learning techniques. This list is by no means comprehensive. Most prediction problems found in practice can be cast as learning problems and the practical application area of machine learning keeps expanding. The algorithms and techniques discussed in this book can be used to derive solutions for all of these problems, though we will not discuss in detail these applications. **1.3** **Some standard learning tasks** **3** **1.3** **Some standard learning tasks** The following are some standard machine learning tasks that have been extensively studied: _\u2022_ _Classification_ : this is the problem of assigning a category to each item. For example, document classification consists of assigning a category such as _politics_, _business_, _sports_, or _weather_ to each document, while image classification consists of assigning to each image a category",
    "chunk_id": "foundations_machine_learning_5"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "such as _car_, _train_, or _plane_ . The number of categories in such tasks is often less than a few hundreds, but it can be much larger in some difficult tasks and even unbounded as in OCR, text classification, or speech recognition. _\u2022_ _Regression_ : this is the problem of predicting a real value for each item. Examples of regression include prediction of stock values or that of variations of economic variables. In regression, the penalty for an incorrect prediction depends on the magnitude of the difference between the true and predicted values, in contrast with the classification problem, where there is typically no notion of closeness between various categories. _\u2022_ _Ranking_ : this is the problem of learning to order items according to some criterion. Web search, e.g., returning web pages relevant to a search query, is the canonical ranking example. Many other similar ranking problems arise in the context of the design of information extraction or natural language processing systems. _\u2022_ _Clustering_ : this is the problem of partitioning a set of items into homogeneous subsets. Clustering is often used to analyze very large data sets. For example, in the context of social network analysis, clustering algorithms attempt to identify natural _communities_ within large groups of people. _\u2022_ _Dimensionality reduction_ or _manifold learning_ : this problem consists of transforming an initial representation of items into a lower-dimensional representation while preserving some properties of the initial representation. A common example involves preprocessing digital images in computer vision tasks. The main practical objectives of machine learning consist of generating accurate predictions for unseen items and of designing efficient and robust algorithms to produce these predictions, even for large-scale problems. To do so, a number of algorithmic and theoretical questions arise. Some fundamental questions include: Which concept families can actually be learned, and under what conditions? How well can these concepts be learned computationally? **4** **Chapter 1** **Introduction** **1.4** **Learning stages** Here, we will use the canonical problem of spam detection as a running example to illustrate some basic definitions and describe the use and evaluation of machine learning algorithms in practice, including their different stages. Spam detection is the problem of learning to automatically classify email messages as either spam or non-spam. The following is a list of definitions and terminology commonly used in machine learning: _\u2022_ _Examples_ : Items or instances of data used for learning or evaluation. In our spam problem, these examples correspond to the collection of email messages we will use for learning and testing. _\u2022_ _Features_ : The set of attributes, often represented as a vector, associated to an example. In the case of email messages, some relevant features may include the length of the message, the name of the sender, various characteristics of the header, the presence of certain keywords in the body of the message, and so on. _\u2022_ _Labels_ : Values or categories assigned to examples. In classification problems, examples are assigned specific categories, for instance, the spam and non-spam categories in our binary classification problem. In regression, items are assigned real-valued labels. _\u2022_",
    "chunk_id": "foundations_machine_learning_6"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_Hyperparameters_ : Free parameters that are not determined by the learning algorithm, but rather specified as inputs to the learning algorithm. _\u2022_ _Training sample_ : Examples used to train a learning algorithm. In our spam problem, the training sample consists of a set of email examples along with their associated labels. The training sample varies for different learning scenarios, as described in section 1.5. _\u2022_ _Validation sample_ : Examples used to tune the parameters of a learning algorithm when working with labeled data. The validation sample is used to select appropriate values for the learning algorithm\u2019s free parameters (hyperparameters). _\u2022_ _Test sample_ : Examples used to evaluate the performance of a learning algorithm. The test sample is separate from the training and validation data and is not made available in the learning stage. In the spam problem, the test sample consists of a collection of email examples for which the learning algorithm must predict labels based on features. These predictions are then compared with the labels of the test sample to measure the performance of the algorithm. _\u2022_ _Loss function_ : A function that measures the difference, or loss, between a predicted label and a true label. Denoting the set of all labels as Y and the set of possible predictions as Y _[\u2032]_, a loss function _L_ is a mapping _L_ : Y _\u00d7_ Y _[\u2032]_ _\u2192_ R + . In most cases, Y _[\u2032]_ = Y and the loss function is bounded, but these conditions do not always hold. Common examples of loss functions include the zero-one (or **1.4** **Learning stages** **5** _\u0338_ prior knowledge _\u0338_ features _\u0338_ parameter selection _\u0338_ labeled data _\u0338_ algorithm _\u0338_ |training sample|A(\u0398)<br>A(\u0398 )<br>0<br>evaluation| |---|---| |validation data|validation data| |test sample|test sample| **Figure 1.1** Illustration of the typical stages of a learning process. misclassification) loss defined over _{\u2212_ 1 _,_ +1 _} \u00d7 {\u2212_ 1 _,_ +1 _}_ by _L_ ( _y, y_ _[\u2032]_ ) = 1 _y_ _\u2032_ = _\u0338_ _y_ and the squared loss defined over I _\u00d7_ I by _L_ ( _y, y_ _[\u2032]_ ) = ( _y_ _[\u2032]_ _\u2212_ _y_ ) [2], where I _\u2286_ R is typically a bounded interval. _\u2022_ _Hypothesis set_ : A set of functions mapping features (feature vectors) to the set of labels Y. In our example, these may be a set of functions mapping email features to Y = _{_ spam _,_ non-spam _}_ . More generally, hypotheses may be functions mapping features to a different set Y _[\u2032]_ . They could be linear functions mapping email feature vectors to real numbers interpreted as _scores_ (Y _[\u2032]_ = R), with higher score values more indicative of spam than lower ones. We now define the learning stages of our spam problem (see figure 1.1). We start with a given collection of labeled examples. We first randomly partition the data into a training sample, a validation sample, and a test sample. The size of each of these samples depends on a number of different considerations. For example, the amount of data reserved for validation depends on the number",
    "chunk_id": "foundations_machine_learning_7"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "of hyperparameters of the algorithm, which are represented here by the vector \u0398. Also, when the labeled sample is relatively small, the amount of training data is often chosen to be larger than that of the test data since the learning performance directly depends on the training sample. Next, we associate relevant features to the examples. This is a critical step in the design of machine learning solutions. Useful features can effectively guide the learning algorithm, while poor or uninformative ones can be misleading. Although it is critical, to a large extent, the choice of the features is left to the user. This choice reflects the user\u2019s _prior knowledge_ about the learning task which in practice can have a dramatic effect on the performance results. Now, we use the features selected to train our learning algorithm A by tuning the values of its free parameters \u0398 (also called _hyperparameters_ ). For each value of these parameters, the algorithm selects a different hypothesis out of the hypothesis set. We choose the one resulting in the best performance on the validation sample (\u0398 0 ). Finally, using that hypothesis, we predict the labels of the examples in the test sample. The performance of the algorithm is evaluated by using the loss **6** **Chapter 1** **Introduction** function associated to the task, e.g., the zero-one loss in our spam detection task, to compare the predicted and true labels. Thus, the performance of an algorithm is of course evaluated based on its test error and not its error on the training sample. **1.5** **Learning scenarios** We next briefly describe some common machine learning scenarios. These scenarios differ in the types of training data available to the learner, the order and method by which training data is received and the test data used to evaluate the learning algorithm. _\u2022_ _Supervised learning_ : The learner receives a set of labeled examples as training data and makes predictions for all unseen points. This is the most common scenario associated with classification, regression, and ranking problems. The spam detection problem discussed in the previous section is an instance of supervised learning. _\u2022_ _Unsupervised learning_ : The learner exclusively receives unlabeled training data, and makes predictions for all unseen points. Since in general no labeled example is available in that setting, it can be difficult to quantitatively evaluate the performance of a learner. Clustering and dimensionality reduction are example of unsupervised learning problems. _\u2022_ _Semi-supervised learning_ : The learner receives a training sample consisting of both labeled and unlabeled data, and makes predictions for all unseen points. Semi-supervised learning is common in settings where unlabeled data is easily accessible but labels are expensive to obtain. Various types of problems arising in applications, including classification, regression, or ranking tasks, can be framed as instances of semi-supervised learning. The hope is that the distribution of unlabeled data accessible to the learner can help him achieve a better performance than in the supervised setting. The analysis of the conditions under which this can indeed be realized is the topic of much modern theoretical and applied",
    "chunk_id": "foundations_machine_learning_8"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "machine learning research. _\u2022_ _Transductive inference_ : As in the semi-supervised scenario, the learner receives a labeled training sample along with a set of unlabeled test points. However, the objective of transductive inference is to predict labels only for these particular test points. Transductive inference appears to be an easier task and matches the scenario encountered in a variety of modern applications. However, as in the semi-supervised setting, the assumptions under which a better performance can be achieved in this setting are research questions that have not been fully resolved. **1.6** **Generalization** **7** _\u2022_ _On-line learning_ : In contrast with the previous scenarios, the online scenario involves multiple rounds where training and testing phases are intermixed. At each round, the learner receives an unlabeled training point, makes a prediction, receives the true label, and incurs a loss. The objective in the on-line setting is to minimize the cumulative loss over all rounds or to minimize the _regret_, that is the difference of the cumulative loss incurred and that of the best expert in hindsight. Unlike the previous settings just discussed, no distributional assumption is made in on-line learning. In fact, instances and their labels may be chosen adversarially within this scenario. _\u2022_ _Reinforcement learning_ : The training and testing phases are also intermixed in reinforcement learning. To collect information, the learner actively interacts with the environment and in some cases affects the environment, and receives an immediate reward for each action. The object of the learner is to maximize his reward over a course of actions and iterations with the environment. However, no long-term reward feedback is provided by the environment, and the learner is faced with the _exploration versus exploitation_ dilemma, since he must choose between exploring unknown actions to gain more information versus exploiting the information already collected. _\u2022_ _Active learning_ : The learner adaptively or interactively collects training examples, typically by querying an oracle to request labels for new points. The goal in active learning is to achieve a performance comparable to the standard supervised learning scenario (or _passive learning_ scenario), but with fewer labeled examples. Active learning is often used in applications where labels are expensive to obtain, for example computational biology applications. In practice, many other intermediate and somewhat more complex learning scenarios may be encountered. **1.6** **Generalization** Machine learning is fundamentally about _generalization_ . As an example, the standard supervised learning scenario consists of using a finite sample of labeled examples to make accurate predictions about unseen examples. The problem is typically formulated as that of selecting a function out of a _hypothesis set_, that is a subset of the family of all functions. The function selected is subsequently used to label all instances, including unseen examples. How should a hypothesis set be chosen? With a rich or _complex_ hypothesis set, the learner may choose a function or predictor that is _consistent_ with the training sample, that is one that commits no error on the training sample. With a less complex family, incurring some errors on the training sample may be unavoidable. But, **8** **Chapter 1**",
    "chunk_id": "foundations_machine_learning_9"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**Introduction** **Image:** [No caption returned] Figure 1.2 illustrates these two types of solution: one is a zig-zag line that perfectly |Col1|Col2| |---|---| |Figure 1.2<br>The zig-zag line on the left panel is consistent over the bl<br>a complex separation surface that is not likely to general<br>the decision surface on the right panel is simpler and m<br>misclassification of a few points of the training sample.<br>which will lead to a better generalization? How sh<br>a hypothesis set?<br>Figure 1.2 illustrates these two types of solution:|r the bl<br>general<br>and m<br>ple.<br>How sh| separates the two populations of blue and red points and that is chosen from a complex family; the other one is a smoother line chosen from a simpler family that only imperfectly discriminates between the two sets. We will see that, in general, the best predictor on the training sample may not be the best overall. A predictor chosen from a very complex family can essentially memorize the data, but generalization is distinct from the memorization of the training labels. We will see that the trade-off between the sample size and complexity plays a critical role in generalization. When the sample size is relatively small, choosing from a too complex a family may lead to poor generalization, which is also known as _overfitting_ . On the other hand, with a too simple a family it may not be possible to achieve a sufficient accuracy, which is known as _underfitting_ . In the next chapters, we will analyze more in detail the problem of generalization and will seek to derive theoretical guarantees for learning. This will depend on different notions of complexity that we will thoroughly discuss. # 2 The PAC Learning Framework Several fundamental questions arise when designing and analyzing algorithms that learn from examples: What can be learned efficiently? What is inherently hard to learn? How many examples are needed to learn successfully? Is there a general model of learning? In this chapter, we begin to formalize and address these questions by introducing the _Probably Approximately Correct_ (PAC) learning framework. The PAC framework helps define the class of learnable concepts in terms of the number of sample points needed to achieve an approximate solution, _sample_ _complexity_, and the time and space complexity of the learning algorithm, which depends on the cost of the computational representation of the concepts. We first describe the PAC framework and illustrate it, then present some general learning guarantees within this framework when the hypothesis set used is finite, both for the _consistent_ case where the hypothesis set used contains the concept to learn and for the opposite _inconsistent_ case. **2.1** **The PAC learning model** We first introduce several definitions and the notation needed to present the PAC model, which will also be used throughout much of this book. We denote by X the set of all possible _examples_ or _instances_ . X is also sometimes referred to as the _input space_ . The set of all possible _labels_ or _target values_ is denoted by Y. For the purpose of this introductory chapter, we will limit ourselves to the case where Y is reduced",
    "chunk_id": "foundations_machine_learning_10"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "to two labels, Y = _{_ 0 _,_ 1 _}_, which corresponds to the so-called _binary classification_ . Later chapters will extend these results to more general settings. A _concept c_ : X _\u2192_ Y is a mapping from X to Y. Since Y = _{_ 0 _,_ 1 _}_, we can identify _c_ with the subset of X over which it takes the value 1. Thus, in the following, we equivalently refer to a concept to learn as a mapping from X to _{_ 0 _,_ 1 _}_, or as a subset of X. As an example, a concept may be the set of points inside a triangle **10** **Chapter 2** **The PAC Learning Framework** or the indicator function of these points. In such cases, we will say in short that the concept to learn is a triangle. A _concept class_ is a set of concepts we may wish to learn and is denoted by C. This could, for example, be the set of all triangles in the plane. We assume that examples are independently and identically distributed (i.i.d.) according to some fixed but unknown distribution D. The learning problem is then formulated as follows. The learner considers a fixed set of possible concepts H, called a _hypothesis set_, which might not necessarily coincide with C. It receives a sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ) drawn i.i.d. according to D as well as the labels ( _c_ ( _x_ 1 ) _, . . ., c_ ( _x_ _m_ )), which are based on a specific target concept _c \u2208_ C to learn. The task is then to use the labeled sample _S_ to select a hypothesis _h_ _S_ _\u2208_ H that has a small _generalization error_ with respect to the concept _c_ . The generalization error of a hypothesis _h \u2208_ H, also referred to as the _risk_ or _true error_ (or simply _error_ ) of _h_ is denoted by _R_ ( _h_ ) and defined as follows. [1] **Definition 2.1 (Generalization error)** _Given a hypothesis h \u2208_ H _, a target concept c \u2208_ C _,_ _and an underlying distribution_ D _, the_ generalization error _or_ risk _of h is defined by_ _R_ ( _h_ ) = _x\u223c_ P D [[] _[h]_ [(] _[x]_ [)] _[ \u0338]_ [=] _[ c]_ [(] _[x]_ [)] =] _x\u223c_ E D \ufffd1 _h_ ( _x_ )= _\u0338_ _c_ ( _x_ ) \ufffd _,_ (2.1) _where_ 1 _\u03c9_ _is the indicator function of the event \u03c9._ [2] The generalization error of a hypothesis is not directly accessible to the learner since both the distribution D and the target concept _c_ are unknown. However, the learner can measure the _empirical error_ of a hypothesis on the labeled sample _S_ . **Definition 2.2 (Empirical error)** _Given a hypothesis h \u2208_ H _, a target concept c \u2208_ C _, and_ _a sample S_ = ( _x_ 1 _, . . ., x_ _m_ ) _, the_ empirical error _or_ empirical risk _of h is defined by_ _\u0338_ _\u0338_ \ufffd",
    "chunk_id": "foundations_machine_learning_11"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_R_ _S_ ( _h_ ) = [1] _\u0338_ _m_ _\u0338_ _m_ \ufffd 1 _h_ ( _x_ _i_ )= _\u0338_ _c_ ( _x_ _i_ ) _._ (2.2) _i_ =1 _\u0338_ _\u0338_ Thus, the empirical error of _h \u2208_ H is its average error over the sample _S_, while the generalization error is its expected error based on the distribution D. We will see in this chapter and the following chapters a number of guarantees relating these two quantities with high probability, under some general assumptions. We can already note that for a fixed _h \u2208_ H, the expectation of the empirical error based on an i.i.d. 1 The choice of _R_ instead of _E_ to denote an error avoids possible confusions with the notation for expectations and is further justified by the fact that the term _risk_ is also used in machine learning and statistics to refer to an error. 2 For this and other related definitions, the family of functions H and the target concept _c_ must be measurable. The function classes we consider in this book all have this property. **2.1** **The PAC learning model** **11** sample _S_ is equal to the generalization error: E (2.3) _S\u223c_ D _[m]_ [[] _[R]_ [ \ufffd] _[S]_ [(] _[h]_ [)] =] _[ R]_ [(] _[h]_ [)] _[.]_ Indeed, by the linearity of the expectation and the fact that the sample is drawn i.i.d., we can write _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _m_ _[\u0338]_ _[\u0338]_ E [1] _S\u223c_ D _[m]_ [[] _[R]_ [ \ufffd] _[S]_ [(] _[h]_ [)] =] _m_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _m_ \ufffd _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ E [1] \ufffd _S\u223c_ D _[m]_ [[1] _[h]_ [(] _[x]_ _[i]_ [)] _[\u0338]_ [=] _[c]_ [(] _[x]_ _[i]_ [)] [] =] _m_ _[\u0338]_ _i_ =1 _[\u0338]_ _[\u0338]_ _m_ E _[\u0338]_ \ufffd _S\u223c_ D _[m]_ [[1] _[h]_ [(] _[x]_ [)] _[\u0338]_ [=] _[c]_ [(] _[x]_ [)] []] _[,]_ _i_ =1 _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ for any _x_ in sample _S_ . Thus, E E E _S\u223c_ D _[m]_ [[] _[R]_ [ \ufffd] _[S]_ [(] _[h]_ [)] =] _S\u223c_ D _[m]_ [[1] _[h]_ [(] _[x]_ [)] _[\u0338]_ [=] _[c]_ [(] _[x]_ [)] [] =] _x\u223c_ D [[1] _[h]_ [(] _[x]_ [)] _[\u0338]_ [=] _[c]_ [(] _[x]_ [)] [] =] _[ R]_ [(] _[h]_ [)] _[.]_ The following introduces the _Probably Approximately Correct_ (PAC) learning framework. Let _n_ be a number such that the computational cost of representing any element _x \u2208_ X is at most _O_ ( _n_ ) and denote by size( _c_ ) the maximal cost of the computational representation of _c \u2208_ C. For example, _x_ may be a vector in R _[n]_, for which the cost of an array-based representation would be in _O_ ( _n_ ). In addition, let _h_ _S_ denote the hypothesis returned by algorithm _A_ after receiving a labeled sample _S_ . To keep notation simple, the dependency of _h_ _S_ on _A_ is not explicitly indicated. **Definition 2.3 (PAC-learning)** _A concept class_ C _is said to be_ PAC-learnable _if there_ _exists an algorithm A and a polynomial",
    "chunk_id": "foundations_machine_learning_12"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "function poly_ ( _\u00b7, \u00b7, \u00b7, \u00b7_ ) _such that for any_ _\u03f5 >_ 0 _and \u03b4 >_ 0 _, for all distributions_ D _on_ X _and for any target concept c \u2208_ C _, the_ _following holds for any sample size m \u2265_ _poly_ (1 _/\u03f5,_ 1 _/\u03b4, n, size_ ( _c_ )) _:_ P (2.4) _S\u223c_ D _[m]_ [[] _[R]_ [(] _[h]_ _[S]_ [)] _[ \u2264]_ _[\u03f5]_ []] _[ \u2265]_ [1] _[ \u2212]_ _[\u03b4.]_ _If A further runs in poly_ (1 _/\u03f5,_ 1 _/\u03b4, n, size_ ( _c_ )) _, then_ C _is said to be_ efficiently PAClearnable _. When such an algorithm A exists, it is called a_ PAC-learning algorithm _for_ C _._ A concept class C is thus PAC-learnable if the hypothesis returned by the algorithm after observing a number of points polynomial in 1 _/\u03f5_ and 1 _/\u03b4_ is _approximately_ _correct_ (error at most _\u03f5_ ) with high _probability_ (at least 1 _\u2212_ _\u03b4_ ), which justifies the PAC terminology. The parameter _\u03b4 >_ 0 is used to define the _confidence_ 1 _\u2212_ _\u03b4_ and _\u03f5 >_ 0 the _accuracy_ 1 _\u2212_ _\u03f5_ . Note that if the running time of the algorithm is polynomial in 1 _/\u03f5_ and 1 _/\u03b4_, then the sample size _m_ must also be polynomial if the full sample is received by the algorithm. Several key points of the PAC definition are worth emphasizing. First, the PAC framework is a _distribution-free model_ : no particular assumption is made about the distribution D from which examples are drawn. Second, the training sample and the test examples used to define the error are drawn according to the same distribution D. This is a natural and necessary assumption for generalization to **12** **Chapter 2** **The PAC Learning Framework** R _[0]_ **Figure 2.1** |Col1|Col2|Col3|Col4|Col5| |---|---|---|---|---| |R||||| |R||||| |R|||R|| Target concept R and possible hypothesis R _[\u2032]_ . Circles represent training instances. A blue circle is a point labeled w ~~ith~~ ~~1~~, s ~~i~~ nce ~~it~~ ~~f~~ a ~~ll~~ s w ~~ithi~~ n ~~th~~ e rec ~~t~~ ang ~~l~~ e ~~R~~ . ~~Oth~~ ers are re ~~d~~ an ~~d~~ ~~l~~ a ~~b~~ e ~~l~~ e ~~d~~ w ~~ith~~ ~~0~~ . be possible in general. It can be relaxed to include favorable _domain adaptation_ problems. Finally, the PAC framework deals with the question of learnability for a concept class C and not a particular concept. Note that the concept class C is known to the algorithm, but of course the target concept _c \u2208_ C is unknown. In many cases, in particular when the computational representation of the concepts is not explicitly discussed or is straightforward, we may omit the polynomial dependency on _n_ and size( _c_ ) in the PAC definition and focus only on the sample complexity. We now illustrate PAC-learning with a specific learning problem. **Example 2.4 (Learning axis-aligned rectangles)** Consider the case where the set of instances are points in the plane, X = R [2], and the concept class C is the set of all axis-aligned rectangles lying in R [2] . Thus,",
    "chunk_id": "foundations_machine_learning_13"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "each concept _c_ is the set of points inside a particular axis-aligned rectangle. The learning problem consists of determining with small error a target axis-aligned rectangle using the labeled training sample. We will show that the concept class of axis-aligned rectangles is PAC-learnable. Figure 2.1 illustrates the problem. R represents a target axis-aligned rectangle and R _[\u2032]_ a hypothesis. As can be seen from the figure, the error regions of R _[\u2032]_ are formed by the area within the rectangle R but outside the rectangle R _[\u2032]_ and the area within R _[\u2032]_ but outside the rectangle R. The first area corresponds to _false negatives_, that is, points that are labeled as 0 or _negatively_ by R _[\u2032]_, which are in fact _positive_ or labeled with 1. The second area corresponds to _false positives_, that is, points labeled positively by R _[\u2032]_ which are in fact negatively labeled. To show that the concept class is PAC-learnable, we describe a simple PAClearning algorithm _A_ . Given a labeled sample _S_, the algorithm consists of returning the tightest axis-aligned rectangle R _[\u2032]_ = R S containing the points labeled with 1. Figure 2.2 illustrates the hypothesis returned by the algorithm. By definition, R S does not produce any false positives, since its points must be included in the target concept R. Thus, the error region of R S is included in R. **2.1** **The PAC learning model** **13** **Image:** [No caption returned] **Figure 2.2** Illustration of the hypothesis R _[\u2032]_ = R S returned by the algorithm. Let R _\u2208_ C be a target concept. Fix _\u03f5 >_ 0. Let P[R] denote the probability mass of the region defined by R, that is the probability that a point randomly drawn according to D falls within R. Since errors made by our algorithm can be due only to points falling inside R, we can assume that P[R] _> \u03f5_ ; otherwise, the error of R S is less than or equal to _\u03f5_ regardless of the training sample _S_ received. Now, since P[R] _> \u03f5_, we can define four rectangular regions _r_ 1 _, r_ 2 _, r_ 3 _,_ and _r_ 4 along the sides of R, each with probability at least _\u03f5/_ 4. These regions can be constructed by starting with the full rectangle R and then decreasing the size by moving one side as much as possible while keeping a distribution mass of at least _\u03f5/_ 4. Figure 2.3 illustrates the definition of these regions. Let _l_, _r_, _b_, and _t_ be the four real values defining R: R = [ _l, r_ ] _\u00d7_ [ _b, t_ ]. Then, for example, the left rectangle _r_ 4 is defined by _r_ 4 = [ _l, s_ 4 ] _\u00d7_ [ _b, t_ ], with _s_ 4 = inf _{s_ : P[[ _l, s_ ] _\u00d7_ [ _b, t_ ]] _\u2265_ _\u03f5/_ 4 _}_ . It is not hard to see that the probability of the region _r_ 4 = [ _l, s_ 4 [ _\u00d7_ [ _b, t_ ] obtained from _r_",
    "chunk_id": "foundations_machine_learning_14"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "4 by excluding the rightmost side is at most _\u03f5/_ 4. _r_ 1, _r_ 2, _r_ 3 and _r_ 1, _r_ 2, _r_ 3 are defined in a similar way. Observe that if R S meets all of these four regions _r_ _i_, _i \u2208_ [4], then, because it is a rectangle, it will have one side in each of these regions (geometric argument). Its error area, which is the part of R that it does not cover, is thus included in the union of the regions _r_ _i_, _i \u2208_ [4], and cannot have probability mass more than _\u03f5_ . By contraposition, if _R_ (R S ) _> \u03f5_, then R S must miss at least one of the regions _r_ _i_, _i \u2208_ [4]. As a result, we can write P P _i_ =1 _[{]_ [R] [S] _[\u2229]_ _[r]_ _[i]_ [=] _[ \u2205}]_ []] (2.5) _S\u223c_ D _[m]_ [[] _[R]_ [(][R] [S] [)] _[ > \u03f5]_ []] _[ \u2264]_ _S\u223c_ D _[m]_ [[] _[\u222a]_ [4] _\u2264_ 4 \ufffd _S\u223c_ P D _[m]_ [[] _[{]_ [R] [S] _[ \u2229]_ _[r]_ _[i]_ [ =] _[ \u2205}]_ []] (by the union bound) _i_ =1 _\u2264_ 4(1 _\u2212_ _\u03f5/_ 4) _[m]_ (since P[ _r_ _i_ ] _\u2265_ _\u03f5/_ 4) _\u2212_ _\u2264_ 4 exp( _m\u03f5/_ 4) _,_ **14** **Chapter 2** **The PAC Learning Framework** **Image:** [No caption returned] |Col1|r<br>1|Col3|Col4| |---|---|---|---| |r<br>4|||R0<br>r<br>2| ||r<br>3|r<br>3|| **Figure 2.3** Illustration of the regions _r_ 1 _, . . ., r_ 4 . where for the last step we used the general inequality 1 _\u2212_ _x \u2264_ _e_ _[\u2212][x]_ valid for all _x \u2208_ R. For any _\u03b4 >_ 0, to ensure that P _S\u223c_ D _m_ [ _R_ (R S ) _> \u03f5_ ] _\u2264_ _\u03b4_, we can impose 4 exp( _\u2212\u03f5m/_ 4) _\u2264_ _\u03b4 \u21d4_ _m \u2265_ [4] (2.6) _\u03b4_ _[.]_ [4] _\u03f5_ [log 4] _\u03b4_ Thus, for any _\u03f5 >_ 0 and _\u03b4 >_ 0, if the sample size _m_ is greater than [4] [4] _\u03f5_ [log] [4] _\u03b4_ Thus, for any _\u03f5 >_ 0 and _\u03b4 >_ 0, if the sample size _m_ is greater than _\u03f5_ [log] _\u03b4_ [, then] P _S\u223c_ D _m_ [ _R_ (R S ) _> \u03f5_ ] _\u2264_ _\u03b4_ . Furthermore, the computational cost of the representation of points in R [2] and axis-aligned rectangles, which can be defined by their four corners, is constant. This proves that the concept class of axis-aligned rectangles is PAC-learnable and that the sample complexity of PAC-learning axis-aligned rectangles is in _O_ ( [1] _\u03f5_ [log] [1] _\u03b4_ [).] An equivalent way to present sample complexity results like (2.6), which we will often see throughout this book, is to give a _generalization bound_ . A generalization bound states that with probability at least 1 _\u2212_ _\u03b4_, _R_ (R S ) is upper bounded by some quantity that depends on the sample size _m_ and _\u03b4_ . To obtain this, it suffices to set _\u03b4_ to be equal to the upper bound derived in (2.5), that is _\u03b4_ = 4 exp( _\u2212m\u03f5/_ 4) and solve for",
    "chunk_id": "foundations_machine_learning_15"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u03f5_ . This yields that with probability at least 1 _\u2212_ _\u03b4_, the error of the algorithm is bounded as follows: [1] _\u03f5_ [log] [1] _\u03b4_ _R_ (R S ) _\u2264_ [4] (2.7) _\u03b4_ _[.]_ _m_ [4] [log 4] _\u03b4_ Other PAC-learning algorithms could be considered for this example. One alternative is to return the largest axis-aligned rectangle not containing the negative points, for example. The proof of PAC-learning just presented for the tightest axis-aligned rectangle can be easily adapted to the analysis of other such algorithms. Note that the hypothesis set H we considered in this example coincided with the concept class C and that its cardinality was infinite. Nevertheless, the problem admitted a simple proof of PAC-learning. We may then ask if a similar proof can readily apply to other similar concept classes. This is not as straightforward because the specific geometric argument used in the proof is key. It is non-trivial to extend the proof to other concept classes such as that of non-concentric circles **2.2** **Guarantees for finite hypothesis sets \u2014 consistent case** **15** (see exercise 2.4). Thus, we need a more general proof technique and more general results. The next two sections provide us with such tools in the case of a finite hypothesis set. **2.2** **Guarantees for finite hypothesis sets \u2014 consistent case** In the example of axis-aligned rectangles that we examined, the hypothesis _h_ _S_ returned by the algorithm was always _consistent_, that is, it admitted no error on the training sample _S_ . In this section, we present a general sample complexity bound, or equivalently, a generalization bound, for consistent hypotheses, in the case where the cardinality _|_ H _|_ of the hypothesis set is finite. Since we consider consistent hypotheses, we will assume that the target concept _c_ is in H. **Theorem 2.5 (Learning bound \u2014 finite** H **, consistent case)** _Let_ H _be a finite set of func-_ _tions mapping from_ X _to_ Y _. Let A be an algorithm that for any target concept c \u2208_ H _and i.i.d. sample S returns a consistent hypothesis h_ _S_ _:_ _R_ [\ufffd] _S_ ( _h_ _S_ ) = 0 _. Then, for any_ _\u03f5, \u03b4 >_ 0 _, the inequality_ P _S\u223c_ D _m_ [ _R_ ( _h_ _S_ ) _\u2264_ _\u03f5_ ] _\u2265_ 1 _\u2212_ _\u03b4 holds if_ _m \u2265_ [1] _\u03f5_ log _|_ H _|_ + log [1] \ufffd _\u03b4_ _._ (2.8) \ufffd _This sample complexity result admits the following equivalent statement as a gen-_ _eralization bound: for any \u03f5, \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4,_ _R_ ( _h_ _S_ ) _\u2264_ [1] _m_ log _|_ H _|_ + log [1] \ufffd _\u03b4_ _._ (2.9) \ufffd Proof: Fix _\u03f5 >_ 0. We do not know which consistent hypothesis _h_ _S_ _\u2208_ H is selected by the algorithm _A_ . This hypothesis further depends on the training sample _S_ . Therefore, we need to give a _uniform convergence bound_, that is, a bound that holds for the set of all consistent hypotheses, which a fortiori includes _h_",
    "chunk_id": "foundations_machine_learning_16"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_S_ . Thus, we will bound the probability that some _h \u2208_ H would be consistent and have error more than _\u03f5_ . For any _\u03f5 >_ 0, define H _\u03f5_ by H _\u03f5_ = _{h \u2208_ H : _R_ ( _h_ ) _> \u03f5}_ . The probability that a hypothesis _h_ in H _\u03f5_ is consistent on a training sample _S_ drawn i.i.d., that is, that it would have no error on any point in _S_, can be bounded as follows: P[ _R_ [\ufffd] _S_ ( _h_ ) = 0] _\u2264_ (1 _\u2212_ _\u03f5_ ) _[m]_ _._ Thus, by the union bound, the following holds: \ufffd \ufffd P \ufffd _\u2203h \u2208_ H _\u03f5_ : _R_ [\ufffd] _S_ ( _h_ ) = 0\ufffd = P \ufffd _R_ _S_ ( _h_ 1 ) = 0 _\u2228\u00b7 \u00b7 \u00b7 \u2228_ _R_ _S_ ( _h_ _|_ H _\u03f5_ _|_ ) = 0\ufffd \ufffd _\u2264_ \ufffd P \ufffd _R_ _S_ ( _h_ ) = 0\ufffd (union bound) _h\u2208_ H _\u03f5_ _\u2264_ \ufffd (1 _\u2212_ _\u03f5_ ) _[m]_ _\u2264|_ H _|_ (1 _\u2212_ _\u03f5_ ) _[m]_ _\u2264|_ H _|e_ _[\u2212][m\u03f5]_ _._ _h\u2208_ H _\u03f5_ **16** **Chapter 2** **The PAC Learning Framework** Setting the right-hand side to be equal to _\u03b4_ and solving for _\u03f5_ concludes the proof. The theorem shows that when the hypothesis set H is finite, a consistent algorithm _A_ is a PAC-learning algorithm, since the sample complexity given by (2.8) is dominated by a polynomial in 1 _/\u03f5_ and 1 _/\u03b4_ . As shown by (2.9), the generalization error of consistent hypotheses is upper bounded by a term that decreases as a function of the sample size _m_ . This is a general fact: as expected, learning algorithms benefit from larger labeled training samples. The decrease rate of _O_ (1 _/m_ ) guaranteed by this theorem, however, is particularly favorable. The price to pay for coming up with a consistent algorithm is the use of a larger hypothesis set H containing target concepts. Of course, the upper bound (2.9) increases with _|_ H _|_ . However, that dependency is only logarithmic. Note that the term log _|_ H _|_, or the related term log 2 _|_ H _|_ from which it differs by a constant factor, can be interpreted as the number of bits needed to represent H. Thus, the generalization guarantee of the theorem is controlled by the ratio of this number of bits, log 2 _|_ H _|_, and the sample size _m_ . We now use theorem 2.5 to analyze PAC-learning with various concept classes. **Example 2.6 (Conjunction of Boolean literals)** Consider learning the concept class C _n_ of conjunctions of at most _n_ Boolean literals _x_ 1 _, . . ., x_ _n_ . A Boolean literal is either a variable _x_ _i_, _i \u2208_ [ _n_ ], or its negation _x_ _i_ . For _n_ = 4, an example is the conjunction: _x_ 1 _\u2227_ _x_ 2 _\u2227_ _x_ 4, where _x_ 2 denotes the negation of the Boolean literal _x_ 2 . (1 _,_ 0 _,_ 0 _,_",
    "chunk_id": "foundations_machine_learning_17"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "1) is a positive example for this concept while (1 _,_ 0 _,_ 0 _,_ 0) is a negative example. Observe that for _n_ = 4, a positive example (1 _,_ 0 _,_ 1 _,_ 0) implies that the target concept cannot contain the literals _x_ 1 and _x_ 3 and that it cannot contain the literals _x_ 2 and _x_ 4 . In contrast, a negative example is not as informative since it is not known which of its _n_ bits are incorrect. A simple algorithm for finding a consistent hypothesis is thus based on positive examples and consists of the following: for each positive example ( _b_ 1 _, . . ., b_ _n_ ) and _i \u2208_ [ _n_ ], if _b_ _i_ = 1 then _x_ _i_ is ruled out as a possible literal in the concept class and if _b_ _i_ = 0 then _x_ _i_ is ruled out. The conjunction of all the literals not ruled out is thus a hypothesis consistent with the target. Figure 2.4 shows an example training sample as well as a consistent hypothesis for the case _n_ = 6. We have _|_ H _|_ = _|_ C _n_ _|_ = 3 _[n]_, since each literal can be included positively, with negation, or not included. Plugging this into the sample complexity bound for consistent hypotheses yields the following sample complexity bound for any _\u03f5 >_ 0 and _\u03b4 >_ 0: _m \u2265_ [1] _\u03f5_ (log 3) _n_ + log [1] \ufffd _\u03b4_ _._ (2.10) \ufffd Thus, the class of conjunctions of at most _n_ Boolean literals is PAC-learnable. Note that the computational complexity is also polynomial, since the training cost per example is in _O_ ( _n_ ). For _\u03b4_ = 0 _._ 02, _\u03f5_ = 0 _._ 1, and _n_ = 10, the bound becomes **2.2** **Guarantees for finite hypothesis sets \u2014 consistent case** **17** **Figure 2.4** |0|1|1|0|1|1|+| |---|---|---|---|---|---|---| |0|1|1|1|1|1|+| |0|0|1|1|0|1|-| |0|1|1|1|1|1|+| |1|0|0|1|1|0|-| |0|1|0|0|1|1|+| |0|1|?|?|1|1|1| Each of the first six rows of the table represents a training example with its label, + or _\u2212_, indicated in the last column. The last row contains 0 (respectively 1) in column _i \u2208_ [6] if the _i_ th entry is 0 (respectively 1) for all the positive examples. It contains \u201c?\u201d if both 0 and 1 appear as an _i_ th entry for some positive example. Thus, for this training sample, the hypothesis returned by the consistent algorithm described in the text is _x_ 1 _\u2227_ _x_ 2 _\u2227_ _x_ 5 _\u2227_ _x_ 6 . _m \u2265_ 149. Thus, for a labeled sample of at least 149 examples, the bound guarantees 90% accuracy with a confidence of at least 98%. **Example 2.7 (Universal concept class)** Consider the set X = _{_ 0 _,_ 1 _}_ _[n]_ of all Boolean vectors with _n_ components, and let U _n_ be the concept class formed by all subsets of X. Is this concept class PAC-learnable? To guarantee a consistent hypothesis the hypothesis class must include the concept class, thus _|_ H _| \u2265|_ U _n_ _|_ =",
    "chunk_id": "foundations_machine_learning_18"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "2 [(2] _[n]_ [)] . Theorem 2.5 gives the following sample complexity bound: _m \u2265_ [1] _\u03f5_ (log 2)2 _[n]_ + log [1] \ufffd _\u03b4_ _._ (2.11) \ufffd Here, the number of training samples required is exponential in _n_, which is the cost of the representation of a point in X. Thus, PAC-learning is not guaranteed by the theorem. In fact, it is not hard to show that this universal concept class is not PAC-learnable. **Example 2.8 (** _k_ **-term DNF formulae)** A disjunctive normal form (DNF) formula is a formula written as the disjunction of several terms, each term being a conjunction of Boolean literals. A _k_ -term DNF is a DNF formula defined by the disjunction of _k_ terms, each term being a conjunction of at most _n_ Boolean literals. Thus, for _k_ = 2 and _n_ = 3, an example of a _k_ -term DNF is ( _x_ 1 _\u2227_ _x_ 2 _\u2227_ _x_ 3 ) _\u2228_ ~~(~~ _x_ 1 _\u2227_ _x_ 3 ). Is the class C of _k_ -term DNF formulae PAC-learnable? The cardinality of the class is 3 _[nk]_, since each term is a conjunction of at most _n_ variables and there are 3 _[n]_ such conjunctions, as seen previously. The hypothesis set H must contain C for **18** **Chapter 2** **The PAC Learning Framework** consistency to be possible, thus _|_ H _| \u2265_ 3 _[nk]_ . Theorem 2.5 gives the following sample complexity bound: _m \u2265_ [1] _,_ (2.12) \ufffd _\u03f5_ (log 3) _nk_ + log [1] \ufffd _\u03b4_ _\u03b4_ which is polynomial. However, it can be shown by a reduction from the graph 3-coloring problem that the problem of learning _k_ -term DNF, even for _k_ = 3, is not efficiently PAC-learnable, unless RP, the complexity class of problems that admit a randomized polynomial-time decision solution, coincides with NP(RP = NP) _,_ which is commonly conjectured not to be the case. Thus, while the sample size needed for learning _k_ -term DNF formulae is only polynomial, efficient PAC-learning of this class is not possible if RP _\u0338_ = NP. **Example 2.9 (** _k_ **-CNF formulae)** A conjunctive normal form (CNF) formula is a conjunction of disjunctions. A _k_ -CNF formula is an expression of the form _T_ 1 _\u2227_ _. . ._ _\u2227_ _T_ _j_ with arbitrary length _j \u2208_ N and with each term _T_ _i_ being a disjunction of at most _k_ Boolean attributes. The problem of learning _k_ -CNF formulae can be reduced to that of learning conjunctions of Boolean literals, which, as seen previously, is a PAC-learnable concept class. This can be done at the cost of introducing (2 _n_ ) _[k]_ new variables _Y_ _u_ 1 _,...,u_ _k_ using the following bijection: ( _u_ 1 _, . . ., u_ _k_ ) _\u2192_ _Y_ _u_ 1 _,...,u_ _k_ _,_ (2.13) where _u_ 1 _, . . ., u_ _k_ are Boolean literals over the original variables _x_ 1 _, . . ., x_ _n_ . The value of _Y_ _u_ 1 _,...,u_ _k_ is determined by _Y_ _u_ 1",
    "chunk_id": "foundations_machine_learning_19"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_,...,u_ _k_ = _u_ 1 _\u2228\u00b7 \u00b7 \u00b7 \u2228_ _u_ _k_ . Using this mapping, the original training sample can be transformed into one defined in terms of the new variables and any _k_ -CNF formula over the original variables can be written as a conjunction over the variables _Y_ _u_ 1 _,...,u_ _k_ . This reduction to PAC-learning of conjunctions of Boolean literals can affect the original distribution of examples, but this is not an issue since in the PAC framework no assumption is made about the distribution. Thus, using this transformation, the PAC-learnability of conjunctions of Boolean literals implies that of _k_ -CNF formulae. This is a surprising result, however, since any _k_ -term DNF formula can be written as a _k_ -CNF formula. Indeed, using associativity, a _k_ -term DNF _T_ 1 _\u2228\u00b7 \u00b7 \u00b7 \u2228_ _T_ _k_ with _T_ _i_ = _u_ _i,_ 1 _\u2227\u00b7 \u00b7 \u00b7 \u2227_ _u_ _i,n_ _i_ for _i \u2208_ [ _k_ ] can be rewritten as a _k_ -CNF formula via _k_ \ufffd _u_ 1 _,j_ 1 _\u2228\u00b7 \u00b7 \u00b7 \u2228_ _u_ _k,j_ _k_ _,_ _j_ 1 _\u2208_ [ _n_ 1 ] _,...,j_ _k_ _\u2208_ [ _n_ _k_ ] \ufffd _u_ _i,_ 1 _\u2227\u00b7 \u00b7 \u00b7 \u2227_ _u_ _i,n_ _i_ = \ufffd _i_ =1 _n_ To illustrate this rewriting in a specific case, observe, for example, that ( _u_ 1 _\u2227_ _u_ 2 _\u2227_ _u_ 3 ) _\u2228_ ( _v_ 1 _\u2227_ _v_ 2 _\u2227_ _v_ 3 ) = 3 \ufffd ( _u_ _i_ _\u2228_ _v_ _j_ ) _._ _i,j_ =1 **2.3** **Guarantees for finite hypothesis sets \u2014 inconsistent case** **19** But, as we previously saw, _k_ -term DNF formulae are not efficiently PAC-learnable if RP _\u0338_ = NP! What can explain this apparent inconsistency? The issue is that converting into a _k_ -term DNF a _k_ -CNF formula we have learned (which is equivalent to a _k_ -term DNF) is in general intractable if RP _\u0338_ = NP. This example reveals some key aspects of PAC-learning, which include the cost of the representation of a concept and the choice of the hypothesis set. For a fixed concept class, learning can be intractable or not depending on the choice of the representation. **2.3** **Guarantees for finite hypothesis sets \u2014 inconsistent case** In the most general case, there may be no hypothesis in H consistent with the labeled training sample. This, in fact, is the typical case in practice, where the learning problems may be somewhat difficult or the concept classes more complex than the hypothesis set used by the learning algorithm. However, inconsistent hypotheses with a small number of errors on the training sample can be useful and, as we shall see, can benefit from favorable guarantees under some assumptions. This section presents learning guarantees precisely for this inconsistent case and finite hypothesis sets. To derive learning guarantees in this more general setting, we will use Hoeffding\u2019s inequality (theorem D.2) or the following corollary, which relates the generalization error and empirical error of a single hypothesis. **Corollary 2.10** _Fix \u03f5 >_ 0 _.",
    "chunk_id": "foundations_machine_learning_20"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Then, for any hypothesis h_ : _X \u2192{_ 0 _,_ 1 _}, the following_ _inequalities hold:_ P _S\u223c_ D _[m]_ \ufffd _R_ _S_ ( _h_ ) _\u2212_ _R_ ( _h_ ) _\u2265_ _\u03f5_ _\u2264_ exp( _\u2212_ 2 _m\u03f5_ [2] ) (2.14) \ufffd \ufffd \ufffd P _R_ _S_ ( _h_ ) _\u2212_ _R_ ( _h_ ) _\u2264\u2212\u03f5_ _\u2264_ exp( _\u2212_ 2 _m\u03f5_ [2] ) _._ (2.15) _S\u223c_ D _[m]_ \ufffd \ufffd _By the union bound, this implies the following two-sided inequality:_ _S\u223c_ P D _[m]_ \ufffd\ufffd\ufffd\ufffd _R_ _S_ ( _h_ ) _\u2212_ _R_ ( _h_ )\ufffd\ufffd _\u2265_ _\u03f5_ \ufffd _\u2264_ 2 exp( _\u2212_ 2 _m\u03f5_ [2] ) _._ (2.16) Proof: The result follows immediately from theorem D.2. Setting the right-hand side of (2.16) to be equal to _\u03b4_ and solving for _\u03f5_ yields immediately the following bound for a single hypothesis. **Corollary 2.11 (Generalization bound \u2014 single hypothesis)** _Fix a hypothesis h_ : X _\u2192{_ 0 _,_ 1 _}._ _Then, for any \u03b4 >_ 0 _, the following inequality holds with probability at least_ 1 _\u2212_ _\u03b4:_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + \ufffd log [2] _\u03b4_ (2.17) 2 _m_ _[.]_ The following example illustrates this corollary in a simple case. **20** **Chapter 2** **The PAC Learning Framework** **Example 2.12 (Tossing a coin)** Imagine tossing a biased coin that lands heads with probability _p_, and let our hypothesis be the one that always guesses tails. Then the true error rate is _R_ ( _h_ ) = _p_ and the empirical error rate _R_ [\ufffd] _S_ ( _h_ ) = \ufffd _p_, where \ufffd _p_ is the empirical probability of heads based on the training sample drawn i.i.d. Thus, corollary 2.11 guarantees with probability at least 1 _\u2212_ _\u03b4_ that \ufffd _|p \u2212_ _p| \u2264_ \ufffd log [2] _\u03b4_ (2.18) 2 _m_ _[.]_ Therefore, if we choose _\u03b4_ = 0 _._ 02 and use a sample of size 500, with probability at least 98%, the following approximation quality is guaranteed for \ufffd _p_ : \ufffd _|p \u2212_ _p| \u2264_ ~~\ufffd~~ log(10) _\u2248_ 0 _._ 048 _._ (2.19) 1000 Can we readily apply corollary 2.11 to bound the generalization error of the hypothesis _h_ _S_ returned by a learning algorithm when training on a sample _S_ ? No, since _h_ _S_ is not a fixed hypothesis, but a random variable depending on the training sample _S_ drawn. Note also that unlike the case of a fixed hypothesis for which the expectation of the empirical error is the generalization error (equation (2.3)), the generalization error _R_ ( _h_ _S_ ) is a random variable and in general distinct from the expectation E[ _R_ [\ufffd] _S_ ( _h_ _S_ )], which is a constant. Thus, as in the proof for the consistent case, we need to derive a uniform convergence bound, that is a bound that holds with high probability for all hypotheses _h \u2208_ H. **Theorem 2.13 (Learning bound \u2014 finite** H **, inconsistent case)** _Let_ H _be a finite hypoth-_ _esis set. Then, for any \u03b4 >_ 0 _,",
    "chunk_id": "foundations_machine_learning_21"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "with probability at least_ 1 _\u2212_ _\u03b4, the following inequality_ _holds:_ _\u2200h \u2208_ H _,_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + \ufffd log _|_ H _|_ + log [2] _\u03b4_ _._ (2.20) 2 _m_ log _|_ H _|_ + log [2] Proof: Let _h_ 1 _, . . ., h_ _|_ H _|_ be the elements of H. Using the union bound and applying corollary 2.11 to each hypothesis yield: P \ufffd _\u2203h \u2208_ H\ufffd\ufffd\ufffd _R_ _S_ ( _h_ ) _\u2212_ _R_ ( _h_ )\ufffd\ufffd _> \u03f5_ \ufffd = P \ufffd\ufffd\ufffd\ufffd\ufffd _R_ _S_ ( _h_ 1 ) _\u2212_ _R_ ( _h_ 1 )\ufffd\ufffd _> \u03f5_ \ufffd _\u2228_ _. . . \u2228_ \ufffd\ufffd\ufffd\ufffd _R_ _S_ ( _h_ _|_ H _|_ ) _\u2212_ _R_ ( _h_ _|_ H _|_ )\ufffd\ufffd _> \u03f5_ \ufffd [\ufffd] _\u2264_ \ufffd P \ufffd\ufffd\ufffd\ufffd _R_ _S_ ( _h_ ) _\u2212_ _R_ ( _h_ )\ufffd\ufffd _> \u03f5_ \ufffd _h\u2208_ H _\u2264_ 2 _|_ H _|_ exp( _\u2212_ 2 _m\u03f5_ [2] ) _._ Setting the right-hand side to be equal to _\u03b4_ completes the proof. **2.4** **Generalities** **21** Thus, for a finite hypothesis set H, _[\u0338]_ \ufffd _[\u0338]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + _O_ _[\u0338]_ \ufffd ~~\ufffd~~ _[\u0338]_ log 2 _|_ H _|_ _m_ _[\u0338]_ _._ _[\u0338]_ As already pointed out, log 2 _|_ H _|_ can be interpreted as the number of bits needed to represent H. Several other remarks similar to those made on the generalization bound in the consistent case can be made here: a larger sample size _m_ guarantees better generalization, and the bound increases with _|_ H _|_, but only logarithmically. But, here, the bound is a less favorable function of [lo][g] [2] _m_ _[ |]_ [H] _[|]_ ; it varies as the square root of this term. This is not a minor price to pay: for a fixed _|_ H _|_, to attain the same guarantee as in the consistent case, a quadratically larger labeled sample is needed. Note that the bound suggests seeking a trade-off between reducing the empirical error versus controlling the size of the hypothesis set: a larger hypothesis set is penalized by the second term but could help reduce the empirical error, that is the first term. But, for a similar empirical error, it suggests using a smaller hypothesis set. This can be viewed as an instance of the so-called _Occam\u2019s Razor principle_ named after the theologian William of Occam: _Plurality should not be posited with-_ _out necessity_, also rephrased as, _the simplest explanation is best_ . In this context, it could be expressed as follows: All other things being equal, a simpler (smaller) hypothesis set is better. **2.4** **Generalities** In this section we will discuss some general aspects of the learning scenario, which, for simplicity, we left out of the discussion of the earlier sections. **2.4.1** **Deterministic versus stochastic scenarios** In the most general scenario of supervised learning, the distribution D is defined over X _\u00d7_ Y, and the training data is a labeled sample _S_ drawn i.i.d.",
    "chunk_id": "foundations_machine_learning_22"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "according to D: _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _._ The learning problem is to find a hypothesis _h \u2208_ H with small generalization error _R_ ( _h_ ) = P E ( _x,y_ ) _\u223c_ D [[] _[h]_ [(] _[x]_ [)] _[ \u0338]_ [=] _[ y]_ [] =] ( _x,y_ ) _\u223c_ D [[1] _[h]_ [(] _[x]_ [)] _[\u0338]_ [=] _[y]_ []] _[.]_ This more general scenario is referred to as the _stochastic scenario_ . Within this setting, the output label is a probabilistic function of the input. The stochastic scenario captures many real-world problems where the label of an input point is not unique. For example, if we seek to predict gender based on input pairs formed by the height and weight of a person, then the label will typically not be unique. **22** **Chapter 2** **The PAC Learning Framework** For most pairs, both male and female are possible genders. For each fixed pair, there would be a probability distribution of the label being male. The natural extension of the PAC-learning framework to this setting is known as the _agnostic PAC-learning_ . **Definition 2.14 (Agnostic PAC-learning)** _Let_ H _be a hypothesis set. A is an_ agnostic PAC-learning _algorithm if there exists a polynomial function poly_ ( _\u00b7, \u00b7, \u00b7, \u00b7_ ) _such that_ _for any \u03f5 >_ 0 _and \u03b4 >_ 0 _, for all distributions_ D _over_ X _\u00d7_ Y _, the following holds for_ _any sample size m \u2265_ _poly_ (1 _/\u03f5,_ 1 _/\u03b4, n, size_ ( _c_ )) _:_ P (2.21) _S\u223c_ D _[m]_ [[] _[R]_ [(] _[h]_ _[S]_ [)] _[ \u2212]_ _h_ [min] _\u2208_ H _[R]_ [(] _[h]_ [)] _[ \u2264]_ _[\u03f5]_ []] _[ \u2265]_ [1] _[ \u2212]_ _[\u03b4.]_ _If A further runs in poly_ (1 _/\u03f5,_ 1 _/\u03b4, n_ ) _, then it is said to be an_ efficient agnostic PAC-learning algorithm _._ When the label of a point can be uniquely determined by some measurable function _f_ : X _\u2192_ Y (with probability one), then the scenario is said to be _deterministic_ . In that case, it suffices to consider a distribution D over the input space. The training sample is obtained by drawing ( _x_ 1 _, . . ., x_ _m_ ) according to D and the labels are obtained via _f_ : _y_ _i_ = _f_ ( _x_ _i_ ) for all _i \u2208_ [ _m_ ]. Many learning problems can be formulated within this deterministic scenario. In the previous sections, as well as in most of the material presented in this book, we have restricted our presentation to the deterministic scenario in the interest of simplicity. However, for all of this material, the extension to the stochastic scenario should be straightforward for the reader. **2.4.2** **Bayes error and noise** In the deterministic case, by definition, there exists a target function _f_ with no generalization error: _R_ ( _h_ ) = 0. In the stochastic case, there is a minimal non-zero error for any hypothesis. **Definition 2.15",
    "chunk_id": "foundations_machine_learning_23"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(Bayes error)** _Given a distribution_ D _over_ X _\u00d7_ Y _, the_ Bayes error _R_ _[\u2217]_ _is defined as the infimum of the errors achieved by measurable functions h_ : X _\u2192_ Y _:_ _R_ _[\u22c6]_ = inf _R_ ( _h_ ) _._ (2.22) _h_ _h measurable_ _A hypothesis h with R_ ( _h_ ) = _R_ _[\u2217]_ _is called a_ Bayes hypothesis _or_ Bayes classifier _._ By definition, in the deterministic case, we have _R_ _[\u2217]_ = 0, but, in the stochastic case, _R_ _[\u2217]_ _\u0338_ = 0. Clearly, the Bayes classifier _h_ Bayes can be defined in terms of the conditional probabilities as: _\u2200x \u2208_ X _,_ _h_ Bayes ( _x_ ) = argmax P[ _y|x_ ] _._ (2.23) _y\u2208{_ 0 _,_ 1 _}_ **2.5** **Chapter notes** **23** The average error made by _h_ Bayes on _x \u2208_ X is thus min _{_ P[0 _|x_ ] _,_ P[1 _|x_ ] _}_, and this is the minimum possible error. This leads to the following definition of _noise_ . **Definition 2.16 (Noise)** _Given a distribution_ D _over_ X _\u00d7_ Y _, the_ noise _at point x \u2208_ X _is defined by_ _noise_ ( _x_ ) = min _{_ P[1 _|x_ ] _,_ P[0 _|x_ ] _}._ (2.24) _The_ average noise _or the_ noise _associated to_ D _is_ E[ _noise_ ( _x_ )] _._ Thus, the average noise is precisely the Bayes error: noise = E[noise( _x_ )] = _R_ _[\u2217]_ . The noise is a characteristic of the learning task indicative of its level of difficulty. A point _x \u2208_ X, for which noise( _x_ ) is close to 1 _/_ 2, is sometimes referred to as _noisy_ and is of course a challenge for accurate prediction. **2.5** **Chapter notes** The PAC learning framework was introduced by Valiant [1984]. The book of Kearns and Vazirani [1994] is an excellent reference dealing with most aspects of PAClearning and several other foundational questions in machine learning. Our example of learning axis-aligned rectangles, also discussed in that reference, is originally due to Blumer et al. [1989]. The PAC learning framework is a computational framework since it takes into account the cost of the computational representations and the time complexity of the learning algorithm. If we omit the computational aspects, it is similar to the learning framework considered earlier by Vapnik and Chervonenkis [see Vapnik, 2000]. The definition of noise presented in this chapter can be generalized to arbitrary loss functions (see exercise 2.14). Occam\u2019s razor principle is invoked in a variety of contexts, such as in linguistics to justify the superiority of a set of rules or syntax. The Kolmogorov complexity can be viewed as the corresponding framework in information theory. In the context of the learning guarantees presented in this chapter, the principle suggests selecting the most parsimonious explanation (the hypothesis set with the smallest cardinality). We will see in the next sections other applications of this principle with different notions of simplicity or complexity. **2.6** **Exercises** 2.1 Two-oracle variant of the PAC model. Assume that positive and negative examples are now drawn from",
    "chunk_id": "foundations_machine_learning_24"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "two separate distributions D + and D _\u2212_ . For an accuracy (1 _\u2212_ _\u03f5_ ), the learning algorithm must find a hypothesis _h_ such that: P P (2.25) _x\u223c_ D + [[] _[h]_ [(] _[x]_ [) = 0]] _[ \u2264]_ _[\u03f5]_ [ and] _x\u223c_ D _\u2212_ [[] _[h]_ [(] _[x]_ [) = 1]] _[ \u2264]_ _[\u03f5 .]_ **24** **Chapter 2** **The PAC Learning Framework** _r_ 1 _r_ 1 _r_ 3 _r_ 3 _r_ 2 (a) (b) **Figure 2.5** (a) Gertrude\u2019s regions _r_ 1 _, r_ 2 _, r_ 3 . (b) Hint for solution. _r_ 2 Thus, the hypothesis must have a small error on both distributions. Let C be any concept class and H be any hypothesis space. Let _h_ 0 and _h_ 1 represent the identically 0 and identically 1 functions, respectively. Prove that C is efficiently PAC-learnable using H in the standard (one-oracle) PAC model if and only if it is efficiently PAC-learnable using H _\u222a{h_ 0 _, h_ 1 _}_ in this two-oracle PAC model. 2.2 PAC learning of hyper-rectangles. An axis-aligned hyper-rectangle in R _[n]_ is a set of the form [ _a_ 1 _, b_ 1 ] _\u00d7 . . . \u00d7_ [ _a_ _n_ _, b_ _n_ ]. Show that axis-aligned hyper-rectangles are PAC-learnable by extending the proof given in Example 2.4 for the case _n_ = 2. 2.3 Concentric circles. Let X = R [2] and consider the set of concepts of the form _c_ = _{_ ( _x, y_ ): _x_ [2] + _y_ [2] _\u2264_ _r_ [2] _}_ for some real number _r_ . Show that this class can be ( _\u03f5, \u03b4_ )-PAC-learned from training data of size _m \u2265_ (1 _/\u03f5_ ) log(1 _/\u03b4_ ). 2.4 Non-concentric circles. Let X = R [2] and consider the set of concepts of the form _c_ = _{x \u2208_ R [2] : _||x \u2212_ _x_ 0 _|| \u2264_ _r}_ for some point _x_ 0 _\u2208_ R [2] and real number _r_ . Gertrude, an aspiring machine learning researcher, attempts to show that this class of concepts may be ( _\u03f5, \u03b4_ )-PAC-learned with sample complexity _m \u2265_ (3 _/\u03f5_ ) log(3 _/\u03b4_ ), but she is having trouble with her proof. Her idea is that the learning algorithm would select the smallest circle consistent with the training data. She has drawn three regions _r_ 1 _, r_ 2 _, r_ 3 around the edge of concept _c_, with each region having probability _\u03f5/_ 3 (see figure 2.5(a)). She wants to argue that if the generalization error is greater than or equal to _\u03f5_, then one of these regions must have been missed by the training data, and hence this event will occur with probability at most _\u03b4_ . Can you tell Gertrude if her approach works? ( _Hint_ : You may wish to use figure 2.5(b) in your solution). **2.6** **Exercises** **25** C |C\u201d<br>A\u201d B\u201d<br>A\u2019 B\u2019|Col2| |---|---| |C\u201d<br>A\u201d B\u201d<br>A\u2019 B\u2019|| |A\u2019|A\u2019| ||| A B **Figure 2.6** Axis-aligned right triangles. 2.5 Triangles. Let X = R [2] with orthonormal basis ( **e** 1",
    "chunk_id": "foundations_machine_learning_25"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_,_ **e** 2 ), and consider the set of concepts defined by the area inside a right triangle _ABC_ with two sides parallel to the axes, with _[\u2212\u2212\u2192]_ _AB/\u2225AB_ _[\u2212\u2212\u2192]_ _\u2225_ = **e** 1 and _[\u2212\u2192]_ _AC/\u2225AC_ _[\u2212\u2192]_ _\u2225_ = **e** 2, and _\u2225AB_ _[\u2212\u2212\u2192]_ _\u2225/\u2225AC_ _[\u2212\u2192]_ _\u2225_ = _\u03b1_ for some positive real _\u03b1 \u2208_ R + . Show, using similar methods to those used in the chapter for the axis-aligned rectangles, that this class can be ( _\u03f5, \u03b4_ )-PAC-learned from training data of size _m \u2265_ (3 _/\u03f5_ ) log(3 _/\u03b4_ ). ( _Hint_ : You may consider using figure 2.6 in your solution). 2.6 Learning in the presence of noise \u2014 rectangles. In example 2.4, we showed that the concept class of axis-aligned rectangles is PAC-learnable. Consider now the case where the training points received by the learner are subject to the following noise: points negatively labeled are unaffected by noise but the label of a positive training point is randomly flipped to negative with probability _\u03b7 \u2208_ (0 _,_ 2 [1] [). The exact value of the noise rate] _[ \u03b7]_ [ is not known to the learner but an] upper bound _\u03b7_ _[\u2032]_ is supplied to him with _\u03b7 \u2264_ _\u03b7_ _[\u2032]_ _<_ 1 _/_ 2. Show that the algorithm returning the tightest rectangle containing positive points can still PAC-learn axis-aligned rectangles in the presence of this noise. To do so, you can proceed using the following steps: (a) Using the same notation as in example 2.4, assume that P[R] _> \u03f5_ . Suppose that _R_ (R _[\u2032]_ ) _> \u03f5_ . Give an upper bound on the probability that R _[\u2032]_ misses a region _r_ _j_, _j \u2208_ [4] in terms of _\u03f5_ and _\u03b7_ _[\u2032]_ ? (b) Use that to give an upper bound on P[ _R_ (R _[\u2032]_ ) _> \u03f5_ ] in terms of _\u03f5_ and _\u03b7_ _[\u2032]_ and conclude by giving a sample complexity bound. 2.7 Learning in the presence of noise \u2014 general case. In this question, we will seek a result that is more general than in the previous question. We consider a finite hypothesis set H, assume that the target concept is in H, and adopt the following noise model: the label of a training point received by the learner is **26** **Chapter 2** **The PAC Learning Framework** randomly changed with probability _\u03b7 \u2208_ (0 _,_ [1] 2 [). The exact value of the noise rate] _\u03b7_ is not known to the learner but an upper bound _\u03b7_ _[\u2032]_ is supplied to him with _\u03b7 \u2264_ _\u03b7_ _[\u2032]_ _<_ 1 _/_ 2. (a) For any _h \u2208_ H, let _d_ ( _h_ ) denote the probability that the label of a training point received by the learner disagrees with the one given by _h_ . Let _h_ _[\u2217]_ be the target hypothesis, show that _d_ ( _h_ _[\u2217]_ ) = _\u03b7_ . (b) More generally, show that for any _h \u2208_ H, _d_ ( _h_ ) = _\u03b7_ + (1 _\u2212_ 2 _\u03b7_ ) _R_ ( _h_ ),",
    "chunk_id": "foundations_machine_learning_26"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "where _R_ ( _h_ ) denotes the generalization error of _h_ . (c) Fix _\u03f5 >_ 0 for this and all the following questions. Use the previous questions to show that if _R_ ( _h_ ) _> \u03f5_, then _d_ ( _h_ ) _\u2212_ _d_ ( _h_ _[\u2217]_ ) _\u2265_ _\u03f5_ _[\u2032]_, where _\u03f5_ _[\u2032]_ = _\u03f5_ (1 _\u2212_ 2 _\u03b7_ _[\u2032]_ ). (d) For any hypothesis _h \u2208_ H and sample _S_ of size _m_, let _d_ [\ufffd] ( _h_ ) denote the fraction of the points in _S_ whose labels disagree with those given by _h_ . We will consider the algorithm _L_ which, after receiving _S_, returns the hypothesis _h_ _S_ with the smallest number of disagreements (thus _d_ [\ufffd] ( _h_ _S_ ) is minimal). To show PAC-learning for _L_, we will show that for any _h_, if _R_ ( _h_ ) _> \u03f5_, then with high probability _d_ [\ufffd] ( _h_ ) _\u2265_ _d_ [\ufffd] ( _h_ _[\u2217]_ ). First, show that for any _\u03b4 >_ 0, with 2 probability at least 1 _\u2212_ _\u03b4/_ 2, for _m \u2265_ _\u03f5_ _[\u2032]_ [2] [ log] [2] _\u03b4_ [, the following holds:] \ufffd _d_ ( _h_ _[\u2217]_ ) _\u2212_ _d_ ( _h_ _[\u2217]_ ) _\u2264_ _\u03f5_ _[\u2032]_ _/_ 2 (e) Second, show that for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4/_ 2, for _m \u2265_ _\u03f5_ 2 _[\u2032]_ [2] [ (log] _[ |]_ [H] _[|]_ [ + log] [2] _\u03b4_ [), the following holds for all] _[ h][ \u2208]_ [H][:] _d_ ( _h_ ) _\u2212_ _d_ [\ufffd] ( _h_ ) _\u2264_ _\u03f5_ _[\u2032]_ _/_ 2 (f) Finally, show that for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, for _m \u2265_ _\u03f5_ [2] (1 _\u2212_ 22 _\u03b7_ _[\u2032]_ ) [2] [ (log] _[ |]_ [H] _[|]_ [ + log] [2] _\u03b4_ [), the following holds for all] _[ h][ \u2208]_ [H][ with] _[ R]_ [(] _[h]_ [)] _[ > \u03f5]_ [:] \ufffd \ufffd _d_ ( _h_ ) _\u2212_ _d_ ( _h_ _[\u2217]_ ) _\u2265_ 0 _._ ( _Hint_ : use _d_ [\ufffd] ( _h_ ) _\u2212_ _d_ [\ufffd] ( _h_ _[\u2217]_ ) = [ _d_ [\ufffd] ( _h_ ) _\u2212_ _d_ ( _h_ )] + [ _d_ ( _h_ ) _\u2212_ _d_ ( _h_ _[\u2217]_ )] + [ _d_ ( _h_ _[\u2217]_ ) _\u2212_ _d_ [\ufffd] ( _h_ _[\u2217]_ )] and use previous questions to lower bound each of these three terms). 2.8 Learning intervals. Give a PAC-learning algorithm for the concept class C formed by closed intervals [ _a, b_ ] with _a, b \u2208_ R. 2.9 Learning union of intervals. Give a PAC-learning algorithm for the concept class C 2 formed by unions of two closed intervals, that is [ _a, b_ ] _\u222a_ [ _c, d_ ], with _a, b, c, d \u2208_ R. Extend your result to derive a PAC-learning algorithm for the concept class C _p_ formed by unions of _p \u2265_ 1 closed intervals, thus [ _a_ 1 _, b_ 1 ] _\u222a\u00b7 \u00b7 \u00b7 \u222a_",
    "chunk_id": "foundations_machine_learning_27"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[ _a_ _p_ _, b_ _p_ ], with _a_ _k_ _, b_ _k_ _\u2208_ R for _k \u2208_ [ _p_ ]. What are the time and sample complexities of your algorithm as a function of _p_ ? **2.6** **Exercises** **27** 2.10 Consistent hypotheses. In this chapter, we showed that for a finite hypothesis set H, a consistent learning algorithm _A_ is a PAC-learning algorithm. Here, we consider a converse question. Let Z be a finite set of _m_ labeled points. Suppose that you are given a PAC-learning algorithm _A_ . Show that you can use _A_ and a finite training sample _S_ to find in polynomial time a hypothesis _h \u2208_ H that is consistent with Z, with high probability. ( _Hint_ : you can select an appropriate distribution D over Z and give a condition on _R_ ( _h_ ) for _h_ to be consistent.) 2.11 Senate laws. For important questions, President Mouth relies on expert advice. He selects an appropriate advisor from a collection of H = 2 _,_ 800 experts. (a) Assume that laws are proposed in a random fashion independently and identically according to some distribution D determined by an unknown group of senators. Assume that President Mouth can find and select an expert senator out of H who has consistently voted with the majority for the last _m_ = 200 laws. Give a bound on the probability that such a senator incorrectly predicts the global vote for a future law. What is the value of the bound with 95% confidence? (b) Assume now that President Mouth can find and select an expert senator out of H who has consistently voted with the majority for all but _m_ _[\u2032]_ = 20 of the last _m_ = 200 laws. What is the value of the new bound? 2.12 Bayesian bound. Let H be a countable hypothesis set of functions mapping X to _{_ 0 _,_ 1 _}_ and let _p_ be a probability measure over H. This probability measure represents the _prior probability_ over the hypothesis class, i.e. the probability that a particular hypothesis is selected by the learning algorithm. Use Hoeffding\u2019s inequality to show that for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, the following inequality holds: _\u2200h \u2208_ H _, R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + ~~\ufffd~~ log _p_ (1 _h_ ) [+ log] [1] _\u03b4_ _._ (2.26) 2 _m_ Compare this result with the bound given in the inconsistent case for finite hypothesis sets ( _Hint_ : you could use _\u03b4_ _[\u2032]_ = _p_ ( _h_ ) _\u03b4_ as confidence parameter in Hoeffding\u2019s inequality). 2.13 Learning with an unknown parameter. In example 2.9, we showed that the concept class of _k_ -CNF is PAC-learnable. Note, however, that the learning algorithm is given _k_ as input. Is PAC-learning possible even when _k_ is not provided? More generally, consider a family of concept classes _{_ C _s_ _}_ _s_ where C _s_ is the set of concepts in C with size at most _s_ . Suppose we have",
    "chunk_id": "foundations_machine_learning_28"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "a PAC-learning algorithm _A_ that can be used for learning any concept class C _s_ when _s_ is given. **28** **Chapter 2** **The PAC Learning Framework** Can we convert _A_ into a PAC-learning algorithm _B_ that does not require the knowledge of _s_ ? This is the main objective of this problem. To do this, we first introduce a method for testing a hypothesis _h_, with high probability. Fix _\u03f5 >_ 0, _\u03b4 >_ 0, and _i \u2265_ 1 and define the sample size _n_ by _n_ = [32] [2] _\u03f5_ [[] _[i]_ [ log 2 + log] _\u03b4_ []. Suppose we draw an i.i.d. sample] _[ S]_ [ of size] _[ n]_ [ according] to some unknown distribution D. We will say that a hypothesis _h_ is _accepted_ if it makes at most 3 _/_ 4 _\u03f5_ errors on _S_ and that it is _rejected_ otherwise. Thus, _h_ is accepted iff _R_ [\ufffd] ( _h_ ) _\u2264_ 3 _/_ 4 _\u03f5_ . (a) Assume that _R_ ( _h_ ) _\u2265_ _\u03f5_ . Use the (multiplicative) Chernoff bound to show that in that case P _S\u223c_ D _[n]_ [ _h_ is accepted] _\u2264_ 2 _[i]_ _\u03b4_ [+1] [ .] (b) Assume that _R_ ( _h_ ) _\u2264_ _\u03f5/_ 2. Use the (multiplicative) Chernoff bounds to show _\u03b4_ that in that case P _S\u223c_ D _n_ [ _h_ is rejected] _\u2264_ 2 _[i]_ [+1] [ .] (c) Algorithm _B_ is defined as follows: we start with _i_ = 1 and, at each round _i \u2265_ 1, we guess the parameter size _s_ to be \ufffd _s_ = _\u230a_ 2 [(] _[i][\u2212]_ [1)] _[/]_ [ log] _\u03b4_ [2] _\u230b_ . We draw a sample _S_ of size _n_ (which depends on _i_ ) to test the hypothesis _h_ _i_ returned by _A_ when it is trained with a sample of size _S_ _A_ ( _\u03f5/_ 2 _,_ 1 _/_ 2 _,_ \ufffd _s_ ), that is the sample complexity of _A_ for a required precision _\u03f5/_ 2, confidence 1 _/_ 2, and size _s_ \ufffd (we ignore the size of the representation of each example here). If _h_ _i_ is accepted, the algorithm stops and returns _h_ _i_, otherwise it proceeds to the next iteration. Show that if at iteration _i_, the estimate \ufffd _s_ is larger than or equal to _s_, then P[ _h_ _i_ is accepted] _\u2265_ 3 _/_ 8. (d) Show that the probability that _B_ does not halt after _j_ = _\u2308_ log [2] [2] _\u03b4_ _[/]_ [ log] [8] 5 (d) Show that the probability that _B_ does not halt after _j_ = _\u2308_ log _\u03b4_ _[/]_ [ log] 5 _[\u2309]_ [iter-] ations with \ufffd _s \u2265_ _s_ is at most _\u03b4/_ 2. (e) Show that for _i \u2265\u2308_ 1 + (log 2 _s_ ) log [2] _\u03b4_ _[\u2309]_ [, the inequality][ \ufffd] _[s][ \u2265]_ _[s]_ [ holds.] (f) Show that with probability at least 1 _\u2212_ _\u03b4_, algorithm _B_ halts after at most _j_ _[\u2032]_ = _\u2308_ 1 + (log 2 _s_ ) log [2] _\u03b4_ _[\u2309]_ [+]",
    "chunk_id": "foundations_machine_learning_29"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[ j]_ [ iterations and returns a hypothesis with error at] most _\u03f5_ . 2.14 In this exercise, we generalize the notion of noise to the case of an arbitrary loss function _L_ : Y _\u00d7_ Y _\u2192_ R + . (a) Justify the following definition of the noise at point _x \u2208_ X: noise( _x_ ) = min _y_ _[\u2032]_ _\u2208Y_ [E] _y_ [[] _[L]_ [(] _[y, y]_ _[\u2032]_ [)] _[|][x]_ []] _[.]_ What is the value of noise( _x_ ) in a deterministic scenario? Does the definition match the one given in this chapter for binary classification? (b) Show that the average noise coincides with the Bayes error (minimum loss achieved by a measurable function). # 3 Rademacher Complexity and VC-Dimension The hypothesis sets typically used in machine learning are infinite. But the sample complexity bounds of the previous chapter are uninformative when dealing with infinite hypothesis sets. One could ask whether efficient learning from a finite sample is even possible when the hypothesis set H is infinite. Our analysis of the family of axis-aligned rectangles (Example 2.4) indicates that this is indeed possible at least in some cases, since we proved that that infinite concept class was PAClearnable. Our goal in this chapter will be to generalize that result and derive general learning guarantees for infinite hypothesis sets. A general idea for doing so consists of reducing the infinite case to the analysis of finite sets of hypotheses and then proceed as in the previous chapter. There are different techniques for that reduction, each relying on a different notion of complexity for the family of hypotheses. The first complexity notion we will use is that of _Rademacher complexity_ . This will help us derive learning guarantees using relatively simple proofs based on McDiarmid\u2019s inequality, while obtaining highquality bounds, including data-dependent ones, which we will frequently make use of in future chapters. However, the computation of the empirical Rademacher complexity is NP-hard for some hypothesis sets. Thus, we subsequently introduce two other purely combinatorial notions, the _growth function_ and the _VC-dimension_ . We first relate the Rademacher complexity to the growth function and then bound the growth function in terms of the VC-dimension. The VC-dimension is often easier to bound or estimate. We will review a series of examples showing how to compute or bound it, then relate the growth function and the VC-dimensions. This leads to generalization bounds based on the VC-dimension. Finally, we present lower bounds based on the VC-dimension for two different settings: The _realizable_ setting, where there is at least one hypothesis in the hypothesis set under consideration that achieves zero expected error, as well as the _non-realizable_ setting, where no hypothesis in the set achieves zero expected error. **30** **Chapter 3** **Rademacher Complexity and VC-Dimension** **3.1** **Rademacher complexity** We will continue to use H to denote a hypothesis set as in the previous chapters. Many of the results of this section are general and hold for an arbitrary loss function _L_ : Y _\u00d7_ Y _\u2192_ R. In what follows, G will generally be interpreted",
    "chunk_id": "foundations_machine_learning_30"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "as _the family of loss_ _functions associated to_ H mapping from Z = X _\u00d7_ Y to R: G = _{g_ : ( _x, y_ ) _\ufffd\u2192_ _L_ ( _h_ ( _x_ ) _, y_ ): _h \u2208_ H _}._ However, the definitions are given in the general case of a family of functions G mapping from an arbitrary input space Z to R. The Rademacher complexity captures the richness of a family of functions by measuring the degree to which a hypothesis set can fit random noise. The following states the formal definitions of the empirical and average Rademacher complexity. **Definition 3.1 (Empirical Rademacher complexity)** _Let_ G _be a family of functions map-_ _ping from_ Z _to_ [ _a, b_ ] _and S_ = ( _z_ 1 _, . . ., z_ _m_ ) _a fixed sample of size m with elements_ _in_ Z _. Then, the_ empirical Rademacher complexity _of_ G _with respect to the sample_ _S is defined as:_ \ufffd _m_ \ufffd \ufffd _\u03c3_ _i_ _g_ ( _z_ _i_ ) _i_ =1 \ufffd R _S_ (G) = E _**\u03c3**_ \ufffd sup _g\u2208_ G _,_ (3.1) 1 _m_ _where_ _**\u03c3**_ = ( _\u03c3_ 1 _, . . ., \u03c3_ _m_ ) _[\u22a4]_ _, with \u03c3_ _i_ _s independent uniform random variables taking_ _values in {\u2212_ 1 _,_ +1 _}._ [3] _The random variables \u03c3_ _i_ _are called_ Rademacher variables _._ Let **g** _S_ denote the vector of values taken by function _g_ over the sample _S_ : **g** _S_ = ( _g_ ( _z_ 1 ) _, . . ., g_ ( _z_ _m_ )) _[\u22a4]_ . Then, the empirical Rademacher complexity can be rewritten as \ufffd _**\u03c3**_ _\u00b7_ **g** _S_ R _S_ (G) = E sup _._ _**\u03c3**_ \ufffd _**\u03c3**_ _\u00b7_ **g** _S_ \ufffd sup _g\u2208_ G _._ _m_ The inner product _**\u03c3**_ _\u00b7_ **g** _S_ measures the correlation of **g** _S_ with the vector of random _**\u03c3**_ _\u00b7_ **g** _S_ noise _**\u03c3**_ . The supremum sup _g\u2208_ G _m_ is a measure of how well the function class G correlates with _**\u03c3**_ over the sample _S_ . Thus, the empirical Rademacher complexity measures on average how well the function class G correlates with random noise on _S_ . This describes the richness of the family G: richer or more complex families G can generate more vectors **g** _S_ and thus better correlate with random noise, on average. 3 We assume implicitly that the supremum over the family G in this definition is measurable and in general will adopt the same assumption throughout this book for other suprema over a class of functions. This assumption does not hold for arbitrary function classes but it is valid for the hypotheses sets typically considered in practice in machine learning, and the instances discussed in this book. **3.1** **Rademacher complexity** **31** **Definition 3.2 (Rademacher complexity)** _Let_ D _denote the distribution according to_ _which samples are drawn. For any integer m \u2265_ 1 _, the_ Rademacher complexity _of_ G _is the expectation of the empirical Rademacher complexity over all samples of_ _size m",
    "chunk_id": "foundations_machine_learning_31"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "drawn according to_ D _:_ R _m_ (G) = E (3.2) _S\u223c_ D _[m]_ [[][R][\ufffd] _[S]_ [(][G][)]] _[.]_ We are now ready to present our first generalization bounds based on Rademacher complexity. **Theorem 3.3** _Let_ G _be a family of functions mapping from_ Z _to_ [0 _,_ 1] _. Then, for any_ _\u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the draw of an i.i.d. sample S of size m,_ _each of the following holds for all g \u2208_ G _:_ \ufffd log [1] _\u03b4_ (3.3) 2 _m_ E[ _g_ ( _z_ )] _\u2264_ [1] _m_ _m_ \ufffd _g_ ( _z_ _i_ ) + 2R _m_ (G) + _i_ =1 \ufffd log [2] _\u03b4_ (3.4) 2 _m_ _[.]_ _and_ E[ _g_ ( _z_ )] _\u2264_ [1] _m_ _m_ \ufffd _g_ ( _z_ _i_ ) + 2R [\ufffd] _S_ (G) + 3 _i_ =1 Proof: For any sample _S_ = ( _z_ 1 _, . . ., z_ _m_ ) and any _g \u2208_ G, we denote by E [\ufffd] _S_ [ _g_ ] the em1 _m_ pirical average of _g_ over _S_ : E [\ufffd] _S_ [ _g_ ] = _m_ \ufffd _i_ =1 _[g]_ [(] _[z]_ _[i]_ [). The proof consists of applying] McDiarmid\u2019s inequality to function \u03a6 defined for any sample _S_ by \u03a6( _S_ ) = sup _g\u2208_ G E[ _g_ ] _\u2212_ E [\ufffd] _S_ [ _g_ ] _._ (3.5) \ufffd \ufffd Let _S_ and _S_ _[\u2032]_ be two samples differing by exactly one point, say _z_ _m_ in _S_ and _z_ _m_ _[\u2032]_ in _S_ _[\u2032]_ . Then, since the difference of suprema does not exceed the supremum of the difference, we have _g_ ( _z_ _m_ ) _\u2212_ _g_ ( _z_ _m_ _[\u2032]_ [)] _\u2264_ [1] (3.6) _m_ _m_ _[.]_ \u03a6( _S_ _[\u2032]_ ) _\u2212_ \u03a6( _S_ ) _\u2264_ sup _g\u2208_ G \ufffd \ufffd E _S_ [ _g_ ] _\u2212_ E _S_ _\u2032_ [ _g_ ] = sup \ufffd \ufffd _g\u2208_ G Similarly, we can obtain \u03a6( _S_ ) _\u2212_ \u03a6( _S_ _[\u2032]_ ) _\u2264_ 1 _/m_, thus _|_ \u03a6( _S_ ) _\u2212_ \u03a6( _S_ _[\u2032]_ ) _| \u2264_ 1 _/m_ . Then, by McDiarmid\u2019s inequality, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4/_ 2, the following holds: \u03a6( _S_ ) _\u2264_ E _S_ [[\u03a6(] _[S]_ [)] +] \ufffd log [2] _\u03b4_ (3.7) 2 _m_ _[.]_ **32** **Chapter 3** **Rademacher Complexity and VC-Dimension** We next bound the expectation of the right-hand side as follows: \ufffd E[ _g_ ] _\u2212_ E [\ufffd] _S_ ( _g_ )\ufffd [\ufffd] E _S_ [[\u03a6(] _[S]_ [)] =][ E] _S_ = E _S_ sup \ufffd _g\u2208_ G \ufffd \ufffd sup _g\u2208_ G _S_ E _[\u2032]_ \ufffd\ufffdE _S_ _\u2032_ ( _g_ ) _\u2212_ E _S_ ( _g_ )\ufffd [\ufffd] (3.8) \ufffd \ufffd\ufffdE _S_ _\u2032_ ( _g_ ) _\u2212_ E _S_ ( _g_ )\ufffd [\ufffd] (3.9) _\u2264_ E _S,S_ _[\u2032]_ = E _S,S_ _[\u2032]_ sup \ufffd _g\u2208_ G sup \ufffd _g\u2208_ G 1 _m_ _m_ \ufffd( _g_ ( _z_ _i_ _[\u2032]_ [)] _[ \u2212]_ _[g]_ [(]",
    "chunk_id": "foundations_machine_learning_32"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[z]_ _[i]_ [))] \ufffd (3.10) _i_ =1 _m_ \ufffd _\u03c3_ _i_ ( _g_ ( _z_ _i_ _[\u2032]_ [)] _[ \u2212]_ _[g]_ [(] _[z]_ _[i]_ [))] \ufffd (3.11) _i_ =1 1 _m_ = E _**\u03c3**_ _,S,S_ _[\u2032]_ sup \ufffd _g\u2208_ G _m_ \ufffd _\u03c3_ _i_ _g_ ( _z_ _i_ _[\u2032]_ [)] \ufffd + E _**\u03c3**_ _,S_ _i_ =1 sup \ufffd _g\u2208_ G _m_ \ufffd _\u2212\u03c3_ _i_ _g_ ( _z_ _i_ )\ufffd (3.12) _i_ =1 1 _m_ 1 _m_ _\u2264_ E _**\u03c3**_ _,S_ _[\u2032]_ sup \ufffd _g\u2208_ G _m_ \ufffd _\u03c3_ _i_ _g_ ( _z_ _i_ )\ufffd = 2R _m_ (G) _._ (3.13) _i_ =1 1 _m_ = 2 E _**\u03c3**_ _,S_ sup \ufffd _g\u2208_ G Equation (3.8) uses the fact that points in _S_ _[\u2032]_ are sampled in an i.i.d. fashion and thus E[ _g_ ] = E _S_ _\u2032_ [E [\ufffd] _S_ _\u2032_ ( _g_ )], as in (2.3). Inequality 3.9 holds due to the sub-additivity of the supremum function. In equation (3.11), we introduce Rademacher variables _\u03c3_ _i_, which are uniformly distributed independent random variables taking values in _{\u2212_ 1 _,_ +1 _}_ as in definition 3.2. This does not change the expectation appearing in (3.10): when _\u03c3_ _i_ = 1, the associated summand remains unchanged; when _\u03c3_ _i_ = _\u2212_ 1, the associated summand flips signs, which is equivalent to swapping _z_ _i_ and _z_ _i_ _[\u2032]_ [between] _[ S]_ [ and] _[ S]_ _[\u2032]_ [. Since] we are taking the expectation over all possible _S_ and _S_ _[\u2032]_, this swap does not affect the overall expectation; we are simply changing the order of the summands within the expectation. Equation (3.12) holds by the sub-additivity of the supremum function, that is the inequality sup( _U_ + _V_ ) _\u2264_ sup( _U_ ) + sup( _V_ ). Finally, (3.13) stems from the definition of Rademacher complexity and the fact that the variables _\u03c3_ _i_ and _\u2212\u03c3_ _i_ are distributed in the same way. The reduction to R _m_ (G) in equation (3.13) yields the bound in equation (3.3), using _\u03b4_ instead of _\u03b4/_ 2. To derive a bound in terms of R [\ufffd] _S_ (G), we observe that, by definition 3.1, changing one point in _S_ changes R [\ufffd] _S_ (G) by at most 1 _/m_ . Then, using again McDiarmid\u2019s inequality, with probability 1 _\u2212_ _\u03b4/_ 2 the following holds: R _m_ (G) _\u2264_ R [\ufffd] _S_ (G) + \ufffd log [2] _\u03b4_ (3.14) 2 _m_ _[.]_ **3.1** **Rademacher complexity** **33** Finally, we use the union bound to combine inequalities 3.7 and 3.14, which yields with probability at least 1 _\u2212_ _\u03b4_ : _\u0338_ _\u0338_ _\u0338_ \u03a6( _S_ ) _\u2264_ 2R [\ufffd] _S_ (G) + 3 _\u0338_ _\u0338_ _\u0338_ \ufffd _\u0338_ _\u0338_ _\u0338_ log [2] _\u03b4_ (3.15) 2 _m_ _[,]_ _\u0338_ _\u0338_ _\u0338_ which matches (3.4). The following result relates the empirical Rademacher complexities of a hypothesis set H and to the family of loss functions G associated to H in the case of binary loss (zero-one loss). **Lemma 3.4** _Let_ H _be a family of functions taking values in",
    "chunk_id": "foundations_machine_learning_33"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "{\u2212_ 1 _,_ +1 _} and let_ G _be_ _the family of loss functions associated to_ H _for the zero-one loss:_ G = _{_ ( _x, y_ ) _\ufffd\u2192_ 1 _h_ ( _x_ )= _\u0338_ _y_ : _h \u2208_ H\ufffd _._ _For any sample S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _of elements in_ X _\u00d7 {\u2212_ 1 _,_ +1 _}, let S_ X _denote its projection over_ X _: S_ X = ( _x_ 1 _, . . ., x_ _m_ ) _._ _Then,_ _the following relation holds between the empirical Rademacher complexities of_ G _and_ H _:_ \ufffd \ufffd R _S_ (G) = [1] 2 R _S_ X (H) _._ (3.16) Proof: For any sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) of elements in X _\u00d7 {\u2212_ 1 _,_ +1 _}_, by definition, the empirical Rademacher complexity of G can be written as: _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd _\u03c3_ _i_ 1 _h_ ( _x_ _i_ )= _\u0338_ _y_ _i_ \ufffd _i_ =1 _\u0338_ _\u0338_ _\u0338_ 1 _\u2212y_ _i_ _h_ ( _x_ _i_ ) \ufffd _\u03c3_ _i_ 2 _i_ =1 _\u0338_ _\u0338_ 1 _\u0338_ _m_ 1 _m_ _\u0338_ _\u0338_ \ufffd R _S_ (G) = E _**\u03c3**_ _\u0338_ = E _**\u03c3**_ _\u0338_ _\u0338_ sup _\u0338_ \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ _\u0338_ _i_ _h_ ( _x_ _i_ ) 2 \ufffd _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd _\u2212\u03c3_ _i_ _y_ _i_ _h_ ( _x_ _i_ )\ufffd _i_ =1 _\u0338_ _\u0338_ _\u0338_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ )\ufffd = [1] 2 _i_ =1 _\u0338_ _\u0338_ _\u0338_ = [1] 2 [E] _**\u03c3**_ = [1] 2 [E] _**\u03c3**_ _\u0338_ _\u0338_ _\u0338_ sup \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H _\u0338_ _\u0338_ _\u0338_ 1 _m_ 1 _m_ _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ _\u0338_ \ufffd 2 R _S_ X (H) _,_ _\u0338_ _\u0338_ _\u0338_ where we used the fact that 1 _h_ ( _x_ _i_ )= _\u0338_ _y_ _i_ = (1 _\u2212_ _y_ _i_ _h_ ( _x_ _i_ )) _/_ 2 and the fact that for a fixed _y_ _i_ _\u2208{\u2212_ 1 _,_ +1 _}_, _\u03c3_ _i_ and _\u2212y_ _i_ _\u03c3_ _i_ are distributed in the same way. Note that the lemma implies, by taking expectations, that for any _m \u2265_ 1, R _m_ (G) = 1 2 [R] _[m]_ [(][H][). These connections between the empirical and average Rademacher com-] plexities can be used to derive generalization bounds for binary classification in terms of the Rademacher complexity of the hypothesis set H. **Theorem 3.5 (Rademacher complexity bounds \u2013 binary classification )** _Let_ H _be a family_ _of functions taking values in {\u2212_ 1 _,_ +1 _} and let_ D _be the distribution over the input_ _space_ X _. Then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over a sample S of_ **34** **Chapter 3** **Rademacher Complexity and VC-Dimension** _size m drawn according to_ D",
    "chunk_id": "foundations_machine_learning_34"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_, each of the following holds for any h \u2208_ H _:_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + R _m_ (H) + ~~\ufffd~~ log [1] _\u03b4_ (3.17) 2 _m_ _and_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + R [\ufffd] _S_ (H) + 3 ~~\ufffd~~ log [2] _\u03b4_ (3.18) 2 _m_ _[.]_ Proof: The result follows immediately by theorem 3.3 and lemma 3.4. The theorem provides two generalization bounds for binary classification based on the Rademacher complexity. Note that the second bound, (3.18), is data-dependent: the empirical Rademacher complexity R [\ufffd] _S_ (H) is a function of the specific sample _S_ \ufffd drawn. Thus, this bound could be particularly informative if we could compute R _S_ (H). But, how can we compute the empirical Rademacher complexity? Using again the fact that _\u03c3_ _i_ and _\u2212\u03c3_ _i_ are distributed in the same way, we can write _m_ \ufffd _\u2212\u03c3_ _i_ _h_ ( _x_ _i_ )\ufffd = _\u2212_ E _**\u03c3**_ _i_ =1 1 inf \ufffd _h\u2208_ H _m_ _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ )\ufffd _._ _i_ =1 1 _m_ \ufffd R _S_ (H) = E _**\u03c3**_ sup \ufffd _h\u2208_ H _m_ Now, for a fixed value of _**\u03c3**_, computing inf _h\u2208_ H _m_ [1] \ufffd _i_ =1 _[\u03c3]_ _[i]_ _[h]_ [(] _[x]_ _[i]_ [) is equivalent to an] _empirical risk minimization_ problem, which is known to be computationally hard for some hypothesis sets. Thus, in some cases, computing R [\ufffd] _S_ (H) could be computationally hard. In the next sections, we will relate the Rademacher complexity to combinatorial measures that are easier to compute and also of independent interest for their usefulness in the analysis of learning in many contexts. **3.2** **Growth function** Here we will show how the Rademacher complexity can be bounded in terms of the _growth function_ . **Definition 3.6 (Growth function)** _The_ growth function \u03a0 H : N _\u2192_ N _for a hypothesis_ _set_ H _is defined by:_ _\u2200m \u2208_ N _,_ \u03a0 H ( _m_ ) = max _{x_ 1 _,...,x_ _m_ _}\u2286X_ \ufffd\ufffd\ufffd\ufffd\ufffd _h_ ( _x_ 1 ) _, . . ., h_ ( _x_ _m_ )\ufffd : _h \u2208_ H\ufffd [\ufffd] \ufffd\ufffd _._ (3.19) In other words, \u03a0 H ( _m_ ) is the maximum number of distinct ways in which _m_ points can be classified using hypotheses in H. Each one of these distinct classifications is called a _dichotomy_ and, thus, the growth function counts the number of dichotomies that are realized by the hypothesis. This provides another measure of the richness of the hypothesis set H. However, unlike the Rademacher complexity, this measure does not depend on the distribution, it is purely combinatorial. **3.2** **Growth function** **35** To relate the Rademacher complexity to the growth function, we will use Mas sart\u2019s lemma. **Theorem 3.7 (Massart\u2019s lemma)** _Let_ A _\u2286_ R _[m]_ _be a finite set, with r_ = max **x** _\u2208_ A _\u2225_ **x** _\u2225_ 2 _,_ _then the following holds:_ \ufffd _\u2264_ _[r]_ \ufffd 2 log _|_ A _|_ _,_ (3.20)",
    "chunk_id": "foundations_machine_learning_35"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_m_ 1 \ufffd _m_ **x** [sup] _\u2208_ A \ufffd 1 _m_ \ufffd _\u03c3_ _i_ _x_ _i_ _i_ =1 _m_ \ufffd E _**\u03c3**_ 2 log _|_ A _|_ _where \u03c3_ _i_ _s are independent uniform random variables taking values in {\u2212_ 1 _,_ +1 _} and_ _x_ 1 _, . . ., x_ _m_ _are the components of vector_ **x** _._ Proof: The result follows immediately from the bound on the expectation of a maximum given by Corollary D.11 since the random variables _\u03c3_ _i_ _x_ _i_ are independent and each _\u03c3_ _i_ _x_ _i_ takes values in [ _\u2212|x_ _i_ _|, |x_ _i_ _|_ ] with \ufffd\ufffd _mi_ =1 _[x]_ _i_ [2] _[\u2264]_ _[r]_ [2] [.] Using this result, we can now bound the Rademacher complexity in terms of the growth function. **Corollary 3.8** _Let_ G _be a family of functions taking values in {\u2212_ 1 _,_ +1 _}. Then the_ _following holds:_ R _m_ (G) _\u2264_ \ufffd 2 log \u03a0 G ( _m_ ) _._ (3.21) _m_ Proof: For a fixed sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ), we denote by G _|S_ the set of vectors of function values ( _g_ ( _x_ 1 ) _, . . ., g_ ( _x_ _m_ )) _[\u22a4]_ where _g_ is in G. Since _g \u2208_ G takes values in _{\u2212_ 1 _,_ +1 _}_, the norm of these vectors is bounded by _[\u221a]_ _m_ . We can then apply Massart\u2019s lemma as follows: \ufffd \ufffd\ufffd \ufffd _m_ \ufffd _\u03c3_ _i_ _u_ _i_ _i_ =1 R _m_ (G) = E _S_ \ufffd E _**\u03c3**_ sup _u\u2208_ G _|S_ \ufffd _\u221am_ \ufffd 2 log _|_ G _|S_ _|_ _m_ 1 _m_ _\u2264_ E _S_ _._ By definition, _|_ G _|S_ _|_ is bounded by the growth function, thus, \ufffd _\u221am_ \ufffd \ufffd R _m_ (G) _\u2264_ E _S_ 2 log \u03a0 G ( _m_ ) _m_ = ~~\ufffd~~ 2 log \u03a0 G ( _m_ ) _,_ _m_ which concludes the proof. Combining the generalization bound (3.17) of theorem 3.5 with corollary 3.8 yields immediately the following generalization bound in terms of the growth function. **Corollary 3.9 (Growth function generalization bound)** _Let_ H _be a family of functions_ _taking values in {\u2212_ 1 _,_ +1 _}. Then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4,_ _for any h \u2208_ H _,_ \ufffd \ufffd log [1] _\u03b4_ (3.22) 2 _m_ _[.]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + 2 log \u03a0 H ( _m_ ) + _m_ 2 log \u03a0 H ( _m_ ) **36** **Chapter 3** **Rademacher Complexity and VC-Dimension** - - + - + + - + + + (a) (b) **Figure 3.1** VC-dimension of intervals on the real line. (a) Any two points can be shattered. (b) No sample of three points can be shattered as the (+ _, \u2212,_ +) labeling cannot be realized. Growth function bounds can be also derived directly (without using Rademacher complexity bounds first). The resulting bound is then the following: \ufffd",
    "chunk_id": "foundations_machine_learning_36"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "P _R_ ( _h_ ) _\u2212_ _R_ _S_ ( _h_ ) _> \u03f5_ _\u2264_ 4\u03a0 H (2 _m_ ) exp _\u2212_ _[m\u03f5]_ [2] \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd \ufffd 8 _,_ (3.23) \ufffd which only differs from (3.22) by constants. The computation of the growth function may not be always convenient since, by definition, it requires computing \u03a0 H ( _m_ ) for all _m \u2265_ 1. The next section introduces an alternative measure of the complexity of a hypothesis set H that is based instead on a single scalar, which will turn out to be in fact deeply related to the behavior of the growth function. **3.3** **VC-dimension** Here, we introduce the notion of _VC-dimension_ (Vapnik-Chervonenkis dimension). The VC-dimension is also a purely combinatorial notion but it is often easier to compute than the growth function (or the Rademacher Complexity). As we shall see, the VC-dimension is a key quantity in learning and is directly related to the growth function. To define the VC-dimension of a hypothesis set H, we first introduce the concept of _shattering_ . Recall from the previous section, that given a hypothesis set H, a dichotomy of a set _S_ is one of the possible ways of labeling the points of _S_ using a hypothesis in H. A set _S_ of _m \u2265_ 1 points is said to be shattered by a hypothesis set H when H realizes all possible dichotomies of _S_, that is when \u03a0 H ( _m_ ) = 2 _[m]_ . **Definition 3.10 (VC-dimension)** _The VC-dimension of a hypothesis set_ H _is the size of_ _the largest set that can be shattered by_ H _:_ VCdim(H) = max _{m_ : \u03a0 H ( _m_ ) = 2 _[m]_ _}._ (3.24) Note that, by definition, if VCdim(H) = _d_, there exists a set of size _d_ that can be shattered. However, this does not imply that all sets of size _d_ or less are shattered and, in fact, this is typically not the case. **3.3** **VC-dimension** **37** ### + - - + ### + ### + ### + (a) (b) **Figure 3.2** Unrealizable dichotomies for four points using hyperplanes in R [2] . (a) All four points lie on the convex hull. (b) Three points lie on the convex hull while the remaining point is interior. To further illustrate this notion, we will examine a series of examples of hypothesis sets and will determine the VC-dimension in each case. To compute the VC-dimension we will typically show a lower bound for its value and then a matching upper bound. To give a lower bound _d_ for VCdim(H), it suffices to show that a set _S_ of cardinality _d_ can be shattered by H. To give an upper bound, we need to prove that no set _S_ of cardinality _d_ + 1 can be shattered by H, which is typically more difficult. **Example 3.11 (Intervals on the real line)** Our first example involves the hypothesis class of intervals on the real line. It is clear that the VC-dimension is at least two, since all four dichotomies (+",
    "chunk_id": "foundations_machine_learning_37"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_,_ +) _,_ ( _\u2212, \u2212_ ) _,_ (+ _, \u2212_ ) _,_ ( _\u2212,_ +) can be realized, as illustrated in figure 3.1(a). In contrast, by the definition of intervals, no set of three points can be shattered since the (+ _, \u2212,_ +) labeling cannot be realized. Hence, VCdim(intervals in R) = 2. **Example 3.12 (Hyperplanes)** Consider the set of hyperplanes in R [2] . We first observe that any three non-collinear points in R [2] can be shattered. To obtain the first three dichotomies, we choose a hyperplane that has two points on one side and the third point on the opposite side. To obtain the fourth dichotomy we have all three points on the same side of the hyperplane. The remaining four dichotomies are realized by simply switching signs. Next, we show that four points cannot be shattered by considering two cases: (i) the four points lie on the convex hull defined by the four points, and (ii) three of the four points lie on the convex hull and the remaining point is internal. In the first case, a positive labeling for one diagonal pair and a negative labeling for the other diagonal pair cannot be realized, as illustrated in figure 3.2(a). In the second case, a labeling which is positive for the points on the convex hull and negative for the interior point cannot be realized, as illustrated in figure 3.2(b). Hence, VCdim(hyperplanes in R [2] ) = 3. More generally in R _[d]_, we derive a lower bound by starting with a set of _d_ +1 points in R _[d]_, setting **x** 0 to be the origin and defining **x** _i_, for _i \u2208{_ 1 _, . . ., d}_, as the point whose _i_ th coordinate is 1 and all others are 0. Let _y_ 0 _, y_ 1 _, . . ., y_ _d_ _\u2208{\u2212_ 1 _,_ +1 _}_ be an **38** **Chapter 3** **Rademacher Complexity and VC-Dimension** arbitrary set of labels for **x** 0 _,_ **x** 1 _, . . .,_ **x** _d_ . Let **w** be the vector whose _i_ th coordinate is _y_ _i_ . Then the classifier defined by the hyperplane of equation **w** _\u00b7_ **x** + _[y]_ 2 [0] [= 0] shatters **x** 0 _,_ **x** 1 _, . . .,_ **x** _d_ since for any _i \u2208{_ 0 _, . . ., d}_, = _y_ _i_ _._ (3.25) \ufffd sgn **w** _\u00b7_ **x** _i_ + _[y]_ [0] \ufffd 2 = sgn _y_ _i_ + _[y]_ [0] \ufffd \ufffd 2 To obtain an upper bound, it suffices to show that no set of _d_ + 2 points can be shattered by halfspaces. To prove this, we will use the following general theorem. **Theorem 3.13 (Radon\u2019s theorem)** Any set X of _d_ + 2 points in R _[d]_ can be partitioned into two subsets X 1 and X 2 such that the convex hulls of X 1 and X 2 intersect. Proof: Let X = _{_ **x** 1 _, . . .,_ **x** _d_ +2 _} \u2282_ R _[d]_ .",
    "chunk_id": "foundations_machine_learning_38"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "The following is a system of _d_ + 1 linear equations in _\u03b1_ 1 _, . . ., \u03b1_ _d_ +2 : _d_ +2 \ufffd _\u03b1_ _i_ **x** _i_ = 0 and _i_ =1 _d_ +2 \ufffd _\u03b1_ _i_ = 0 _,_ (3.26) _i_ =1 since the first equality leads to _d_ equations, one for each component. The number of unknowns, _d_ + 2, is larger than the number of equations, _d_ + 1, therefore the system admits a non-zero solution _\u03b2_ 1 _, . . ., \u03b2_ _d_ +2 . Since [\ufffd] _[d]_ _i_ =1 [+2] _[\u03b2]_ _[i]_ [ = 0, both][ I] [1] [ =] _{i \u2208_ [ _d_ + 2]: _\u03b2_ _i_ _>_ 0 _}_ and I 2 = _{i \u2208_ [ _d_ + 2]: _\u03b2_ _i_ _\u2264_ 0 _}_ are non-empty sets and X 1 = _{_ **x** _i_ : _i \u2208_ I 1 _}_ and X 2 = _{_ **x** _i_ : _i \u2208_ I 2 _}_ form a partition of X. By the last equation of (3.26), [\ufffd] _i\u2208_ I 1 _[\u03b2]_ _[i]_ [ =] _[ \u2212]_ [\ufffd] _i\u2208_ I 2 _[\u03b2]_ _[i]_ [. Let] _[ \u03b2]_ [ =][ \ufffd] _i\u2208_ I 1 _[\u03b2]_ _[i]_ [. Then, the first part] of (3.26) implies _\u03b2_ _i_ _\u2212\u03b2_ _i_ \ufffd **[x]** _[i]_ [ =] \ufffd **[x]** _[i]_ _[,]_ _i\u2208_ I 1 _[\u03b2]_ _[i]_ [ =] _[ \u2212]_ [\ufffd] _i\u2208_ I 2 _[\u03b2]_ _[i]_ [. Let] _[ \u03b2]_ [ =][ \ufffd] _\u03b2_ _i_ _\u2212\u03b2_ _i_ _i\u2208_ I 1 _\u03b2_ _i_ **[x]** _[i]_ [ =] \ufffd _i\u2208_ I 2 _\u03b2_ **[x]** _[i]_ _[,]_ _\u03b2_ _i_ _i\u2208_ I 1 _\u03b2_ _\u03b2_ _i_ _i\u2208_ I 1 _\u03b2_ _\u2212\u03b2_ _i_ _i\u2208_ I 2 _\u03b2_ _\u03b2_ _[i]_ _[\u2265]_ [0 for] _[ i][ \u2208]_ [I] [1] [ and] _[ \u2212]_ _\u03b2_ _[\u03b2]_ _[i]_ with [\ufffd] _i_ _\u03b2_ [=][ \ufffd] _\u03b2\u03b2_ _i_ = 1, and _[\u03b2]_ _\u03b2_ _[i]_ with [\ufffd] _i\u2208_ I 1 _\u03b2_ _i_ [=][ \ufffd] _i\u2208_ I 2 _\u2212\u03b2_ _i_ = 1, and _\u03b2_ _[i]_ _[\u2265]_ [0 for] _[ i][ \u2208]_ [I] [1] [ and] _[ \u2212]_ _\u03b2_ _[i]_ _\u2265_ 0 for _i \u2208_ I 2 . By definition of the convex hulls (B.6), this implies that [\ufffd] _i\u2208_ I 1 _\u03b2\u03b2_ _i_ **[x]** _[i]_ [ belongs both to] the convex hull of X 1 and to that of X 2 . Now, let X be a set of _d_ + 2 points. By Radon\u2019s theorem, it can be partitioned into two sets X 1 and X 2 such that their convex hulls intersect. Observe that when two sets of points X 1 and X 2 are separated by a hyperplane, their convex hulls are also separated by that hyperplane. Thus, X 1 and X 2 cannot be separated by a hyperplane and X is not shattered. Combining our lower and upper bounds, we have proven that VCdim(hyperplanes in R _[d]_ ) = _d_ + 1. **Example 3.14 (Axis-aligned Rectangles)** We first show that the VC-dimension is at least four, by considering four points in a diamond pattern. Then, it is clear that all",
    "chunk_id": "foundations_machine_learning_39"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "16 dichotomies can be realized, some of which are illustrated in figure 3.3(a). In contrast, for any set of five distinct points, if we construct the minimal axisaligned rectangle containing these points, one of the five points is in the interior of **3.3** **VC-dimension** **39** #### + #### + #### ~~+~~ #### - #### - - - - + + #### + #### + - + #### + (a) #### **+** **+** **- +** **+** (b) **Figure 3.3** ~~VC~~ - ~~di~~ mens ~~i~~ on o ~~f~~ ax ~~i~~ s-a ~~li~~ gne ~~d~~ rec ~~t~~ ang ~~l~~ es. ~~(~~ a ~~)~~ ~~E~~ xamp ~~l~~ es o ~~f~~ rea ~~li~~ za ~~bl~~ e ~~di~~ c ~~h~~ o ~~t~~ om ~~i~~ es ~~f~~ or ~~f~~ our po ~~i~~ n ~~t~~ s in a diamond pattern. (b) No sample of five points can be realized if the interior point and the remaining points have opposite labels. this rectangle. Imagine that we assign a negative label to this interior point and a positive label to each of the remaining four points, as illustrated in figure 3.3(b). There is no axis-aligned rectangle that can realize this labeling. Hence, no set of five distinct points can be shattered and VCdim(axis-aligned rectangles) = 4. **Example 3.15 (Convex Polygons)** We focus on the class of convex _d_ -gons in the plane. To get a lower bound, we show that any set of 2 _d_ +1 points can be shattered. To do this, we select 2 _d_ +1 points that lie on a circle, and for a particular labeling, if there are more negative than positive labels, then the points with the positive labels are used as the polygon\u2019s vertices, as in figure 3.4(a). Otherwise, the tangents of the negative points serve as the edges of the polygon, as shown in (3.4)(b). To derive an upper bound, it can be shown that choosing points on the circle maximizes the number of possible dichotomies, and thus VCdim(convex _d_ -gons) = 2 _d_ + 1. Note also that VCdim(convex polygons) = + _\u221e_ . **Example 3.16 (Sine Functions)** The previous examples could suggest that the VCdimension of H coincides with the number of free parameters defining H. For example, the number of parameters defining hyperplanes matches their VC-dimension. However, this does not hold in general. Several of the exercises in this chapter illustrate this fact. The following provides a striking example from this point of view. Consider the following family of sine functions: _{t \ufffd\u2192_ sin( _\u03c9t_ ): _\u03c9 \u2208_ R _}_ . One instance of this function class is shown in figure 3.5. These sine functions can be **40** **Chapter 3** **Rademacher Complexity and VC-Dimension** #### **Image:** [No caption returned] **Image:** [No caption returned] #### **+** #### - - **+** |positive points| < |negative points| #### **+** |positive points| > |negative points| (a) (b) **Figure 3.4** Convex _d_ -gons in the plane can shatter 2 _d_ + 1 points. (a) _d_ -gon construction when there are more negative labels. (b) _d_ -gon construction when there are more positive labels. **Figure 3.5** **Image:** [No",
    "chunk_id": "foundations_machine_learning_40"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "caption returned] An example of a sine function (with _\u03c9_ = 50) used for classification. used to classify the points on the real line: a point is labeled positively if it is above the curve, negatively otherwise. Although this family of sine functions is defined via a single parameter, _\u03c9_, it can be shown that VCdim(sine functions) = + _\u221e_ (exercise 3.20). The VC-dimension of many other hypothesis sets can be determined or upperbounded in a similar way (see this chapter\u2019s exercises). In particular, the VCdimension of any vector space of dimension _r < \u221e_ can be shown to be at most _r_ (exercise 3.19). The next result, known as _Sauer\u2019s lemma_, clarifies the connection between the notions of growth function and VC-dimension. **3.3** **VC-dimension** **41** **Figure 3.6** |1 = G|S0 G2 = \ufffd g0 \u2286S0 : (g0 \u2208G) \u2227(g|Col2|Col3|Col4|Col5|Col6| |---|---|---|---|---|---| |x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7| |x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|x<br>1|x<br>2|\u00b7 \u00b7 \u00b7|x<br>m\u22121|x<br>m| |x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|1|1|0|1|0| |x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1",
    "chunk_id": "foundations_machine_learning_41"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|1|1|0|1|1| |x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|0|1|1|1|1| |x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|1|0|0|1|0| |x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|1|0|0|0|1| |x x \u00b7 \u00b7 \u00b7 x x<br>1 2 m\u22121 m<br>1 1 0 1 0<br>1 1 0 1 1<br>0 1 1 1 1<br>1 0 0 1 0<br>1 0 0 0 1<br>\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7| Illustration of how G 1 and G 2 are constructed in the proof of Sauer\u2019s lemma. **Theorem 3.17 (Sauer\u2019s lemma)** _Let_ H _be a hypothesis set with_ VCdim(H) = _d. Then,_ _for all m \u2208_ N _, the following inequality holds:_ \u03a0 H ( _m_ ) _\u2264_ _d_ \ufffd _i_ =0 _m_ _._ (3.27) \ufffd _i_ \ufffd Proof: The proof is by induction on _m_ + _d_ . The statement clearly holds for _m_ = 1 and _d_ = 0 or _d_ = 1. Now, assume that it holds for ( _m \u2212_ 1 _, d \u2212_ 1) and ( _m \u2212_ 1 _, d_ ). Fix a set S = _{x_ 1 _, . . ., x_ _m_ _}_ with \u03a0 H ( _m_ ) dichotomies and let G = H _|_ S be the set of concepts H induced by restriction to S. Now consider the following families over S _[\u2032]_ = _{x_ 1 _, . . ., x_ _m\u2212_ 1 _}_ . We define G 1 = G _|_ S _\u2032_ as the set of concepts H induced by restriction to _S_ _[\u2032]_ . Next, by identifying each concept as the set of points (in S _[\u2032]_ or S) for which it is non-zero, we can define 2 as G 2 = _{g_ _[\u2032]_ _\u2286_ S _[\u2032]_ : ( _g_ _[\u2032]_ _\u2208_ G) _\u2227_ ( _g_ _[\u2032]_ _\u222a{x_ _m_ _} \u2208_ G) _}._ Since _g_ _[\u2032]_ _\u2286_ S _[\u2032]_, _g_ _[\u2032]_ _\u2208_ G means that without adding _x_ _m_ it is a concept of G. Further, the constraint _g_ _[\u2032]_ _\u222a{x_ _m_ _} \u2208_ G means that adding _x_ _m_ to _g_ _[\u2032]_ also makes it a concept of G. The construction of G 1 and G 2 is illustrated pictorially in figure 3.6. Given our definitions of G 1 and",
    "chunk_id": "foundations_machine_learning_42"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "G 2, observe that _|_ G 1 _|_ + _|_ G 2 _|_ = _|_ G _|_ . Since VCdim(G 1 ) _\u2264_ VCdim(G) _\u2264_ _d_, then by definition of the growth function and using the induction hypothesis, _|_ G 1 _| \u2264_ \u03a0 G 1 ( _m \u2212_ 1) _\u2264_ _d_ \ufffd _i_ =0 \ufffd _m \u2212i_ 1 _._ \ufffd Further, by definition of G 2, if a set Z _\u2286_ S _[\u2032]_ is shattered by G 2, then the set Z _\u222a{x_ _m_ _}_ is shattered by G. Hence, VCdim(G 2 ) _\u2264_ VCdim(G) _\u2212_ 1 = _d \u2212_ 1 _,_ **42** **Chapter 3** **Rademacher Complexity and VC-Dimension** and by definition of the growth function and using the induction hypothesis, _d\u2212_ 1 \ufffd _i_ =0 _|_ G 2 _| \u2264_ \u03a0 G 2 ( _m \u2212_ 1) _\u2264_ _m \u2212_ 1 \ufffd _i_ _._ \ufffd Thus, _|_ G _|_ = _|_ G 1 _|_ + _|_ G 2 _| \u2264_ _d_ \ufffd _i_ =0 \ufffd _m\u2212i_ 1 \ufffd + _d\u2212_ 1 \ufffd _i_ =0 _m\u2212_ 1 = \ufffd _i_ \ufffd _d_ \ufffd _i_ =0 _m\u2212_ 1 _m\u2212_ 1 \ufffd _i_ \ufffd + \ufffd _i\u2212_ 1 \ufffd = _d_ \ufffd _i_ =0 _m_ \ufffd _i_ \ufffd _,_ which completes the inductive proof. The significance of Sauer\u2019s lemma can be seen by corollary 3.18, which remarkably shows that growth function only exhibits two types of behavior: either VCdim(H) = _d <_ + _\u221e_, in which case \u03a0 H ( _m_ ) = _O_ ( _m_ _[d]_ ), or VCdim(H) = + _\u221e_, in which case \u03a0 H ( _m_ ) = 2 _[m]_ . **Corollary 3.18** _Let_ H _be a hypothesis set with_ VCdim(H) = _d. Then for all m \u2265_ _d,_ _em_ \u03a0 H ( _m_ ) _\u2264_ \ufffd _d_ _d_ = _O_ ( _m_ _[d]_ ) _._ (3.28) \ufffd Proof: The proof begins by using Sauer\u2019s lemma. The first inequality multiplies each summand by a factor that is greater than or equal to one since _m \u2265_ _d_, while the second inequality adds non-negative summands to the summation. \u03a0 H ( _m_ ) _\u2264_ _\u2264_ _\u2264_ _d_ \ufffd _i_ =0 _d_ \ufffd _i_ =0 _m_ \ufffd _i_ =0 _m_ \ufffd _i_ \ufffd _m_ \ufffd _i_ _m_ \ufffd _i_ _m_ \ufffd\ufffd _d_ _m_ \ufffd\ufffd _d_ _d_ _d_ \ufffd _d\u2212i_ \ufffd _d\u2212i_ _d_ _m_ \ufffd \ufffd _i_ =0 \ufffd _i_ _m_ = \ufffd _d_ _m_ = \ufffd _d_ _m_ \ufffd _i_ _d_ \ufffd \ufffd _m_ _m_ _d_ [\ufffd] 1 + _[d]_ \ufffd _m_ _d_ [\ufffd] 1 + _[d]_ \ufffd _m_ _m_ _m_ _d_ _\u2264_ _e_ _[d]_ _._ \ufffd \ufffd _d_ \ufffd _m_ = \ufffd _d_ _m_ = \ufffd _d_ After simplifying the expression using the binomial theorem, the final inequality follows using the general inequality (1 _\u2212_ _x_ ) _\u2264_ _e_ _[\u2212][x]_ . The explicit relationship just formulated between VC-dimension and the growth function combined with corollary 3.9 leads immediately to the following generaliza tion bounds based on the VC-dimension. **Corollary 3.19 (VC-dimension generalization bounds)** _Let_ H _be a family of functions_",
    "chunk_id": "foundations_machine_learning_43"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_taking values in {\u2212_ 1 _,_ +1 _} with VC-dimension d. Then, for any \u03b4 >_ 0 _, with proba-_ **3.4** **Lower bounds** **43** _bility at least_ 1 _\u2212_ _\u03b4, the following holds for all h \u2208_ H _:_ \ufffd log [1] _\u03b4_ (3.29) 2 _m_ _[.]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + ~~\ufffd~~ 2 _d_ log _[em]_ _d_ + _m_ Thus, the form of this generalization bound is \ufffd _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + _O_ \ufffd\ufffd log( _m/d_ ) ( _m/d_ ) _,_ (3.30) which emphasizes the importance of the ratio _m/d_ for generalization. The theorem provides another instance of Occam\u2019s razor principle where simplicity is measured in terms of smaller VC-dimension. VC-dimension bounds can be derived directly without using an intermediate Rademacher complexity bound, as for (3.23): combining Sauer\u2019s lemma with (3.23) leads to the following high-probability bound _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + \ufffd 8 _d_ log [2] _[em]_ _[em]_ _d_ + 8 log [4] _\u03b4_ _\u03b4_ _,_ _m_ which has the general form of (3.30). The log factor plays only a minor role in these bounds. A finer analysis can be used in fact to eliminate that factor. **3.4** **Lower bounds** In the previous section, we presented several upper bounds on the generalization error. In contrast, this section provides lower bounds on the generalization error of any learning algorithm in terms of the VC-dimension of the hypothesis set used. These lower bounds are shown by finding for any algorithm a \u2018bad\u2019 distribution. Since the learning algorithm is arbitrary, it will be difficult to specify that particular distribution. Instead, it suffices to prove its existence non-constructively. At a high level, the proof technique used to achieve this is the _probabilistic method_ of Paul Erd\u00a8os. In the context of the following proofs, first a lower bound is given on the expected error over the parameters defining the distributions. From that, the lower bound is shown to hold for at least one set of parameters, that is one distribution. **Theorem 3.20 (Lower bound, realizable case)** _Let_ H _be a hypothesis set with VC-_ _dimension d >_ 1 _._ _Then, for any m \u2265_ 1 _and any learning algorithm A, there_ _exist a distribution_ D _over_ X _and a target function f \u2208_ H _such that_ P _S\u223c_ D _[m]_ _R_ D ( _h_ _S_ _, f_ ) _>_ _[d][ \u2212]_ [1] _\u2265_ 1 _/_ 100 _._ (3.31) \ufffd 32 _m_ \ufffd Proof: Let X = _{x_ 0 _, x_ 1 _, . . ., x_ _d\u2212_ 1 _} \u2286_ X be a set that is shattered by H. For any _\u03f5 >_ 0, we choose D such that its support is reduced to X and so that one point ( _x_ 0 ) **44** **Chapter 3** **Rademacher Complexity and VC-Dimension** has very high probability (1 _\u2212_ 8 _\u03f5_ ), with the rest of the probability mass distributed uniformly among the other points: 8 _\u03f5_ P and _\u2200i \u2208_ [ _d \u2212_ 1]",
    "chunk_id": "foundations_machine_learning_44"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_,_ P (3.32) D [[] _[x]_ [0] [] = 1] _[ \u2212]_ [8] _[\u03f5]_ D [[] _[x]_ _[i]_ [] =] _d \u2212_ 1 _[.]_ With this definition, most samples would contain _x_ 0 and, since X is shattered, _A_ can essentially do no better than tossing a coin when determining the label of a point _x_ _i_ not falling in the training set. We assume without loss of generality that _A_ makes no error on _x_ 0 . For a sample _S_, we let _S_ denote the set of its elements falling in _{x_ 1 _, . . ., x_ _d\u2212_ 1 _}_, and let S be the set of samples _S_ of size _m_ such that _|S| \u2264_ ( _d \u2212_ 1) _/_ 2. Now, fix a sample _S \u2208_ S, and consider the uniform distribution U over all labelings _f_ : X _\u2192{_ 0 _,_ 1 _}_, which are all in H since the set is shattered. Then, the following lower bound holds: _\u0338_ _\u0338_ _\u0338_ E _f_ _\u223c_ U [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [)] =] \ufffd _\u0338_ _f_ _\u2265_ \ufffd _\u0338_ _f_ = \ufffd _\u0338_ _x\u0338\u2208S_ \ufffd 1 _h_ _S_ ( _x_ )= _\u0338_ _f_ ( _x_ ) P[ _x_ ] P[ _f_ ] _x\u2208_ X _\u0338_ _\u0338_ _\u0338_ \ufffd 1 _h_ _S_ ( _x_ )= _\u0338_ _f_ ( _x_ ) P[ _x_ ] P[ _f_ ] _x\u0338\u2208S_ _\u0338_ _\u0338_ _\u0338_ \ufffd\ufffd 1 _h_ _S_ ( _x_ )= _\u0338_ _f_ ( _x_ ) P[ _f_ ]\ufffd P[ _x_ ] _f_ _\u0338_ _\u0338_ _\u0338_ = [1] 2 _\u0338_ _\u0338_ _\u0338_ \ufffd P[ _x_ ] _\u2265_ 2 [1] _x\u0338\u2208S_ _\u0338_ _\u0338_ _\u0338_ 1 8 _\u03f5_ (3.33) 2 _d \u2212_ 1 [= 2] _[\u03f5.]_ _\u0338_ _\u0338_ _\u0338_ \ufffd _\u0338_ _\u0338_ _\u0338_ [1] _d \u2212_ 1 2 2 _\u0338_ _\u0338_ _\u0338_ The first lower bound holds because we remove non-negative terms from the summation when we only consider _x \u0338\u2208_ _S_ instead of all _x_ in X. After rearranging terms, the subsequent equality holds since we are taking an expectation over _f \u2208_ H with uniform weight on each _f_ and H shatters X. The final lower bound holds due to the definitions of D and _S_, the latter which implies that _|_ X _\u2212_ _S| \u2265_ ( _d \u2212_ 1) _/_ 2. Since (3.33) holds for all _S \u2208_ S, it also holds in expectation over all _S \u2208_ S: E _S\u2208_ S \ufffd E _f_ _\u223c_ U [ _R_ D ( _h_ _S_ _, f_ )]\ufffd _\u2265_ 2 _\u03f5_ . By Fubini\u2019s theorem, the expectations can be permuted, thus, E E _\u2265_ 2 _\u03f5._ (3.34) _f_ _\u223c_ U \ufffd _S\u2208_ S [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [)]] \ufffd This implies that E _S\u2208_ S [ _R_ D ( _h_ _S_ _, f_ 0 )] _\u2265_ 2 _\u03f5_ for at least one labeling _f_ 0 _\u2208_ H. Decomposing this expectation into two parts and using _R_ D ( _h_ _S_ _, f_ 0 ) _\u2264_ P D [X _\u2212{x_ 0 _}_ ], we obtain: _\u0338_",
    "chunk_id": "foundations_machine_learning_45"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u0338_ _\u0338_ _S_ E _\u2208_ S [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [0] [)] =] \ufffd _\u0338_ _\u0338_ _\u0338_ [)] =] \ufffd _R_ D ( _h_ _S_ _, f_ 0 ) P[ _R_ D ( _h_ _S_ _, f_ 0 )] + \ufffd _S_ : _R_ D ( _h_ _S_ _,f_ 0 ) _\u2265\u03f5_ _S_ : _R_ D ( _h_ _S_ _\u0338_ _\u0338_ _\u0338_ )] + \ufffd _R_ D ( _h_ _S_ _, f_ 0 ) P[ _R_ D ( _h_ _S_ _, f_ 0 )] _S_ : _R_ D ( _h_ _S_ _,f_ 0 ) _<\u03f5_ _\u0338_ _\u0338_ _\u0338_ _\u2264_ P D [[][X] _[ \u2212{][x]_ [0] _[}]_ []][ P] _S\u2208_ S [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [0] [)] _[ \u2265]_ _[\u03f5]_ [] +] _[ \u03f5]_ [ P] _S\u2208_ S [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [0] [)] _[ < \u03f5]_ []] _\u2264_ 8 _\u03f5_ P \ufffd1 _\u2212_ P \ufffd _._ _S\u2208_ S [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [0] [)] _[ \u2265]_ _[\u03f5]_ [] +] _[ \u03f5]_ _S\u2208_ S [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [0] [)] _[ \u2265]_ _[\u03f5]_ []] **3.4** **Lower bounds** **45** Collecting terms in P _S\u2208_ S [ _R_ D ( _h_ _S_ _, f_ 0 ) _\u2265_ _\u03f5_ ] yields P _S\u2208_ S [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [0] [)] _[ \u2265]_ _[\u03f5]_ []] _[ \u2265]_ 7 [1] [1] 7 _\u03f5_ [(2] _[\u03f5][ \u2212]_ _[\u03f5]_ [) = 1] 7 (3.35) 7 _[.]_ Thus, the probability over all samples _S_ (not necessarily in S) can be lower bounded as P (3.36) _S_ [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [0] [)] _[ \u2265]_ _[\u03f5]_ []] _[ \u2265]_ _S_ [P] _\u2208_ S [[] _[R]_ [D] [(] _[h]_ _[S]_ _[, f]_ [0] [)] _[ \u2265]_ _[\u03f5]_ []][ P][[][S][]] _[ \u2265]_ [1] 7 [P][[][S][]] _[.]_ This leads us to find a lower bound for P[S]. By the multiplicative Chernoff bound (Theorem D.4), for any _\u03b3 >_ 0, the probability that more than ( _d \u2212_ 1) _/_ 2 points are drawn in a sample of size _m_ verifies: 1 _\u2212_ P[S] = P[ _S_ _m_ _\u2265_ 8 _\u03f5m_ (1 + _\u03b3_ )] _\u2264_ _e_ _[\u2212]_ [8] _[\u03f5m]_ _[\u03b3]_ 3 [2] _._ (3.37) Therefore, for _\u03f5_ = ( _d \u2212_ 1) _/_ (32 _m_ ) and _\u03b3_ = 1, P[ _S_ _m_ _\u2265_ _[d][\u2212]_ 2 [1] []] _[ \u2264]_ _[e]_ _[\u2212]_ [(] _[d][\u2212]_ [1)] _[/]_ [12] _[ \u2264]_ _[e]_ _[\u2212]_ [1] _[/]_ [12] _[ \u2264]_ [1] _[ \u2212]_ [7] _[\u03b4,]_ (3.38) for _\u03b4 \u2264_ _._ 01. Thus P[S] _\u2265_ 7 _\u03b4_ and P _S_ [ _R_ D ( _h_ _S_ _, f_ 0 ) _\u2265_ _\u03f5_ ] _\u2265_ _\u03b4_ . The theorem shows that for any algorithm _A_, there exists a \u2018bad\u2019 distribution over X and a target function _f_ for which the error of the hypothesis returned by _A_ is a constant times _md_ [with some constant probability. This further demonstrates the] key role played by the VC-dimension in learning. The result implies in particular",
    "chunk_id": "foundations_machine_learning_46"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "that PAC-learning in the realizable case is not possible when the VC-dimension is infinite. Note that the proof shows a stronger result than the statement of the theorem: the distribution D is selected independently of the algorithm _A_ . We now present a theorem giving a lower bound in the non-realizable case. The following two lemmas will be needed for the proof. **Lemma 3.21** _Let \u03b1 be a uniformly distributed random variable taking values in_ _{\u03b1_ _\u2212_ _, \u03b1_ + _}, where \u03b1_ _\u2212_ = [1] 2 _[\u2212]_ 2 _[\u03f5]_ _[and][ \u03b1]_ [+] [ =] [1] 2 [+] 2 _[\u03f5]_ _[, and let][ S][ be a sample of][ m][ \u2265]_ [1] _random variables X_ 1 _, . . ., X_ _m_ _taking values in {_ 0 _,_ 1 _} and drawn i.i.d. according to_ _the distribution_ D _\u03b1_ _defined by_ P D _\u03b1_ [ _X_ = 1] = _\u03b1. Let h be a function from_ X _[m]_ _to_ _{\u03b1_ _\u2212_ _, \u03b1_ + _}, then the following holds:_ _[\u03f5]_ [1] 2 _[and][ \u03b1]_ [+] [ =] 2 2 _[\u2212]_ 2 _[\u03f5]_ _[\u03f5]_ 2 [+] 2 E _\u03b1_ P [ _h_ ( _S_ ) _\u0338_ = _\u03b1_ ] _\u2265_ \u03a6(2 _\u2308m/_ 2 _\u2309, \u03f5_ ) _,_ (3.39) \ufffd _S\u223c_ D _[m]_ _\u03b1_ \ufffd [1] 4 \ufffd1 _\u2212_ ~~\ufffd~~ 1 _[m\u03f5]_ _\u2212\u03f5_ [2][2] \ufffd [\ufffd] _for all m and \u03f5._ _where_ \u03a6( _m, \u03f5_ ) = [1] 1 _\u2212_ exp \ufffd _\u2212_ 1 _[m\u03f5]_ _\u2212\u03f5_ [2] Proof: The lemma can be interpreted in terms of an experiment with two coins with biases _\u03b1_ _\u2212_ and _\u03b1_ + . It implies that for a discriminant rule _h_ ( _S_ ) based on a sample _S_ drawn from D _\u03b1_ _\u2212_ or D _\u03b1_ +, to determine which coin was tossed, the sample size _m_ must be at least \u2126(1 _/\u03f5_ [2] ). The proof is left as an exercise (exercise D.3). \u25a1 **46** **Chapter 3** **Rademacher Complexity and VC-Dimension** We will make use of the fact that for any fixed _\u03f5_ the function _m \ufffd\u2192_ \u03a6( _m, x_ ) is convex, which is not hard to establish. **Lemma 3.22** _Let Z be a random variable taking values in_ [0 _,_ 1] _._ _Then, for any_ _\u03b3 \u2208_ [0 _,_ 1) _,_ P[ _z > \u03b3_ ] _\u2265_ [E][[] _[Z]_ []] _[ \u2212]_ _[\u03b3]_ _>_ E[ _Z_ ] _\u2212_ _\u03b3._ (3.40) 1 _\u2212_ _\u03b3_ Proof: Since the values taken by _Z_ are in [0 _,_ 1], E[ _Z_ ] = \ufffd \ufffd P[ _Z_ = _z_ ] _z_ + \ufffd _z\u2264\u03b3_ _z>\u03b3_ \ufffd P[ _Z_ = _z_ ] _\u03b3_ + \ufffd _z\u2264\u03b3_ _z>\u03b3_ \ufffd P[ _Z_ = _z_ ] _z_ _z>\u03b3_ _\u2264_ \ufffd \ufffd P[ _Z_ = _z_ ] _z>\u03b3_ = _\u03b3_ P[ _Z \u2264_ _\u03b3_ ] + P[ _Z > \u03b3_ ] = _\u03b3_ (1 _\u2212_ P[ _Z > \u03b3_ ]) + P[ _Z > \u03b3_ ] = (1 _\u2212_ _\u03b3_ ) P[ _Z > \u03b3_ ] + _\u03b3,_ which concludes the proof. **Theorem 3.23 (Lower bound, non-realizable case)** _Let_ H _be a hypothesis set",
    "chunk_id": "foundations_machine_learning_47"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "with VC-_ _dimension d >_ 1 _. Then, for any m \u2265_ 1 _and any learning algorithm A, there exists_ _a distribution_ D _over_ X _\u00d7 {_ 0 _,_ 1 _} such that:_ ~~\ufffd~~ _\u2265_ 1 _/_ 64 _._ (3.41) \ufffd P _S\u223c_ D _[m]_ \ufffd _R_ D ( _h_ _S_ ) _\u2212_ _h_ inf _\u2208_ H _[R]_ [D] [(] _[h]_ [)] _[ >]_ _d_ 320 _m_ _Equivalently, for any learning algorithm, the sample complexity verifies_ _d_ _m \u2265_ (3.42) 320 _\u03f5_ [2] _[ .]_ Proof: Let X = _{x_ 1 _, . . ., x_ _d_ _} \u2286_ X be a set shattered by H. For any _\u03b1 \u2208_ [0 _,_ 1] and any vector _**\u03c3**_ = ( _\u03c3_ 1 _, . . ., \u03c3_ _d_ ) _[\u22a4]_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[d]_, we define a distribution D _**\u03c3**_ with support X _\u00d7 {_ 0 _,_ 1 _}_ as follows: _\u2200i \u2208_ [ _d_ ] _,_ P [1] D _**\u03c3**_ [[(] _[x]_ _[i]_ _[,]_ [ 1)] =] _d_ \ufffd 1 2 2 [+] _[ \u03c3]_ 2 _[i]_ _[\u03b1]_ _._ (3.43) \ufffd Thus, the label of each point _x_ _i_, _i \u2208_ [ _d_ ], follows the distribution P D _**\u03c3**_ [ _\u00b7|x_ _i_ ], that of a biased coin where the bias is determined by the sign of _\u03c3_ _i_ and the magnitude of _\u03b1_ . To determine the most likely label of each point _x_ _i_, the learning algorithm will therefore need to estimate P D _**\u03c3**_ [1 _|x_ _i_ ] with an accuracy better than _\u03b1_ . To make this further difficult, _\u03b1_ and _**\u03c3**_ will be selected based on the algorithm, requiring, as in lemma 3.21, \u2126(1 _/\u03b1_ [2] ) instances of each point _x_ _i_ in the training sample. **3.4** **Lower bounds** **47** Clearly, the Bayes classifier _h_ _[\u2217]_ D _**\u03c3**_ [is defined by] _[ h]_ D _[\u2217]_ _**\u03c3**_ [(] _[x]_ _[i]_ [) = argmax] _y\u2208{_ 0 _,_ 1 _}_ [P][[] _[y][|][x]_ _[i]_ [] =] 1 _\u03c3_ _i_ _>_ 0 for all _i \u2208_ [ _d_ ]. _h_ _[\u2217]_ D _**\u03c3**_ [is in][ H][ since][ X][ is shattered. For all] _[ h][ \u2208]_ [H][,] _\u0338_ _\u0338_ _\u0338_ _R_ D _**\u03c3**_ ( _h_ ) _\u2212_ _R_ D _**\u03c3**_ ( _h_ _[\u2217]_ D _**\u03c3**_ [) =] [1] _\u0338_ _\u0338_ _d_ _\u0338_ \ufffd _\u0338_ _\u0338_ _x\u2208_ X _\u0338_ _\u0338_ _\u0338_ 2 _\u0338_ _\u0338_ \ufffd 1 _h_ ( _x_ )= _\u0338_ _h_ _[\u2217]_ D _**\u03c3**_ [(] _[x]_ [)] _[.]_ [ (3.44)] _x\u2208_ X _\u0338_ _\u03b1_ _\u0338_ _\u0338_ \ufffd 2 _\u0338_ _\u03b1_ _\u0338_ _\u0338_ 2 [+] _[ \u03b1]_ 2 _\u0338_ \ufffd1 _h_ ( _x_ )= _\u0338_ _h_ _[\u2217]_ D _**\u03c3**_ [(] _[x]_ [)] [ =] _[\u03b1]_ _d_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ Let _h_ _S_ denote the hypothesis returned by the learning algorithm _A_ after receiving a labeled sample _S_ drawn according to D _**\u03c3**_ . We will denote by _|S|_ _x_ the number of occurrences of a point _x_ in _S_ . Let U denote the uniform distribution over _{\u2212_ 1 _,_ +1 _}_ _[d]_ . Then, in view of (3.44),",
    "chunk_id": "foundations_machine_learning_48"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the following holds: _\u0338_ _\u0338_ _\u0338_ E _**\u03c3**_ _\u223c_ U _S\u223c_ D _[m]_ _**\u03c3**_ = [1] _\u0338_ _d_ = [1] _d_ = [1] _d_ _\u2265_ [1] _d_ _\u2265_ [1] _d_ _\u0338_ _\u0338_ 1 \ufffd _R_ D _**\u03c3**_ ( _h_ _S_ ) _\u2212_ _R_ D _**\u03c3**_ ( _h_ _[\u2217]_ D _**\u03c3**_ [)] \ufffd [\ufffd] \ufffd _\u03b1_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd \u03a6( _m/d_ + 1 _, \u03b1_ ) (convexity of \u03a6( _\u00b7, \u03b1_ ) and Jensen\u2019s ineq.) _x\u2208_ X _\u0338_ _\u0338_ E \ufffd _**\u03c3**_ _\u223c_ U _\u0338_ _x\u2208_ X _S\u223c_ D _[m]_ _**\u03c3**_ _\u0338_ _\u0338_ \ufffd1 _h_ _S_ ( _x_ )= _\u0338_ _h_ _[\u2217]_ D _**\u03c3**_ [(] _[x]_ [)] \ufffd _\u0338_ _\u0338_ _\u0338_ \ufffd _h_ _S_ ( _x_ ) _\u0338_ = _h_ _[\u2217]_ D _**\u03c3**_ [(] _[x]_ [)] \ufffd [\ufffd] _\u0338_ _\u0338_ _\u0338_ \ufffd _**\u03c3**_ E _\u223c_ U _x\u2208_ X _\u0338_ _\u0338_ _\u0338_ P \ufffd _S\u223c_ D _[m]_ _**\u03c3**_ _\u0338_ _\u0338_ _\u0338_ \ufffd _h_ _S_ ( _x_ ) _\u0338_ = _h_ _[\u2217]_ D _**\u03c3**_ [(] _[x]_ [)] \ufffd\ufffd _|S|_ _x_ = _n_ \ufffd P[ _|S|_ _x_ = _n_ ]\ufffd _\u0338_ _\u0338_ _\u0338_ \ufffd _x\u2208_ X \ufffd _x\u2208_ X _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd _**\u03c3**_ E _\u223c_ U _n_ =0 _\u0338_ _\u0338_ _\u0338_ P \ufffd _S\u223c_ D _[m]_ _**\u03c3**_ _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd \u03a6( _n_ + 1 _, \u03b1_ ) P[ _|S|_ _x_ = _n_ ] (lemma 3.21) _n_ =0 _\u0338_ _\u0338_ _\u0338_ = \u03a6( _m/d_ + 1 _, \u03b1_ ) _._ Since the expectation over _**\u03c3**_ is lower-bounded by \u03a6( _m/d_ + 1 _, \u03b1_ ), there must exist some _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[d]_ for which _\u0338_ _\u0338_ _\u0338_ E _S\u223c_ D _[m]_ _**\u03c3**_ _\u0338_ _\u0338_ _\u0338_ 1 \ufffd _R_ D _**\u03c3**_ ( _h_ _S_ ) _\u2212_ _R_ D _**\u03c3**_ ( _h_ _[\u2217]_ D _**\u03c3**_ [)] \ufffd [\ufffd] _>_ \u03a6( _m/d_ + 1 _, \u03b1_ ) _._ (3.45) \ufffd _\u03b1_ _\u0338_ _\u0338_ _\u0338_ Then, by lemma 3.22, for that _**\u03c3**_, for any _\u03b3 \u2208_ [0 _,_ 1], _\u0338_ _\u0338_ _\u0338_ P _S\u223c_ D _[m]_ _**\u03c3**_ _\u0338_ _\u0338_ _\u0338_ 1 \ufffd _R_ D _**\u03c3**_ ( _h_ _S_ ) _\u2212_ _R_ D _**\u03c3**_ ( _h_ _[\u2217]_ D _**\u03c3**_ [)] \ufffd _> \u03b3u_ _>_ (1 _\u2212_ _\u03b3_ ) _u,_ (3.46) \ufffd _\u03b1_ \ufffd _\u0338_ _\u0338_ _\u0338_ where _u_ = \u03a6( _m/d_ + 1 _, \u03b1_ ). Selecting _\u03b4_ and _\u03f5_ such that _\u03b4 \u2264_ (1 _\u2212_ _\u03b3_ ) _u_ and _\u03f5 \u2264_ _\u03b3\u03b1u_ gives _S\u223c_ P D _[m]_ _**\u03c3**_ \ufffd _R_ D _**\u03c3**_ ( _h_ _S_ ) _\u2212_ _R_ D _**\u03c3**_ ( _h_ _[\u2217]_ D _**\u03c3**_ [)] _[ > \u03f5]_ \ufffd _> \u03b4._ (3.47) **48** **Chapter 3** **Rademacher Complexity and VC-Dimension** To satisfy the inequalities defining _\u03f5_ and _\u03b4_, let _\u03b3_ = 1 _\u2212_ 8 _\u03b4_ . Then, _\u03b4 \u2264_ (1 _\u2212_ _\u03b3_ ) _u \u21d0\u21d2_ _u \u2265_ [1] (3.48) 8 \ufffd \ufffd [\ufffd] _\u21d0\u21d2_ [1] 4 1 _\u2212_ \ufffd \ufffd 1 _\u2212_ exp _\u2212_ [(] _[m][/][d]_ [ + 1][)] _[\u03b1]_ [2] \ufffd 1 _\u2212_ _\u03b1_ [2] _\u2265_ [1] (3.49) 8 _\u21d0\u21d2_ [(] _[m][/][d]_ [ + 1][)] _[\u03b1]_ [2] (3.50) 3 [4]",
    "chunk_id": "foundations_machine_learning_49"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(3.51) 3 _[\u2212]_ [1] _[.]_ _[d]_ [ + 1][)] _[\u03b1]_ _\u2264_ log [4] 1 _\u2212_ _\u03b1_ [2] 3 _d_ _[\u2264]_ \ufffd _\u03b1_ 1 _\u21d0\u21d2_ _[m]_ 1 _\u03b1_ [2] _[ \u2212]_ [1] \ufffd log [4] 3 Selecting _\u03b1_ = 8 _\u03f5/_ (1 _\u2212_ 8 _\u03b4_ ) gives _\u03f5_ = _\u03b3\u03b1/_ 8 and the condition _m_ (1 _\u2212_ 8 _\u03b4_ ) 2 _d_ _[\u2264]_ \ufffd 64 _\u03f5_ [2] _m_ 2 _\u2212_ 8 _\u03b4_ ) _\u2212_ 1 log [4] 64 _\u03f5_ [2] \ufffd 3 (3.52) 3 _[\u2212]_ [1] _[.]_ Let _f_ (1 _/\u03f5_ [2] ) denote the right-hand side. We are seeking a sufficient condition of the form _m/d \u2264_ _\u03c9/\u03f5_ [2] . Since _\u03f5 \u2264_ 1 _/_ 64, to ensure that _\u03c9/\u03f5_ [2] _\u2264_ _f_ (1 _/\u03f5_ [2] ), it suffices to impose (1 _/\u03c9_ 64) [2] [ =] _[ f]_ \ufffd (1 _/_ 164) [2] \ufffd. This condition gives _\u03c9_ = (7 _/_ 64) [2] log(4 _/_ 3) _\u2212_ (1 _/_ 64) [2] (log(4 _/_ 3) + 1) _\u2248_ _._ 003127 _\u2265_ 1 _/_ 320 = _._ 003125 _._ Thus, _\u03f5_ [2] _\u2264_ 320(1 _m/d_ ) [is sufficient to ensure the inequalities.] The theorem shows that for any algorithm _A_, in the non-realizable case, there exists a \u2018bad\u2019 distribution over X _\u00d7 {_ 0 _,_ 1 _}_ such that the error of the hypothesis returned by _A_ is a constant times \ufffd _md_ [with some constant probability. The VC-] dimension appears as a critical quantity in learning in this general setting as well. In particular, with an infinite VC-dimension, agnostic PAC-learning is not possible. **3.5** **Chapter notes** The use of Rademacher complexity for deriving generalization bounds in learning was first advocated by Koltchinskii [2001], Koltchinskii and Panchenko [2000], and Bartlett, Boucheron, and Lugosi [2002a], see also [Koltchinskii and Panchenko, 2002, Bartlett and Mendelson, 2002]. Bartlett, Bousquet, and Mendelson [2002b] introduced the notion of _local Rademacher complexity_, that is the Rademacher complexity restricted to a subset of the hypothesis set limited by a bound on the variance. This can be used to derive better guarantees under some regularity assumptions about the noise. Theorem 3.7 is due to Massart [2000]. The notion of VC-dimension was introduced by Vapnik and Chervonenkis [1971] and has been since extensively studied [Vapnik, 2006, Vapnik and Chervonenkis, 1974, Blumer et al., 1989, Assouad, 1983, Dudley, **3.5** **Chapter notes** **49** 1999]. In addition to the key role it plays in machine learning, the VC-dimension is also widely used in a variety of other areas of computer science and mathematics (e.g., see Shelah [1972], Chazelle [2000]). Theorem 3.17 is known as _Sauer\u2019s lemma_ in the learning community, however the result was first given by Vapnik and Chervonenkis [1971] (in a somewhat different version) and later independently by Sauer [1972] and Shelah [1972]. In the realizable case, lower bounds for the expected error in terms of the VCdimension were given by Vapnik and Chervonenkis [1974] and Haussler et al. [1988]. Later, a lower bound for the probability of error such as that of theorem 3.20 was given by Blumer et al. [1989]. Theorem",
    "chunk_id": "foundations_machine_learning_50"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "3.20 and its proof, which improves upon this previous result, are due to Ehrenfeucht, Haussler, Kearns, and Valiant [1988]. Devroye and Lugosi [1995] gave slightly tighter bounds for the same problem with a more complex expression. Theorem 3.23 giving a lower bound in the non-realizable case and the proof presented are due to Anthony and Bartlett [1999]. For other examples of application of the probabilistic method demonstrating its full power, consult the reference book of Alon and Spencer [1992]. There are several other measures of the complexity of a family of functions used in machine learning, including _covering numbers_, _packing numbers_, and some other complexity measures discussed in chapter 11. A covering number _N_ _p_ (G _, \u03f5_ ) is the minimal number of _L_ _p_ balls of radius _\u03f5 >_ 0 needed to cover a family of loss functions G. A packing number _M_ _p_ (G _, \u03f5_ ) is the maximum number of non-overlapping _L_ _p_ balls of radius _\u03f5_ centered in G. The two notions are closely related, in particular it can be shown straightforwardly that _M_ _p_ (G _,_ 2 _\u03f5_ ) _\u2264N_ _p_ (G _, \u03f5_ ) _\u2264M_ _p_ (G _, \u03f5_ ) for G and _\u03f5 >_ 0. Each complexity measure naturally induces a different reduction of infinite hypothesis sets to finite ones, thereby resulting in generalization bounds for infinite hypothesis sets. Exercise 3.31 illustrates the use of covering numbers for deriving generalization bounds using a very simple proof. There are also close relationships between these complexity measures: for example, by Dudley\u2019s theorem, the empirical Rademacher complexity can be bounded in terms of _N_ 2 (G _, \u03f5_ ) [Dudley, 1967, 1987] and the covering and packing numbers can be bounded in terms of the VC-dimension [Haussler, 1995]. See also [Ledoux and Talagrand, 1991, Alon et al., 1997, Anthony and Bartlett, 1999, Cucker and Smale, 2001, Vidyasagar, 1997] for a number of upper bounds on the covering number in terms of other complexity measures. **50** **Chapter 3** **Rademacher Complexity and VC-Dimension** **3.6** **Exercises** 3.1 Growth function of intervals in R. Let H be the set of intervals in R. The VC-dimension of H is 2. Compute its shattering coefficient \u03a0 H ( _m_ ), _m \u2265_ 0. Compare your result with the general bound for growth functions. 3.2 Growth function and Rademacher complexity of thresholds in R. Let H be the family of threshold functions over the real line: H = _{x \ufffd\u2192_ 1 _x\u2264\u03b8_ : _\u03b8 \u2208_ R _}\u222a{x \ufffd\u2192_ 1 _x\u2265\u03b8_ : _\u03b8 \u2208_ R _}_ . Give an upper bound on the growth function \u03a0 _m_ (H). Use that to derive an upper bound on R _m_ (H). 3.3 Growth function of linear combinations. A _linearly separable labeling_ of a set X of vectors in R _[d]_ is a classification of X into two sets X [+] and X _[\u2212]_ with X [+] = _{_ **x** _\u2208_ X : **w** _\u00b7_ **x** _>_ 0 _}_ and X _[\u2212]_ = _{_ **x** _\u2208_ X : **w** _\u00b7_ **x** _<_ 0 _}_ for some **w**",
    "chunk_id": "foundations_machine_learning_51"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2208_ R _[d]_ . Let X = _{_ **x** 1 _, . . .,_ **x** _m_ _}_ be a subset of R _[d]_ . (a) Let _{_ X [+] _,_ X _[\u2212]_ _}_ be a dichotomy of X and let **x** _m_ +1 _\u2208_ R _[d]_ . Show that _{_ X [+] _\u222a_ _{_ **x** _m_ +1 _},_ X _[\u2212]_ _}_ and _{_ X [+] _,_ X _[\u2212]_ _\u222a{_ **x** _m_ +1 _}}_ are linearly separable by a hyperplane going through the origin if and only if _{_ X [+] _,_ X _[\u2212]_ _}_ is linearly separable by a hyperplane going through the origin and **x** _m_ +1 . (b) Let X = _{_ **x** 1 _, . . .,_ **x** _m_ _}_ be a subset of R _[d]_ such that any _k_ -element subset of X with _k \u2264_ _d_ is linearly independent. Then, show that the number of linearly separable labelings of X is _C_ ( _m, d_ ) = 2 [\ufffd] _[d]_ _k_ _[\u2212]_ =0 [1] \ufffd _mk\u2212_ 1 \ufffd. ( _Hint_ : prove by induction that _C_ ( _m_ + 1 _, d_ ) = _C_ ( _m, d_ ) + _C_ ( _m, d \u2212_ 1). (c) Let _f_ 1 _, . . ., f_ _p_ be _p_ functions mapping R _[d]_ to R. Define F as the family of classifiers based on linear combinations of these functions: _p_ F = _x \ufffd\u2192_ sgn \ufffd _a_ _k_ _f_ _k_ ( _x_ ) : _a_ 1 _, . . ., a_ _p_ _\u2208_ R _._ \ufffd \ufffd _k_ =1 \ufffd \ufffd Define \u03a8 by \u03a8( _x_ ) = ( _f_ 1 ( _x_ ) _, . . ., f_ _p_ ( _x_ )). Assume that there exists _x_ 1 _, . . ., x_ _m_ _\u2208_ R _[d]_ such that every _p_ -subset of _{_ \u03a8( _x_ 1 ) _, . . .,_ \u03a8( _x_ _m_ ) _}_ is linearly independent. Then, show that \u03a0 F ( _m_ ) = 2 _p\u2212_ 1 \ufffd _i_ =0 _m \u2212_ 1 \ufffd _i_ \ufffd _._ 3.4 Lower bound on growth function. Prove that Sauer\u2019s lemma (theorem 3.17) is tight, i.e., for any set X of _m > d_ elements, show that there exists a hypothesis class H of VC-dimension _d_ such that \u03a0 H ( _m_ ) = [\ufffd] _[d]_ _i_ =0 \ufffd _mi_ \ufffd. **3.6** **Exercises** **51** 3.5 Finer Rademacher upper bound. Show that a finer upper bound on the Rademacher complexity of the family G can be given in terms of E _S_ [\u03a0(G _, S_ )], where \u03a0(G _, S_ ) is the number of ways to label the points in sample _S_ . 3.6 Singleton hypothesis class. Consider the trivial hypothesis set H = _{h_ 0 _}_ . (a) Show that R _m_ (H) = 0 for any _m >_ 0. (b) Use a similar construction to show that Massart\u2019s lemma (theorem 3.7) is tight. 3.7 Two function hypothesis class. Let H be a hypothesis set reduced to two functions: H = _{h_ _\u2212_ 1 _,",
    "chunk_id": "foundations_machine_learning_52"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "h_ +1 _}_ and let _S_ = ( _x_ 1 _, . . ., x_ _m_ ) _\u2286_ X be a sample of size _m_ . (a) Assume that _h_ _\u2212_ 1 is the constant function taking value _\u2212_ 1 and _h_ +1 the constant function taking the value +1. What is the VC-dimension _d_ of H? Upper bound the empirical Rademacher complexity\ufffd R [\ufffd] _S_ (H) ( _Hint_ : express R _S_ (H) in terms of the absolute value of a sum of Rademacher variables and apply Jensen\u2019s inequality) and compare your bound with \ufffd _d/m_ . (b) Assume that _h_ _\u2212_ 1 is the constant function taking value _\u2212_ 1 and _h_ +1 the function taking value _\u2212_ 1 everywhere except at _x_ 1 where it takes the value +1. What is the VC-dimension _d_ of H? Compute the empirical Rademacher complexity R [\ufffd] _S_ (H). 3.8 Rademacher identities. Fix _m \u2265_ 1. Prove the following identities for any _\u03b1 \u2208_ R and any two hypothesis sets H and H _[\u2032]_ of functions mapping from X to R: (a) R _m_ ( _\u03b1_ H) = _|\u03b1|_ R _m_ (H) _._ (b) R _m_ (H + H _[\u2032]_ ) = R _m_ (H) + R _m_ (H _[\u2032]_ ) _._ (c) R _m_ ( _{_ max( _h, h_ _[\u2032]_ ): _h \u2208_ H _, h_ _[\u2032]_ _\u2208_ H _[\u2032]_ _}_ ) _\u2264_ R _m_ (H) + R _m_ (H _[\u2032]_ ) _,_ where max( _h, h_ _[\u2032]_ ) denotes the function _x \ufffd\u2192_ max _x\u2208_ X ( _h_ ( _x_ ) _, h_ _[\u2032]_ ( _x_ )) ( _Hint_ : you could use the identity max( _a, b_ ) = [1] 2 [[] _[a]_ [ +] _[ b]_ [ +] _[ |][a][ \u2212]_ _[b][|]_ [] valid for all] _[ a, b][ \u2208]_ [R][ and] Talagrand\u2019s contraction lemma (see lemma 5.7)). 3.9 Rademacher complexity of intersection of concepts. Let H 1 and H 2 be two families of functions mapping X to _{_ 0 _,_ 1 _}_ and let H = _{h_ 1 _h_ 2 : _h_ 1 _\u2208_ H 1 _, h_ 2 _\u2208_ H 2 _}_ . Show that the empirical Rademacher complexity of H for any sample _S_ of size _m_ can be bounded as follows: \ufffd \ufffd R _S_ ( _H_ ) _\u2264_ R _S_ (H 1 ) + \ufffdR _S_ (H 2 ) _._ **52** **Chapter 3** **Rademacher Complexity and VC-Dimension** _Hint_ : use the Lipschitz function _x \ufffd\u2192_ max(0 _, x_ _\u2212_ 1) and Talagrand\u2019s contraction lemma. Use that to bound the Rademacher complexity R _m_ (U) of the family U of intersections of two concepts _c_ 1 and _c_ 2 with _c_ 1 _\u2208_ C 1 and _c_ 2 _\u2208_ C 2 in terms of the Rademacher complexities of C 1 and C 2 . 3.10 Rademacher complexity of prediction vector. Let _S_ = ( _x_ 1 _, . . ., x_ _m_ ) be a sample of size _m_ and fix _h_ : X _\u2192_ R. _h_ ( _x_ 1 ) (a) Denote by",
    "chunk_id": "foundations_machine_learning_53"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**u** the vector of predictions of _h_ for _S_ : **u** = ... \ufffd _h_ ( _x_ _m_ ) . Give \ufffd an upper bound on the empirical Rademacher complexity R [\ufffd] _S_ (H) of H = _{h, \u2212h}_ in terms of _\u2225_ **u** _\u2225_ 2 ( _Hint_ : express R [\ufffd] _S_ (H) in terms of the expectation of an absolute value and apply Jensen\u2019s inequality). Suppose that _h_ ( _x_ _i_ ) _\u2208_ _{_ 0 _, \u2212_ 1 _,_ +1 _}_ for all _i \u2208_ [ _m_ ]. Express the bound on the Rademacher complexity in terms of the sparsity measure _n_ = _|{i | h_ ( _x_ _i_ ) _\u0338_ = 0 _}|_ . What is that upper bound for the extreme values of the sparsity measure? (b) Let F be a family of functions mapping X to R. Give an upper bound on the empirical Rademacher complexity of F + _h_ = _{f_ + _h_ : _f \u2208_ F _}_ and that of F _\u00b1 h_ = (F + _h_ ) _\u222a_ (F _\u2212_ _h_ ) in terms of R [\ufffd] _S_ (F) and _\u2225_ **u** _\u2225_ 2 . 3.11 Rademacher complexity of regularized neural networks. Let the input space be X = R _[n]_ [1] . In this problem, we consider the family of regularized neural networks defined by the following set of functions mapping X to R: \uf8fc \uf8fd \uf8fe _[,]_ H = \uf8f1 \uf8f2 \uf8f3 **[x]** _[ \ufffd\u2192]_ _n_ 2 \ufffd \ufffd _w_ _j_ _\u03c3_ ( **u** _j_ _\u00b7_ **x** ): _\u2225_ **w** _\u2225_ 1 _\u2264_ \u039b _[\u2032]_ _, \u2225_ **u** _j_ _\u2225_ 2 _\u2264_ \u039b _, \u2200j \u2208_ [ _n_ 2 ] _j_ =1 where _\u03c3_ is an _L_ -Lipschitz function. As an example, _\u03c3_ could be the sigmoid function which is 1-Lipschitz. (a) Show that R [\ufffd] _S_ (H) = [\u039b] _m_ _[\u2032]_ [E] _**[\u03c3]**_ \ufffdsup _\u2225_ **u** _\u2225_ 2 _\u2264_ \u039b _|_ [\ufffd] _[m]_ _i_ =1 _[\u03c3]_ _[i]_ _[\u03c3]_ [(] **[u]** _[ \u00b7]_ **[ x]** _[i]_ [)] _[|]_ \ufffd. (b) Use the following form of Talagrand\u2019s lemma valid for all hypothesis sets H and _L_ -Lipschitz function \u03a6: \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd sup _h\u2208_ H 1 _m_ [E] _**\u03c3**_ \ufffd sup _h\u2208_ H \ufffd _\u2264_ _[L]_ _m_ [E] _**\u03c3**_ \ufffd \ufffd\ufffd\ufffd\ufffd\ufffd _m_ \ufffd \ufffd _\u03c3_ _i_ (\u03a6 _\u25e6_ _h_ )( _x_ _i_ ) _i_ =1 \ufffd\ufffd\ufffd\ufffd\ufffd _m_ \ufffd \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ ) _i_ =1 _,_ **3.6** **Exercises** **53** to upper bound R [\ufffd] _S_ (H) in terms of the empirical Rademacher complexity of H _[\u2032]_, where H _[\u2032]_ is defined by H _[\u2032]_ = _{_ **x** _\ufffd\u2192_ _s_ ( **u** _\u00b7_ **x** ): _\u2225_ **u** _\u2225_ 2 _\u2264_ \u039b _, s \u2208{\u2212_ 1 _,_ +1 _}} ._ (c) Use the Cauchy-Schwarz inequality to show that \ufffd R _S_ (H _[\u2032]_ ) = [\u039b] _m_ [E] _**\u03c3**_ \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u03c3_ _i_ **x** _i_ _i_ =1 \ufffd\ufffd\ufffd\ufffd\ufffd 2 \ufffd _._ (d) Use the inequality E **v** [ _\u2225_ **v** _\u2225_ 2 ] _\u2264_ \ufffdE **v** [ _\u2225_ **v** _\u2225_ [2]",
    "chunk_id": "foundations_machine_learning_54"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "2 [], which holds by Jensen\u2019s inequal-] ity to upper bound R [\ufffd] _S_ (H _[\u2032]_ ). (e) Assume that for all **x** _\u2208_ _S_, _\u2225_ **x** _\u2225_ 2 _\u2264_ _r_ for some _r >_ 0. Use the previous questions to derive an upper bound on the Rademacher complexity of H in terms of _r_ . 3.12 Rademacher complexity. Professor Jesetoo claims to have found a better bound on the Rademacher complexity of any hypothesis set H of functions taking values in _{\u2212_ 1 _,_ +1 _}_, in terms of its VC-dimension VCdim(H). His bound is of VCdim(H) the form R _m_ (H) _\u2264_ _O_ \ufffd _m_ \ufffd. Can you show that Professor Jesetoo\u2019s claim cannot be correct? ( _Hint_ : consider a hypothesis set H reduced to just two simple functions.) 3.13 VC-dimension of union of _k_ intervals. What is the VC-dimension of subsets of the real line formed by the union of _k_ intervals? 3.14 VC-dimension of finite hypothesis sets. Show that the VC-dimension of a finite hypothesis set H is at most log 2 _|_ H _|_ . 3.15 VC-dimension of subsets. What is the VC-dimension of the set of subsets _I_ _\u03b1_ of the real line parameterized by a single parameter _\u03b1_ : _I_ _\u03b1_ = [ _\u03b1, \u03b1_ +1] _\u222a_ [ _\u03b1_ +2 _,_ + _\u221e_ )? 3.16 VC-dimension of axis-aligned squares and triangles. (a) What is the VC-dimension of axis-aligned squares in the plane? (b) Consider right triangles in the plane with the sides adjacent to the right angle both parallel to the axes and with the right angle in the lower left corner. What is the VC-dimension of this family? **54** **Chapter 3** **Rademacher Complexity and VC-Dimension** 3.17 VC-dimension of closed balls in R _[n]_ . Show that the VC-dimension of the set of all closed balls in R _[n]_, i.e., sets of the form _{x \u2208_ R _[n]_ : _\u2225x \u2212_ _x_ 0 _\u2225_ [2] _\u2264_ _r}_ for some _x_ 0 _\u2208_ R _[n]_ and _r \u2265_ 0, is less than or equal to _n_ + 2. 3.18 VC-dimension of ellipsoids. What is the VC-dimension of the set of all ellipsoids in R _[n]_ ? 3.19 VC-dimension of a vector space of real functions. Let _F_ be a finite-dimensional vector space of real functions on R _[n]_, dim( _F_ ) = _r < \u221e_ . Let H be the set of hypotheses: H = _{{x_ : _f_ ( _x_ ) _\u2265_ 0 _}_ : _f \u2208_ _F_ _}._ Show that _d_, the VC-dimension of H, is finite and that _d \u2264_ _r_ . ( _Hint_ : select an arbitrary set of _m_ = _r_ + 1 points and consider linear mapping _u_ : _F \u2192_ R _[m]_ defined by: _u_ ( _f_ ) = ( _f_ ( _x_ 1 ) _, . . ., f_ ( _x_ _m_ )).) 3.20 VC-dimension of sine functions. Consider the hypothesis family of sine functions (Example 3.16): _{x \u2192_ sin( _\u03c9x_ ): _\u03c9 \u2208_ R _}_ . (a) Show that for any _x \u2208_ R the points _x,_",
    "chunk_id": "foundations_machine_learning_55"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "2 _x,_ 3 _x_ and 4 _x_ cannot be shattered by this family of sine functions. (b) Show that the VC-dimension of the family of sine functions is infinite. ( _Hint_ : show that _{_ 2 _[\u2212][i]_ : _i \u2264_ _m}_ can be shattered for any _m >_ 0.) 3.21 VC-dimension of union of halfspaces. Provide an upper bound on the VCdimension of the class of hypotheses described by the unions of _k_ halfspaces. 3.22 VC-dimension of intersection of halfspaces. Consider the class C _k_ of convex intersections of _k_ halfspaces. Give lower and upper bound estimates for VCdim(C _k_ ). 3.23 VC-dimension of intersection concepts. (a) Let C 1 and C 2 be two concept classes. Show that for any concept class C = _{c_ 1 _\u2229_ _c_ 2 : _c_ 1 _\u2208_ C 1 _, c_ 2 _\u2208_ C 2 _}_, \u03a0 C ( _m_ ) _\u2264_ \u03a0 C 1 ( _m_ ) \u03a0 C 2 ( _m_ ) _._ (3.53) (b) Let C be a concept class with VC-dimension _d_ and let C _s_ be the concept class formed by all intersections of _s_ concepts from C, _s \u2265_ 1. Show that the VC-dimension of C _s_ is bounded by 2 _ds_ log 2 (3 _s_ ). ( _Hint_ : show that log 2 (3 _x_ ) _<_ 9 _x/_ (2 _e_ ) for any _x \u2265_ 2.) **3.6** **Exercises** **55** 3.24 VC-dimension of union of concepts. Let A and B be two sets of functions mapping from X into _{_ 0 _,_ 1 _}_, and assume that both A and B have finite VCdimension, with VCdim(A) = _d_ A and VCdim(B) = _d_ B . Let C = A _\u222a_ B be the union of A and B. (a) Prove that for all _m_, \u03a0 C ( _m_ ) _\u2264_ \u03a0 A ( _m_ ) + \u03a0 B ( _m_ ). (b) Use Sauer\u2019s lemma to show that for _m \u2265_ _d_ A + _d_ B + 2, \u03a0 C ( _m_ ) _<_ 2 _[m]_ _,_ and give a bound on the VC-dimension of C. 3.25 VC-dimension of symmetric difference of concepts. For two sets A and B, let A\u2206B denote the symmetric difference of A and B, i.e., A\u2206B = (A _\u222a_ B) _\u2212_ (A _\u2229_ B). Let H be a non-empty family of subsets of X with finite VC-dimension. Let A be an element of H and define H\u2206A = _{X_ \u2206A : _X \u2208_ H _}_ . Show that VCdim(H\u2206A) = VCdim(H) _._ 3.26 Symmetric functions. A function _h_ : _{_ 0 _,_ 1 _}_ _[n]_ _\u2192{_ 0 _,_ 1 _}_ is _symmetric_ if its value is uniquely determined by the number of 1\u2019s in the input. Let C denote the set of all symmetric functions. (a) Determine the VC-dimension of C. (b) Give lower and upper bounds on the sample complexity of any consistent PAC learning algorithm for C. (c) Note that any hypothesis _h \u2208_ C can be represented by a vector ( _y_ 0 _, y_ 1 _, . . .,",
    "chunk_id": "foundations_machine_learning_56"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "y_ _n_ ) _\u2208{_ 0 _,_ 1 _}_ _[n]_ [+1], where _y_ _i_ is the value of _h_ on examples having precisely _i_ 1\u2019s. Devise a consistent learning algorithm for C based on this representation. 3.27 VC-dimension of neural networks. Let C be a concept class over R _[r]_ with VC-dimension _d_ . A C-neural network with one intermediate layer is a concept defined over R _[n]_ that can be represented by a directed acyclic graph such as that of Figure 3.7, in which the input nodes are those at the bottom and in which each other node is labeled with a concept _c \u2208_ C. The output of the neural network for a given input vector ( _x_ 1 _, . . ., x_ _n_ ) is obtained as follows. First, each of the _n_ input nodes is labeled with the corresponding value _x_ _i_ _\u2208_ R. Next, the value at a node _u_ in the higher layer and labeled with _c_ is obtained by applying _c_ to the values of the input nodes admitting an **56** **Chapter 3** **Rademacher Complexity and VC-Dimension** **Figure 3.7** **Image:** [No caption returned] A neural network with one intermediate layer. edge ending in _u_ . Note that since _c_ takes values in _{_ 0 _,_ 1 _}_, the value at _u_ is in _{_ 0 _,_ 1 _}_ . The value at the top or output node is obtained similarly by applying the corresponding concept to the values of the nodes admitting an edge to the output node. (a) Let H denote the set of all neural networks defined as above with _k \u2265_ 2 internal nodes. Show that the growth function \u03a0 H ( _m_ ) can be upper bounded in terms of the product of the growth functions of the hypothesis sets defined at each intermediate layer. (b) Use that to upper bound the VC-dimension of the C-neural networks ( _Hint_ : you can use the implication _m_ = 2 _x_ log 2 ( _xy_ ) _\u21d2_ _m > x_ log 2 ( _ym_ ) valid for _m \u2265_ 1, and _x, y >_ 0 with _xy >_ 4). (c) Let C be the family of concept classes defined by threshold functions C = _{_ sgn( [\ufffd] _[r]_ _j_ =1 _[w]_ _[j]_ _[x]_ _[j]_ [):] **[ w]** _[ \u2208]_ [R] _[r]_ _[}]_ [. Give an upper bound on the VC-dimension of] H in terms of _k_ and _r_ . 3.28 VC-dimension of convex combinations. Let H be a family of functions mapping from an input space X to _{\u2212_ 1 _,_ +1 _}_ and let _T_ be a positive integer. Give an upper bound on the VC-dimension of the family of functions F _T_ defined by _T_ \ufffd _\u03b1_ _t_ _h_ _t_ \ufffd _t_ =1 \ufffd \ufffd F = sgn \ufffd : _h_ _t_ _\u2208_ H _, \u03b1_ _t_ _\u2265_ 0 _,_ _T_ \ufffd _\u03b1_ _t_ _\u2264_ 1 _t_ =1 _._ ( _Hint_ : you can use exercise 3.27 and its solution). 3.29 Infinite VC-dimension. **3.6** **Exercises** **57** (a) Show that if a",
    "chunk_id": "foundations_machine_learning_57"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "concept class C has infinite VC-dimension, then it is not PAC-learnable. (b) In the standard PAC-learning scenario, the learning algorithm receives all examples first and then computes its hypothesis. Within that setting, PAClearning of concept classes with infinite VC-dimension is not possible as seen in the previous question. Imagine now a different scenario where the learning algorithm can alternate between drawing more examples and computation. The objective of this problem is to prove that PAC-learning can then be possible for some concept classes with infinite VC-dimension. Consider for example the special case of the concept class C of all subsets of natural numbers. Professor Vitres has an idea for the first stage of a learning algorithm _L_ PAC-learning C. In the first stage, _L_ draws a sufficient number of points _m_ such that the probability of drawing a point beyond the maximum value _M_ observed be small with high confidence. Can you complete Professor Vitres\u2019 idea by describing the second stage of the algorithm so that it PAClearns C? The description should be augmented with the proof that _L_ can PAC-learn C. 3.30 VC-dimension generalization bound \u2013 realizable case. In this exercise we show that the bound given in corollary 3.19 can be improved to _O_ ( _[d]_ [ lo][g(] _m_ _[m][/][d]_ [)] ) in the realizable setting. Assume we are in the realizable scenario, i.e. the target concept is included in our hypothesis class H. We will show that if a hypothesis _h_ is consistent with a sample _S \u223c_ D _[m]_ then for any _\u03f5 >_ 0 such that _m\u03f5 \u2265_ 8 2 _em_ P[ _R_ ( _h_ ) _> \u03f5_ ] _\u2264_ 2 \ufffd _d_ _d_ 2 _[\u2212][m\u03f5/]_ [2] _._ (3.54) \ufffd (a) Let\ufffd H _S_ _\u2286_ H be the subset of hypotheses consistent with the sample _S_, let _R_ _S_ ( _h_ ) denote the empirical error with respect to the sample _S_ and define _S_ _[\u2032]_ as another independent sample drawn from D _[m]_ . Show that the following inequality holds for any _h_ 0 _\u2208_ H _S_ : P sup _|R_ [\ufffd] _S_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ _\u2032_ ( _h_ ) _| >_ _[\u03f5]_ \ufffd _h\u2208_ H _S_ 2 _\u2265_ P _B_ ( _m, \u03f5_ ) _>_ _[m\u03f5]_ \ufffd \ufffd 2 _\u2265_ P _B_ ( _m, \u03f5_ ) _>_ _[m\u03f5]_ \ufffd \ufffd 2 P[ _R_ ( _h_ 0 ) _> \u03f5_ ] _,_ \ufffd where _B_ ( _m, \u03f5_ ) is a binomial random variable with parameters ( _m, \u03f5_ ). ( _Hint_ : prove and use the fact that P[ _R_ [\ufffd] _S_ ( _h_ ) _\u2265_ _[\u03f5]_ []] _[ \u2265]_ [P][[] _[R]_ [ \ufffd] _[S]_ [(] _[h]_ [)] _[ >]_ _[\u03f5]_ _[\u2227]_ _[R]_ [(] _[h]_ [)] _[ > \u03f5]_ [].)] 2 _[\u03f5]_ []] _[ \u2265]_ [P][[] _[R]_ [ \ufffd] _[S]_ [(] _[h]_ [)] _[ >]_ 2 _[\u03f5]_ _[\u03f5]_ 2 _[\u2227]_ _[R]_ [(] _[h]_ [)] _[ > \u03f5]_ [].)] **58** **Chapter 3** **Rademacher Complexity and VC-Dimension** (b) Prove that P _B_ ( _m, \u03f5_ ) _>_ _[m\u03f5]_ 2 \ufffd 2 _\u2265_ 2",
    "chunk_id": "foundations_machine_learning_58"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[1] \ufffd P _B_ ( _m, \u03f5_ ) _>_ _[m\u03f5]_ 2 _\u2265_ 2 [. Use this inequality along with the result] from (a) to show that for any _h_ 0 _\u2208_ H _S_ P _R_ ( _h_ 0 ) _> \u03f5_ _\u2264_ 2 P sup _|R_ [\ufffd] _S_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ _\u2032_ ( _h_ ) _| >_ _[\u03f5]_ \ufffd \ufffd \ufffd _h\u2208_ H _S_ 2 _._ \ufffd (c) Instead of drawing two samples, we can draw one sample _T_ of size 2 _m_ then uniformly at random split it into _S_ and _S_ _[\u2032]_ . The right hand side of part (b) can then be rewritten as: P sup _|R_ [\ufffd] _S_ ( _h_ ) _\u2212R_ [\ufffd] _S_ _\u2032_ ( _h_ ) _| >_ _[\u03f5]_ \ufffd _h\u2208_ H _S_ 2 = P \ufffd _T \u223c_ D [2] _[m]_ : _T \u2192_ [ _S,S_ _[\u2032]_ ] _\u2203h_ _\u2208_ H : _R_ [\ufffd] _S_ ( _h_ ) = 0 _\u2227_ _R_ [\ufffd] _S_ _\u2032_ ( _h_ ) _>_ _[\u03f5]_ \ufffd 2 2 _._ \ufffd Let _h_ 0 be a hypothesis such that _R_ [\ufffd] _T_ ( _h_ 0 ) _>_ _[\u03f5]_ _[\u03f5]_ _[m\u03f5]_ 2 [and let] _[ l >]_ 2 Let _h_ 0 be a hypothesis such that _R_ _T_ ( _h_ 0 ) _>_ 2 _[\u03f5]_ [and let] _[ l >]_ _[m\u03f5]_ 2 be the total number of errors _h_ 0 makes on _T_ . Show that the probability of all _l_ errors falling into _S_ _[\u2032]_ is upper bounded by 2 _[\u2212][l]_ . (d) Part (b) implies that for any _h \u2208_ H \ufffd \ufffd _R_ _S_ ( _h_ ) = 0 _\u2227_ _R_ _S_ _\u2032_ ( _h_ ) _>_ _[\u03f5]_ \ufffd 2 P _T \u223c_ D [2] _[m]_ : _T \u2192_ ( _S,S_ _[\u2032]_ ) 2 _\u03f5_ _R_ _T_ ( _h_ 0 ) _>_ \ufffd\ufffd\ufffd\ufffd 2 _\u2264_ 2 _[\u2212][l]_ _._ \ufffd Use this bound to show that for any _h \u2208_ H \ufffd \ufffd _R_ _S_ ( _h_ ) = 0 _\u2227_ _R_ _S_ _\u2032_ ( _h_ ) _>_ _[\u03f5]_ \ufffd 2 P _T \u223c_ D [2] _[m]_ : _T \u2192_ ( _S,S_ _[\u2032]_ ) 2 _\u2264_ 2 _[\u2212]_ _[\u03f5m]_ 2 _._ \ufffd (e) Complete the proof of inequality (3.54) by using the union bound to up \ufffd _\u03f5_ per bound P _T \u223c_ D 2 _m_ : _\u2203h \u2208_ H : _R_ [\ufffd] _S_ ( _h_ ) = 0 _\u2227_ _R_ _S_ _\u2032_ ( _h_ ) _>_ 2 . Show that \ufffd \ufffd _T \u2192_ ( _S,S_ _[\u2032]_ ) \ufffd _\u03f5_ _\u2203h \u2208_ H : _R_ [\ufffd] _S_ ( _h_ ) = 0 _\u2227_ _R_ _S_ _\u2032_ ( _h_ ) _>_ 2 . Show that \ufffd \ufffd we can achieve a high probability generalization bound that is of the order _O_ ( _[d]_ [ lo][g(] _[m][/][d]_ [)] ). _m_ _[m]_ ). 3.31 Generalization bound based on covering numbers. Let H be a family of functions mapping X to a subset of real numbers Y _\u2286_ R. For any _\u03f5 >_ 0, the _covering_ _number N_ (H",
    "chunk_id": "foundations_machine_learning_59"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_, \u03f5_ ) of H for the _L_ _\u221e_ norm is the minimal _k \u2208_ N such that H can be covered with _k_ balls of radius _\u03f5_, that is, there exists _{h_ 1 _, . . ., h_ _k_ _} \u2286_ H such that, for all _h \u2208_ H, there exists _i \u2264_ _k_ with _\u2225h \u2212_ _h_ _i_ _\u2225_ _\u221e_ = max _x\u2208_ X _|h_ ( _x_ ) _\u2212_ _h_ _i_ ( _x_ ) _| \u2264_ _\u03f5_ . In particular, when H is a compact set, a finite covering can be extracted from a covering of H with balls of radius _\u03f5_ and thus _N_ (H _, \u03f5_ ) is finite. Covering numbers provide a measure of the complexity of a class of functions: the larger the covering number, the richer is the family of functions. The objective of this problem is to illustrate this by proving a learning bound in the case of the squared loss. Let D denote a distribution over X _\u00d7_ Y according to which **3.6** **Exercises** **59** labeled examples are drawn. Then, the generalization error of _h \u2208_ H for the squared loss is defined by _R_ ( _h_ ) = E ( _x,y_ ) _\u223c_ D [( _h_ ( _x_ ) _\u2212y_ ) [2] ] and its empirical error for a labeled sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) by _R_ [\ufffd] _S_ ( _h_ ) = _m_ [1] \ufffd _mi_ =1 [(] _[h]_ [(] _[x]_ _[i]_ [)] _[ \u2212]_ _[y]_ _[i]_ [)] [2] [.] We will assume that H is bounded, that is there exists _M >_ 0 such that _|h_ ( _x_ ) _\u2212_ _y| \u2264_ _M_ for all ( _x, y_ ) _\u2208_ X _\u00d7_ Y. The following is the generalization bound proven in this problem: _\u2212m\u03f5_ 2 2 exp \ufffd \ufffd 2 _M_ [4] P _S\u223c_ D _[m]_ _\u03f5_ sup _|R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _| \u2265_ _\u03f5_ _\u2264N_ H _,_ \ufffd _h\u2208_ H \ufffd \ufffd 8 _M_ _._ (3.55) \ufffd The proof is based on the following steps. (a) Let _L_ _S_ = _R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ), then show that for all _h_ 1 _, h_ 2 _\u2208_ H and any labeled sample _S_, the following inequality holds: _|L_ _S_ ( _h_ 1 ) _\u2212_ _L_ _S_ ( _h_ 2 ) _| \u2264_ 4 _M_ _\u2225h_ 1 _\u2212_ _h_ 2 _\u2225_ _\u221e_ _._ (b) Assume that H can be covered by _k_ subsets B 1 _, . . .,_ B _k_, that is H = B 1 _\u222a_ _. . . \u222a_ B _k_ . Then, show that, for any _\u03f5 >_ 0, the following upper bound holds: _k_ \ufffd _S\u223c_ P D _[m]_ _i_ =1 sup _|L_ _S_ ( _h_ ) _| \u2265_ _\u03f5_ _._ \ufffd _h\u2208_ B _i_ \ufffd P _S\u223c_ D _[m]_ sup _|L_ _S_ ( _h_ ) _| \u2265_ _\u03f5_ _\u2264_ \ufffd _h\u2208_ H \ufffd _\u03f5_ (c) Finally,",
    "chunk_id": "foundations_machine_learning_60"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "let _k_ = _N_ (H _,_ 8 _M_ [) and let][ B] [1] _[, . . .,]_ [ B] _[k]_ [ be balls of radius] _[ \u03f5/]_ [(8] _[M]_ [)] centered at _h_ 1 _, . . ., h_ _k_ covering H. Use part (a) to show that for all _i \u2208_ [ _k_ ], \ufffd _|L_ _S_ ( _h_ _i_ ) _| \u2265_ 2 _[\u03f5]_ P _S\u223c_ D _[m]_ sup _|L_ _S_ ( _h_ ) _| \u2265_ _\u03f5_ _\u2264_ P \ufffd _h\u2208_ B _i_ \ufffd _S\u223c_ D _[m]_ _,_ \ufffd and apply Hoeffding\u2019s inequality (theorem D.2) to prove (3.55). # 4 Model Selection A key problem in the design of learning algorithms is the choice of the hypothesis set H. This is known as the _model selection_ problem. How should the hypothesis set H be chosen? A rich or complex enough hypothesis set could contain the ideal Bayes classifier. On the other hand, learning with such a complex family becomes a very difficult task. More generally, the choice of H is subject to a trade-off that can be analyzed in terms of the _estimation_ and _approximation errors_ . Our discussion will focus on the particular case of binary classification but much of what is discussed can be straightforwardly extended to different tasks and loss functions. **4.1** **Estimation and approximation errors** Let H be a family of functions mapping X to _{\u2212_ 1 _,_ +1 _}_ . The _excess error_ of a hypothesis _h_ chosen from H, that is the difference between its error _R_ ( _h_ ) and the Bayes error _R_ _[\u2217]_, can be decomposed as follows: _R_ ( _h_ ) _\u2212_ _R_ _[\u2217]_ = _R_ ( _h_ ) _\u2212_ inf \ufffd _h\u2208_ H _[R]_ [(] _[h]_ [)] \ufffd \ufffd ~~\ufffd~~ \ufffd ~~\ufffd~~ estimation + inf \ufffd _h\u2208_ H _[R]_ [(] _[h]_ [)] _[ \u2212]_ _[R]_ _[\u2217]_ [\ufffd] \ufffd ~~\ufffd~~ \ufffd ~~\ufffd~~ approximation _._ (4.1) The first term is called the _estimation error_, the second term the _approximation_ _error_ . The estimation error depends on the hypothesis _h_ selected. It measures the error of _h_ with respect to the infimum of the errors achieved by hypotheses in H, or that of the best-in-class hypothesis _h_ _[\u2217]_ when that infimum is reached. Note that the definition of agnostic PAC-learning is precisely based on the estimation error. The approximation error measures how well the Bayes error can be approximated using H. It is a property of the hypothesis set H, a measure of its richness. For a more complex or richer hypothesis H, the approximation error tends to be smaller at the price of a larger estimation error. This is illustrated by Figure 4.1. **62** **Chapter 4** **Model Selection** **Image:** [No caption returned] Bayes **Figure 4.1** Illustration of the estimation error (in green) and approximation error (in orange). Here, it is assumed that there exists a best-in-class hypothesis, that is _h_ _[\u2217]_ such that _R_ ( _h_ _[\u2217]_ ) = inf _h\u2208_ H _R_ ( _h_ ). Model selection consists of choosing H with a favorable trade-off between the approximation and estimation",
    "chunk_id": "foundations_machine_learning_61"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "errors. Note, however, that the approximation error is not accessible, since in general the underlying distribution D needed to determine _R_ _[\u2217]_ is not known. Even with various noise assumptions, estimating the approximation error is difficult. In contrast, the _estimation error of an algorithm A_, that is, the estimation error of the hypothesis _h_ _S_ returned after training on a sample _S_, can sometimes be bounded using generalization bounds as shown in the next section. **4.2** **Empirical risk minimization (ERM)** A standard algorithm for which the estimation error can be bounded is _Empiri-_ _cal Risk Minimization_ (ERM). ERM seeks to minimize the error on the training sample: [4] \ufffd _h_ [ERM] _S_ = argmin _R_ _S_ ( _h_ ) _._ (4.2) _h\u2208_ H **Proposition 4.1** _For any sample S, the following inequality holds for the hypothesis_ _returned by ERM:_ P _R_ ( _h_ [ERM] _S_ ) _\u2212_ inf _\u2264_ P sup _|R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _| >_ _[\u03f5]_ \ufffd _h\u2208_ H _[R]_ [(] _[h]_ [)] _[ > \u03f5]_ \ufffd \ufffd _h\u2208_ H 2 _._ (4.3) \ufffd Proof: By definition of inf _h\u2208_ H _R_ ( _h_ ), for any _\u03f5 >_ 0, there exists _h_ _\u03f5_ such that _R_ ( _h_ _\u03f5_ ) _\u2264_ inf _h\u2208_ H _R_ ( _h_ ) + _\u03f5_ . Thus, using _R_ [\ufffd] _S_ ( _h_ [ERM] _S_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ _\u03f5_ ), which holds by the 4 Note that, if there exists multiple hypotheses with minimal error on the training sample, then ERM returns an arbitrary one. **4.2** **Empirical risk minimization (ERM)** **63** increasing _\u03b3_ **Image:** [No caption returned] **Figure 4.2** Illustration of the decomposition of a rich family H = [\ufffd] _\u03b3\u2208_ \u0393 [H] _[\u03b3]_ [.] definition of the algorithm, we can write _R_ ( _h_ [ERM] _S_ ) _\u2212_ inf _S_ ) _\u2212_ _R_ ( _h_ _\u03f5_ ) + _R_ ( _h_ _\u03f5_ ) _\u2212_ inf _h\u2208_ H _[R]_ [(] _[h]_ [) =] _[ R]_ [(] _[h]_ [ERM] _h\u2208_ H _[R]_ [(] _[h]_ [)] _\u2264_ _R_ ( _h_ [ERM] _S_ ) _\u2212_ _R_ ( _h_ _\u03f5_ ) + _\u03f5_ = _R_ ( _h_ [ERM] _S_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ [ERM] _S_ ) + _R_ [\ufffd] _S_ ( _h_ [ERM] _S_ ) _\u2212_ _R_ ( _h_ _\u03f5_ ) + _\u03f5_ _\u2264_ _R_ ( _h_ [ERM] _S_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ [ERM] _S_ ) + _R_ [\ufffd] _S_ ( _h_ _\u03f5_ ) _\u2212_ _R_ ( _h_ _\u03f5_ ) + _\u03f5_ _\u2264_ 2 sup _|R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _|_ + _\u03f5._ _h\u2208_ H Since the inequality holds for all _\u03f5 >_ 0, it implies the following: _R_ ( _h_ [ERM] _S_ ) _\u2212_ inf _|R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _|,_ _h\u2208_ H _[R]_ [(] _[h]_ [)] _[ \u2264]_ [2 sup] _h\u2208_ H which concludes the proof. The right-hand side of (4.3) can be upper-bounded using the generalization bounds presented in the previous chapter in terms of the",
    "chunk_id": "foundations_machine_learning_62"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Rademacher complexity, the growth function, or the VC-dimension of H. In particular, it can be bounded by 2 _e_ _[\u2212]_ [2] _[m]_ [[] _[\u03f5][\u2212]_ [R] _[m]_ [(][H][)]] [2] . Thus, when H admits a favorable Rademacher complexity, for example a finite VC-dimension, for a sufficiently large sample, with high probability, the estimation error is guaranteed to be small. Nevertheless, the performance of ERM is typically very poor. This is because the algorithm disregards the complexity of the hypothesis set H: in practice, either H is not complex enough, in which case the approximation error can be very large, or H is very rich, in which case the bound on the estimation error becomes very loose. Additionally, in many cases, determining the ERM solution is computationally intractable. For example, finding **64** **Chapter 4** **Model Selection** |Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8| |---|---|---|---|---|---|---|---| ||||||||| ||||||||| ||||||||| ||||||estimat|ion|| ||||||approxi<br>upper b|mation<br>ound|| ||||||||| ||||||||| ||\u03b3\u2217||||||| _\u03b3_ **Figure 4.3** Choice of _\u03b3_ _[\u2217]_ with the most favorable trade-off between estimation and approximation errors. a linear hypothesis with the smallest error on the training sample is NP-hard, as a function of the dimension of the space. **4.3** **Structural risk minimization (SRM)** In the previous section, we showed that the estimation error can be sometimes bounded or estimated. But, since the approximation error cannot be estimated, how should we choose H? One way to proceed is to choose a very complex family H with no approximation error or a very small one. H may be too rich for generalization bounds to hold for H, but suppose we can decompose H as a union of increasingly complex hypothesis sets H _\u03b3_, that is H = [\ufffd] _\u03b3\u2208_ \u0393 [H] _[\u03b3]_ [, with the complexity of][ H] _[\u03b3]_ increasing with _\u03b3_, for some set \u0393. Figure 4.2 illustrates this decomposition. The problem then consists of selecting the parameter _\u03b3_ _[\u2217]_ _\u2208_ \u0393 and thus the hypothesis set H _\u03b3_ _\u2217_ with the most favorable trade-off between estimation and approximation errors. Since these quantities are not known, instead, as illustrated by Figure 4.3, a uniform upper bound on their sum, the excess error (also called excess risk), can be used. This is precisely the idea behind the _Structural Risk Minimization_ (SRM) method. For SRM, H is assumed to be decomposable into a countable set, thus, we will write its decomposition as H = [\ufffd] _k\u2265_ 1 [H] _[k]_ [. Additionally, the hypothesis sets][ H] _[k]_ [ are] assumed to be nested: H _k_ _\u2282_ H _k_ +1 for all _k \u2265_ 1. However, many of the results presented in this section also hold for non-nested hypothesis sets. Thus, we will not make use of that assumption, unless explicitly specified. SRM consists of choosing the index _k_ _[\u2217]_ _\u2265_ 1 and the ERM hypothesis _h_ in H _k_ _\u2217_ that minimize an upper bound on the excess error. **4.3** **Structural risk minimization (SRM)** **65** |Col1|Col2|Col3|Col4|Col5|Col6| |---|---|---|---|---|---| ||||||| ||||||| ||||||n bound| ||||gen<br>pen|eralizatio<br>alty term|n bound| ||||em|pirical erro|r| ||||||| ||||||| _k_ **Figure 4.4** Illustration of structural risk minimization. The plots of three errors are shown as a function of the index _k_",
    "chunk_id": "foundations_machine_learning_63"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ". Clearly, as _k_, or equivalently the complexity the hypothesis set H _k_, increases, the training error decreases, while the penalty term increases. SRM selects the hypothesis minimizing a bound on the generalization error, which is a sum of the empirical error and the penalty term. As we shall see, the following learning bound holds for all _h \u2208_ H: for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_ over the draw of a sample _S_ of size _m_ from D _[m]_, for all _h \u2208_ H _k_ and _k \u2265_ 1, \ufffd log [2] _\u03b4_ 2 _m_ _[.]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + R _m_ (H _k_ ( _h_ ) ) + \ufffd log _k_ + _m_ Thus, to minimize the resulting bound on the excess error ( _R_ ( _h_ ) _\u2212_ _R_ _[\u2217]_ ), the index _k_ and the hypothesis _h \u2208_ H _k_ should be chosen to minimize the following objective function: ~~\ufffd~~ _F_ _k_ ( _h_ ) = _R_ [\ufffd] _S_ ( _h_ ) + R _m_ (H _k_ ) + log _k_ _m_ _[.]_ This is precisely the definition of the SRM solution _h_ [SRM] _S_ : \ufffd _h_ [SRM] _S_ = argmin _F_ _k_ ( _h_ ) = argmin _R_ _S_ ( _h_ ) + _R_ _m_ (H _k_ ) + _k\u2265_ 1 _,h\u2208_ H _k_ _k\u2265_ 1 _,h\u2208_ H _k_ \ufffd log _k_ (4.4) _m_ _[.]_ Thus, SRM identifies an optimal index _k_ _[\u2217]_ and therefore hypothesis set H _k_ _\u2217_, and returns the ERM solution based on that hypothesis set. Figure 4.4 further illustrates the selection of the index _k_ _[\u2217]_ and hypothesis set H _k_ _\u2217_ by SRM by minimizing an upper bound on the sum of the training error and the penalty term _R_ _m_ (H _k_ )+ \ufffdlog _k/m_ . The following theorem shows that the SRM solution benefits from a strong learning guarantee. For any _h \u2208_ H, we will denote by H _k_ ( _h_ ) the least complex hypothesis set among the H _k_ s that contain _h_ . **66** **Chapter 4** **Model Selection** **Theorem 4.2 (SRM Learning guarantee)** _For any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4_ _over the draw of an i.i.d. sample S of size m from_ D _[m]_ _, the generalization error of_ _the hypothesis h_ [SRM] _S_ _returned by the SRM method is bounded as follows:_ \ufffd _R_ ( _h_ [SRM] _S_ ) _\u2264_ inf _h\u2208_ H _R_ ( _h_ ) + 2R _m_ (H _k_ ( _h_ ) ) + \ufffd log _k_ ( _h_ ) \ufffd _m_ + \ufffd 2 log [3] _\u03b4_ _._ _m_ Proof: Observe first that, by the union bound, the following general inequality holds: P sup _R_ ( _h_ ) _\u2212_ _F_ _k_ ( _h_ ) ( _h_ ) _> \u03f5_ \ufffd _h\u2208_ H \ufffd = P sup sup _R_ ( _h_ ) _\u2212_ _F_ _k_ ( _h_ ) _> \u03f5_ \ufffd _k\u2265_ 1 _h\u2208_ H _k_ \ufffd _\u221e_ \ufffd P sup",
    "chunk_id": "foundations_machine_learning_64"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_R_ ( _h_ ) _\u2212_ _F_ _k_ ( _h_ ) _> \u03f5_ _k_ =1 \ufffd _h\u2208_ H _k_ \ufffd _\u2264_ = _\u2264_ _\u2264_ _\u221e_ \ufffd P sup _R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _\u2212_ R _m_ (H _k_ ) _> \u03f5_ + _k_ =1 \ufffd _h\u2208_ H _k_ ~~\ufffd~~ (4.5) log _k_ _m_ \ufffd _\u221e_ \ufffd exp \ufffd _\u2212_ 2 _m_ \ufffd _\u03f5_ + ~~\ufffd~~ _k_ =1 2 log _k_ _m_ \ufffd \ufffd _\u221e_ \ufffd _e_ _[\u2212]_ [2] _[m\u03f5]_ [2] _e_ _[\u2212]_ [2 log] _[ k]_ _k_ =1 _\u221e_ = _e_ _[\u2212]_ [2] _[m\u03f5]_ [2] \ufffd _k_ =1 1 _[\u03c0]_ [2] _k_ [2] [ =] 6 _[e]_ _[\u2212]_ [2] _[m\u03f5]_ [2] _[ \u2264]_ [2] _[e]_ _[\u2212]_ [2] _[m\u03f5]_ [2] _[.]_ Next, for any two random variables _X_ 1 and _X_ 2, if _X_ 1 + _X_ 2 _> \u03f5_, then either _X_ 1 or _X_ 2 must be larger than _\u03f5/_ 2. In view of that, by the union bound, P[ _X_ 1 + _X_ 2 _> \u03f5_ ] _\u2264_ P[ _X_ 1 _>_ 2 _[\u03f5]_ [] +][ P][[] _[X]_ [2] _[ >]_ 2 _[\u03f5]_ []. Using this inequality, inequality (4.5), and the inequality] _F_ _k_ ( _h_ SRM _S_ ) [(] _[h]_ _S_ [SRM] ) _\u2264_ _F_ _k_ ( _h_ ) ( _h_ ), which holds for all _h \u2208_ H, by definition of _h_ [SRM] _S_, we **4.3** **Structural risk minimization (SRM)** **67** can write, for any _h \u2208_ H, P \ufffd _R_ ( _h_ [SRM] _S_ ) _\u2212_ _R_ ( _h_ ) _\u2212_ 2R _m_ (H _k_ ( _h_ ) ) _\u2212_ \ufffd log _k_ ( _h_ ) _> \u03f5_ _m_ \ufffd _\u2264_ P \ufffd _R_ ( _h_ [SRM] _S_ ) _\u2212_ _F_ _k_ ( _h_ SRM _S_ ) [(] _[h]_ _S_ [SRM] ) _>_ 2 _[\u03f5]_ \ufffd + P \ufffd _F_ _k_ ( _h_ SRM _S_ ) [(] _[h]_ _S_ [SRM] ) _\u2212_ _R_ ( _h_ ) _\u2212_ 2R _m_ (H _k_ ( _h_ ) ) _\u2212_ \ufffd 2 log _k_ ( _h_ ) _\u2264_ 2 _e_ _[\u2212]_ _[m\u03f5]_ 2 [2] + P \ufffd _F_ _k_ ( _h_ ) ( _h_ ) _\u2212_ _R_ ( _h_ ) _\u2212_ 2R _m_ (H _k_ ( _h_ ) ) _\u2212_ \ufffd [2] 2 + P \ufffd _F_ _k_ ( _h_ ) ( _h_ ) _\u2212_ _R_ ( _h_ ) _\u2212_ 2R _m_ (H _k_ ( _h_ ) ) _\u2212_ \ufffd _k_ ( _h_ ) _>_ _[\u03f5]_ _m_ 2 _k_ ( _h_ ) _>_ _[\u03f5]_ _m_ 2 log _k_ ( _h_ ) \ufffd \ufffd = 2 _e_ _[\u2212]_ _[m\u03f5]_ 2 [2] = 2 _e_ _[\u2212]_ _[m\u03f5]_ 2 [2] [2] 2 = 3 _e_ _[\u2212]_ _[m\u03f5]_ 2 [2] [2] \ufffd 2 + P \ufffd _R_ _S_ ( _h_ ) _\u2212_ _R_ ( _h_ ) _\u2212_ R _m_ (H _k_ ( _h_ ) ) _>_ 2 _[\u03f5]_ 2 \ufffd [2] 2 + _e_ _[\u2212]_ _[m\u03f5]_ 2 [2] 2 _._ Setting the right-hand side to be equal to _\u03b4_ completes the proof. The learning guarantee just proven for SRM is",
    "chunk_id": "foundations_machine_learning_65"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "remarkable. To simplify its discussion, let us assume that there exists _h_ _[\u2217]_ such that _R_ ( _h_ _[\u2217]_ ) = inf _h\u2208_ H _R_ ( _h_ ), that is, that there exists a best-in-class classifier _h_ _[\u2217]_ _\u2208_ H. Then, the theorem implies in particular that, with probability at least 1 _\u2212_ _\u03b4_, the following inequality holds for all _h \u2208_ H: ~~\ufffd~~ 2 log [3] _\u03b4_ _._ (4.6) _m_ _R_ ( _h_ [SRM] _S_ ) _\u2264_ _R_ ( _h_ _[\u2217]_ ) + 2R _m_ (H _k_ ( _h_ _\u2217_ ) ) + ~~\ufffd~~ log _k_ ( _h_ _[\u2217]_ ) + _m_ Observe that, remarkably, this bound is similar to the estimation error bound for H _k_ ( _h_ _\u2217_ ) : it differs from it only by the term \ufffdlog _k_ ( _h_ _[\u2217]_ ) _/m_ . Thus, modulo that term, the guarantee for SRM is as favorable as the one we would have obtained, had an oracle informed us of the index _k_ ( _h_ _[\u2217]_ ) of the best-in-class classifier\u2019s hypothesis set. Furthermore, observe that when H is rich enough that _R_ ( _h_ _[\u2217]_ ) is close to the Bayes error, the learning bound (4.6) is approximately a bound on the excess error of the SRM solution. Note that, if for some _k_ 0, the empirical error of the ERM solution for H _k_ 0 is zero, which holds in particular if H _k_ 0 contains the Bayes error, then, we have min _h\u2208_ H _k_ _F_ _k_ 0 ( _h_ ) _\u2264_ min _h\u2208_ H _k_ _F_ _k_ ( _h_ ) for all _k > k_ 0 and only finitely many indices need to be considered in SRM. Assume more generally that if min _h\u2208_ H _k_ _F_ _k_ ( _h_ ) _\u2264_ min _h\u2208_ H _k_ +1 _F_ _k_ ( _h_ ) for some _k_, then indices beyond _k_ + 1 need not be inspected. This property may hold for example if the empirical error cannot be further improved after some index _k_ . In that case, the minimizing index _k_ _[\u2217]_ can be determined via a binary search in the interval [1 _, k_ max ], given some maximum value _k_ max . _k_ max itself can be found by inspecting min _h\u2208_ H 2 _n_ _F_ _k_ ( _h_ ) for exponentially growing indices 2 _[n]_, _n \u2265_ 1, and setting _k_ max = 2 _[n]_ for _n_ such that min _h\u2208_ H 2 _n_ _F_ _k_ ( _h_ ) _\u2264_ min _h\u2208_ H 2 _n_ +1 _F_ _k_ ( _h_ ). The number of ERM computations needed to find _k_ max is in _O_ ( _n_ ) = _O_ (log _k_ max ) and similarly the **68** **Chapter 4** **Model Selection** number of ERM computations due to the binary search is in _O_ (log _k_ max ). Thus, if _n_ is the smallest integer such that _k_ _[\u2217]_ _<_ 2 _[n]_, the overall number of ERM computations is in _O_ (log _k_ _[\u2217]_ ). While it benefits from a very favorable guarantee,",
    "chunk_id": "foundations_machine_learning_66"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "SRM admits several drawbacks. First, the decomposability of H into countably many hypothesis sets, each with a converging Rademacher complexity, remains a strong assumption. As an example, the family of all measurable functions cannot be written as a union of countably many hypothesis sets with finite VC-dimension. Thus, the choice of H or that of the hypothesis sets H _k_ is a key component of SRM. Second, and this is the main disadvantage of SRM, the method is typically computationally intractable: for most hypothesis sets, finding the solution of ERM is NP-hard and in general SRM requires determining that solution for a large number of indices _k_ . **4.4** **Cross-validation** An alternative method for model selection, _cross-validation_, consists of using some fraction of the training sample as a _validation set_ to select a hypothesis set H _k_ . This is in contrast with the SRM model which relies on a theoretical learning bound assigning a penalty to each hypothesis set. In this section, we analyze the crossvalidation method and compare its performance to that of SRM. As in the previous section, let (H _k_ ) _k\u2265_ 1 be a countable sequence of hypothesis sets with increasing complexities. The cross-validation (CV) solution is obtained as follows. Let _S_ be an i.i.d. labeled sample of size _m_ . _S_ is divided into a sample _S_ 1 of size (1 _\u2212_ _\u03b1_ ) _m_ and a sample _S_ 2 of size _\u03b1m_, with _\u03b1 \u2208_ (0 _,_ 1) typically chosen to be relatively small. _S_ 1 is reserved for training, _S_ 2 for validation. For any _k \u2208_ N, let _h_ [ERM] _S_ 1 _,k_ [denote the solution of ERM run on] _[ S]_ [1] [ using the hypothesis set][ H] _[k]_ [. The] hypothesis _h_ [CV] _S_ returned by cross-validation is the ERM solution _h_ [ERM] _S_ 1 _,k_ [with the] best performance on _S_ 2 : _h_ [CV] _S_ = argmin (4.7) _h\u2208_ \ufffd _h_ [ERM] _S_ 1 _,k_ [:] _[ k][\u2265]_ [1] \ufffd _[R]_ [\ufffd] _[S]_ [2] [(] _[h]_ [)] _[.]_ The following general result will help us derive learning guarantees for cross-validation. **Proposition 4.3** _For any \u03b1 >_ 0 _and any sample size m \u2265_ 1 _, the following general_ _inequality holds:_ \ufffd \ufffd log _k_ _\u03b1m_ P \ufffd sup _k\u2265_ 1 \ufffd\ufffd\ufffd _R_ ( _h_ ERM _S_ 1 _,k_ [)] _[ \u2212]_ _[R]_ [\ufffd] _[S]_ 2 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] \ufffd\ufffd\ufffd _> \u03f5_ + _\u2264_ 4 _e_ _[\u2212]_ [2] _[\u03b1m\u03f5]_ [2] _._ **4.4** **Cross-validation** **69** Proof: By the union bound, we can write \ufffd \ufffd log _k_ _\u03b1m_ P \ufffd sup _k\u2265_ 1 \ufffd\ufffd\ufffd _R_ ( _h_ ERM _S_ 1 _,k_ [)] _[ \u2212]_ _[R]_ [\ufffd] _[S]_ 2 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] \ufffd\ufffd\ufffd _> \u03f5_ + \ufffd _R_ ( _h_ ERM _S_ 1 _,k_ [)] _[ \u2212]_ _[R]_ [\ufffd] _[S]_ 2 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] \ufffd\ufffd\ufffd _> \u03f5_ + \ufffd\ufffd\ufffd\ufffd log _k_ \ufffd _\u03b1m_ _\u2264_ = _\u221e_ \ufffd P _k_ =1 _\u221e_ \ufffd E _k_ =1 P (4.8) \ufffd \ufffd\ufffd _S_",
    "chunk_id": "foundations_machine_learning_67"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "1 \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd _R_ ( _h_ ERM _S_ 1 _,k_ [)] _[ \u2212]_ _[R]_ [\ufffd] _[S]_ 2 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] \ufffd\ufffd\ufffd _> \u03f5_ + \ufffd log _k_ _\u03b1m_ _._ The hypothesis _h_ [ERM] _S_ 1 _,k_ is fixed conditioned on _S_ 1 . Furthermore, the sample _S_ 2 is independent from _S_ 1 . Therefore, by Hoeffding\u2019s inequality, we can bound the conditional probability as follows: _S_ 1 \ufffd\ufffd\ufffd\ufffd \ufffd 2 _\u2264_ 2 _e_ _[\u2212]_ [2] _[\u03b1m]_ \ufffd _\u03f5_ + _[\u221a]_ [lo] _\u03b1m_ [g] _[ k]_ \ufffd _._ P _R_ ( _h_ ERM _S_ 1 _,k_ [)] _[ \u2212]_ _[R]_ [\ufffd] _[S]_ 2 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] \ufffd\ufffd\ufffd _> \u03f5_ + \ufffd\ufffd\ufffd\ufffd ~~\ufffd~~ log _k_ _\u03b1m_ _\u2264_ 2 _e_ _[\u2212]_ [2] _[\u03b1m\u03f5]_ [2] _[\u2212]_ [2 log] _[ k]_ = [2] _k_ [2] _[ e]_ _[\u2212]_ [2] _[\u03b1m\u03f5]_ [2] _[.]_ Plugging in the right-hand side of this bound in (4.8) and summing over _k_ yields \ufffd _\u2264_ _[\u03c0]_ [2] 3 _[e]_ _[\u2212]_ [2] _[\u03b1m\u03f5]_ [2] _[ <]_ [ 4] _[e]_ _[\u2212]_ [2] _[\u03b1m\u03f5]_ [2] _[,]_ \ufffd log _k_ _\u03b1m_ P sup \ufffd _k\u2265_ 1 \ufffd\ufffd\ufffd _R_ ( _h_ ERM _S_ 1 _,k_ [)] _[ \u2212]_ _[R]_ [\ufffd] _[S]_ 2 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] \ufffd\ufffd\ufffd _> \u03f5_ + which completes the proof. Let _R_ ( _h_ [SRM] _S_ 1 ) be the generalization error of the SRM solution using a sample _S_ 1 of size (1 _\u2212_ _\u03b1m_ ) and _R_ ( _h_ [CV] _S_ _[, S]_ [) the generalization error of the cross-validation] solution using a sample _S_ of size _m_ . Then, using Proposition 4.3, the following learning guarantee can be derived which compares the error of the CV method to that of SRM. **Theorem 4.4 (Cross-validation versus SRM)** _For any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, the following holds:_ _R_ ( _h_ [CV] _S_ [)] _[ \u2212]_ _[R]_ [(] _[h]_ [SRM] _S_ 1 ) _\u2264_ 2 \ufffd log max( _k_ ( _h_ [CV] _S_ [)] _[, k]_ [(] _[h]_ [SRM] _S_ 1 )) + 2 _\u03b1m_ ~~\ufffd~~ log [4] _\u03b4_ 2 _\u03b1m_ _[,]_ _where, for any h, k_ ( _h_ ) _denotes the smallest index of a hypothesis set containing h._ Proof: By Proposition 4.3 and Theorem 4.2, using the property of _h_ [CV] _S_ as a minimizer, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, the following inequalities **70** **Chapter 4** **Model Selection** hold: ~~\ufffd~~ log [4] _\u03b4_ 2 _\u03b1m_ _R_ ( _h_ [CV] _S_ [)] _[ \u2264]_ _[R]_ [\ufffd] _[S]_ 2 [(] _[h]_ [CV] _S_ [) +] log( _k_ ( _h_ [CV] _S_ [))] + \ufffd _\u03b1m_ _\u2264_ _R_ [\ufffd] _S_ 2 ( _h_ [SRM] _S_ 1 ) + \ufffd log( _k_ ( _h_ [CV] _S_ [))] + _\u03b1m_ \ufffd log [4] _\u03b4_ 2 _\u03b1m_ \ufffd \ufffd log( _k_ ( _h_ [SRM] _S_ 1 )) + 2 _\u03b1m_ log [4] _\u03b4_ 2 _\u03b1m_ _\u2264_ _R_ ( _h_ [SRM] _S_ 1 ) + log( _k_ ( _h_ [CV] _S_ [))] +",
    "chunk_id": "foundations_machine_learning_68"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd _\u03b1m_ ~~\ufffd~~ \ufffd log [4] _\u03b4_ 2 _\u03b1m_ _[,]_ _\u2264_ _R_ ( _h_ [SRM] _S_ 1 ) + 2 log(max( _k_ ( _h_ [CV] _S_ [)] _[, k]_ [(] _[h]_ [SRM] _S_ 1 )) + 2 _\u03b1m_ which completes the proof. The learning guarantee just proven shows that, with high probability, the generalization error of the CV solution for a sample of size _m_ is close to that of the SRM solution for a sample of size (1 _\u2212_ _\u03b1_ ) _m_ . For _\u03b1_ relatively small, this suggests a guarantee similar to that of SRM, which, as previously discussed, is very favorable. However, in some unfavorable regimes, an algorithm (here SRM) trained on (1 _\u2212_ _\u03b1_ ) _m_ points may have a significantly worse performance than when trained on _m_ points (avoiding this phase transition issue is one of the main motivations behind the use of the _n_ -fold cross-validation method in practice, see section 4.5). Thus, the bound suggests in fact a trade-off: _\u03b1_ should be chosen sufficiently small to avoid the unfavorable regimes just mentioned and yet sufficiently large for the right-hand side of the bound to be small and thus informative. The learning bound for CV can be made more explicit in some cases in practice. Assume for example that the hypothesis sets H _k_ are nested and that the empirical errors of the ERM solutions\ufffd _h_ [ERM] _S_ 1 _,k_ [are decreasing before reaching zero: for any] _[ k]_ [,] _R_ _S_ 1 ( _h_ [ERM] _S_ 1 _,k_ +1 [)] _[ <]_ [ \ufffd] _[R]_ _[S]_ 1 [(] _[h]_ [ERM] _S_ 1 _,k_ [) for all] _[ k]_ [ such that][ \ufffd] _[R]_ _[S]_ 1 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] _[ >]_ [ 0 and][ \ufffd] _[R]_ _[S]_ 1 [(] _[h]_ [ERM] _S_ 1 _,k_ +1 [)] _[ \u2264]_ \ufffd _R_ _S_ 1 ( _h_ [ERM] _S_ 1 _,k_ [) otherwise. Observe that][ \ufffd] _[R]_ _[S]_ 1 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] _[ >]_ [ 0 implies at least one error for] _h_ [ERM] _S_ 1 _,k_ [, therefore][ \ufffd] _[R]_ _[S]_ 1 [(] _[h]_ [ERM] _S_ 1 _,k_ [)] _[ >]_ _m_ [1] [. In view of that, we must then have][ \ufffd] _[R]_ _[S]_ [1] [(] _[h]_ _S_ [ERM] 1 _,n_ [) =] 0 for all _n \u2265_ _m_ + 1. Thus, we have _h_ [ERM] _S_ 1 _,n_ [=] _[ h]_ [ERM] _S_ 1 _,m_ +1 [for all] _[ n][ \u2265]_ _[m]_ [ + 1 and we] can assume that _k_ ( _f_ _CV_ ) _\u2264_ _m_ + 1. Since the complexity of H _k_ increases with _k_ we also have _k_ ( _f_ _SRM_ ) _\u2264_ _m_ + 1. In view of that, we obtain the following more explicit learning bound for cross-validation: _R_ ( _f_ _CV_ _, S_ ) _\u2212_ _R_ ( _f_ _SRM_ _, S_ 1 ) _\u2264_ 2 \ufffd log( [4] _\u03b4_ [)] 2 _\u03b1m_ [+ 2] \ufffd log( _m_ + 1) _._ _\u03b1m_ **4.5** _n_ **-Fold cross-validation** **71** **4.5** _**n**_ **-Fold cross-validation** In",
    "chunk_id": "foundations_machine_learning_69"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "practice, the amount of labeled data available is often too small to set aside a validation sample since that would leave an insufficient amount of training data. Instead, a widely adopted method known as _n-fold cross-validation_ is used to exploit the labeled data both for _model selection_ and for training. Let _**\u03b8**_ denote the vector of free parameters of the algorithm. For a fixed value of _**\u03b8**_, the method consists of first randomly partitioning a given sample _S_ of _m_ labeled examples into _n_ subsamples, or folds. The _i_ th fold is thus a labeled sample (( _x_ _i_ 1 _, y_ _i_ 1 ) _, . . .,_ ( _x_ _im_ _i_ _, y_ _im_ _i_ )) of size _m_ _i_ . Then, for any _i \u2208_ [ _n_ ], the learning algorithm is trained on all but the _i_ th fold to generate a hypothesis _h_ _i_, and the performance of _h_ _i_ is tested on the _i_ th fold, as illustrated in figure 4.5a. The parameter value _**\u03b8**_ is evaluated based on the average error of the hypotheses _h_ _i_, which is called the _cross-validation error_ . This quantity is denoted by _R_ [\ufffd] CV ( _**\u03b8**_ ) and defined by \ufffd _R_ CV ( _**\u03b8**_ ) = [1] _n_ _n_ \ufffd _i_ =1 1 _m_ _i_ _m_ _i_ \ufffd _L_ ( _h_ _i_ ( _x_ _ij_ ) _, y_ _ij_ ) _j_ =1 _._ \ufffd ~~\ufffd~~ \ufffd ~~\ufffd~~ error of _h_ _i_ on the _i_ th fold The folds are generally chosen to have equal size, that is _m_ _i_ = _m/n_ for all _i \u2208_ [ _n_ ]. How should _n_ be chosen? The appropriate choice is subject to a trade-off. For a large _n_, each training sample used in _n_ -fold cross-validation has size _m \u2212_ _m/n_ = _m_ (1 _\u2212_ 1 _/n_ ) (illustrated by the right vertical red line in figure 4.5b), which is close to _m_, the size of the full sample, and also implies all training samples are quite similar. At the same time, the _i_ th fold used to measure the error is relatively small and thus the cross-validation error tends to have a small bias but a large variance. In contrast, smaller values of _n_ lead to more diverse training samples but their size (shown by the left vertical red line in figure 4.5b) is significantly less than _m_ . In this regime, the _i_ th fold is relatively large and thus the cross-validation error tends to have a smaller variance but a larger bias. In applications, _n_ is typically chosen to be 5 or 10. _n_ -fold cross-validation is used as follows in model selection. The full labeled data is first split into a training and a test sample. The training sample of size _m_ is then used to compute the _n_ fold cross-validation error _R_ [\ufffd] CV ( _**\u03b8**_ ) for a small number of possible values of _**\u03b8**_ . The free parameter _**\u03b8**_ is next set to the value _**\u03b8**_ 0 for which _R_ [\ufffd] CV (",
    "chunk_id": "foundations_machine_learning_70"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_**\u03b8**_ ) is smallest and the algorithm is trained with the parameter setting _**\u03b8**_ 0 over the full training sample of size _m_ . Its performance is evaluated on the test sample as already described in the previous section. The special case of _n_ -fold cross-validation where _n_ = _m_ is called _leave-one-out_ _cross-validation_, since at each iteration exactly one instance is left out of the train **72** **Chapter 4** **Model Selection** ~~train~~ ~~train~~ ~~train~~ ~~train~~ ~~test~~ ~~train~~ ~~train~~ ~~train~~ ~~test~~ ~~train~~ ... ... ... ~~test~~ **Image:** [No caption returned] ~~train~~ ~~train~~ ~~train~~ ~~train~~ (a) (b) **Figure 4.5** _n_ -fold cross-validation. (a) Illustration of the partitioning of the training data into 5 folds. (b) Typical plot of a classifier\u2019s prediction error as a function of the size of the training sample _m_ : the error decreases as a function of the number of training points. The red line on the left side marks the region for small values of _n_, while the red line on the right side marks the region for large values of _n_ . ing sample. As shown in chapter 5, the average leave-one-out error is an approximately unbiased estimate of the average error of an algorithm and can be used to derive simple guarantees for some algorithms. In general, the leave-one-out error is very costly to compute, since it requires training _m_ times on samples of size _m \u2212_ 1, but for some algorithms it admits a very efficient computation (see exercise 11.9). In addition to model selection, _n_ -fold cross-validation is also commonly used for performance evaluation. In that case, for a fixed parameter setting _**\u03b8**_, the full labeled sample is divided into _n_ random folds with no distinction between training and test samples. The performance reported is the _n_ -fold cross-validation error on the full sample as well as the standard deviation of the errors measured on each fold. **4.6** **Regularization-based algorithms** A broad family of algorithms inspired by the SRM method is that of _regularization-_ _based algorithm_ . This consists of selecting a very complex family H that is an uncountable union of nested hypothesis sets H _\u03b3_ : H = [\ufffd] _\u03b3>_ 0 [H] _[\u03b3]_ [.][ H][ is often chosen] to be dense in the space of continuous functions over X. For example, H may be chosen to be the set of all linear functions in some high-dimensional space and H _\u03b3_ the subset of those functions whose norm is bounded by _\u03b3_ : H _\u03b3_ = _{x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ): _\u2225_ **w** _\u2225\u2264_ _\u03b3}_ . For some choices of **\u03a6** and the high-dimensional space, it can be shown that H is indeed dense in the space of continuous functions over X. **4.7** **Convex surrogate losses** **73** Given a labeled sample _S_, the extension of the SRM method to an uncountable union would then suggest selecting _h_ based on the following optimization problem: \ufffd argmin _R_ _S_ ( _h_ ) + R _m_ (H _\u03b3_ ) + _\u03b3>_ 0 _,h\u2208H_ _\u03b3_ \ufffd log _\u03b3_ _m_ _[,]_ where other penalty terms",
    "chunk_id": "foundations_machine_learning_71"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "pen( _\u03b3, m_ ) can be chosen in lieu of the specific choice pen( _\u03b3, m_ ) = R _m_ (H _\u03b3_ ) + \ufffd lo _m_ g _\u03b3_ [. Often, there exists a function] _[ R]_ [:][ H] _[ \u2192]_ [R][ such] that, for any _\u03b3 >_ 0, the constrained optimization problem argmin _\u03b3>_ 0 _,h\u2208H_ _\u03b3_ _R_ [\ufffd] _S_ ( _h_ )+ pen( _\u03b3, m_ ) can be equivalently written as the unconstrained optimization problem \ufffd argmin _R_ _S_ ( _h_ ) + _\u03bbR_ ( _h_ ) _,_ _h\u2208_ H for some _\u03bb >_ 0. _R_ ( _h_ ) is called a _regularization term_ and _\u03bb >_ 0 is treated as a hyperparameter since its optimal value is often not known. For most algorithms, the regularization term _R_ ( _h_ ) is chosen to be an increasing function of _\u2225h\u2225_ for some choice of the norm _\u2225\u00b7 \u2225_, when H is the subset of a Hilbert space. The variable _\u03bb_ is often called a _regularization parameter_ . Larger values of _\u03bb_ further penalize more complex hypotheses, while, for _\u03bb_ close or equal to zero, the regularization term has no effect and the algorithm coincides with ERM. In practice, _\u03bb_ is typically selected via cross-validation or using _n_ -fold cross-validation. When the regularization term is chosen to be _\u2225h\u2225_ _p_ for some choice of the norm and _p \u2265_ 1, then it is a convex function of _h_, since any norm is convex. However, for the zero-one loss, the first term of the objective function is non-convex, thereby making the optimization problem computationally hard. In practice, most regularization-based algorithms instead use a convex upper bound on the zero-one loss and replace the empirical zero-one term with the empirical value of that convex surrogate. The resulting optimization problem is then convex and therefore admits more efficient solutions than SRM. The next section studies the properties of such convex surrogate losses. **4.7** **Convex surrogate losses** The guarantees for the estimation error that we presented in previous sections hold either for ERM or for SRM, which itself is defined in terms of ERM. However, as already mentioned, for many choices of the hypothesis set H, including that of linear functions, solving the ERM optimization problem is NP-hard mainly because the zero-one loss function is not convex. One common method for addressing this problem consists of using a convex surrogate loss function that upper bounds the zero-one loss. This section analyzes learning guarantees for such surrogate losses in terms of the original loss. **74** **Chapter 4** **Model Selection** The hypotheses we consider are real-valued functions _h_ : X _\u2192_ R. The sign of _h_ defines a binary classifier _f_ _h_ : X _\u2192{\u2212_ 1 _,_ +1 _}_ defined for all _x \u2208_ X by _\u0338_ _\u0338_ _\u0338_ _f_ _h_ ( _x_ ) = _\u0338_ _\u0338_ _\u0338_ +1 if _h_ ( _x_ ) _\u2265_ 0 \ufffd _\u2212_ 1 if _h_ ( _x_ ) _<_ 0 _._ _\u0338_ _\u0338_ _\u0338_ The loss or error of _h_ at point ( _x, y_ ) _\u2208_ X _\u00d7 {\u2212_ 1",
    "chunk_id": "foundations_machine_learning_72"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_,_ +1 _}_ is defined as the binary classification error of _f_ _h_ : 1 _f_ _h_ ( _x_ )= _\u0338_ _y_ = 1 _yh_ ( _x_ ) _<_ 0 + 1 _h_ ( _x_ )=0 _\u2227y_ = _\u2212_ 1 _\u2264_ 1 _yh_ ( _x_ ) _\u2264_ 0 _._ We will denote by _R_ ( _h_ ) the expected error of _h_ : _R_ ( _h_ ) = E ( _x,y_ ) _\u223c_ D \ufffd1 _f_ _h_ ( _x_ )= _\u0338_ _y_ \ufffd. For any _x \u2208_ X, let _\u03b7_ ( _x_ ) denote _\u03b7_ ( _x_ ) = P[ _y_ = +1 _|x_ ] and let D X denote the marginal distribution over X. Then, for any _h_, we can write _\u0338_ _\u0338_ _\u0338_ _R_ ( _h_ ) = ( _x,y_ E ) _\u223c_ D _\u0338_ _\u0338_ _\u0338_ \ufffd1 _f_ _h_ ( _x_ )= _\u0338_ _y_ \ufffd _\u0338_ _\u0338_ _\u0338_ = E _x\u223c_ D X = E _x\u223c_ D X _\u0338_ _\u0338_ _\u0338_ \ufffd _\u03b7_ ( _x_ )1 _h_ ( _x_ ) _<_ 0 + (1 _\u2212_ _\u03b7_ ( _x_ ))1 _h_ ( _x_ ) _>_ 0 + (1 _\u2212_ _\u03b7_ ( _x_ ))1 _h_ ( _x_ )=0 \ufffd \ufffd _\u03b7_ ( _x_ )1 _h_ ( _x_ ) _<_ 0 + (1 _\u2212_ _\u03b7_ ( _x_ ))1 _h_ ( _x_ ) _\u2265_ 0 \ufffd _._ _\u0338_ _\u0338_ _\u0338_ In view of that, the Bayes classifier can be defined as assigning label +1 to _x_ when _\u03b7_ ( _x_ ) _\u2265_ 2 [1] [,] _[ \u2212]_ [1 otherwise. It can therefore be induced by the function] _[ h]_ _[\u2217]_ [defined by] _\u0338_ _\u0338_ _\u0338_ _h_ _[\u2217]_ ( _x_ ) = _\u03b7_ ( _x_ ) _\u2212_ [1] 2 _[.]_ (4.9) _\u0338_ _\u0338_ _\u0338_ We will refer to _h_ _[\u2217]_ : X _\u2192_ R as the _Bayes scoring function_ and will denote by _R_ _[\u2217]_ the error of the Bayes classifier or Bayes scoring function: _R_ _[\u2217]_ = _R_ ( _h_ _[\u2217]_ ). **Lemma 4.5** _The excess error of any hypothesis h_ : X _\u2192_ R _can be expressed as follows_ _in terms of \u03b7 and the Bayes scoring function h_ _[\u2217]_ _:_ _\u0338_ _\u0338_ _\u0338_ _R_ ( _h_ ) _\u2212_ _R_ _[\u2217]_ = 2 E _x\u223c_ D X Proof: For any _h_, we can write _\u0338_ _\u0338_ _\u0338_ \ufffd _|h_ _[\u2217]_ ( _x_ ) _|_ 1 _h_ ( _x_ ) _h_ _\u2217_ ( _x_ ) _\u2264_ 0 \ufffd _._ _\u0338_ _\u0338_ _\u0338_ _R_ ( _h_ ) = E _x\u223c_ D X = E _x\u223c_ D X = E _x\u223c_ D X = E _x\u223c_ D X _\u0338_ _\u0338_ _\u0338_ \ufffd _\u03b7_ ( _x_ )1 _h_ ( _x_ ) _<_ 0 + (1 _\u2212_ _\u03b7_ ( _x_ ))1 _h_ ( _x_ ) _\u2265_ 0 \ufffd \ufffd _\u03b7_ ( _x_ )1 _h_ ( _x_ ) _<_ 0 + (1 _\u2212_ _\u03b7_ ( _x_ ))(1 _\u2212_ 1 _h_ ( _x_ ) _<_ 0 )\ufffd \ufffd[2 _\u03b7_ ( _x_ ) _\u2212_ 1]1 _h_ ( _x_ ) _<_ 0 + (1 _\u2212_ _\u03b7_ ( _x_ ))\ufffd \ufffd2 _h_",
    "chunk_id": "foundations_machine_learning_73"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u2217]_ ( _x_ )1 _h_ ( _x_ ) _<_ 0 + (1 _\u2212_ _\u03b7_ ( _x_ ))\ufffd _,_ **4.7** **Convex surrogate losses** **75** where we used for the last step equation (4.9). In view of that, for any _h_, the following holds: _\u0338_ _R_ ( _h_ ) _\u2212_ _R_ ( _h_ _[\u2217]_ ) = E _x\u223c_ D X = E _x\u223c_ D X _\u0338_ \ufffd2[ _h_ _[\u2217]_ ( _x_ )](1 _h_ ( _x_ ) _\u2264_ 0 _\u2212_ 1 _h_ _\u2217_ ( _x_ ) _\u2264_ 0 )\ufffd \ufffd2[ _h_ _[\u2217]_ ( _x_ )] sgn( _h_ _[\u2217]_ ( _x_ ))1 ( _h_ ( _x_ ) _h_ _\u2217_ ( _x_ ) _\u2264_ 0) _\u2227_ (( _h_ ( _x_ ) _,h_ _\u2217_ ( _x_ )) _\u0338_ =(0 _,_ 0)) \ufffd _\u0338_ = 2 E _x\u223c_ D X _\u0338_ \ufffd _|h_ _[\u2217]_ ( _x_ ) _|_ 1 _h_ ( _x_ ) _h_ _\u2217_ ( _x_ ) _\u2264_ 0 \ufffd _,_ _\u0338_ which completes the proof, since _R_ ( _h_ _[\u2217]_ ) = _R_ _[\u2217]_ . Let \u03a6: R _\u2192_ R be a convex and non-decreasing function so that for any _u \u2208_ R, 1 _u\u2264_ 0 _\u2264_ \u03a6( _\u2212u_ ). The \u03a6-loss of a function _h_ : X _\u2192_ R at point ( _x, y_ ) _\u2208_ X _\u00d7 {\u2212_ 1 _,_ +1 _}_ is defined as \u03a6( _\u2212yh_ ( _x_ )) and its expected loss given by _\u0338_ _L_ \u03a6 ( _h_ ) = E ( _x,y_ ) _\u223c_ D _\u0338_ \ufffd\u03a6( _\u2212yh_ ( _x_ ))\ufffd _\u0338_ = E _x\u223c_ D X _\u0338_ _\u03b7_ ( _x_ )\u03a6( _\u2212h_ ( _x_ )) + (1 _\u2212_ _\u03b7_ ( _x_ ))\u03a6( _h_ ( _x_ )) _._ (4.10) \ufffd \ufffd _\u0338_ Notice that since 1 _yh_ ( _x_ ) _\u2264_ 0 _\u2264_ \u03a6( _\u2212yh_ ( _x_ )), we have _R_ ( _h_ ) _\u2264L_ \u03a6 ( _h_ ). For any _x \u2208_ X, let _u \ufffd\u2192_ _L_ \u03a6 ( _x, u_ ) be the function defined for all _u \u2208_ R by _L_ \u03a6 ( _x, u_ ) = _\u03b7_ ( _x_ )\u03a6( _\u2212u_ ) + (1 _\u2212_ _\u03b7_ ( _x_ ))\u03a6( _u_ ) _._ Then, _L_ \u03a6 ( _h_ ) = E _x\u223c_ D X [ _L_ \u03a6 ( _x, h_ ( _x_ ))]. Since \u03a6 is convex, _u \ufffd\u2192_ _L_ \u03a6 ( _x, u_ ) is convex as a sum of two convex functions. Define _h_ _[\u2217]_ \u03a6 [:][ X] _[ \u2192]_ [[] _[\u2212\u221e][,]_ [ +] _[\u221e]_ [] as the] _[ Bayes solution]_ _for the loss function L_ \u03a6 . That is, for any _x_, _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) is a solution of the following] convex optimization problem: _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) =] argmin _L_ \u03a6 ( _x, u_ ) _u\u2208_ [ _\u2212\u221e,_ + _\u221e_ ] = argmin _\u03b7_ ( _x_ )\u03a6( _\u2212u_ ) + (1 _\u2212_ _\u03b7_ ( _x_ ))\u03a6( _u_ ) _._ _u\u2208_ [ _\u2212\u221e,_ + _\u221e_ ] The solution of this optimization is in general not unique. When _\u03b7_ ( _x_ ) = 0, _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) is",
    "chunk_id": "foundations_machine_learning_74"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "a] minimizer of _u \ufffd\u2192_ \u03a6( _u_ ) and since \u03a6 is non-decreasing, we can choose _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) =] _[ \u2212\u221e]_ [in] that case. Similarly, when _\u03b7_ ( _x_ ) = 1, we can choose _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) = +] _[\u221e]_ [. When] _[ \u03b7]_ [(] _[x]_ [) =] [1] 2 [,] _L_ \u03a6 ( _x, u_ ) = [1] 2 [[\u03a6(] _[\u2212][u]_ [) + \u03a6(] _[u]_ [)], thus, by convexity,] _[ L]_ [\u03a6] [(] _[x, u]_ [)] _[ \u2265]_ [\u03a6(] _[\u2212]_ _[u]_ 2 [+] _[u]_ 2 [) = \u03a6(0).] Thus, we can choose _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) = 0 in that case. For all other values of] _[ \u03b7]_ [(] _[x]_ [), in case of] non-uniqueness, an arbitrary minimizer is chosen in this definition. We will denote by _L_ _[\u2217]_ \u03a6 [the \u03a6-loss of] _[ h]_ _[\u2217]_ \u03a6 [:] _[ L]_ _[\u2217]_ \u03a6 [=][ E] [(] _[x,y]_ [)] _[\u223c]_ [D] \ufffd\u03a6( _\u2212yh_ _[\u2217]_ \u03a6 [(] _[x]_ [))] \ufffd. _\u0338_ 2 [[\u03a6(] _[\u2212][u]_ [) + \u03a6(] _[u]_ [)], thus, by convexity,] _[ L]_ [\u03a6] [(] _[x, u]_ [)] _[ \u2265]_ [\u03a6(] _[\u2212]_ _[u]_ 2 _\u0338_ _[u]_ _[u]_ 2 [+] 2 _\u0338_ **Proposition 4.6** _Let_ \u03a6 _be a convex and non-decreasing function that is differentiable_ _at_ 0 _with_ \u03a6 _[\u2032]_ (0) _>_ 0 _. Then, the minimizer of_ \u03a6 _defines the Bayes classifier: for_ _any x \u2208_ X _, h_ _[\u2217]_ \u03a6 [(] _[x]_ [)] _[ >]_ [ 0] _[ iff][ h]_ _[\u2217]_ [(] _[x]_ [)] _[ >]_ [ 0] _[ and][ h]_ _[\u2217]_ [(] _[x]_ [) = 0] _[ iff][ h]_ _[\u2217]_ \u03a6 [(] _[x]_ [) = 0] _[, which implies]_ _L_ _[\u2217]_ \u03a6 [=] _[ R]_ _[\u2217]_ _[.]_ **76** **Chapter 4** **Model Selection** Proof: Fix _x \u2208_ X. If _\u03b7_ ( _x_ ) = 0, then _h_ _[\u2217]_ ( _x_ ) = _\u2212_ [1] Proof: Fix _x \u2208_ X. If _\u03b7_ ( _x_ ) = 0, then _h_ _[\u2217]_ ( _x_ ) = _\u2212_ 2 [and] _[ h]_ \u03a6 _[\u2217]_ [(] _[x]_ [) =] _[ \u2212\u221e]_ [, thus] _[ h]_ _[\u2217]_ [(] _[x]_ [)] and _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) admit the same sign.] Similarly, if _\u03b7_ ( _x_ ) = 1, then _h_ _[\u2217]_ ( _x_ ) = + [1] 2 [and] _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) = +] _[\u221e]_ [, and] _[ h]_ _[\u2217]_ [(] _[x]_ [) and] _[ h]_ _[\u2217]_ \u03a6 [(] _[x]_ [) admit the same sign.] Let _u_ _[\u2217]_ denote the minimizer defining _h_ _[\u2217]_ \u03a6 [(] _[x]_ [).] _[ u]_ _[\u2217]_ [is a minimizer of] _[ u][ \ufffd\u2192]_ _[L]_ [\u03a6] [(] _[x, u]_ [)] iff the subdifferential of that function at _u_ _[\u2217]_ contains 0, that is, since _\u2202L_ \u03a6 ( _x, u_ _[\u2217]_ ) = _\u2212\u03b7_ ( _x_ ) _\u2202_ \u03a6( _\u2212u_ _[\u2217]_ ) + (1 _\u2212_ _\u03b7_ ( _x_ )) _\u2202_ \u03a6( _u_ _[\u2217]_ ), iff there exist _v_ 1 _\u2208_ _\u2202_ \u03a6( _\u2212u_ _[\u2217]_ ) and _v_ 2 _\u2208_ _\u2202_ \u03a6( _u_ _[\u2217]_ ) such that _\u03b7_ ( _x_ ) _v_",
    "chunk_id": "foundations_machine_learning_75"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "1 = (1 _\u2212_ _\u03b7_ ( _x_ )) _v_ 2 _._ (4.11) If _u_ _[\u2217]_ = 0, by the differentiability of \u03a6 at 0 we have _v_ 1 = _v_ 2 = \u03a6 _[\u2032]_ (0) _>_ 0 and thus _\u03b7_ ( _x_ ) = [1] 2 [, that is] _[ h]_ _[\u2217]_ [(] _[x]_ [) = 0. Conversely, If] _[ h]_ _[\u2217]_ [(] _[x]_ [) = 0, that is] _[ \u03b7]_ [(] _[x]_ [) =] [1] 2 [, then, by] definition, we have _h_ _[\u2217]_ \u03a6 [(] _[x]_ [) = 0. Thus,] _[ h]_ _[\u2217]_ [(] _[x]_ [) = 0 iff] _[ h]_ _[\u2217]_ \u03a6 [(] _[x]_ [) = 0 iff] _[ \u03b7]_ [(] _[x]_ [) =] [1] 2 [.] We can assume now that _\u03b7_ ( _x_ ) is not in _{_ 0 _,_ 1 _,_ [1] 2 _[}]_ [. We first show that for any] _u_ 1 _, u_ 2 _\u2208_ R with _u_ 1 _< u_ 2, and any two choices of the subgradients at _u_ 1 and _u_ 2, _v_ 1 _\u2208_ _\u2202_ \u03a6( _u_ 1 ) and _v_ 2 _\u2208_ _\u2202_ \u03a6( _u_ 2 ), we have _v_ 1 _\u2264_ _v_ 2 . By definition of the subgradients at _u_ 1 and _u_ 2, the following inequalities hold: [1] [1] 2 [, that is] _[ h]_ _[\u2217]_ [(] _[x]_ [) = 0. Conversely, If] _[ h]_ _[\u2217]_ [(] _[x]_ [) = 0, that is] _[ \u03b7]_ [(] _[x]_ [) =] 2 \u03a6 \u03a6 2 [.] We can assume now that _\u03b7_ ( _x_ ) is not in _{_ 0 _,_ 1 _,_ [1] _[}]_ \u03a6( _u_ 2 ) _\u2212_ \u03a6( _u_ 1 ) _\u2265_ _v_ 1 ( _u_ 2 _\u2212_ _u_ 1 ) \u03a6( _u_ 1 ) _\u2212_ \u03a6( _u_ 2 ) _\u2265_ _v_ 2 ( _u_ 1 _\u2212_ _u_ 2 ) _._ Summing up these inequalities yields _v_ 2 ( _u_ 2 _\u2212_ _u_ 1 ) _\u2265_ _v_ 1 ( _u_ 2 _\u2212_ _u_ 1 ) and thus _v_ 2 _\u2265_ _v_ 1, since _u_ 1 _< u_ 2 . Now, if _u_ _[\u2217]_ _>_ 0, then we have _\u2212u_ _[\u2217]_ _< u_ _[\u2217]_ . By the property shown above, this implies _v_ 1 _\u2264_ _v_ 2 . We cannot have _v_ 1 = _v_ 2 _\u0338_ = 0 since (4.11) would then imply _\u03b7_ ( _x_ ) = [1] 2 [. We also cannot have] _[ v]_ [1] [ =] _[ v]_ [2] [ = 0 since by the property shown above, we] must have \u03a6 _[\u2032]_ (0) _\u2264_ _v_ 2 and thus _v_ 2 _>_ 0. Thus, we must have _v_ 1 _< v_ 2 with _v_ 2 _>_ 0, which, by (4.11), implies _\u03b7_ ( _x_ ) _>_ 1 _\u2212_ _\u03b7_ ( _x_ ), that is _h_ _[\u2217]_ ( _x_ ) _>_ 0. Conversely, if _h_ _[\u2217]_ ( _x_ ) _>_ 0 then _\u03b7_ ( _x_ ) _>_ 1 _\u2212_ _\u03b7_ ( _x_ ). We cannot have _v_ 1 = _v_ 2 = 0 or _v_ 1 = _v_ 2 _\u0338_ = 0",
    "chunk_id": "foundations_machine_learning_76"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "as already shown. Thus, since _\u03b7_ ( _x_ ) _\u0338_ = 1, by (4.11), this implies _v_ 1 _< v_ 2 . We cannot have _u_ _[\u2217]_ _< \u2212u_ _[\u2217]_ since, by the property shown above, this would imply _v_ 2 _\u2264_ _v_ 1 . Thus, we must have _\u2212u_ _[\u2217]_ _\u2264_ _u_ _[\u2217]_, that is _u_ _[\u2217]_ _\u2265_ 0, and more specifically _u_ _[\u2217]_ _>_ 0 since, as already shown above, _u_ _[\u2217]_ = 0 implies _h_ _[\u2217]_ ( _x_ ) = 0. **Theorem 4.7** _Let_ \u03a6 _be a convex and non-decreasing function._ _Assume that there_ _exists s \u2265_ 1 _and c >_ 0 _such that the following holds for all x \u2208_ X _:_ _|h_ _[\u2217]_ ( _x_ ) _|_ _[s]_ = \ufffd\ufffd _\u03b7_ ( _x_ ) _\u2212_ 12 \ufffd\ufffd _s_ _\u2264_ _c_ _s_ \ufffd _L_ \u03a6 ( _x,_ 0) _\u2212_ _L_ \u03a6 ( _x, h_ _[\u2217]_ \u03a6 [(] _[x]_ [))] \ufffd _._ _Then, for any hypothesis h, the excess error of h is bounded as follows:_ _s_ _R_ ( _h_ ) _\u2212_ _R_ _[\u2217]_ _\u2264_ 2 _c_ \ufffd _L_ \u03a6 ( _h_ ) _\u2212L_ _[\u2217]_ \u03a6 \ufffd [1] **4.8** **Chapter notes** **77** Proof: We will use the following inequality which holds by the convexity of \u03a6: \u03a6\ufffd _\u2212_ 2 _h_ _[\u2217]_ ( _x_ ) _h_ ( _x_ )\ufffd = \u03a6\ufffd(1 _\u2212_ 2 _\u03b7_ ( _x_ )) _h_ ( _x_ )\ufffd = \u03a6\ufffd _\u03b7_ ( _x_ )( _\u2212h_ ( _x_ )) + (1 _\u2212_ _\u03b7_ ( _x_ )) _h_ ( _x_ )\ufffd _\u2264_ _\u03b7_ ( _x_ )\u03a6(( _\u2212h_ ( _x_ ))) + (1 _\u2212_ _\u03b7_ ( _x_ ))\u03a6( _h_ ( _x_ )) = _L_ \u03a6 ( _x, h_ ( _x_ )) _._ (4.12) By Lemma 4.5, Jensen\u2019s inequality, and _h_ _[\u2217]_ ( _x_ ) = _\u03b7_ ( _x_ ) _\u2212_ [1] 2 [, we can write] _R_ ( _h_ ) _\u2212_ _R_ ( _h_ _[\u2217]_ ) = E _x\u223c_ D X _\u2264_ E _x\u223c_ D X \ufffd _|_ 2 _\u03b7_ ( _x_ ) _\u2212_ 1 _|_ 1 _h_ ( _x_ ) _h_ _\u2217_ ( _x_ ) _\u2264_ 0 \ufffd _s_ \ufffd _|_ 2 _\u03b7_ ( _x_ ) _\u2212_ 1 _|_ _[s]_ 1 _h_ ( _x_ ) _h_ _\u2217_ ( _x_ ) _\u2264_ 0 \ufffd [1] (Jensen\u2019s ineq.) _\u2264_ 2 _c_ E _x\u223c_ D X _\u2264_ 2 _c_ E _x\u223c_ D X _\u2264_ 2 _c_ E _x\u223c_ D X _\u2264_ 2 _c_ E _x\u223c_ D X _s_ \ufffd\ufffd\u03a6(0) _\u2212_ _L_ \u03a6 ( _x, h_ _[\u2217]_ \u03a6 [(] _[x]_ [))] \ufffd 1 _h_ ( _x_ ) _h_ _\u2217_ ( _x_ ) _\u2264_ 0 \ufffd [1] (assumption) _s_ \ufffd\ufffd\u03a6\ufffd _\u2212_ 2 _h_ _[\u2217]_ ( _x_ ) _h_ ( _x_ )\ufffd _\u2212_ _L_ \u03a6 ( _x, h_ _[\u2217]_ \u03a6 [(] _[x]_ [))] \ufffd 1 _h_ ( _x_ ) _h_ _\u2217_ ( _x_ ) _\u2264_ 0 \ufffd [1] (\u03a6 non-decreasing) \ufffd[ _L_ \u03a6 ( _x, h_ ( _x_ )) _\u2212_ _L_ \u03a6 ( _x, h_ _[\u2217]_ \u03a6 [(] _[x]_ [))] 1] _h_ ( _x_ ) _h_ _[\u2217]_ ( _x_ ) _\u2264_ 0 \ufffd [1] _s_",
    "chunk_id": "foundations_machine_learning_77"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(convexity ineq. (4.12)) _L_ \u03a6 ( _x, h_ ( _x_ )) _\u2212_ _L_ \u03a6 ( _x, h_ _[\u2217]_ \u03a6 [(] _[x]_ [))] _s_ _,_ \ufffd \ufffd [1] which completes the proof, since E _x\u223c_ D X [ _L_ \u03a6 ( _x, h_ _[\u2217]_ \u03a6 [(] _[x]_ [))] =] _[ L]_ _[\u2217]_ \u03a6 [.] The theorem shows that, when the assumption holds, the excess error of _h_ can be upper bounded in terms of the excess \u03a6-loss. The assumption of the theorem holds in particular for the following convex loss functions: _\u2022_ Hinge loss, where \u03a6( _u_ ) = max(0 _,_ 1 + _u_ ), with _s_ = 1 and _c_ = [1] 2 [.] _\u2022_ Exponential loss, where \u03a6( _u_ ) = exp( _u_ ), with _s_ = 2 and _c_ = ~~_\u221a_~~ 12 [.] _\u2022_ Logistic loss, where \u03a6( _u_ ) = log 2 (1 + _e_ _[u]_ ), with _s_ = 2 and _c_ = ~~_\u221a_~~ 12 [.] They also hold for the square loss and the squared Hinge loss (see Exercises 4.2 and 4.3). **4.8** **Chapter notes** The structural risk minimization (SRM) technique is due to Vapnik [1998]. The original penalty term used by Vapnik [1998] is based on the VC-dimension of the hypothesis set. The version of SRM with Rademacher complexity-based penalties that we present here leads to finer data-dependent learning guarantees. Penalties based on alternative complexity measures can be used similarly leading to learning bounds in terms of the corresponding complexity measure [Bartlett et al., 2002a]. **78** **Chapter 4** **Model Selection** An alternative model selection theory of _Voted Risk Minimization_ (VRM) has been recently developed by Cortes, Mohri, and Syed [2014] and other related publications [Kuznetsov et al., 2014, DeSalvo et al., 2015, Cortes et al., 2015]. Theorem 4.7 is due to Zhang [2003a]. The proof given here is somewhat different and simpler. **4.9** **Exercises** 4.1 For any hypothesis set H, show that the following inequalities hold: \ufffd _R_ \ufffd _h_ [ERM] _S_ \ufffd [\ufffd] _._ (4.13) E _S\u223c_ D _[m]_ \ufffd _R_ \ufffd _S_ \ufffd _h_ [ERM] _S_ \ufffd [\ufffd] _\u2264_ _h_ inf _\u2208_ H _[R]_ [(] _[h]_ [)] _[ \u2264]_ _S\u223c_ E D _[m]_ 4.2 Show that for the squared loss, \u03a6( _u_ ) = (1 + _u_ ) [2], the statement of Theorem 4.7 holds with _s_ = 2 and _c_ = [1] holds with _s_ = 2 and _c_ = 2 [and therefore that the excess error can be upper] bounded as follows: 2 _R_ ( _h_ ) _\u2212_ _R_ _[\u2217]_ _\u2264_ \ufffd _L_ \u03a6 ( _h_ ) _\u2212L_ _[\u2217]_ \u03a6 \ufffd [1] _._ 2 _._ 4.3 Show that for the squared Hinge loss, \u03a6( _u_ ) = max(0 _,_ 1 + _u_ ) [2], the statement of Theorem 4.7 holds with _s_ = 2 and _c_ = [1] 2 [and therefore that the excess error can] be upper bounded as follows: 2 _R_ ( _h_ ) _\u2212_ _R_ _[\u2217]_ _\u2264_ \ufffd _L_ \u03a6 ( _h_ ) _\u2212L_ _[\u2217]_ \u03a6 \ufffd [1] _._ 4.4 In this problem, the loss of _h_ : X",
    "chunk_id": "foundations_machine_learning_78"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2192_ R at point ( _x, y_ ) _\u2208_ X _\u00d7 {\u2212_ 1 _,_ +1 _}_ is defined to be 1 _yh_ ( _x_ ) _\u2264_ 0 . (a) Define the Bayes classifier and a Bayes scoring function _h_ _[\u2217]_ for this loss. (b) Express the excess error of _h_ in terms of _h_ _[\u2217]_ (counterpart of Lemma 4.5, for loss considered here). (c) Give a counterpart of the result of Theorem 4.7 for this loss. 4.5 Same questions as in Exercise 4.5 with the loss of _h_ : X _\u2192_ R at point ( _x, y_ ) _\u2208_ X _\u00d7 {\u2212_ 1 _,_ +1 _}_ defined instead to be 1 _yh_ ( _x_ ) _<_ 0 . # 5 Support Vector Machines This chapter presents one of the most theoretically well motivated and practically most effective classification algorithms in modern machine learning: Support Vector Machines (SVMs). We first introduce the algorithm for separable datasets, then present its general version designed for non-separable datasets, and finally provide a theoretical foundation for SVMs based on the notion of margin. We start with the description of the problem of linear classification. **5.1** **Linear classification** Consider an input space X that is a subset of R _[N]_ with _N \u2265_ 1, and the output or target space Y = _{\u2212_ 1 _,_ +1 _}_, and let _f_ : X _\u2192_ Y be the target function. Given a hypothesis set H of functions mapping X to Y, the binary classification task is formulated as follows. The learner receives a training sample _S_ of size _m_ drawn i.i.d. from X according to some unknown distribution D, _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_, with _y_ _i_ = _f_ ( _x_ _i_ ) for all _i \u2208_ [ _m_ ]. The problem consists of determining a hypothesis _h \u2208_ H, a _binary classifier_, with small generalization error: _R_ D ( _h_ ) = P (5.1) _x\u223c_ D [[] _[h]_ [(] _[x]_ [)] _[ \u0338]_ [=] _[ f]_ [(] _[x]_ [)]] _[.]_ Different hypothesis sets H can be selected for this task. In view of the results presented in chapter 3, which formalized Occam\u2019s razor principle, hypothesis sets with smaller complexity \u2014 e.g., smaller VC-dimension or Rademacher complexity \u2014 provide better learning guarantees, everything else being equal. A natural hypothesis set with relatively small complexity is that of _linear classifiers_, or hyperplanes, which can be defined as follows: H = _{_ **x** _\ufffd\u2192_ sign( **w** _\u00b7_ **x** + _b_ ): **w** _\u2208_ R _[N]_ _, b \u2208_ R _}._ (5.2) The learning problem is then referred to as a _linear classification problem_ . The general equation of a hyperplane in R _[N]_ is **w** _\u00b7_ **x** + _b_ = 0, where **w** _\u2208_ R _[N]_ is a **80** **Chapter 5** **Support Vector Machines** **w** _\u00b7_ **x** + _b_ =0 **w** _\u00b7_ **x** + _b_ =0 **Figure 5.1** **Image:** [No caption returned] **Image:** [No caption returned] Two possible separating hyperplanes. The right-hand",
    "chunk_id": "foundations_machine_learning_79"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "side figure shows a hyperplane that maximizes the margin. non-zero vector normal to the hyperplane and _b \u2208_ R a scalar. A hypothesis of the form **x** _\ufffd_ ~~_\u2192_~~ s ~~i~~ gn ~~(~~ **w** _\u00b7_ **x** ~~+~~ ~~_b_~~ ~~)~~ ~~th~~ us ~~l~~ a ~~b~~ e ~~l~~ s pos ~~iti~~ ve ~~l~~ y a ~~ll~~ po ~~i~~ n ~~t~~ s ~~f~~ a ~~lli~~ ng on one s ~~id~~ e o ~~f~~ ~~th~~ e hyperplane **w** _\u00b7_ **x** + _b_ = 0 and negatively all others. **5.2** **Separable case** In this section, we assume that the training sample _S_ can be linearly separated, that is, we assume the existence of a hyperplane that perfectly separates the training sample into two populations of positively and negatively labeled points, as illustrated by the left panel of figure 5.1. This is equivalent to the existence of ( **w** _, b_ ) _\u2208_ (R _[N]_ _\u2212{_ **0** _}_ ) _\u00d7_ R such that _\u2200i \u2208_ [ _m_ ] _,_ _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2265_ 0 _._ (5.3) But, as can be seen from figure 5.1, there are then infinitely many such separating hyperplanes. Which hyperplane should a learning algorithm select? The definition of the SVM solution is based on the notion of _geometric margin_ . **Definition 5.1 (Geometric margin)** _The_ geometric margin _\u03c1_ _h_ ( **x** ) _of a linear classifier_ _h_ : **x** _\ufffd\u2192_ **w** _\u00b7_ **x** + _b at a point_ **x** _is its Euclidean distance to the hyperplane_ **w** _\u00b7_ **x** + _b_ = 0 _:_ _\u03c1_ _h_ ( _x_ ) = _[|]_ **[w]** _[ \u00b7]_ **[ x]** [ +] _[ b][|]_ _._ (5.4) _\u2225_ **w** _\u2225_ 2 _The_ geometric margin _\u03c1_ _h_ _of a linear classifier h for a sample S_ = ( **x** 1 _, . . .,_ **x** _m_ ) _is_ _the minimum geometric margin over the points in the sample, \u03c1_ _h_ = min _i\u2208_ [ _m_ ] _\u03c1_ _h_ ( _x_ _i_ ) _,_ _that is the distance of the hyperplane defining h to the closest sample points._ The SVM solution is the separating hyperplane with the maximum geometric margin and is thus known as the _maximum-margin hyperplane_ . The right panel of figure 5.1 illustrates the maximum-margin hyperplane returned by the SVM **5.2** **Separable case** **81** **w** _\u00b7_ **x** =0 **Image:** [No caption returned] **w** _\u00b7_ **x** + _b_ =0 **w** **x** _k_ **w** _k_ **Figure 5.2** An illustration of the geometric margin of a point **x** in the case **w** _\u00b7_ **x** _>_ 0 and _b >_ 0. algorithm in the separable case. We will present later in this chapter a theory that provides a strong justification for this solution. We can observe already, however, that the SVM solution can also be viewed as the \u201csafest\u201d choice in the following sense: a test point is classified correctly by a separating hyperplane with geometric margin _\u03c1_ even when it falls within a distance _\u03c1_ of the training samples sharing the same label; for the SVM solution, _\u03c1_ is the maximum geometric margin and",
    "chunk_id": "foundations_machine_learning_80"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "thus the \u201csafest\u201d value. **5.2.1** **Primal optimization problem** We now derive the equations and optimization problem that define the SVM solution. By definition of the geometric margin (see also figure 5.2), the maximum margin _\u03c1_ of a separating hyperplane is given by **x** _i_ + _b|_ _y_ _i_ \ufffd **w** _\u00b7_ **x** _i_ + _b_ \ufffd _\u2225_ **w** _\u2225_ = max **w** _,b_ _i_ [min] _\u2208_ [ _m_ ] _\u2225_ **w** _\u2225_ _|_ **w** _\u00b7_ **x** _i_ + _b|_ _\u03c1_ = max **w** _,b_ : _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2265_ 0 _i_ [min] _\u2208_ [ _m_ ] _\u2225_ **w** _\u2225_ _._ (5.5) _\u2225_ **w** _\u2225_ The second equality follows from the fact that, since the sample is linearly separable, for the maximizing pair ( **w** _, b_ ), _y_ _i_ \ufffd **w** _\u00b7_ **x** _i_ + _b_ \ufffd must be non-negative for all _i \u2208_ [ _m_ ]. Now, observe that the last expression is invariant to multiplication of ( **w** _, b_ ) by a positive scalar. Thus, we can restrict ourselves to pairs ( **w** _, b_ ) scaled such that min _i\u2208_ [ _m_ ] _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) = 1: _\u03c1_ = max **w** _,b_ : min _i\u2208_ [ _m_ ] _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ )=1 1 max _\u2225_ **w** _\u2225_ [=] **w** _,b_ : _\u2200i\u2208_ [ _m_ ] _,y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2265_ 1 1 (5.6) _\u2225_ **w** _\u2225_ _[.]_ The second equality results from the fact that for the maximizing pair ( **w** _, b_ ), the minimum of _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) is 1. Figure 5.3 illustrates the solution ( **w** _, b_ ) of the maximization (5.6). In addition to the maximum-margin hyperplane, it also shows the _marginal hyperplanes_, which are **82** **Chapter 5** **Support Vector Machines** **w** _\u00b7_ **x** + _b_ =0 margin **w** _\u00b7_ **x** + _b_ =+1 **Figure 5.3** **Image:** [No caption returned] Maximum-margin hyperplane solution of (5.6). The marginal hyperplanes are represented by dashed lines on the figure. the hyperplanes parallel to the separating hyperplane and passing through the closest points on the negative or positive sides. Since they are parallel to the separating hyperplane, they admit the same normal vector **w** . Furthermore, since _|_ **w** _\u00b7_ **x** + _b|_ = 1 for the closest points, the equations of the marginal hyperplanes are **w** _\u00b7_ **x** + _b_ = _\u00b1_ 1. Since maximizing 1 _/\u2225_ **w** _\u2225_ is equivalent to minimizing [1] 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [, in view of (5.6), the] pair ( **w** _, b_ ) returned by SVM in the separable case is the solution of the following convex optimization problem: 1 min (5.7) **w** _,b_ 2 _[\u2225]_ **[w]** _[\u2225]_ [2] subject to: _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2265_ 1 _, \u2200i \u2208_ [ _m_ ] _._ The objective function _F_ : **w** _\ufffd\u2192_ [1] 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [",
    "chunk_id": "foundations_machine_learning_81"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "is infinitely differentiable. Its gradient is] _\u2207F_ ( **w** ) = **w** and its Hessian is the identity matrix _\u2207_ [2] _F_ ( **w** ) = **I**, whose eigenvalues are strictly positive. Therefore, _\u2207_ [2] _F_ ( **w** ) _\u227b_ **0** and _F_ is strictly convex. The constraints are all defined by affine functions _g_ _i_ : ( **w** _, b_ ) _\ufffd\u2192_ 1 _\u2212_ _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) and are therefore qualified. Thus, in view of the results known for convex optimization (see appendix B for details), the optimization problem of (5.7) admits a unique solution, an important and favorable property that does not hold for all learning algorithms. Moreover, since the objective function is quadratic and the constraints are affine, the optimization problem of (5.7) is in fact a specific instance of _quadratic program-_ _ming_ (QP), a family of problems extensively studied in optimization. A variety of commercial and open-source solvers are available for solving convex QP problems. Additionally, motivated by the empirical success of SVMs along with its rich theoretical underpinnings, specialized methods have been developed to more efficiently solve this particular convex QP problem, notably the block coordinate descent algorithms with blocks of just two coordinates. **5.2** **Separable case** **83** **5.2.2** **Support vectors** Returning to the optimization problem (5.7), we note that the constraints are affine and thus qualified. The objective function as well as the affine constraints are convex and differentiable. Thus, the requirements of theorem B.30 hold and the KKT conditions apply at the optimum. We shall use these conditions to both analyze the algorithm and demonstrate several of its crucial properties, and subsequently derive the dual optimization problem associated to SVMs in section 5.2.3. We introduce Lagrange variables _\u03b1_ _i_ _\u2265_ 0, _i \u2208_ [ _m_ ], associated to the _m_ constraints and denote by _**\u03b1**_ the vector ( _\u03b1_ 1 _, . . ., \u03b1_ _m_ ) _[\u22a4]_ . The Lagrangian can then be defined for all **w** _\u2208_ R _[N]_, _b \u2208_ R, and _**\u03b1**_ _\u2208_ R _[m]_ + [, by] _L_ ( **w** _, b,_ _**\u03b1**_ ) = [1] 2 _[\u2225]_ **[w]** _[\u2225]_ [2] _[ \u2212]_ _m_ \ufffd _\u03b1_ _i_ [ _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2212_ 1] _._ (5.8) _i_ =1 The KKT conditions are obtained by setting the gradient of the Lagrangian with respect to the primal variables **w** and _b_ to zero and by writing the complementarity conditions: _m_ _\u2207_ **w** _L_ = **w** _\u2212_ \ufffd _\u03b1_ _i_ _y_ _i_ **x** _i_ = 0 = _\u21d2_ **w** = _i_ =1 _m_ \ufffd _\u03b1_ _i_ _y_ _i_ **x** _i_ (5.9) _i_ =1 _m_ \ufffd _\u03b1_ _i_ _y_ _i_ = 0 (5.10) _i_ =1 _\u2207_ _b_ _L_ = _\u2212_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ = 0 = _\u21d2_ _i_ =1 _\u2200i, \u03b1_ _i_ [ _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2212_ 1] = 0 = _\u21d2_ _\u03b1_ _i_ = 0 _\u2228_ _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) = 1 _._ (5.11)",
    "chunk_id": "foundations_machine_learning_82"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "By equation (5.9), the weight vector **w** at the solution of the SVM problem is a linear combination of the training set vectors **x** 1 _, . . .,_ **x** _m_ . A vector **x** _i_ appears in that expansion iff _\u03b1_ _i_ _\u0338_ = 0. Such vectors are called _support vectors_ . By the complementarity conditions (5.11), if _\u03b1_ _i_ _\u0338_ = 0, then _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) = 1. Thus, support vectors lie on the marginal hyperplanes **w** _\u00b7_ **x** _i_ + _b_ = _\u00b1_ 1. Support vectors fully define the maximum-margin hyperplane or SVM solution, which justifies the name of the algorithm. By definition, vectors not lying on the marginal hyperplanes do not affect the definition of these hyperplanes \u2014 in their absence, the solution to the SVM problem remains unchanged. Note that while the solution **w** of the SVM problem is unique, the support vectors are not. In dimension _N_, _N_ + 1 points are sufficient to define a hyperplane. Thus, when more than _N_ + 1 points lie on a marginal hyperplane, different choices are possible for the _N_ + 1 support vectors. **5.2.3** **Dual optimization problem** To derive the dual form of the constrained optimization problem (5.7), we plug into the Lagrangian the definition of **w** in terms of the dual variables as expressed in **84** **Chapter 5** **Support Vector Machines** (5.9) and apply the constraint (5.10). This yields _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _y_ _i_ _y_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ ) _i,j_ =1 _L_ = [1] 2 _[\u2225]_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ **x** _i_ _\u2225_ [2] _\u2212_ _i_ =1 _m_ \ufffd _\u03b1_ _i_ _,_ (5.12) _i_ =1 _\u2212_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ _b_ _i_ =1 ~~\ufffd~~ ~~\ufffd~~ \ufffd ~~\ufffd~~ 0 + ~~\ufffd~~ \ufffd ~~\ufffd~~ \ufffd _\u2212_ [1] 2 \ufffd _mi,j_ =1 _[\u03b1]_ _[i]_ _[\u03b1]_ _[j]_ _[y]_ _[i]_ _[y]_ _[j]_ [(] **[x]** _[i]_ _[\u00b7]_ **[x]** _[j]_ [)] which simplifies to 2 _L_ = _m_ \ufffd \ufffd _\u03b1_ _i_ _\u2212_ [1] 2 _i_ =1 _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _y_ _i_ _y_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ ) _._ (5.13) _i,j_ =1 This leads to the following dual optimization problem for SVMs in the separable case: 2 max _**\u03b1**_ _m_ \ufffd \ufffd _\u03b1_ _i_ _\u2212_ [1] 2 _i_ =1 _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _y_ _i_ _y_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ ) (5.14) _i,j_ =1 subject to: _\u03b1_ _i_ _\u2265_ 0 _\u2227_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ = 0 _, \u2200i \u2208_ [ _m_ ] _._ _i_ =1 The objective function _G_ : _**\u03b1**_ _\ufffd\u2192_ [\ufffd] _[m]_ _i_ =1 _[\u03b1]_ _[i]_ _[ \u2212]_ [1] 2 _m_ The objective function _G_ : _**\u03b1**_ _\ufffd\u2192_ [\ufffd] _[m]_ _i_ =1 _[\u03b1]_ _[i]_ _[ \u2212]_ [1] 2 \ufffd _i,j_ =1 _[\u03b1]_ _[i]_ _[\u03b1]_ _[j]_ _[y]_ _[i]_ _[y]_ _[j]_ [(] **[x]** _[i]_ _[ \u00b7]_ **[ x]** _[j]_ [) is infinitely] differentiable. Its Hessian is given by _\u2207_ [2] _G_ = _\u2212_ **A**, with **A** = \ufffd _y_ _i_ **x** _i_",
    "chunk_id": "foundations_machine_learning_83"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u00b7 y_ _j_ **x** _j_ \ufffd [.] **[ A]** [ is] differentiable. Its Hessian is given by _\u2207_ _G_ = _\u2212_ **A**, with **A** = \ufffd _y_ _i_ **x** _i_ _\u00b7 y_ _j_ **x** _j_ \ufffd _ij_ [.] **[ A]** [ is] the Gram matrix associated to the vectors _y_ 1 **x** 1 _, . . ., y_ _m_ **x** _m_ and is therefore positive semidefinite (see section A.2.3), which shows that _\u2207_ [2] _G \u2aaf_ **0** and that _G_ is a concave function. Since the constraints are affine and convex, the maximization problem (5.14) is a convex optimization problem. Since _G_ is a quadratic function of _**\u03b1**_, this dual optimization problem is also a QP problem, as in the case of the primal optimization and once again both general-purpose and specialized QP solvers can be used to obtain the solution (see exercise 5.4 for details on the SMO algorithm, which is often used to solve the dual form of the SVM problem in the more general non-separable setting). Moreover, since the constraints are affine, they are qualified and strong duality holds (see appendix B). Thus, the primal and dual problems are equivalent, i.e., the solution _**\u03b1**_ of the dual problem (5.14) can be used directly to determine the hypothesis returned by SVMs, using equation (5.9): _h_ ( **x** ) = sgn( **w** _\u00b7_ **x** + _b_ ) = sgn \ufffd \ufffd _[m]_ _\u03b1_ _i_ _y_ _i_ ( **x** _i_ _\u00b7_ **x** ) + _b_ \ufffd _._ (5.15) _i_ =1 **5.2** **Separable case** **85** Since support vectors lie on the marginal hyperplanes, for any support vector **x** _i_, **w** _\u00b7_ **x** _i_ + _b_ = _y_ _i_, and thus _b_ can be obtained via _\u0338_ _b_ = _y_ _i_ _\u2212_ _\u0338_ _m_ \ufffd _\u03b1_ _j_ _y_ _j_ ( **x** _j_ _\u00b7_ **x** _i_ ) _._ (5.16) _j_ =1 _\u0338_ The dual optimization problem (5.14) and the expressions (5.15) and (5.16) reveal an important property of SVMs: the hypothesis solution depends only on inner products between vectors and not directly on the vectors themselves. This observation is key and its importance will become clear in Chapter 6 where we introduce kernel methods. Equation (5.16) can now be used to derive a simple expression of the geometric margin _\u03c1_ in terms of _**\u03b1**_ . Since (5.16) holds for all _i_ with _\u03b1_ _i_ _\u0338_ = 0, multiplying both sides by _\u03b1_ _i_ _y_ _i_ and taking the sum leads to _\u0338_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ _b_ = _i_ =1 _\u0338_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ [2] _[\u2212]_ _i_ =1 _\u0338_ _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _y_ _i_ _y_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ ) _._ (5.17) _i,j_ =1 _\u0338_ Using the fact that _y_ _i_ [2] [= 1 along with equation (5.9) then yields] _\u0338_ 0 = _\u0338_ _m_ \ufffd _\u03b1_ _i_ _\u2212\u2225_ **w** _\u2225_ [2] _._ (5.18) _i_ =1 _\u0338_ Noting that _\u03b1_ _i_ _\u2265_ 0, we obtain the following expression of the margin _\u03c1_ in terms of the _L_ 1 norm of _**\u03b1**_ : 1 1",
    "chunk_id": "foundations_machine_learning_84"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "1 _\u03c1_ [2] = = _m_ = _._ (5.19) _\u2225_ **w** _\u2225_ [2] 2 \ufffd _i_ =1 _[\u03b1]_ _[i]_ _\u2225_ _**\u03b1**_ _\u2225_ 1 **5.2.4** **Leave-one-out analysis** We now use the notion of _leave-one-out error_ to derive a first learning guarantee for SVMs based on the fraction of support vectors in the training set. **Definition 5.2 (Leave-one-out error)** _Let h_ _S_ _denote the hypothesis returned by a learn-_ _ing algorithm A, when trained on a fixed sample S. Then, the_ leave-one-out error _of A on a sample S of size m is defined by_ _\u0338_ \ufffd _R_ LOO ( _A_ ) = [1] _\u0338_ _m_ _m_ \ufffd 1 _h_ _S\u2212{xi}_ ( _x_ _i_ )= _\u0338_ _y_ _i_ _._ _i_ =1 _\u0338_ Thus, for each _i \u2208_ [ _m_ ], _A_ is trained on all the points in _S_ except for _x_ _i_, i.e., _S_ _\u2212{x_ _i_ _}_, and its error is then computed using _x_ _i_ . The leave-one-out error is the average of these errors. We will use an important property of the leave-one-out error stated in the following lemma. **86** **Chapter 5** **Support Vector Machines** **Lemma 5.3** _The average leave-one-out error for samples of size m \u2265_ 2 _is an unbiased_ _estimate of the average generalization error for samples of size m \u2212_ 1 _:_ E E (5.20) _S\u223c_ D _[m]_ [[] _[R]_ [ \ufffd] [LOO] [(] _[A]_ [)] =] _S_ _[\u2032]_ _\u223c_ D _[m][\u2212]_ [1] [[] _[R]_ [(] _[h]_ _[S]_ _[\u2032]_ [)]] _[,]_ _where_ D _denotes the distribution according to which points are drawn._ Proof: By the linearity of expectation, we can write _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _\u0338_ _\u0338_ E [1] _S\u223c_ D _[m]_ [[] _[R]_ [ \ufffd] [LOO] [(] _[A]_ [)] =] _m_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _\u0338_ _\u0338_ _m_ \ufffd _S\u223c_ E D _[m]_ [[1] _[h]_ _[S][\u2212{][xi][}]_ [(] _[x]_ _[i]_ [)] _[\u0338]_ [=] _[y]_ _[i]_ []] _i_ =1 _[\u0338]_ _[\u0338]_ _[\u0338]_ _\u0338_ _\u0338_ _[\u0338]_ = E _S\u223c_ D _[m]_ [[1] _[h]_ _[S][\u2212{][x]_ [1] _[}]_ [(] _[x]_ [1] [)] _[\u0338]_ [=] _[y]_ [1] []] = E _S_ _[\u2032]_ _\u223c_ D _[m][\u2212]_ [1] _,x_ 1 _\u223c_ D [[1] _[h]_ _[S][\u2032]_ [(] _[x]_ [1] [)] _[\u0338]_ [=] _[y]_ [1] []] = E _S_ _[\u2032]_ _\u223c_ D _[m][\u2212]_ [1] [[] _x_ [ E] 1 _\u223c_ D [[1] _[h]_ _[S][\u2032]_ [(] _[x]_ [1] [)] _[\u0338]_ [=] _[y]_ [1] []]] = E _S_ _[\u2032]_ _\u223c_ D _[m][\u2212]_ [1] [[] _[R]_ [(] _[h]_ _[S]_ _[\u2032]_ [)]] _[.]_ For the second equality, we used the fact that, since the points of _S_ are drawn in an i.i.d. fashion, the expectation E _S\u223c_ D _m_ [1 _h_ _S\u2212{xi}_ ( _x_ _i_ )= _\u0338_ _y_ _i_ ] does not depend on the choice of _i \u2208_ [ _m_ ] and is thus equal to E _S\u223c_ D _[m]_ [1 _h_ _S\u2212{x_ 1 _}_ ( _x_ 1 )= _\u0338_ _y_ 1 ]. In general, computing the leave-one-out error may be costly since it requires training _m_ times on samples of size _m\u2212_ 1. In some situations however, it is possible to derive the expression of _R_ [\ufffd] LOO (",
    "chunk_id": "foundations_machine_learning_85"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_A_ ) much more efficiently (see exercise 11.9). **Theorem 5.4** _Let h_ _S_ _be the hypothesis returned by SVMs for a sample S, and let_ _N_ _SV_ ( _S_ ) _be the number of support vectors that define h_ _S_ _. Then,_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _\u0338_ _\u0338_ _._ \ufffd _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _\u0338_ _\u0338_ E E _S\u223c_ D _[m]_ [[] _[R]_ [(] _[h]_ _[S]_ [)]] _[ \u2264]_ _S\u223c_ D _[m]_ [+1] _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _\u0338_ _\u0338_ _N_ SV ( _S_ ) \ufffd _m_ + 1 _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _\u0338_ _\u0338_ Proof: Let _S_ be a linearly separable sample of _m_ + 1. If _x_ is not a support vector for _h_ _S_, removing it does not change the SVM solution. Thus, _h_ _S\u2212{x}_ = _h_ _S_ and _h_ _S\u2212{x}_ correctly classifies _x_ . By contraposition, if _h_ _S\u2212{x}_ misclassifies _x_, _x_ must be a support vector, which implies \ufffd _R_ LOO (SVM) _\u2264_ _[N]_ [SV] [(] _[S]_ [)] (5.21) _m_ + 1 _[.]_ Taking the expectation of both sides and using lemma 5.3 yields the result. Theorem 5.4 gives a sparsity argument in favor of SVMs: the average error of the algorithm is upper bounded by the average fraction of support vectors. One may hope that for many distributions seen in practice, a relatively small number of the training points will lie on the marginal hyperplanes. The solution will then be sparse in the sense that a small fraction of the dual variables _\u03b1_ _i_ will be non **5.3** **Non-separable case** **87** **Image:** [No caption returned] **w** _\u00b7_ **x** + _b_ =+1 **Figure 5.4** A separating hyperplane with point **x** _i_ classified incorrectly and point **x** _j_ correctly classified, but with margin less than 1. zero. Note, however, that this bound is relatively weak since it applies only to the average generalization error of the algorithm over all samples of size _m_ . It provides no information about the variance of the generalization error. In section 5.4, we present stronger high-probability bounds using a different argument based on the notion of margin. **5.3** **Non-separable case** In most practical settings, the training data is not linearly separable, which implies that for any hyperplane **w** _\u00b7_ **x** + _b_ = 0, there exists **x** _i_ _\u2208_ _S_ such that _y_ _i_ [ **w** _\u00b7_ **x** _i_ + _b_ ] _\u0338\u2265_ 1 _._ (5.22) Thus, the constraints imposed in the linearly separable case discussed in section 5.2 cannot all hold simultaneously. However, a relaxed version of these constraints can indeed hold, that is, for each _i \u2208_ [ _m_ ], there exist _\u03be_ _i_ _\u2265_ 0 such that _y_ _i_ [ **w** _\u00b7_ **x** _i_ + _b_ ] _\u2265_ 1 _\u2212_ _\u03be_ _i_ _._ (5.23) The variables _\u03be_ _i_ are known as _slack variables_ and are commonly used in optimization to define relaxed versions of constraints. Here, a slack variable _\u03be_ _i_ measures the distance by which vector **x** _i_ violates the desired inequality, _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2265_ 1. Figure 5.4 illustrates the situation.",
    "chunk_id": "foundations_machine_learning_86"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "For a hyperplane **w** _\u00b7_ **x** + _b_ = 0, a vector **x** _i_ with _\u03be_ _i_ _>_ 0 can be viewed as an _outlier_ . Each **x** _i_ must be positioned on the correct side of the appropriate marginal hyperplane to not be considered an outlier. As a consequence, a vector **x** _i_ with 0 _< y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _<_ 1 is correctly classified by the hyperplane **w** _\u00b7_ **x** + _b_ = 0 but is nonetheless considered to be an outlier, that **88** **Chapter 5** **Support Vector Machines** is, _\u03be_ _i_ _>_ 0. If we omit the outliers, the training data is correctly separated by **w** _\u00b7_ **x** + _b_ = 0 with a margin _\u03c1_ = 1 _/\u2225_ **w** _\u2225_ that we refer to as the _soft margin_, as opposed to the _hard margin_ in the separable case. How should we select the hyperplane in the non-separable case? One idea consists of selecting the hyperplane that minimizes the empirical error. But, that solution will not benefit from the large-margin guarantees we will present in section 5.4. Furthermore, the problem of determining a hyperplane with the smallest zero-one loss, that is the smallest number of misclassifications, is NP-hard as a function of the dimension _N_ of the space. Here, there are two conflicting objectives: on one hand, we wish to limit the total amount of slack due to outliers, which can be measured by [\ufffd] _[m]_ _i_ =1 _[\u03be]_ _[i]_ [, or, more] generally by [\ufffd] _[m]_ _i_ =1 _[\u03be]_ _i_ _[p]_ [for some] _[ p][ \u2265]_ [1; on the other hand, we seek a hyperplane with] a large margin, though a larger margin can lead to more outliers and thus larger amounts of slack. **5.3.1** **Primal optimization problem** This leads to the following general optimization problem defining SVMs in the non-separable case where the parameter _C \u2265_ 0 determines the trade-off between margin-maximization (or minimization of _\u2225_ **w** _\u2225_ [2] ) and the minimization of the slack penalty [\ufffd] _[m]_ _i_ =1 _[\u03be]_ _i_ _[p]_ [:] 1 min **w** _,b,_ _**\u03be**_ 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [ +] _[ C]_ _m_ \ufffd _\u03be_ _i_ _[p]_ (5.24) _i_ =1 subject to _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2265_ 1 _\u2212_ _\u03be_ _i_ _\u2227_ _\u03be_ _i_ _\u2265_ 0 _, i \u2208_ [ _m_ ] _,_ where _**\u03be**_ = ( _\u03be_ 1 _, . . ., \u03be_ _m_ ) _[\u22a4]_ . The parameter _C_ is typically determined via _n_ -fold crossvalidation (see section 4.5). As in the separable case, (5.24) is a convex optimization problem since the constraints are affine and thus convex and since the objective function is convex for any _p \u2265_ 1. In particular, _**\u03be**_ _\ufffd\u2192_ [\ufffd] _[m]_ _i_ =1 _[\u03be]_ _i_ _[p]_ [=] _[ \u2225]_ _**[\u03be]**_ _[\u2225]_ _[p]_ _p_ [is convex in view of the convexity of] the norm _\u2225\u00b7 \u2225_ _p_ . There are many possible choices for _p_ leading to more or less aggressive penalizations of the slack terms (see exercise 5.1). The choices _p_",
    "chunk_id": "foundations_machine_learning_87"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "= 1 and _p_ = 2 lead to the most straightforward solutions and analyses. The loss functions associated with _p_ = 1 and _p_ = 2 are called the _hinge loss_ and the _quadratic hinge loss_, respectively. Figure 5.5 shows the plots of these loss functions as well as that of the standard zero-one loss function. Both hinge losses are convex upper bounds on the zero-one loss, thus making them well suited for optimization. In what follows, the analysis is presented in the case of the hinge loss ( _p_ = 1), which is the most widely used loss function for SVMs. **5.3** **Non-separable case** **89** **Figure 5.5** **Image:** [No caption returned] Both the hinge loss and the quadratic hinge loss provide convex upper bounds on the binary zero-one loss. **5.3.2** **Support vectors** As in the separable case, the constraints are affine and thus qualified. The objective function as well as the affine constraints are convex and differentiable. Thus, the hypotheses of theorem B.30 hold and the KKT conditions apply at the optimum. We use these conditions to both analyze the algorithm and demonstrate several of its crucial properties, and subsequently derive the dual optimization problem associated to SVMs in section 5.3.3. We introduce Lagrange variables _\u03b1_ _i_ _\u2265_ 0, _i \u2208_ [ _m_ ], associated to the first _m_ constraints and _\u03b2_ _i_ _\u2265_ 0, _i \u2208_ [ _m_ ] associated to the non-negativity constraints of the slack variables. We denote by _**\u03b1**_ the vector ( _\u03b1_ 1 _, . . ., \u03b1_ _m_ ) _[\u22a4]_ and by _**\u03b2**_ the vector ( _\u03b2_ 1 _, . . ., \u03b2_ _m_ ) _[\u22a4]_ . The Lagrangian can then be defined for all **w** _\u2208_ R _[N]_, _b \u2208_ R, and _**\u03be**_ _,_ _**\u03b1**_ _,_ _**\u03b2**_ _\u2208_ R _[m]_ + [, by] _m_ \ufffd _\u03b1_ _i_ [ _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2212_ 1+ _\u03be_ _i_ ] _\u2212_ _i_ =1 _m_ \ufffd _\u03b2_ _i_ _\u03be_ _i_ _._ (5.25) _i_ =1 _L_ ( **w** _, b,_ _**\u03be**_ _,_ _**\u03b1**_ _,_ _**\u03b2**_ ) = [1] 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [+] _[C]_ _m_ \ufffd _\u03be_ _i_ _\u2212_ _i_ =1 The KKT conditions are obtained by setting the gradient of the Lagrangian with respect to the primal variables **w**, _b_, and _\u03be_ _i_ s to zero and by writing the complemen **90** **Chapter 5** **Support Vector Machines** tarity conditions: _m_ _\u2207_ **w** _L_ = **w** _\u2212_ \ufffd _\u03b1_ _i_ _y_ _i_ **x** _i_ = 0 = _\u21d2_ **w** = _i_ =1 _m_ \ufffd _\u03b1_ _i_ _y_ _i_ **x** _i_ (5.26) _i_ =1 _m_ \ufffd _\u03b1_ _i_ _y_ _i_ = 0 (5.27) _i_ =1 _\u2207_ _b_ _L_ = _\u2212_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ = 0 = _\u21d2_ _i_ =1 _\u2207_ _\u03be_ _i_ _L_ = _C \u2212_ _\u03b1_ _i_ _\u2212_ _\u03b2_ _i_ = 0 = _\u21d2_ _\u03b1_ _i_ + _\u03b2_ _i_ = _C_ (5.28) _\u2200i, \u03b1_ _i_ [ _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) _\u2212_ 1 + _\u03be_ _i_ ] = 0 = _\u21d2_ _\u03b1_ _i_ =",
    "chunk_id": "foundations_machine_learning_88"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "0 _\u2228_ _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) = 1 _\u2212_ _\u03be_ _i_ (5.29) _\u2200i, \u03b2_ _i_ _\u03be_ _i_ = 0 = _\u21d2_ _\u03b2_ _i_ = 0 _\u2228_ _\u03be_ _i_ = 0 _._ (5.30) By equation (5.26), as in the separable case, the weight vector **w** at the solution of the SVM problem is a linear combination of the training set vectors **x** 1 _, . . .,_ **x** _m_ . A vector **x** _i_ appears in that expansion iff _\u03b1_ _i_ _\u0338_ = 0. Such vectors are called _support_ _vectors_ . Here, there are two types of support vectors. By the complementarity condition (5.29), if _\u03b1_ _i_ _\u0338_ = 0, then _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) = 1 _\u2212\u03be_ _i_ . If _\u03be_ _i_ = 0, then _y_ _i_ ( **w** _\u00b7_ **x** _i_ + _b_ ) = 1 and **x** _i_ lies on a marginal hyperplane, as in the separable case. Otherwise, _\u03be_ _i_ _\u0338_ = 0 and **x** _i_ is an outlier. In this case, (5.30) implies _\u03b2_ _i_ = 0 and (5.28) then requires _\u03b1_ _i_ = _C_ . Thus, support vectors **x** _i_ are either outliers, in which case _\u03b1_ _i_ = _C_, or vectors lying on the marginal hyperplanes. As in the separable case, note that while the weight vector **w** solution is unique, the support vectors are not. **5.3.3** **Dual optimization problem** To derive the dual form of the constrained optimization problem (5.24), we plug into the Lagrangian the definition of **w** in terms of the dual variables (5.26) and apply the constraint (5.27). This yields _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _y_ _i_ _y_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ ) _i,j_ =1 _L_ = [1] 2 _[\u2225]_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ **x** _i_ _\u2225_ [2] _\u2212_ _i_ =1 _m_ \ufffd _\u03b1_ _i_ _._ (5.31) _i_ =1 _\u2212_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ _b_ _i_ =1 ~~\ufffd~~ ~~\ufffd~~ \ufffd ~~\ufffd~~ 0 + ~~\ufffd~~ \ufffd ~~\ufffd~~ \ufffd _\u2212_ [1] 2 \ufffd _mi,j_ =1 _[\u03b1]_ _[i]_ _[\u03b1]_ _[j]_ _[y]_ _[i]_ _[y]_ _[j]_ [(] **[x]** _[i]_ _[\u00b7]_ **[x]** _[j]_ [)] Remarkably, we find that the objective function is no different than in the separable case: 2 _L_ = _m_ \ufffd \ufffd _\u03b1_ _i_ _\u2212_ [1] 2 _i_ =1 _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _y_ _i_ _y_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ ) _._ (5.32) _i,j_ =1 However, here, in addition to _\u03b1_ _i_ _\u2265_ 0, we must impose the constraint on the Lagrange variables _\u03b2_ _i_ _\u2265_ 0. In view of (5.28), this is equivalent to _\u03b1_ _i_ _\u2264_ _C_ . This leads to the following dual optimization problem for SVMs in the non-separable case, which **5.4** **Margin theory** **91** only differs from that of the separable case (5.14) by the constraints _\u03b1_ _i_ _\u2264_ _C_ : 2 max _**\u03b1**_ _m_ \ufffd \ufffd _\u03b1_ _i_ _\u2212_ [1] 2 _i_ =1 _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _y_ _i_ _y_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ ) (5.33) _i,j_ =1 subject",
    "chunk_id": "foundations_machine_learning_89"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "to: 0 _\u2264_ _\u03b1_ _i_ _\u2264_ _C \u2227_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ = 0 _, i \u2208_ [ _m_ ] _._ _i_ =1 Thus, our previous comments about the optimization problem (5.14) apply to (5.33) as well. In particular, the objective function is concave and infinitely differentiable and (5.33) is equivalent to a convex QP. The problem is equivalent to the primal problem (5.24). The solution _**\u03b1**_ of the dual problem (5.33) can be used directly to determine the hypothesis returned by SVMs, using equation (5.26): _h_ ( **x** ) = sgn( **w** _\u00b7_ **x** + _b_ ) = sgn \ufffd \ufffd _[m]_ _\u03b1_ _i_ _y_ _i_ ( **x** _i_ _\u00b7_ **x** ) + _b_ \ufffd _._ (5.34) _i_ =1 Moreover, _b_ can be obtained from any support vector **x** _i_ lying on a marginal hyperplane, that is any vector **x** _i_ with 0 _< \u03b1_ _i_ _< C_ . For such support vectors, **w** _\u00b7_ **x** _i_ + _b_ = _y_ _i_ and thus _b_ = _y_ _i_ _\u2212_ _m_ \ufffd _\u03b1_ _j_ _y_ _j_ ( **x** _j_ _\u00b7_ **x** _i_ ) _._ (5.35) _j_ =1 As in the separable case, the dual optimization problem (5.33) and the expressions (5.34) and (5.35) show an important property of SVMs: the hypothesis solution depends only on inner products between vectors and not directly on the vectors themselves. This fact can be used to extend SVMs to define non-linear decision boundaries, as we shall see in chapter 6. **5.4** **Margin theory** This section presents generalization bounds which provide a strong theoretical justification for the SVM algorithm. Recall that the VC-dimension of the family of hyperplanes or linear hypotheses in R _[N]_ is _N_ + 1. Thus, the application of the VC-dimension bound (3.29) of corollary 3.19 to this hypothesis set yields the following: for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, for any _h \u2208_ H, \ufffd log [1] _\u03b4_ (5.36) 2 _m_ _[.]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + ~~\ufffd~~ 2( _N_ + 1) log _Nem_ +1 + _m_ **92** **Chapter 5** **Support Vector Machines** When the dimension of the feature space _N_ is large compared to the sample size _m_, this bound is uninformative. Remarkably, the learning guarantees presented in this section are independent of the dimension _N_ and thus hold regardless of its value. The guarantees we will present hold for real-valued functions such as the function **x** _\ufffd\u2192_ **w** _\u00b7_ **x** + _b_ returned by SVMs, as opposed to classification functions returning +1 or _\u2212_ 1, such as **x** _\ufffd\u2192_ sgn( **w** _\u00b7_ **x** + _b_ ). They are based on the notion of _confidence_ _margin_ . The confidence margin of a real-valued function _h_ at a point _x_ labeled with _y_ is the quantity _yh_ ( _x_ ). Thus, when _yh_ ( _x_ ) _>_ 0, _h_ classifies _x_ correctly but we interpret the magnitude of _|h_ ( _x_ ) _|_ as the _confidence_ of the prediction made by _h_ . The notion of confidence margin is",
    "chunk_id": "foundations_machine_learning_90"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "distinct from that of geometric margin and does not require a linear separability assumption. But, the two notions are related as follows in the separable case: for _h_ : **x** _\ufffd\u2192_ **w** _\u00b7_ **x** + _b_ with geometric margin _\u03c1_ geom, the confidence margin at any point **x** of the training sample with label _y_ is at least _\u03c1_ geom _\u2225_ **w** _\u2225_, i.e. _|yh_ ( **x** ) _| \u2265_ _\u03c1_ geom _\u2225_ **w** _\u2225_ . In view of the definition of the confidence margin, for any parameter _\u03c1 >_ 0, we will define a _\u03c1-margin loss function_ that, as with the zero-one loss, penalizes _h_ with the cost of 1 when it misclassifies point _x_ ( _yh_ ( _x_ ) _\u2264_ 0), but also penalizes _h_ (linearly) when it correctly classifies _x_ with confidence less than or equal to _\u03c1_ ( _yh_ ( _x_ ) _\u2264_ _\u03c1_ ). The main margin-based generalization bounds of this section are presented in terms of this loss function, which is formally defined as follows. **Definition 5.5 (Margin loss function)** _For any \u03c1 >_ 0 _, the \u03c1-margin loss is the function_ _L_ _\u03c1_ : R _\u00d7_ R _\u2192_ R + _defined for all y, y_ _[\u2032]_ _\u2208_ R _by L_ _\u03c1_ ( _y, y_ _[\u2032]_ ) = \u03a6 _\u03c1_ ( _yy_ _[\u2032]_ ) _with,_ \u03a6 _\u03c1_ ( _x_ ) = min 1 _,_ max 0 _,_ 1 _\u2212_ _[x]_ \ufffd \ufffd _\u03c1_ = \ufffd [\ufffd] \uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3 1 _if x \u2264_ 0 0 _if \u03c1 \u2264_ _x._ _[x]_ _\u03c1_ _if_ 0 _\u2264_ _x \u2264_ _\u03c1_ 1 _\u2212_ _[x]_ This loss function is illustrated by figure 5.6. The parameter _\u03c1 >_ 0 can be interpreted as the confidence margin demanded from a hypothesis _h_ . The empirical margin loss is similarly defined as the margin loss over the training sample. **Definition 5.6 (Empirical margin loss)** _Given a sample S_ = ( _x_ 1 _, . . ., x_ _m_ ) _and a hypoth-_ _esis h, the empirical margin loss is defined by_ \ufffd _R_ _S,\u03c1_ ( _h_ ) = [1] _m_ _m_ \ufffd \u03a6 _\u03c1_ ( _y_ _i_ _h_ ( _x_ _i_ )) _._ (5.37) _i_ =1 Note that, for any _i \u2208_ [ _m_ ], \u03a6 _\u03c1_ ( _y_ _i_ _h_ ( _x_ _i_ )) _\u2264_ 1 _y_ _i_ _h_ ( _x_ _i_ ) _\u2264\u03c1_ . Thus, the empirical margin loss can be upper-bounded as follows: \ufffd _R_ _S,\u03c1_ ( _h_ ) _\u2264_ [1] _m_ _m_ \ufffd 1 _y_ _i_ _h_ ( _x_ _i_ ) _\u2264\u03c1_ _._ (5.38) _i_ =1 **5.4** **Margin theory** **93** **Figure 5.6** **Image:** [No caption returned] The margin loss illustrated in red, defined with respect to margin parameter _\u03c1_ = 0 _._ 7. In all the results that follow, the empirical margin loss can be replaced by this upper bound, which admits a simple interpretation: it is the fraction of the points in the training sample _S_ that have been misclassified or classified with confidence less than _\u03c1_ . In other words, the upper bound is then the fraction of the points",
    "chunk_id": "foundations_machine_learning_91"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "in the training data with margin less than _\u03c1_ . This corresponds to the loss function indicated by the blue dotted line in figure 5.6. A key benefit of using a loss function based on \u03a6 _\u03c1_ as opposed to the zero-one loss or the loss defined by the blue dotted line of figure 5.6 is that \u03a6 _\u03c1_ is 1 _/\u03c1_ -Lipschitz, since the absolute value of the slope of the function is at most 1 _/\u03c1_ . The following lemma bounds the empirical Rademacher complexity of a hypothesis set H after composition with such a Lipschitz function in terms of the empirical Rademacher complexity of H. It will be needed for the proof of the margin-based generalization bound. **Lemma 5.7 (Talagrand\u2019s lemma)** _Let_ \u03a6 1 _, . . .,_ \u03a6 _m_ _be l-Lipschitz functions from_ R _to_ R _and \u03c3_ 1 _, . . ., \u03c3_ _m_ _be Rademacher random variables. Then, for any hypothesis set_ H _of real-valued functions, the following inequality holds:_ _m_ [E] _**\u03c3**_ sup \ufffd _h\u2208_ H 1 _m_ [E] _**\u03c3**_ sup \ufffd _h\u2208_ H _m_ \ufffd \ufffd _\u03c3_ _i_ (\u03a6 _i_ _\u25e6_ _h_ )( _x_ _i_ ))\ufffd _\u2264_ _[l]_ _i_ =1 _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ )\ufffd = _l_ R [\ufffd] _S_ (H) _._ _i_ =1 _In particular, if_ \u03a6 _i_ = \u03a6 _for all i \u2208_ [ _m_ ] _, then the following holds:_ \ufffd R _S_ (\u03a6 _\u25e6_ H) _\u2264_ _l_ \ufffdR _S_ (H) _._ Proof: First we fix a sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ), then, by definition, E _m_ _\u03c3_ 1 _,...,\u03c3_ _m\u2212_ 1 E \ufffd _\u03c3_ _m_ 1 _m_ [E] _**\u03c3**_ sup \ufffd _h\u2208_ H sup _u_ _m\u2212_ 1 ( _h_ ) + _\u03c3_ _m_ (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _m_ ) _,_ \ufffd _h\u2208_ H \ufffd\ufffd _m_ \ufffd \ufffd _\u03c3_ _i_ (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _i_ )\ufffd = [1] _i_ =1 **94** **Chapter 5** **Support Vector Machines** where _u_ _m\u2212_ 1 ( _h_ ) = [\ufffd] _[m]_ _i_ =1 _[\u2212]_ [1] _[\u03c3]_ _[i]_ [(\u03a6] _[i]_ _[ \u25e6]_ _[h]_ [)(] _[x]_ _[i]_ [). By definition of the supremum, for any] _\u03f5 >_ 0, there exist _h_ 1 _, h_ 2 _\u2208_ H such that _u_ _m\u2212_ 1 ( _h_ 1 ) + (\u03a6 _m_ _\u25e6_ _h_ 1 )( _x_ _m_ ) _\u2265_ (1 _\u2212_ _\u03f5_ ) sup _u_ _m\u2212_ 1 ( _h_ ) + (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _m_ ) \ufffd _h\u2208_ H \ufffd and _u_ _m\u2212_ 1 ( _h_ 2 ) _\u2212_ (\u03a6 _m_ _\u25e6_ _h_ 2 )( _x_ _m_ ) _\u2265_ (1 _\u2212_ _\u03f5_ ) sup _u_ _m\u2212_ 1 ( _h_ ) _\u2212_ (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _m_ ) _._ \ufffd _h\u2208_ H \ufffd Thus, for any _\u03f5 >_ 0, by definition of E _\u03c3_ _m_, (1 _\u2212_ _\u03f5_ ) E sup _u_ _m\u2212_ 1 ( _h_ ) + _\u03c3_ _m_ (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _m_ ) _\u03c3_ _m_ \ufffd _h\u2208_ H \ufffd 1 = (1 _\u2212_ _\u03f5_ )",
    "chunk_id": "foundations_machine_learning_92"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd 2 _h_ [sup] _\u2208_ H _u_ _m\u2212_ 1 ( _h_ ) + (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _m_ ) + [1] \ufffd \ufffd 2 sup _u_ _m\u2212_ 1 ( _h_ ) _\u2212_ (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _m_ ) \ufffd _h\u2208_ H \ufffd [\ufffd] _\u2264_ [1] 2 [[] _[u]_ _[m][\u2212]_ [1] [(] _[h]_ [2] [)] _[ \u2212]_ [(\u03a6] _[m]_ _[ \u25e6]_ _[h]_ [2] [)(] _[x]_ _[m]_ [)]] _[.]_ [1] 2 [[] _[u]_ _[m][\u2212]_ [1] [(] _[h]_ [1] [) + (\u03a6] _[m]_ _[ \u25e6]_ _[h]_ [1] [)(] _[x]_ _[m]_ [)] + 1] 2 Let _s_ = sgn( _h_ 1 ( _x_ _m_ ) _\u2212_ _h_ 2 ( _x_ _m_ )). Then, the previous inequality implies (1 _\u2212_ _\u03f5_ ) E _\u03c3_ _m_ sup _u_ _m\u2212_ 1 ( _h_ ) + _\u03c3_ _m_ (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _m_ ) \ufffd _h\u2208_ H \ufffd _\u2264_ [1] (Lipschitz property) 2 [[] _[u]_ _[m][\u2212]_ [1] [(] _[h]_ [1] [) +] _[ u]_ _[m][\u2212]_ [1] [(] _[h]_ [2] [) +] _[ sl]_ [(] _[h]_ [1] [(] _[x]_ _[m]_ [)] _[ \u2212]_ _[h]_ [2] [(] _[x]_ _[m]_ [))]] = [1] 2 _\u2264_ [1] [1] 2 [[] _[u]_ _[m][\u2212]_ [1] [(] _[h]_ [1] [) +] _[ slh]_ [1] [(] _[x]_ _[m]_ [)] + 1] 2 [ _u_ _m\u2212_ 1 ( _h_ ) _\u2212_ _slh_ ( _x_ _m_ )] (definition of sup) 2 _h_ [sup] _\u2208_ H (rearranging) 2 [[] _[u]_ _[m][\u2212]_ [1] [(] _[h]_ [2] [)] _[ \u2212]_ _[slh]_ [2] [(] _[x]_ _[m]_ [)]] [1] [ _u_ _m\u2212_ 1 ( _h_ ) + _slh_ ( _x_ _m_ )] + [1] 2 _h_ [sup] _\u2208_ H = E _\u03c3_ _m_ sup _u_ _m\u2212_ 1 ( _h_ ) + _\u03c3_ _m_ _lh_ ( _x_ _m_ ) _._ (definition of E \ufffd _h\u2208_ H \ufffd _\u03c3_ _m_ [)] Since the inequality holds for all _\u03f5 >_ 0, we have sup _u_ _m\u2212_ 1 ( _h_ ) + _\u03c3_ _m_ _lh_ ( _x_ _m_ ) _._ \ufffd _h\u2208_ H \ufffd E _\u03c3_ _m_ sup _u_ _m\u2212_ 1 ( _h_ ) + _\u03c3_ _m_ (\u03a6 _m_ _\u25e6_ _h_ )( _x_ _m_ ) _\u2264_ E \ufffd _h\u2208_ H \ufffd _\u03c3_ _m_ Proceeding in the same way for all other _\u03c3_ _i_ ( _i \u0338_ = _m_ ) proves the lemma. The following is a general margin-based generalization bound that will be used in the analysis of several algorithms. **Theorem 5.8 (Margin bound for binary classification)** _Let_ H _be a set of real-valued func-_ _tions. Fix \u03c1 >_ 0 _, then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, each of the_ _following holds for all h \u2208_ H _:_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] _\u03c1_ [R] _[m]_ [(][H][) +] \ufffd log [1] _\u03b4_ (5.39) 2 _m_ \ufffd _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] R _S_ (H) + 3 _\u03c1_ \ufffd log [2] _\u03b4_ (5.40) 2 _m_ _[.]_ **5.4** **Margin theory** **95** Proof: Let H [\ufffd] = _{z_ = ( _x, y_",
    "chunk_id": "foundations_machine_learning_93"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ") _\ufffd\u2192_ _yh_ ( _x_ ): _h \u2208_ H _}_ . Consider the family of functions taking values in [0 _,_ 1]: \ufffd \ufffd H = _{_ \u03a6 _\u03c1_ _\u25e6_ _f_ : _f \u2208_ H _} ._ By theorem 3.3, with probability at least 1 _\u2212_ _\u03b4_, for all _g \u2208_ H [\ufffd], \ufffd log [1] _\u03b4_ 2 _m_ _[,]_ log [1] E[ _g_ ( _z_ )] _\u2264_ [1] _m_ _m_ \ufffd _g_ ( _z_ _i_ ) + 2R _m_ (H [\ufffd] ) + _i_ =1 _m_ \ufffd and thus, for all _h \u2208_ H, E[\u03a6 _\u03c1_ ( _yh_ ( _x_ ))] _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 2R _m_ \ufffd\u03a6 _\u03c1_ _\u25e6_ H [\ufffd] \ufffd + \ufffd log [1] _\u03b4_ 2 _m_ _[.]_ Since 1 _u\u2264_ 0 _\u2264_ \u03a6 _\u03c1_ ( _u_ ) for all _u \u2208_ R, we have _R_ ( _h_ ) = E[1 _yh_ ( _x_ ) _\u2264_ 0 ] _\u2264_ E[\u03a6 _\u03c1_ ( _yh_ ( _x_ ))], thus _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 2R _m_ \ufffd\u03a6 _\u03c1_ _\u25e6_ H [\ufffd] \ufffd + \ufffd log [1] _\u03b4_ 2 _m_ _[.]_ log [1] Since \u03a6 _\u03c1_ is 1 _/\u03c1_ -Lipschitz, by lemma 5.7, we have R _m_ \ufffd\u03a6 _\u03c1_ _\u25e6_ H [\ufffd] \ufffd _\u2264_ _\u03c1_ 1 [R] _[m]_ [(][H][ \ufffd] [) and] R _m_ (H [\ufffd] ) can be rewritten as follows: _m_ _S,\u03c3_ [E] R _m_ (H [\ufffd] ) = _m_ [1] _S,\u03c3_ [E] sup \ufffd _h\u2208_ H _m_ \ufffd \ufffd _\u03c3_ _i_ _y_ _i_ _h_ ( _x_ _i_ )\ufffd = [1] _i_ =1 sup \ufffd _h\u2208_ H _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ )\ufffd = R _m_ \ufffdH\ufffd _._ _i_ =1 _m_ \ufffd This proves (5.39). The second inequality, (5.40), can be derived in the same way by using the second inequality of theorem 3.3, (3.4), instead of (3.3). The generalization bounds of theorem 5.8 suggest a trade-off: a larger value of _\u03c1_ decreases the complexity term (second term), but tends to increase the empirical margin-loss _R_ [\ufffd] _S,\u03c1_ ( _h_ ) (first term) by requiring from a hypothesis _h_ a higher confidence margin. Thus, if for a relatively large value of _\u03c1_ the empirical margin loss of _h_ remains relatively small, then _h_ benefits from a very favorable guarantee on its generalization error. For theorem 5.8, the margin parameter _\u03c1_ must be selected beforehand. But, the bounds of the theorem can be generalized to hold uniformly for all _\u03c1 \u2208_ (0 _,_ 1] at the cost of a modest additional term ~~\ufffd~~ log log _m_ 2 _\u03c1_ 2, as shown in the following theorem (a version of this theorem with better constants can be derived, see exercise 5.2). **96** **Chapter 5** **Support Vector Machines** **Theorem 5.9** _Let_ H _be a set of real-valued functions. Fix r >_ 0 _. Then, for any_ _\u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, each of the following holds for all h \u2208_ H _and_ _\u03c1 \u2208_ (0 _, r_ ] _:_ ~~\ufffd~~ log [2] _\u03b4_",
    "chunk_id": "foundations_machine_learning_94"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(5.41) 2 _m_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [4] _\u03c1_ [R] _[m]_ [(][H][) +] log log 2 2 _\u03c1r_ + _m_ log log 2 2 _r_ ~~\ufffd~~ log [4] _\u03b4_ (5.42) 2 _m_ _[.]_ \ufffd _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [4] R _S_ (H) + _\u03c1_ ~~\ufffd~~ \ufffd log log 2 2 _\u03c1r_ + 3 _m_ log log 2 2 _r_ Proof: Consider two sequences ( _\u03c1_ _k_ ) _k\u2265_ 1 and ( _\u03f5_ _k_ ) _k\u2265_ 1, with _\u03f5_ _k_ _\u2208_ (0 _,_ 1]. By theorem 5.8, for any fixed _k \u2265_ 1, P sup _R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S,\u03c1_ _k_ ( _h_ ) _>_ [2] R _m_ (H) + _\u03f5_ _k_ \ufffd _h\u2208H_ _\u03c1_ _k_ _\u2264_ exp( _\u2212_ 2 _m\u03f5_ [2] _k_ [)] _[.]_ (5.43) \ufffd Choose _\u03f5_ _k_ = _\u03f5_ + \ufffd lo _m_ g _k_ [, then, by the union bound, the following holds:] R _m_ (H) _\u2212_ _\u03f5_ _k_ _>_ 0 _\u03c1_ _k_ \uf8f9 \uf8fb P \uf8ee sup _R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S,\u03c1_ _k_ ( _h_ ) _\u2212_ [2] _h\u2208H_ _\u03c1_ _k_ \uf8f0 _k\u2265_ 1 _\u2264_ \ufffd exp( _\u2212_ 2 _m\u03f5_ [2] _k_ [)] _k\u2265_ 1 = \ufffd exp \ufffd _\u2212_ 2 _m_ ( _\u03f5_ + \ufffd _k\u2265_ 1 = \ufffd (log _k_ ) _/m_ ) [2] [\ufffd] _\u2264_ \ufffd exp( _\u2212_ 2 _m\u03f5_ [2] ) exp( _\u2212_ 2 log _k_ ) _k\u2265_ 1 = \ufffd [\ufffd] 1 _/k_ [2] [\ufffd] exp( _\u2212_ 2 _m\u03f5_ [2] ) _k\u2265_ 1 = _[\u03c0]_ [2] 6 [exp(] _[\u2212]_ [2] _[m\u03f5]_ [2] [)] _[ \u2264]_ [2 exp(] _[\u2212]_ [2] _[m\u03f5]_ [2] [)] _[.]_ We can choose _\u03c1_ _k_ = _r/_ 2 _[k]_ . For any _\u03c1 \u2208_ (0 _, r_ ], there exists _k \u2265_ 1 such that _\u03c1 \u2208_ ( _\u03c1_ _k_ _,_ _\u03c1_ _k\u2212_ 1 ], with _\u03c1_ 0 = _r_ . For that _k_, _\u03c1 \u2264_ _\u03c1_ _k\u2212_ 1 = 2 _\u03c1_ _k_, thus 1 _/\u03c1_ _k_ _\u2264_ 2 _/\u03c1_ and _[\u221a]_ log _k_ = \ufffdlog log 2 ( _r/\u03c1_ _k_ ) _\u2264_ \ufffdlog log 2 (2 _r/\u03c1_ ). Furthermore, for any _h \u2208_ H, _R_ [\ufffd] _S,\u03c1_ _k_ ( _h_ ) _\u2264_ \ufffd\ufffd log log 2 ( _r/\u03c1_ _k_ ) _\u2264_ \ufffdlog log 2 (2 _r/\u03c1_ ). Furthermore, for any _h \u2208_ H, _R_ [\ufffd] _S,\u03c1_ _k_ ( _h_ ) _\u2264_ _R_ _S,\u03c1_ ( _h_ ). Thus, the following inequality holds: log log 2 ( _r/\u03c1_ _k_ ) _\u2264_ \ufffd \uf8f9 \uf8fa\uf8fb _\u2264_ 2 exp( _\u2212_ 2 _m\u03f5_ 2 ) _,_ ~~\ufffd~~ log log 2 (2 _r/\u03c1_ ) P \uf8ee sup _h\u2208H_ \uf8ef\uf8f0 _\u03c1\u2208_ (0 _,r_ ] _R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) _\u2212_ [4] _\u03c1_ [R] _[m]_ [(][H][)] _[ \u2212]_ _\u2212_ _\u03f5 >_ 0 _m_ which proves the first statement. The second statement can be proven in a similar way. **5.4** **Margin theory** **97** The Rademacher complexity of linear hypotheses with bounded weight vector can be bounded as follows. **Theorem 5.10**",
    "chunk_id": "foundations_machine_learning_95"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_Let S \u2286{_ **x** : _\u2225_ **x** _\u2225\u2264_ _r} be a sample of size m and let_ H = _{_ **x** _\ufffd\u2192_ **w** _\u00b7_ **x** : _\u2225_ **w** _\u2225\u2264_ \u039b _}. Then, the empirical Rademacher complexity of_ H _can be bounded_ _as follows:_ \ufffd \ufffd R _S_ (H) _\u2264_ _r_ [2] \u039b [2] _._ _m_ Proof: The proof follows through a series of inequalities: _m_ \ufffd _\u03c3_ _i_ **w** _\u00b7_ **x** _i_ _i_ =1 = [1] \ufffd _m_ [E] _\u03c3_ = [1] \ufffd _m_ sup **w** _\u00b7_ \ufffd _\u2225_ **w** _\u2225\u2264_ \u039b _m_ \ufffd _\u03c3_ _i_ **x** _i_ _i_ =1 \ufffd \ufffd R _S_ (H) = [1] _m_ [E] _\u03c3_ sup \ufffd _\u2225_ **w** _\u2225\u2264_ \u039b _m_ _m_ \ufffd \ufffd _i_ =1 _\u03c3_ _i_ **x** _i_ \ufffd\ufffd\ufffd\ufffd _\u2264_ _m_ [\u039b] \ufffd\ufffd\ufffd\ufffd _m_ 2 [\ufffd\ufffd] [1] 2 \ufffd _\u03c3_ _i_ **x** _i_ \ufffd\ufffd\ufffd _i_ =1 _m_ \ufffd _\u2264_ [\u039b] _m_ [E] _\u03c3_ \ufffd\ufffd\ufffd\ufffd 2 E _\u03c3_ \ufffd _m_ _m_ \ufffd \ufffd 2 _\u221a_ _\u2264_ [\u039b] _m_ \ufffd \ufffd _m_ 2 \ufffd _\u03c3_ _i_ _\u03c3_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ )\ufffd [\ufffd] [1] _i,j_ =1 = [\u039b] _m_ E _\u03c3_ \ufffd 2 _\u2264_ [\u039b] _mr_ [2] \ufffd _r_ [2] \u039b [2] _,_ _m_ _m_ 2 \ufffd _\u2225_ **x** _i_ _\u2225_ [2] _i_ =1 \ufffd [1] = _m_ The first inequality makes use of the Cauchy-Schwarz inequality and the bound on _\u2225_ **w** _\u2225_, the second follows by Jensen\u2019s inequality, the third by E[ _\u03c3_ _i_ _\u03c3_ _j_ ] = E[ _\u03c3_ _i_ ] E[ _\u03c3_ _j_ ] = 0 for _i \u0338_ = _j_, and the last one by _\u2225_ **x** _i_ _\u2225\u2264_ _r_ . Combining theorem 5.10 and theorem 5.8 gives directly the following general margin bound for linear hypotheses with bounded weight vectors, presented in corollary 5.11. **Corollary 5.11** _Let_ H = _{_ **x** _\ufffd\u2192_ **w** _\u00b7_ **x** : _\u2225_ **w** _\u2225\u2264_ \u039b _} and assume that_ X _\u2286{_ **x** : _\u2225_ **x** _\u2225\u2264_ _r}._ _Fix \u03c1 >_ 0 _, then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the choice of a_ _sample S of size m, the following holds for any h \u2208_ H _:_ \ufffd log [1] _\u03b4_ (5.44) 2 _m_ _[.]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 2 ~~\ufffd~~ _r_ [2] \u039b [2] _/\u03c1_ [2] + _m_ As with theorem 5.8, the bound of this corollary can be generalized to hold uni formly for all _\u03c1 \u2208_ (0 _,_ 1] at the cost of an additional term \ufffd log log _m_ 2 _\u03c1_ 2 by combining theorems 5.10 and 5.9. This generalization bound for linear hypotheses is remarkable, since it does not depend directly on the dimension of the feature space, but only on the margin. It suggests that a small generalization error can be achieved when _\u03c1/_ ( _r_ \u039b) is large (small second term) while the empirical margin loss is relatively small (first term). The latter occurs when few points are either classified incorrectly or correctly, but with margin less than _\u03c1_",
    "chunk_id": "foundations_machine_learning_96"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ". When the training sample is linearly separable, for a linear hypothesis with geometric margin _\u03c1_ geom and the choice of the confidence margin **98** **Chapter 5** **Support Vector Machines** parameter _\u03c1_ = _\u03c1_ geom, the empirical margin loss term is zero. Thus, if _\u03c1_ geom is relatively large, this provides a strong guarantee for the generalization error of the corresponding linear hypothesis. The fact that the guarantee does not explicitly depend on the dimension of the feature space may seem surprising and appears to contradict the VC-dimension lower bounds of theorems 3.20 and 3.23. Those lower bounds show that for any learning algorithm _A_ there exists a _bad_ distribution for which the error of the hypothesis returned by the algorithm is \u2126(\ufffd _d/m_ ) with a non-zero probability. The bound of the corollary does not rule out such _bad_ cases, however: for such bad distributions, the empirical margin loss would be large even for a relatively small margin _\u03c1_, and thus the bound of the corollary would be loose in that case. Thus, in some sense, the learning guarantee of the corollary hinges upon the hope of a good margin value _\u03c1_ : if there exists a relatively large margin value _\u03c1 >_ 0 for which the empirical margin loss is small, then a small generalization error is guaranteed by the corollary. This favorable margin situation depends on the distribution: while the learning bound is distribution-independent, the existence of a good margin is in fact distribution-dependent. A favorable margin seems to appear relatively often in applications. The bound of the corollary gives a strong justification for margin-maximization algorithms such as SVMs. Choosing \u039b = 1, by the generalization of corollary 5.11 to a uniform bound over _\u03c1 \u2208_ (0 _, r_ ], for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, the following holds for all _h \u2208_ \ufffd **x** _\ufffd\u2192_ **w** _\u00b7_ **x** : _\u2225_ **w** _\u2225\u2264_ 1\ufffd and _\u03c1 \u2208_ (0 _, r_ ]: ~~\ufffd~~ log [2] _\u03b4_ 2 _m_ _[.]_ ~~\ufffd~~ log log 2 2 _\u03c1r_ + _m_ log log 2 2 _r_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 4 \ufffd _r_ [2] _/\u03c1_ [2] + _m_ The inequality also trivially holds for _\u03c1_ larger than _r_ since in that case, by the Cauchy-Schwarz inequality, for any **w** with _\u2225_ **w** _\u2225\u2264_ 1, we have _y_ _i_ ( **w** _\u00b7_ **x** _i_ ) _\u2264_ _r \u2264_ _\u03c1_ and _R_ [\ufffd] _S,\u03c1_ ( _h_ ) is equal to one for all _h_ . Now, for any _\u03c1 >_ 0, the _\u03c1_ -margin loss function is upper bounded by the _\u03c1_ -hinge loss: _\u2200u \u2208_ R _,_ \u03a6 _\u03c1_ ( _u_ ) = min 1 _,_ max 0 _,_ 1 _\u2212_ _[u]_ \ufffd \ufffd _\u03c1_ _\u2264_ max 0 _,_ 1 _\u2212_ _[u]_ _._ (5.45) \ufffd\ufffd \ufffd _\u03c1_ \ufffd Thus, with probability at least 1 _\u2212_ _\u03b4_, the following holds for all _h \u2208_ \ufffd **x** _\ufffd\u2192_ **w** _\u00b7_ **x** : _\u2225_ **w** _\u2225\u2264_ 1\ufffd and all _\u03c1 >_ 0: ~~\ufffd~~ log",
    "chunk_id": "foundations_machine_learning_97"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[2] _\u03b4_ 2 _m_ _[.]_ _\u03c1_ + 4 \ufffd \ufffd ~~\ufffd~~ log log 2 2 _\u03c1r_ + _m_ _R_ ( _h_ ) _\u2264_ [1] _m_ _m_ \ufffd \ufffd max 0 _,_ 1 _\u2212_ _[y]_ _[i]_ [(] **[w]** _[ \u00b7]_ **[ x]** _[i]_ [)] _i_ =1 \ufffd _\u03c1_ _r_ [2] _/\u03c1_ [2] + _m_ **5.4** **Margin theory** **99** Since for any _\u03c1 >_ 0, _h/\u03c1_ admits the same generalization error as _h_, with probability at least 1 _\u2212_ _\u03b4_, the following holds for all _h \u2208_ \ufffd **x** _\ufffd\u2192_ **w** _\u00b7_ **x** : _\u2225_ **w** _\u2225\u2264_ 1 _/\u03c1_ \ufffd and all _\u03c1 >_ 0: \ufffd log [2] _\u03b4_ 2 _m_ _[.]_ [ (5.46)] ~~\ufffd~~ \ufffd log log 2 2 _\u03c1r_ + _m_ _R_ ( _h_ ) _\u2264_ [1] _m_ _m_ \ufffd \ufffd max \ufffd0 _,_ 1 _\u2212_ _y_ _i_ ( **w** _\u00b7_ **x** _i_ )\ufffd +4 _i_ =1 _r_ [2] _/\u03c1_ [2] + _m_ This inequality can be used to derive an algorithm that selects **w** and _\u03c1 >_ 0 to minimize the right-hand side. The minimization with respect to _\u03c1_ does not lead to a convex optimization and depends on theoretical constant factors affecting the second and third terms, which may not be optimal. Thus, instead, _\u03c1_ is left as a free parameter of the algorithm, typically determined via cross-validation. Now, since only the first term of the right-hand side depends on **w**, for any _\u03c1 >_ 0, the bound suggests selecting **w** as the solution of the following optimization problem: _m_ \ufffd max \ufffd0 _,_ 1 _\u2212_ _y_ _i_ ( **w** _\u00b7_ **x** _i_ )\ufffd _._ (5.47) _i_ =1 min _\u2225_ **w** _\u2225_ [2] _\u2264_ _\u03c1_ [1][2] 1 _m_ Introducing a Lagrange variable _\u03bb \u2265_ 0, the optimization problem can be written equivalently as min _\u03bb\u2225_ **w** _\u2225_ [2] + [1] **w** _m_ _m_ \ufffd max \ufffd0 _,_ 1 _\u2212_ _y_ _i_ ( **w** _\u00b7_ **x** _i_ )\ufffd _._ (5.48) _i_ =1 Since for any choice of _\u03c1_ in the constraint of (5.47) there exists an equivalent dual variable _\u03bb_ in the formulation of (5.48) that achieves the same optimal **w**, _\u03bb_ can be freely selected via cross-validation. [5] The resulting algorithm precisely coincides with SVMs. Note that an alternative objective function and thus algorithm would be based on the empirical margin loss instead of the hinge loss. However, the advantage of the hinge loss is that it is convex, while the margin loss is not. As already pointed out, the bounds just discussed do not directly depend on the dimension of the feature space but guarantee good generalization when given a favorable margin. Thus, they suggest seeking large-margin separating hyperplanes in a very high-dimensional space. In view of the form of the dual optimization problems for SVMs, determining the solution of the optimization and using it for prediction both require computing many inner products in that space. For very high-dimensional spaces, the computation of these inner products could become very costly. The next chapter provides a solution to this problem which further provides a generalization of SVMs to non-vectorial input",
    "chunk_id": "foundations_machine_learning_98"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "spaces. 5 An equivalent analysis consists of choosing _\u03c1_ = 1 _/\u2225_ **w** _\u2225_ in (5.46). **100** **Chapter 5** **Support Vector Machines** **5.5** **Chapter notes** The maximum-margin or _optimal hyperplane_ solution described in section 5.2 was introduced by Vapnik and Chervonenkis [1964]. The algorithm had limited applications since in most tasks in practice the data is not linearly separable. In contrast, the SVM algorithm of section 5.3 for the general non-separable case, introduced by Cortes and Vapnik [1995] under the name _support-vector networks_, has been widely adopted and been shown to be effective in practice. The algorithm and its theory have had a profound impact on theoretical and applied machine learning and inspired research on a variety of topics. Several specialized algorithms have been suggested for solving the specific QP that arises when solving the SVM problem, for example the SMO algorithm of Platt [1999] (see exercise 5.4) and a variety of other decomposition methods such as those used in the LibLinear software library [Hsieh et al., 2008], and [Allauzen et al., 2010] for solving the problem when using rational kernels (see chapter 6). Much of the theory supporting the SVM algorithm ([Cortes and Vapnik, 1995, Vapnik, 1998]), in particular the margin theory presented in section 5.4, has been adopted in the learning theory and statistics communities and applied to a variety of other problems. The margin bound on the VC-dimension of canonical hyperplanes (exercise 5.7) is by Vapnik [1998], the proof is very similar to Novikoff\u2019s margin bound on the number of updates made by the Perceptron algorithm in the separable case. Our presentation of margin guarantees based on the Rademacher complexity follows the elegant analysis of Koltchinskii and Panchenko [2002] (see also Bartlett and Mendelson [2002], Shawe-Taylor et al. [1998]). Our proof of Talagrand\u2019s lemma 5.7 is a simpler and more concise version of a more general result given by Ledoux and Talagrand [1991, pp. 112\u2013114]. See H\u00a8offgen et al. [1995] for hardness results related to the problem of finding a hyperplane with the minimal number of errors on a training sample. **5.6** **Exercises** 5.1 Soft margin hyperplanes. The function of the slack variables used in the optimization problem for soft margin hyperplanes has the form: _\u03be \ufffd\u2192_ [\ufffd] _[m]_ _i_ =1 _[\u03be]_ _[i]_ [.] Instead, we could use _\u03be \ufffd\u2192_ [\ufffd] _[m]_ _i_ =1 _[\u03be]_ _i_ _[p]_ [, with] _[ p >]_ [ 1.] (a) Give the dual formulation of the problem in this general case. (b) How does this more general formulation ( _p >_ 1) compare to the standard setting ( _p_ = 1)? In the case _p_ = 2 is the optimization still convex? **5.6** **Exercises** **101** 5.2 Tighter Rademacher Bound. Derive the following tighter version of the bound of theorem 5.9: for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, for all _h \u2208_ H and _\u03c1 \u2208_ (0 _,_ 1] the following holds: ~~\ufffd~~ log [2] _\u03b4_ (5.49) 2 _m_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] _[\u03b3]_ _\u03c1_ [R] _[m]_ [(][H][) +] \ufffd log",
    "chunk_id": "foundations_machine_learning_99"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "log _\u03b3_ _\u03b3\u03c1_ + _m_ for any _\u03b3 >_ 1. 5.3 Importance weighted SVM. Suppose you wish to use SVMs to solve a learning problem where some training data points are more important than others. More formally, assume that each training point consists of a triplet ( _x_ _i_ _, y_ _i_ _, p_ _i_ ), where 0 _\u2264_ _p_ _i_ _\u2264_ 1 is the importance of the _i_ th point. Rewrite the primal SVM constrained optimization problem so that the penalty for mis-labeling a point _x_ _i_ is scaled by the priority _p_ _i_ . Then carry this modification through the derivation of the dual solution. 5.4 Sequential minimal optimization (SMO). The SMO algorithm is an optimization algorithm introduced to speed up the training of SVMs. SMO reduces a (potentially) large quadratic programming (QP) optimization problem into a series of small optimizations involving only two Lagrange multipliers. SMO reduces memory requirements, bypasses the need for numerical QP optimization and is easy to implement. In this question, we will derive the update rule for the SMO algorithm in the context of the dual formulation of the SVM problem. (a) Assume that we want to optimize equation (5.33) only over _\u03b1_ 1 and _\u03b1_ 2 . Show that the optimization problem reduces to max _\u03b1_ 1 _,\u03b1_ 2 _[\u03b1]_ [1] [ +] _[ \u03b1]_ [2] _[ \u2212]_ [1] 2 [1] 1 _[\u2212]_ [1] 2 _[K]_ [11] _[\u03b1]_ [2] 2 1 _[\u2212]_ 2 _[\u2212]_ _[sK]_ [12] _[\u03b1]_ [1] _[\u03b1]_ [2] _[\u2212]_ _[y]_ [1] _[\u03b1]_ [1] _[v]_ [1] _[\u2212]_ _[y]_ [2] _[\u03b1]_ [2] _[v]_ [2] _[\u03b1]_ [1] [ +] _[ \u03b1]_ [2] _[ \u2212]_ 2 _[K]_ [11] _[\u03b1]_ [2] 2 _[K]_ [22] _[\u03b1]_ [2] ~~\ufffd~~ ~~\ufffd~~ \ufffd ~~\ufffd~~ \u03a8 1 ( _\u03b1_ 1 _,\u03b1_ 2 ) subject to: 0 _\u2264_ _\u03b1_ 1 _, \u03b1_ 2 _\u2264_ _C \u2227_ _\u03b1_ 1 + _s\u03b1_ 2 = _\u03b3,_ _m_ where _\u03b3_ = _y_ 1 \ufffd _i_ =3 _[y]_ _[i]_ _[\u03b1]_ _[i]_ [,] _[ s]_ [ =] _[ y]_ [1] _[y]_ [2] _[ \u2208{\u2212]_ [1] _[,]_ [ +1] _[}]_ [,] _[ K]_ _[ij]_ [ = (] **[x]** _[i]_ _[ \u00b7]_ **[ x]** _[j]_ [) and] _[ v]_ _[i]_ [ =] \ufffd _mj_ =3 _[\u03b1]_ _[j]_ _[y]_ _[j]_ _[K]_ _[ij]_ [ for] _[ i]_ [ = 1] _[,]_ [ 2.] (b) Substitute the linear constraint _\u03b1_ 1 = _\u03b3 \u2212_ _s\u03b1_ 2 into \u03a8 1 to obtain a new objective function \u03a8 2 that depends only on _\u03b1_ 2 . Show that the _\u03b1_ 2 that maximizes \u03a8 2 (without the constraints 0 _\u2264_ _\u03b1_ 1 _, \u03b1_ 2 _\u2264_ _C_ ) can be expressed as _\u03b1_ 2 = _[s]_ [(] _[K]_ [11] _[ \u2212]_ _[K]_ [12] [)] _[\u03b3]_ [ +] _[y]_ [2] [(] _[v]_ [1] _[ \u2212]_ _[v]_ [2] [)] _[ \u2212]_ _[s]_ [ + 1] _,_ _\u03b7_ where _\u03b7_ = _K_ 11 + _K_ 22 _\u2212_ 2 _K_ 12 . **102** **Chapter 5** **Support Vector Machines** (c) Show that _v_ 1 _\u2212_ _v_ 2 = _f_ ( **x** 1 ) _\u2212_ _f_ ( **x** 2 ) + _\u03b1_",
    "chunk_id": "foundations_machine_learning_100"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "2 _[\u2217]_ _[y]_ [2] _[\u03b7][ \u2212]_ _[sy]_ [2] _[\u03b3]_ [(] _[K]_ [11] _[\u2212]_ _[K]_ [12] [)] where _f_ ( **x** ) = [\ufffd] _[m]_ _i_ =1 _[\u03b1]_ _i_ _[\u2217]_ _[y]_ _[i]_ [(] **[x]** _[i]_ _[ \u00b7]_ **[ x]** [) +] _[ b]_ _[\u2217]_ [and] _[ \u03b1]_ _i_ _[\u2217]_ [are values for the Lagrange] multipliers prior to optimization over _\u03b1_ 1 and _\u03b1_ 2 (similarly, _b_ _[\u2217]_ is the previous value for the offset). (d) Show that ( _y_ 2 _\u2212_ _f_ ( **x** 2 )) _\u2212_ ( _y_ 1 _\u2212_ _f_ ( **x** 1 )) _\u03b1_ 2 = _\u03b1_ 2 _[\u2217]_ [+] _[ y]_ [2] _._ _\u03b7_ (e) For _s_ = +1, define _L_ = max _{_ 0 _, \u03b3 \u2212_ _C}_ and _H_ = min _{C, \u03b3}_ as the lower and upper bounds on _\u03b1_ 2 . Similarly, for _s_ = _\u2212_ 1, define _L_ = max _{_ 0 _, \u2212\u03b3}_ and _H_ = min _{C, C \u2212_ _\u03b3}_ . The update rule for SMO involves \u201cclipping\u201d the value of _\u03b1_ 2, i.e., _\u03b1_ 2 _[clip]_ = \uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3 _\u03b1_ 2 if _L < \u03b1_ 2 _< H_ _L_ if _\u03b1_ 2 _\u2264_ _L_ _H_ if _\u03b1_ 2 _\u2265_ _H_ _._ We subsequently solve for _\u03b1_ 1 such that we satisfy the equality constraint, resulting in _\u03b1_ 1 = _\u03b1_ 1 _[\u2217]_ [+] _[ s]_ [(] _[\u03b1]_ 2 _[\u2217]_ _[\u2212]_ _[\u03b1]_ 2 _[clip]_ ). Why is \u201cclipping\u201d is required? How are _L_ and _H_ derived for the case _s_ = +1? 5.5 SVMs hands-on. (a) Download and install the `libsvm` software library from: `http://www.csie.ntu.edu.tw/~cjlin/libsvm/` . (b) Download the `satimage` data set found at: `http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/` . Merge the training and validation sets into one. We will refer to the resulting set as the training set from now on. Normalize both the training and test vectors. (c) Consider the binary classification that consists of distinguishing class 6 from the rest of the data points. Use SVMs combined with polynomial kernels (see chapter 6) to solve this classification problem. To do so, randomly split the training data into ten equal-sized disjoint sets. For each value of the polynomial degree, _d_ = 1 _,_ 2 _,_ 3 _,_ 4, plot the average cross-validation error plus or minus one standard deviation as a function of _C_ (let the other parameters of polynomial kernels in `libsvm`, _\u03b3_ and _c_, be equal to their default values 1). **5.6** **Exercises** **103** Report the best value of the trade-off constant _C_ measured on the validation set. (d) Let ( _C_ _[\u2217]_ _, d_ _[\u2217]_ ) be the best pair found previously. Fix _C_ to be _C_ _[\u2217]_ . Plot the ten-fold cross-validation training and test errors for the hypotheses obtained as a function of _d_ . Plot the average number of support vectors obtained as a function of _d_ . (e) How many of the support vectors lie on the margin hyperplanes? (f) In the standard two-group classification, errors on positive or negative points are treated in the same manner. Suppose, however, that we wish to penalize an error on a",
    "chunk_id": "foundations_machine_learning_101"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "negative point (false positive error) _k >_ 0 times more than an error on a positive point. Give the dual optimization problem corresponding to SVMs modified in this way. (g) Assume that _k_ is an integer. Show how you can use `libsvm` without writing any additional code to find the solution of the modified SVMs just described. (h) Apply the modified SVMs to the classification task previously examined and compare with your previous SVMs results for _k_ = 2 _,_ 4 _,_ 8 _,_ 16. 5.6 Sparse SVM. One can give two types of arguments in favor of the SVM algorithm: one based on the sparsity of the support vectors, another based on the notion of margin. Suppose that instead of maximizing the margin, we choose instead to maximize sparsity by minimizing the _L_ _p_ norm of the vector _**\u03b1**_ that defines the weight vector **w**, for some _p \u2265_ 1. First, consider the case _p_ = 2. This gives the following optimization problem: _m_ \ufffd _\u03be_ _i_ (5.50) _i_ =1 1 min _**\u03b1**_ _,b_ 2 _m_ \ufffd _\u03b1_ _i_ [2] [+] _[ C]_ _i_ =1 subject to _y_ _i_ \ufffd \ufffd _[m]_ _\u03b1_ _j_ _y_ _j_ **x** _i_ _\u00b7_ **x** _j_ + _b_ \ufffd _\u2265_ 1 _\u2212_ _\u03be_ _i_ _, i \u2208_ [ _m_ ] _j_ =1 _\u03be_ _i_ _, \u03b1_ _i_ _\u2265_ 0 _, i \u2208_ [ _m_ ] _._ (a) Show that modulo the non-negativity constraint on _**\u03b1**_, the problem coincides with an instance of the primal optimization problem of SVM. (b) Derive the dual optimization of problem of (5.50). (c) Setting _p_ = 1 will induce a more sparse _**\u03b1**_ . Derive the dual optimization in this case. 5.7 VC-dimension of canonical hyperplanes. The objective of this problem is derive a bound on the VC-dimension of canonical hyperplanes that does not depend on **104** **Chapter 5** **Support Vector Machines** the dimension of feature space. Let _S \u2286{_ **x** : _\u2225_ **x** _\u2225\u2264_ _r}_ . We will show that the VC-dimension _d_ of the set of canonical hyperplanes _{x \ufffd\u2192_ sgn( **w** _\u00b7_ **x** ): min _x\u2208S_ _|_ **w** _\u00b7_ **x** _|_ = 1 _\u2227\u2225_ **w** _\u2225\u2264_ \u039b _}_ verifies _d \u2264_ _r_ [2] \u039b [2] _._ (5.51) (a) Let _{_ **x** 1 _, . . .,_ **x** _d_ _}_ be a set that can be shattered. Show that for all **y** = ( _y_ 1 _, . . ., y_ _d_ ) _\u2208{\u2212_ 1 _,_ +1 _}_ _[d]_, _d \u2264_ \u039b _\u2225_ [\ufffd] _[d]_ _i_ =1 _[y]_ _[i]_ **[x]** _[i]_ _[\u2225]_ [.] (b) Use randomization over the labels **y** and Jensen\u2019s inequality to show that _d \u2264_ \u039b\ufffd\ufffd _di_ =1 _[\u2225]_ **[x]** _[i]_ _[\u2225]_ [2] [.] (c) Conclude that _d \u2264_ _r_ [2] \u039b [2] . # 6 Kernel Methods _Kernel methods_ are widely used in machine learning. They are flexible techniques that can be used to extend algorithms such as SVMs to define non-linear decision boundaries. Other algorithms that only depend on inner products between sample points can be extended similarly, many of which will be studied",
    "chunk_id": "foundations_machine_learning_102"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "in future chapters. The main idea behind these methods is based on so-called _kernels_ or _kernel func-_ _tions_, which, under some technical conditions of symmetry and _positive-definiteness_, implicitly define an inner product in a high-dimensional space. Replacing the original inner product in the input space with positive definite kernels immediately extends algorithms such as SVMs to a linear separation in that high-dimensional space, or, equivalently, to a non-linear separation in the input space. In this chapter, we present the main definitions and key properties of positive definite symmetric kernels, including the proof of the fact that they define an inner product in a Hilbert space, as well as their closure properties. We then extend the SVM algorithm using these kernels and present several theoretical results including general margin-based learning guarantees for hypothesis sets based on kernels. We also introduce _negative definite symmetric kernels_ and point out their relevance to the construction of positive definite kernels, in particular from distances or metrics. Finally, we illustrate the design of kernels for non-vectorial discrete structures by introducing a general family of kernels for sequences, _rational kernels_ . We describe an efficient algorithm for the computation of these kernels and illustrate them with several examples. **6.1** **Introduction** In the previous chapter, we presented an algorithm for linear classification, SVMs, which is both effective in applications and benefits from a strong theoretical justification. In practice, linear separation is often not possible. Figure 6.1a shows an example where any hyperplane crosses both populations. However, one can use **106** **Chapter 6** **Kernel Methods** **Image:** [No caption returned] **Image:** [No caption returned] (a) (b) **Figure 6.1** Non-linearly separable case. The classification task consists of discriminating between blue and red points. (a) No hyperplane can separate the two populations. (b) A non-linear mapping can be used instead. more complex functions to separate the two sets as in figure 6.1b. One way to define such a non-linear decision boundary is to use a non-linear mapping \u03a6 from the input space X to a higher-dimensional space H, where linear separation is possible (see figure 6.2). The dimension of H can truly be very large in practice. For example, in the case of document classification, one may wish to use as features sequences of three consecutive words, i.e., _trigrams_ . Thus, with a vocabulary of just 100 _,_ 000 words, the dimension of the feature space H reaches 10 [15] . On the positive side, the margin bounds presented in section 5.4 show that, remarkably, the generalization ability of large-margin classification algorithms such as SVMs do not depend on the dimension of the feature space, but only on the margin _\u03c1_ and the number of training examples _m_ . Thus, with a favorable margin _\u03c1_, such algorithms could succeed even in very high-dimensional space. However, determining the hyperplane solution requires multiple inner product computations in high-dimensional spaces, which can become be very costly. A solution to this problem is to use _kernel methods_, which are based on _kernels_ or _kernel functions_ . **Definition 6.1 (Kernels)** _A function K_ : X _\u00d7_ X _\u2192_ R",
    "chunk_id": "foundations_machine_learning_103"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_is called a_ kernel over X _._ The idea is to define a kernel _K_ such that for any two points _x, x_ _[\u2032]_ _\u2208_ X, _K_ ( _x, x_ _[\u2032]_ ) be equal to an inner product of vectors \u03a6( _x_ ) and \u03a6( _y_ ): [6] _\u2200x, x_ _[\u2032]_ _\u2208_ X _,_ _K_ ( _x, x_ _[\u2032]_ ) = _\u27e8_ \u03a6( _x_ ) _,_ \u03a6( _x_ _[\u2032]_ ) _\u27e9_ _,_ (6.1) 6 To differentiate that inner product from the one of the input space, we will typically denote it by _\u27e8\u00b7, \u00b7\u27e9_ . **6.1** **Introduction** **107** **Figure 6.2** |\u03a6|Col2| |---|---| ||| ||| **Image:** [No caption returned] An example of a non-linear mapping from 2-dimensions to 3-dimensions, where the task becomes linearly seperable. for some mapping \u03a6: X _\u2192_ H to a Hilbert space H called a _feature space_ . Since an inner product is a measure of the similarity of two vectors, _K_ is often interpreted as a similarity measure between elements of the input space X. An important advantage of such a kernel _K_ is efficiency: _K_ is often significantly more efficient to compute than \u03a6 and an inner product in H. We will see several common examples where the computation of _K_ ( _x, x_ _[\u2032]_ ) can be achieved in _O_ ( _N_ ) while that of _\u27e8_ \u03a6( _x_ ) _,_ \u03a6( _x_ _[\u2032]_ ) _\u27e9_ typically requires _O_ (dim(H)) work, with dim(H) _\u226b_ _N_ . Furthermore, in some cases, the dimension of H is infinite. Perhaps an even more crucial benefit of such a kernel function _K_ is flexibility: there is no need to explicitly define or compute a mapping \u03a6. The kernel _K_ can be arbitrarily chosen so long as the existence of \u03a6 is guaranteed, i.e. _K_ satisfies _Mercer\u2019s condition_ (see theorem 6.2). **Theorem 6.2 (Mercer\u2019s condition)** _Let_ X _\u2282_ R _[N]_ _be a compact set and let K_ : X _\u00d7_ X _\u2192_ R _be a continuous and symmetric function. Then, K admits a uniformly convergent_ _expansion of the form_ _K_ ( _x, x_ _[\u2032]_ ) = _\u221e_ \ufffd _a_ _n_ _\u03c6_ _n_ ( _x_ ) _\u03c6_ _n_ ( _x_ _[\u2032]_ ) _,_ _n_ =0 _with a_ _n_ _>_ 0 _iff for any square integrable function c (c \u2208_ _L_ 2 (X) _), the following_ _condition holds:_ _c_ ( _x_ ) _c_ ( _x_ _[\u2032]_ ) _K_ ( _x, x_ _[\u2032]_ ) _dxdx_ _[\u2032]_ _\u2265_ 0 _._ \ufffd\ufffd X _\u00d7_ X This condition is important to guarantee the convexity of the optimization problem for algorithms such as SVMs, thereby ensuring convergence to a global minimum. A condition that is equivalent to Mercer\u2019s condition under the assumptions of the theorem is that the kernel _K_ be _positive definite symmetric_ (PDS). This property **108** **Chapter 6** **Kernel Methods** is in fact more general since in particular it does not require any assumption about X. In the next section, we give the definition of this property and present several commonly used examples of PDS kernels, then show that PDS kernels induce an inner product in",
    "chunk_id": "foundations_machine_learning_104"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "a Hilbert space, and prove several general closure properties for PDS kernels. **6.2** **Positive definite symmetric kernels** **6.2.1** **Definitions** **Definition 6.3 (Positive definite symmetric kernels)** _A kernel K_ : X _\u00d7_ X _\u2192_ R _is said to_ _be_ positive definite symmetric _(PDS) if for any {x_ 1 _, . . ., x_ _m_ _} \u2286_ X _, the matrix_ **K** = [ _K_ ( _x_ _i_ _, x_ _j_ )] _ij_ _\u2208_ R _[m][\u00d7][m]_ _is symmetric positive semidefinite (SPSD)._ **K** is SPSD if it is symmetric and one of the following two equivalent conditions holds: _\u2022_ the eigenvalues of **K** are non-negative; _\u2022_ for any column vector **c** = ( _c_ 1 _, . . ., c_ _m_ ) _[\u22a4]_ _\u2208_ R _[m][\u00d7]_ [1], **c** _[\u22a4]_ **Kc** = _m_ \ufffd _c_ _i_ _c_ _j_ _K_ ( _x_ _i_ _, x_ _j_ ) _\u2265_ 0 _._ (6.2) _i,j_ =1 For a sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ), **K** = [ _K_ ( _x_ _i_ _, x_ _j_ )] _ij_ _\u2208_ R _[m][\u00d7][m]_ is called the _kernel_ _matrix_ or the _Gram matrix_ associated to _K_ and the sample _S_ . Let us insist on the terminology: the kernel matrix associated to a _positive defi-_ _nite kernel_ is _positive semidefinite_ . This is the correct mathematical terminology. Nevertheless, the reader should be aware that in the context of machine learning, some authors have chosen to use instead the term _positive definite kernel_ to imply a _positive definite_ kernel matrix or used new terms such as _positive semidefinite_ _kernel_ . The following are some standard examples of PDS kernels commonly used in applications. **Example 6.4 (Polynomial kernels)** For any constant _c >_ 0, a _polynomial kernel of de-_ _gree d \u2208_ N is the kernel _K_ defined over R _[N]_ by: _\u2200_ **x** _,_ **x** _[\u2032]_ _\u2208_ R _[N]_ _,_ _K_ ( **x** _,_ **x** _[\u2032]_ ) = ( **x** _\u00b7_ **x** _[\u2032]_ + _c_ ) _[d]_ _._ (6.3) Polynomial kernels map the input space to a higher-dimensional space of dimension \ufffd _Nd_ + _d_ \ufffd (see exercise 6.12). As an example, for an input space of dimension _N_ = 2, a second-degree polynomial ( _d_ = 2) corresponds to the following inner product in **6.2** **Positive definite symmetric kernels** **109** 2 x 1 x 2 _x_ 2 **Image:** [No caption returned] (1, 1, \u2212\u221a ~~\u221a~~ (1, 1, +\u221a 2, + `\u221a` 2, 1) ~~\u221a~~ 2 x `1` 2, \u2212\u221a 2, 1) **Image:** [No caption returned] (a) (b) **Figure 6.3** Illustration of the XOR classification problem and the use of polynomial kernels. (a) XOR problem linearly non-separable in the input space. (b) Linearly separable using second-degree polynomial kernel. dimension 6: _._ (6.4) _x_ _[\u2032]_ [2] 1 _x_ _[\u2032]_ [2] 2 _\u221a_ 2 _x_ _[\u2032]_ \uf8f9 _\u2200_ **x** _,_ **x** _[\u2032]_ _\u2208_ R [2] _,_ _K_ ( **x** _,_ **x** _[\u2032]_ ) = ( _x_ 1 _x_ _[\u2032]_ 1 [+] _[ x]_ [2] _[x]_ 2 _[\u2032]_ [+] _[ c]_ [)] [2] [ =] \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 _x_ [2] 1 _x_ [2]",
    "chunk_id": "foundations_machine_learning_105"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "2 _\u221a_ 2 _x_ 2 _x_ 1 _x_ 2 _\u221a_ 2 _c x_ 1 _\u221a_ 2 _c x_ 2 _\u00b7_ \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 2 _x_ _[\u2032]_ 1 _[x]_ _[\u2032]_ 2 _\u221a_ 2 _c x_ _[\u2032]_ 1 _\u221a_ 2 _c x_ _[\u2032]_ \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb 2 _c x_ 2 _c_ \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb 2 _c x_ _[\u2032]_ 2 _c_ Thus, the features corresponding to a second-degree polynomial are the original features ( _x_ 1 and _x_ 2 ), as well as products of these features, and the constant feature. More generally, the features associated to a polynomial kernel of degree _d_ are all the monomials of degree at most _d_ based on the original features. The explicit expression of polynomial kernels as inner products, as in (6.4), proves directly that they are PDS kernels. To illustrate the application of polynomial kernels, consider the example of figure 6.3a which shows a simple data set in dimension two that is not linearly separable. This is known as the XOR problem due to its interpretation in terms of the exclusive OR (XOR) function: the label of a point is blue iff exactly one of its coordinates is 1. However, if we map these points to the six-dimensional space defined by a second-degree polynomial as described in (6.4), then the problem becomes separable by the hyperplane of equation _x_ 1 _x_ 2 = 0. Figure 6.3b illustrates that by showing the projection of these points on the two-dimensional space defined by their third and fourth coordinates. **110** **Chapter 6** **Kernel Methods** **Example 6.5 (Gaussian kernels)** For any constant _\u03c3 >_ 0, a _Gaussian kernel_ or _radial_ _basis function (RBF)_ is the kernel _K_ defined over R _[N]_ by: _\u2200_ **x** _,_ **x** _[\u2032]_ _\u2208_ R _[N]_ _,_ _K_ ( **x** _,_ **x** _[\u2032]_ ) = exp _\u2212_ _[\u2225]_ **[x]** _[\u2032]_ _[ \u2212]_ **[x]** _[\u2225]_ [2] \ufffd 2 _\u03c3_ [2] _._ (6.5) \ufffd Gaussian kernels are among the most frequently used kernels in applications. We will prove in section 6.2.3 that they are PDS kernels and that they can be derived by _normalization_ from the kernels _K_ _[\u2032]_ : ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ exp \ufffd **x** _\u03c3\u00b7_ **x** [2] _\u2032_ \ufffd. Using the power series ex pansion of the exponential function, we can rewrite the expression of _K_ _[\u2032]_ as follows: _\u2200_ **x** _,_ **x** _[\u2032]_ _\u2208_ R _[N]_ _,_ _K_ _[\u2032]_ ( **x** _,_ **x** _[\u2032]_ ) = + _\u221e_ \ufffd _n_ =0 ( **x** _\u00b7_ **x** _[\u2032]_ ) _[n]_ _\u03c3_ [2] _[n]_ _n_ ! _[,]_ which shows that the kernels _K_ _[\u2032]_, and thus Gaussian kernels, are positive linear combinations of polynomial kernels of all degrees _n \u2265_ 0. **Example 6.6 (Sigmoid kernels)** For any real constants _a, b \u2265_ 0, a _sigmoid kernel_ is the kernel _K_ defined over R _[N]_ by: _\u2200_ **x** _,_ **x** _[\u2032]_ _\u2208_ R _[N]_ _,_ _K_ ( **x** _,_ **x** _[\u2032]_ ) = tanh \ufffd _a_ ( **x** _\u00b7_ **x** _[\u2032]_ ) + _b_ \ufffd _._ (6.6) Using sigmoid kernels with SVMs leads to an algorithm that is closely",
    "chunk_id": "foundations_machine_learning_106"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "related to learning algorithms based on simple neural networks, which are also often defined via a sigmoid function. When _a <_ 0 or _b <_ 0, the kernel is not PDS and the corresponding neural network does not benefit from the convergence guarantees of convex optimization (see exercise 6.18). **6.2.2** **Reproducing kernel Hilbert space** Here, we prove the crucial property of PDS kernels, which is to induce an inner product in a Hilbert space. The proof will make use of the following lemma. **Lemma 6.7 (Cauchy-Schwarz inequality for PDS kernels)** _Let K be a PDS kernel. Then,_ _for any x, x_ _[\u2032]_ _\u2208_ X _,_ _K_ ( _x, x_ _[\u2032]_ ) [2] _\u2264_ _K_ ( _x, x_ ) _K_ ( _x_ _[\u2032]_ _, x_ _[\u2032]_ ) _._ (6.7) _K_ ( _x,x_ ) _K_ ( _x,x_ _[\u2032]_ ) Proof: Consider the matrix **K** = \ufffd _K_ ( _x_ _[\u2032]_ _,x_ ) _K_ ( _x_ _[\u2032]_ _,x_ _[\u2032]_ ) \ufffd. By definition, if _K_ is PDS, then **K** is SPSD for all _x, x_ _[\u2032]_ _\u2208_ X. In particular, the product of the eigenvalues of **K**, det( **K** ), must be non-negative, thus, using _K_ ( _x_ _[\u2032]_ _, x_ ) = _K_ ( _x, x_ _[\u2032]_ ), we have det( **K** ) = _K_ ( _x, x_ ) _K_ ( _x_ _[\u2032]_ _, x_ _[\u2032]_ ) _\u2212_ _K_ ( _x, x_ _[\u2032]_ ) [2] _\u2265_ 0 _,_ which concludes the proof. The following is the main result of this section. **Theorem 6.8 (Reproducing kernel Hilbert space (RKHS) )** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS_ _kernel. Then, there exists a Hilbert space_ H _(see definition A.2) and a mapping_ \u03a6 **6.2** **Positive definite symmetric kernels** **111** _from_ X _to_ H _such that:_ _\u2200x, x_ _[\u2032]_ _\u2208_ X _,_ _K_ ( _x, x_ _[\u2032]_ ) = _\u27e8_ \u03a6( _x_ ) _,_ \u03a6( _x_ _[\u2032]_ ) _\u27e9_ _._ (6.8) _Furthermore,_ H _has the following property known as the_ reproducing property _:_ _\u2200h \u2208_ H _, \u2200x \u2208_ X _,_ _h_ ( _x_ ) = _\u27e8h, K_ ( _x, \u00b7_ ) _\u27e9_ _._ (6.9) H _is called a_ reproducing kernel Hilbert space _(RKHS) associated to K._ Proof: For any _x \u2208_ X, define \u03a6( _x_ ): X _\u2192_ R [X] as follows: _\u2200x_ _[\u2032]_ _\u2208_ X _,_ \u03a6( _x_ )( _x_ _[\u2032]_ ) = _K_ ( _x, x_ _[\u2032]_ ) _._ We define H 0 as the set of finite linear combinations of such functions \u03a6( _x_ ): H 0 = _a_ _i_ \u03a6( _x_ _i_ ): _a_ _i_ _\u2208_ R _, x_ _i_ _\u2208_ X _, |I| < \u221e_ _._ \ufffd\ufffd _i\u2208I_ \ufffd Now, we introduce an operation _\u27e8\u00b7, \u00b7\u27e9_ on H 0 _\u00d7_ H 0 defined for all _f, g \u2208_ H 0 with _f_ = [\ufffd] _i_ _I_ _[a]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [) and] _[ g]_ [ =][ \ufffd] _J_ _[b]_ _[j]_ [\u03a6(] _[x]_ _[\u2032]_ [) by] _j\u2208J_ _[b]_ _[j]_ [\u03a6(] _[x]_ _[\u2032]_ _j_ [) by] _i\u2208I_ _[a]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [) and] _[ g]_ [ =][",
    "chunk_id": "foundations_machine_learning_107"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd] \ufffd _a_ _i_ _b_ _j_ _K_ ( _x_ _i_ _, x_ _[\u2032]_ _j_ [) =] \ufffd _i\u2208I,j\u2208J_ _j\u2208J_ = _\u27e8f, g\u27e9_ \ufffd \ufffd _b_ _j_ _f_ ( _x_ _[\u2032]_ _j_ [) =] \ufffd _j\u2208J_ _i\u2208I_ _a_ _i_ _g_ ( _x_ _i_ ) _._ _i\u2208I_ By definition, _\u27e8\u00b7, \u00b7\u27e9_ is symmetric. The last two equations show that _\u27e8f, g\u27e9_ does not depend on the particular representations of _f_ and _g_, and also show that _\u27e8\u00b7, \u00b7\u27e9_ is bilinear. Further, for any _f_ = [\ufffd] _i\u2208I_ _[a]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [)] _[ \u2208]_ [H] [0] [, since] _[ K]_ [ is PDS, we have] _\u27e8f, f_ _\u27e9_ = \ufffd _a_ _i_ _a_ _j_ _K_ ( _x_ _i_ _, x_ _j_ ) _\u2265_ 0 _._ _i,j\u2208I_ Thus, _\u27e8\u00b7, \u00b7\u27e9_ is positive semidefinite bilinear form. This inequality implies more generally using the bilinearity of _\u27e8\u00b7, \u00b7\u27e9_ that for any _f_ 1 _, . . ., f_ _m_ and _c_ 1 _, . . ., c_ _m_ _\u2208_ R, _m_ \ufffd _c_ _i_ _c_ _j_ _\u27e8f_ _i_ _, f_ _j_ _\u27e9_ = \ufffd \ufffd _[m]_ _i,j_ =1 _i_ =1 _m_ \ufffd _c_ _i_ _f_ _i_ _,_ _i_ =1 _m_ \ufffd _c_ _j_ _f_ _j_ \ufffd _\u2265_ 0 _._ _j_ =1 Hence, _\u27e8\u00b7, \u00b7\u27e9_ is a PDS kernel on H 0 . Thus, for any _f \u2208_ H 0 and any _x \u2208_ X, by lemma 6.7, we can write _\u27e8f,_ \u03a6( _x_ ) _\u27e9_ [2] _\u2264\u27e8f, f_ _\u27e9\u27e8_ \u03a6( _x_ ) _,_ \u03a6( _x_ ) _\u27e9._ Further, we observe the reproducing property of _\u27e8\u00b7, \u00b7\u27e9_ : for any _f_ = [\ufffd] _i\u2208I_ _[a]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [)] _[ \u2208]_ H 0, by definition of _\u27e8\u00b7, \u00b7\u27e9_, _\u2200x \u2208_ X _,_ _f_ ( _x_ ) = \ufffd _a_ _i_ _K_ ( _x_ _i_ _, x_ ) = _\u27e8f,_ \u03a6( _x_ ) _\u27e9_ _._ (6.10) _i\u2208I_ **112** **Chapter 6** **Kernel Methods** Thus, [ _f_ ( _x_ )] [2] _\u2264\u27e8f, f_ _\u27e9K_ ( _x, x_ ) for all _x \u2208_ X, which shows the definiteness of _\u27e8\u00b7, \u00b7\u27e9_ . This implies that _\u27e8\u00b7, \u00b7\u27e9_ defines an inner product on H 0, which thereby becomes a pre-Hilbert space. H 0 can be completed to form a Hilbert space H in which it is dense, following a standard construction. By the Cauchy-Schwarz inequality, for any _x \u2208_ X, _f \ufffd\u2192\u27e8f,_ \u03a6( _x_ ) _\u27e9_ is Lipschitz, therefore continuous. Thus, since H 0 is dense in H, the reproducing property (6.10) also holds over H. The Hilbert space H defined in the proof of the theorem for a PDS kernel _K_ is called _the reproducing kernel Hilbert space (RKHS) associated to K_ . Any Hilbert space H such that there exists \u03a6: X _\u2192_ H with _K_ ( _x, x_ _[\u2032]_ ) = _\u27e8_ \u03a6( _x_ ) _,_ \u03a6( _x_ _[\u2032]_ ) _\u27e9_ for all _x, x_ _[\u2032]_ _\u2208_ X is called a _feature space_ associated to _K_ and \u03a6 is called a _feature mapping_ . We will denote by _\u2225_ _\u00b7 \u2225_ H the norm induced by the inner",
    "chunk_id": "foundations_machine_learning_108"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "product in feature space H: _\u2225_ **w** _\u2225_ H = \ufffd _\u27e8_ **w** _,_ **w** _\u27e9_ for all **w** _\u2208_ H. Note that the feature spaces associated to _K_ are in general not unique and may have different dimensions. In practice, when referring to the _dimension of the feature space_ associated to _K_, we either refer to the dimension of the feature space based on a feature mapping described explicitly, or to that of the RKHS associated to _K_ . Theorem 6.8 implies that PDS kernels can be used to implicitly define a feature space or feature vectors. As already underlined in previous chapters, the role played by the features in the success of learning algorithms is crucial: with poor features, uncorrelated with the target labels, learning could become very challenging or even impossible; in contrast, good features could provide invaluable clues to the algorithm. Therefore, in the context of learning with PDS kernels and for a fixed input space, the problem of seeking useful features is replaced by that of finding useful PDS kernels. While features represented the user\u2019s prior knowledge about the task in the standard learning problems, here PDS kernels will play this role. Thus, in practice, an appropriate choice of PDS kernel for a task will be crucial. **6.2.3** **Properties** This section highlights several important properties of PDS kernels. We first show that PDS kernels can be _normalized_ and that the resulting normalized kernels are also PDS. We also introduce the definition of _empirical kernel maps_ and describe their properties and extension. We then prove several important closure properties of PDS kernels, which can be used to construct complex PDS kernels from simpler ones. To any kernel _K_, we can associate a _normalized kernel K_ _[\u2032]_ defined by _\u2200x, x_ _[\u2032]_ _\u2208_ X _,_ _K_ _[\u2032]_ ( _x, x_ _[\u2032]_ ) = \uf8f1 \uf8f2 \uf8f3 0 if ( _K_ ( _x, x_ ) = 0) _\u2228_ ( _K_ ( _x_ _[\u2032]_ _, x_ _[\u2032]_ ) = 0) _K_ ( _x,x_ _[\u2032]_ ) otherwise _._ _\u221a_ _K_ ( _x,x_ ) _K_ ( _x_ _[\u2032]_ _,x_ _[\u2032]_ ) _K_ ( _x,x_ _[\u2032]_ ) _\u221a_ _K_ ( _x,x_ ) _K_ ( (6.11) **6.2** **Positive definite symmetric kernels** **113** By definition, for a normalized kernel _K_ _[\u2032]_, _K_ _[\u2032]_ ( _x, x_ ) = 1 for all _x \u2208_ X such that _K_ ( _x, x_ ) _\u0338_ = 0. An example of normalized kernel is the Gaussian kernel with parameter _\u03c3 >_ 0, which is the normalized kernel associated to _K_ _[\u2032]_ : ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ exp \ufffd **x** _\u03c3\u00b7_ **x** [2] _\u2032_ \ufffd: **x** _\u00b7_ **x** _[\u2032]_ _K_ _[\u2032]_ ( **x** _,_ **x** _[\u2032]_ ) = _e_ _\u03c3_ [2] _K_ _[\u2032]_ ( **x** _,_ **x** ) _K_ _[\u2032]_ ( **x** _[\u2032]_ _,_ **x** _[\u2032]_ ) _\u2225_ 2 **x** _\u03c3\u2225_ [2][2] _\u2225_ _K_ _[\u2032]_ ( **x** _,_ **x** _[\u2032]_ ) _\u2200_ **x** _,_ **x** _[\u2032]_ _\u2208_ R _[N]_ _,_ ~~\ufffd~~ _K_ _[\u2032]_ ( **x** _,_ **x** ) _K_ _[\u2032]_ ( _e_ _\u03c3_ [2] _\u2225_ **x** _\u2225_ [2]",
    "chunk_id": "foundations_machine_learning_109"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2225_ **x** _[\u2032]_ _\u2225_ [2] 2 _\u03c3_ [2] _._ (6.12) \ufffd _\u2212_ _[\u2225]_ **[x]** _[\u2032]_ _[ \u2212]_ **[x]** _[\u2225]_ [2] 2 **x** _\u03c3_ _[\u2032]_ _\u2225_ [2][2] = exp \ufffd 2 _\u03c3_ [2] 2 _\u03c3_ [2] _e_ **Lemma 6.9 (Normalized PDS kernels)** _Let K be a PDS kernel. Then, the normalized_ _kernel K_ _[\u2032]_ _associated to K is PDS._ Proof: Let _{x_ 1 _, . . ., x_ _m_ _} \u2286_ X and let **c** be an arbitrary vector in R _[m]_ . We will show that the sum [\ufffd] _[m]_ _i,j_ =1 _[c]_ _[i]_ _[c]_ _[j]_ _[K]_ _[\u2032]_ [(] _[x]_ _[i]_ _[, x]_ _[j]_ [) is non-negative. By lemma 6.7, if] _[ K]_ [(] _[x]_ _[i]_ _[, x]_ _[i]_ [) = 0] then _K_ ( _x_ _i_ _, x_ _j_ ) = 0 and thus _K_ _[\u2032]_ ( _x_ _i_ _, x_ _j_ ) = 0 for all _j \u2208_ [ _m_ ]. Thus, we can assume that _K_ ( _x_ _i_ _, x_ _i_ ) _>_ 0 for all _i \u2208_ [ _m_ ]. Then, the sum can be rewritten as follows: 2 _m_ \ufffd _i,j_ =1 \ufffd\ufffd\ufffd\ufffd\ufffd _\u2265_ 0 _,_ H _c_ _i_ _c_ _j_ _K_ ( _x_ _i_ _, x_ _j_ ) = \ufffd _K_ ( _x_ _i_ _, x_ _i_ ) _K_ ( _x_ _j_ _, x_ _j_ ) _m_ \ufffd _i,j_ =1 _c_ _i_ _c_ _j_ _\u27e8_ \u03a6( _x_ _i_ ) _,_ \u03a6( _x_ _j_ ) _\u27e9_ = _\u2225_ \u03a6( _x_ _i_ ) _\u2225_ H _\u2225_ \u03a6( _x_ _j_ ) _\u2225_ H _m_ \ufffd \ufffd\ufffd\ufffd\ufffd\ufffd _i_ =1 _c_ _i_ \u03a6( _x_ _i_ ) _\u2225_ \u03a6( _x_ _i_ ) _\u2225_ H where \u03a6 is a feature mapping associated to _K_, which exists by theorem 6.8. As indicated earlier, PDS kernels can be interpreted as a similarity measure since they induce an inner product in some Hilbert space H. This is more evident for a normalized kernel _K_ since _K_ ( _x, x_ _[\u2032]_ ) is then exactly the cosine of the angle between the feature vectors \u03a6( _x_ ) and \u03a6( _x_ _[\u2032]_ ), provided that none of them is zero: \u03a6( _x_ ) and \u03a6( _x_ _[\u2032]_ ) are then unit vectors since _\u2225_ \u03a6( _x_ ) _\u2225_ H = _\u2225_ \u03a6( _x_ _[\u2032]_ ) _\u2225_ H = \ufffd _K_ ( _x, x_ ) = 1. While one of the advantages of PDS kernels is an implicit definition of a feature mapping, in some instances, it may be desirable to define an explicit feature mapping based on a PDS kernel. This may be to work in the primal for various optimization and computational reasons, to derive an approximation based on an explicit mapping, or as part of a theoretical analysis where an explicit mapping is more convenient. The _empirical kernel map_ \u03a6 associated to a PDS kernel _K_ is a feature mapping that can be used precisely in such contexts. Given a training sample containing points _x_ 1 _, . . ., x_ _m_ _\u2208_ X, \u03a6: X _\u2192_ R _[m]_ is defined for all _x \u2208_",
    "chunk_id": "foundations_machine_learning_110"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "X by _._ \uf8fa\uf8fa\uf8fb \uf8f9 \u03a6( _x_ ) = \uf8ee \uf8ef\uf8ef\uf8f0 _K_ ( _x, x_ 1 ) ... _K_ ( _x, x_ _m_ ) Thus, \u03a6( _x_ ) is the vector of the _K_ -similarity measures of _x_ with each of the training points. Let **K** be the kernel matrix associated to _K_ and **e** _i_ the _i_ th unit vector. Note that for any _i \u2208_ [ _m_ ], \u03a6( _x_ _i_ ) is the _i_ th column of **K**, that is \u03a6( _x_ _i_ ) = **Ke** _i_ . In **114** **Chapter 6** **Kernel Methods** particular, for all _i, j \u2208_ [ _m_ ], _\u27e8_ \u03a6( _x_ _i_ ) _,_ \u03a6( _x_ _j_ ) _\u27e9_ = ( **Ke** _i_ ) _[\u22a4]_ ( **Ke** _j_ ) = **e** _[\u22a4]_ _i_ **[K]** [2] **[e]** _[j]_ _[.]_ Thus, the kernel matrix **K** _[\u2032]_ associated to \u03a6 is **K** [2] . It may desirable in some cases 1 to define a feature mapping whose kernel matrix coincides with **K** . Let 1 **K** _[\u2020]_ 2 denote the SPSD matrix whose square is **K** _[\u2020]_, the pseudo-inverse of **K** . **K** _[\u2020]_ 2 can be derived 1 from **K** _[\u2020]_ via singular value decomposition and if the matrix **K** is invertible, **K** _[\u2020]_ 2 coincides with **K** _[\u2212]_ [1] _[/]_ [2] (see appendix A for properties of the pseudo-inverse). Then, \u03a8 can be defined as follows using the empirical kernel map \u03a6: 1 _\u2200x \u2208_ X _,_ \u03a8( _x_ ) = **K** _[\u2020]_ 2 \u03a6( _x_ ) _._ Using the identity **KK** _[\u2020]_ **K** = **K** valid for any symmetric matrix **K**, for all _i, j \u2208_ [ _m_ ], the following holds: 1 1 _\u27e8_ \u03a8( _x_ _i_ ) _,_ \u03a8( _x_ _j_ ) _\u27e9_ = ( **K** _[\u2020]_ 2 **Ke** _i_ ) _\u22a4_ ( **K** _\u2020_ 2 **Ke** _j_ ) = **e** _\u22a4i_ **[KK]** _[\u2020]_ **[Ke]** _[j]_ [=] **[ e]** _[\u22a4]_ _i_ **[Ke]** _[j]_ _[.]_ Thus, the kernel matrix associated to \u03a8 is **K** . Finally, note that for the feature mapping \u2126: X _\u2192_ R _[m]_ defined by _\u2200x \u2208_ X _,_ \u2126( _x_ ) = **K** _[\u2020]_ \u03a6( _x_ ) _,_ for all _i, j \u2208_ [ _m_ ], we have _\u27e8_ \u2126( _x_ _i_ ) _,_ \u2126( _x_ _j_ ) _\u27e9_ = **e** _[\u22a4]_ _i_ **[KK]** _[\u2020]_ **[K]** _[\u2020]_ **[Ke]** _[j]_ [ =] **[ e]** _[\u22a4]_ _i_ **[KK]** _[\u2020]_ **[e]** _[j]_ [, using the] identity **K** _[\u2020]_ **K** _[\u2020]_ **K** = **K** _[\u2020]_ valid for any symmetric matrix **K** . Thus, the kernel matrix associated to \u2126is **KK** _[\u2020]_, which reduces to the identity matrix **I** _\u2208_ R _[m][\u00d7][m]_ when **K** is invertible, since **K** _[\u2020]_ = **K** _[\u2212]_ [1] in that case. As pointed out in the previous section, kernels represent the user\u2019s prior knowledge about a task. In some cases, a user may come up with appropriate similarity measures or PDS kernels for some subtasks \u2014 for example, for different subcategories of proteins or text documents to classify. But how can the user combine these PDS kernels to",
    "chunk_id": "foundations_machine_learning_111"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "form a PDS kernel for the entire class? Is the resulting combined kernel guaranteed to be PDS? In the following, we will show that PDS kernels are closed under several useful operations which can be used to design complex PDS kernels. These operations are the sum and the product of kernels, as well as the _tensor product_ of two kernels _K_ and _K_ _[\u2032]_, denoted by _K \u2297_ _K_ _[\u2032]_ and defined by _\u2200x_ 1 _, x_ 2 _, x_ _[\u2032]_ 1 _[, x]_ _[\u2032]_ 2 _[\u2208]_ [X] _[,]_ ( _K \u2297_ _K_ _[\u2032]_ )( _x_ 1 _, x_ _[\u2032]_ 1 _[, x]_ [2] _[, x]_ _[\u2032]_ 2 [) =] _[ K]_ [(] _[x]_ [1] _[, x]_ [2] [)] _[K]_ _[\u2032]_ [(] _[x]_ _[\u2032]_ 1 _[, x]_ _[\u2032]_ 2 [)] _[.]_ They also include the pointwise limit: given a sequence of kernels ( _K_ _n_ ) _n\u2208_ N such that for all _x, x_ _[\u2032]_ _\u2208_ X ( _K_ _n_ ( _x, x_ _[\u2032]_ )) _n\u2208_ N admits a limit, the pointwise limit of ( _K_ _n_ ) _n\u2208_ N is the kernel _K_ defined for all _x, x_ _[\u2032]_ _\u2208_ X by _K_ ( _x, x_ _[\u2032]_ ) = lim _n\u2192_ + _\u221e_ ( _K_ _n_ )( _x, x_ _[\u2032]_ ). Similarly, if [\ufffd] _[\u221e]_ _n_ =0 _[a]_ _[n]_ _[x]_ _[n]_ [ is a power series with radius of convergence] _[ \u03c1 >]_ [ 0 and] _[ K]_ [ a kernel] taking values in ( _\u2212\u03c1,_ + _\u03c1_ ), then [\ufffd] _[\u221e]_ _n_ =0 _[a]_ _[n]_ _[K]_ _[n]_ [ is the kernel obtained by composition] **6.2** **Positive definite symmetric kernels** **115** of _K_ with that power series. The following theorem provides closure guarantees for all of these operations. **Theorem 6.10 (PDS kernels \u2014 closure properties)** _PDS kernels are closed under sum,_ _product, tensor product, pointwise limit, and composition with a power series_ \ufffd _\u221en_ =0 _[a]_ _[n]_ _[x]_ _[n]_ _[ with][ a]_ _[n]_ _[ \u2265]_ [0] _[ for all][ n][ \u2208]_ [N] _[.]_ Proof: We start with two kernel matrices, **K** and **K** _[\u2032]_, generated from PDS kernels _K_ and _K_ _[\u2032]_ for an arbitrary set of _m_ points. By assumption, these kernel matrices are SPSD. Observe that for any **c** _\u2208_ R _[m][\u00d7]_ [1], ( **c** _[\u22a4]_ **Kc** _\u2265_ 0) _\u2227_ ( **c** _[\u22a4]_ **K** _[\u2032]_ **c** _\u2265_ 0) _\u21d2_ **c** _[\u22a4]_ ( **K** + **K** _[\u2032]_ ) **c** _\u2265_ 0 _._ By (6.2), this shows that **K** + **K** _[\u2032]_ is SPSD and thus that _K_ + _K_ _[\u2032]_ is PDS. To show closure under product, we will use the fact that for any SPSD matrix **K** there exists **M** such that **K** = **MM** _[\u22a4]_ . The existence of **M** is guaranteed as it can be generated via, for instance, singular value decomposition of **K**, or by Cholesky decomposition. The kernel matrix associated to _KK_ _[\u2032]_ is ( **K** _ij_ **K** _[\u2032]_ _ij_ [)] _[ij]_ [. For any] **[ c]** _[ \u2208]_ [R] _[m][\u00d7]_ [1] [, expressing] **K** _ij_ in terms of the entries of **M**, we can",
    "chunk_id": "foundations_machine_learning_112"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "write _m_ \ufffd _c_ _i_ _c_ _j_ ( **K** _ij_ **K** _[\u2032]_ _ij_ [) =] _i,j_ =1 = = _m_ \ufffd _c_ _i_ _c_ _j_ _i,j_ =1 _m_ \ufffd **z** _[\u22a4]_ _k_ **[K]** _[\u2032]_ **[z]** _[k]_ _[\u2265]_ [0] _[,]_ _k_ =1 _m_ \ufffd\ufffd _k_ \ufffd =1 **M** _ik_ **M** _jk_ \ufffd **K** _[\u2032]_ _ij_ \ufffd _m_ \ufffd _k_ =1 _m_ \ufffd _c_ _i_ _c_ _j_ **M** _ik_ **M** _jk_ **K** _[\u2032]_ _ij_ \ufffd _i,j_ =1 \ufffd _c_ 1 **M** 1 _k_ with **z** _k_ = ... \ufffd _c_ _m_ **M** _mk_ . This shows that PDS kernels are closed under product. \ufffd The tensor product of _K_ and _K_ _[\u2032]_ is PDS as the product of the two PDS kernels ( _x_ 1 _, x_ _[\u2032]_ 1 _[, x]_ [2] _[, x]_ _[\u2032]_ 2 [)] _[ \ufffd\u2192]_ _[K]_ [(] _[x]_ [1] _[, x]_ [2] [) and (] _[x]_ [1] _[, x]_ _[\u2032]_ 1 _[, x]_ [2] _[, x]_ _[\u2032]_ 2 [)] _[ \ufffd\u2192]_ _[K]_ _[\u2032]_ [(] _[x]_ _[\u2032]_ 1 _[, x]_ _[\u2032]_ 2 [). Next, let (] _[K]_ _[n]_ [)] _[n][\u2208]_ [N] be a sequence of PDS kernels with pointwise limit _K_ . Let **K** be the kernel matrix associated to _K_ and **K** _n_ the one associated to _K_ _n_ for any _n \u2208_ N. Observe that ( _\u2200n,_ **c** _[\u22a4]_ **K** _n_ **c** _\u2265_ 0) _\u21d2_ lim _n\u2192\u221e_ **[c]** _[\u22a4]_ **[K]** _[n]_ **[c]** [ =] **[ c]** _[\u22a4]_ **[Kc]** _[ \u2265]_ [0] _[.]_ This shows the closure under pointwise limit. Finally, assume that _K_ is a PDS kernel with _|K_ ( _x, x_ _[\u2032]_ ) _| < \u03c1_ for all _x, x_ _[\u2032]_ _\u2208_ X and let _f_ : _x \ufffd\u2192_ [\ufffd] _[\u221e]_ _n_ =0 _[a]_ _[n]_ _[x]_ _[n]_ _[, a]_ _[n]_ _[ \u2265]_ [0 be a] power series with radius of convergence _\u03c1_ . Then, for any _n \u2208_ N, _K_ _[n]_ and thus _a_ _n_ _K_ _[n]_ are PDS by closure under product. For any _N \u2208_ N, [\ufffd] _[N]_ _n_ =0 _[a]_ _[n]_ _[K]_ _[n]_ [ is PDS by closure] under sum of _a_ _n_ _K_ _[n]_ s and _f \u25e6_ _K_ is PDS by closure under the limit of [\ufffd] _[N]_ _n_ =0 _[a]_ _[n]_ _[K]_ _[n]_ as _N_ tends to infinity. **116** **Chapter 6** **Kernel Methods** The theorem implies in particular that for any PDS kernel matrix _K_, exp( _K_ ) is PDS, since the radius of convergence of exp is infinite. In particular, the kernel _K_ _[\u2032]_ : ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ exp \ufffd **x** _\u03c3\u00b7_ **x** [2] _\u2032_ \ufffd is PDS since ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ **[x]** _\u03c3_ _[\u00b7]_ **[x]** [2] _[\u2032]_ is PDS. Thus, by lemma 6.9, _\u03c3\u00b7_ **x** [2] _\u2032_ \ufffd is PDS since ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ **[x]** _\u03c3_ _[\u00b7]_ **[x]** [2] _[\u2032]_ _K_ _[\u2032]_ : ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ exp \ufffd **x** _\u03c3\u00b7_ **x** [2] \ufffd is PDS since ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ **[x]** _\u03c3_ _[\u00b7]_ **[x]** [2] is PDS. Thus, by lemma 6.9,",
    "chunk_id": "foundations_machine_learning_113"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "this shows that a Gaussian kernel, which is the normalized kernel associated to _K_ _[\u2032]_, is PDS. **6.3** **Kernel-based algorithms** In this section we discuss how SVMs can be used with kernels and analyze the impact that kernels have on generalization. **6.3.1** **SVMs with PDS kernels** In chapter 5, we noted that the dual optimization problem for SVMs as well as the form of the solution did not directly depend on the input vectors but only on inner products. Since a PDS kernel implicitly defines an inner product (theorem 6.8), we can extend SVMs and combine it with an arbitrary PDS kernel _K_ by replacing each instance of an inner product _x_ _\u00b7_ _x_ _[\u2032]_ with _K_ ( _x, x_ _[\u2032]_ ). This leads to the following general form of the SVM optimization problem and solution with PDS kernels extending (5.33): 2 max _**\u03b1**_ _m_ \ufffd \ufffd _\u03b1_ _i_ _\u2212_ [1] 2 _i_ =1 _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _y_ _i_ _y_ _j_ _K_ ( _x_ _i_ _, x_ _j_ ) (6.13) _i,j_ =1 subject to: 0 _\u2264_ _\u03b1_ _i_ _\u2264_ _C \u2227_ _m_ \ufffd _\u03b1_ _i_ _y_ _i_ = 0 _, i \u2208_ [ _m_ ] _._ _i_ =1 In view of (5.34), the hypothesis _h_ solution can be written as: _h_ ( _x_ ) = sgn \ufffd \ufffd _[m]_ _\u03b1_ _i_ _y_ _i_ _K_ ( _x_ _i_ _, x_ ) + _b_ \ufffd _,_ (6.14) _i_ =1 with _b_ = _y_ _i_ _\u2212_ [\ufffd] _[m]_ _j_ =1 _[\u03b1]_ _[j]_ _[y]_ _[j]_ _[K]_ [(] _[x]_ _[j]_ _[, x]_ _[i]_ [) for any] _[ x]_ _[i]_ [ with 0] _[ < \u03b1]_ _[i]_ _[ < C]_ [. We can rewrite] the optimization problem (6.13) in a vector form, by using the kernel matrix **K** associated to _K_ for the training sample ( _x_ 1 _, . . ., x_ _m_ ) as follows: max 2 **1** _[\u22a4]_ _**\u03b1**_ _\u2212_ ( _**\u03b1**_ _\u25e6_ **y** ) _[\u22a4]_ **K** ( _**\u03b1**_ _\u25e6_ **y** ) (6.15) _**\u03b1**_ subject to: **0** _\u2264_ _**\u03b1**_ _\u2264_ **C** _\u2227_ _**\u03b1**_ _[\u22a4]_ **y** = 0 _._ In this formulation, _**\u03b1**_ _\u25e6_ **y** is the Hadamard product or entry-wise product of the vectors _**\u03b1**_ and **y** . Thus, it is the column vector in R _[m][\u00d7]_ [1] whose _i_ th component equals _\u03b1_ _i_ _y_ _i_ . The solution in vector form is the same as in (6.14), but with _b_ = _y_ _i_ _\u2212_ ( _**\u03b1**_ _\u25e6_ **y** ) _[\u22a4]_ **Ke** _i_ for any _x_ _i_ with 0 _< \u03b1_ _i_ _< C_ . **6.3** **Kernel-based algorithms** **117** This version of SVMs used with PDS kernels is the general form of SVMs we will consider in all that follows. The extension is important, since it enables an implicit non-linear mapping of the input points to a high-dimensional space where large-margin separation is sought. Many other algorithms in areas including regression, ranking, dimensionality reduction or clustering can be extended using PDS kernels following the same scheme (see in particular chapters 9, 10, 11, 15). **6.3.2** **Representer theorem** Observe that modulo the",
    "chunk_id": "foundations_machine_learning_114"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "offset _b_, the hypothesis solution of SVMs can be written as a linear combination of the functions _K_ ( _x_ _i_ _, \u00b7_ ), where _x_ _i_ is a sample point. The following theorem known as the _representer theorem_ shows that this is in fact a general property that holds for a broad class of optimization problems, including that of SVMs with no offset. **Theorem 6.11 (Representer theorem)** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS kernel and_ H _its_ _corresponding RKHS. Then, for any non-decreasing function G_ : R _\u2192_ R _and any_ _loss function L_ : R _[m]_ _\u2192_ R _\u222a{_ + _\u221e}, the optimization problem_ argmin _F_ ( _h_ ) = argmin _G_ ( _\u2225h\u2225_ H ) + _L_ \ufffd _h_ ( _x_ 1 ) _, . . ., h_ ( _x_ _m_ )\ufffd _h\u2208_ H _h\u2208_ H _admits a solution of the form h_ _[\u2217]_ = [\ufffd] _[m]_ _i_ =1 _[\u03b1]_ _[i]_ _[K]_ [(] _[x]_ _[i]_ _[,][ \u00b7]_ [)] _[. If][ G][ is further assumed to be]_ _increasing, then any solution has this form._ Proof: Let H 1 = span( _{K_ ( _x_ _i_ _, \u00b7_ ): _i \u2208_ [ _m_ ] _}_ ). Any _h \u2208_ H admits the decomposition _h_ = _h_ 1 + _h_ _[\u22a5]_ according to H = H 1 _\u2295_ H _[\u22a5]_ 1 [,][ where] _[ \u2295]_ [is the direct sum. Since] _[ G]_ [ is] non-decreasing, _G_ ( _\u2225h_ 1 _\u2225_ H ) _\u2264_ _G_ ( ~~\ufffd~~ _\u2225h_ 1 _\u2225_ [2] H [+] _[ \u2225][h]_ _[\u22a5]_ _[\u2225]_ [2] H [) =] _[ G]_ [(] _[\u2225][h][\u2225]_ [H] [). By the reproducing] property, for all _i \u2208_ [ _m_ ], _h_ ( _x_ _i_ ) = _\u27e8h, K_ ( _x_ _i_ _, \u00b7_ ) _\u27e9_ = _\u27e8h_ 1 _, K_ ( _x_ _i_ _, \u00b7_ ) _\u27e9_ = _h_ 1 ( _x_ _i_ ). Thus, _L_ \ufffd _h_ ( _x_ 1 ) _, . . ., h_ ( _x_ _m_ )\ufffd = _L_ \ufffd _h_ 1 ( _x_ 1 ) _, . . ., h_ 1 ( _x_ _m_ )\ufffd and _F_ ( _h_ 1 ) _\u2264_ _F_ ( _h_ ). This proves the first part of the theorem. If _G_ is further increasing, then _F_ ( _h_ 1 ) _< F_ ( _h_ ) when _\u2225h_ _[\u22a5]_ _\u2225_ H _>_ 0 and any solution of the optimization problem must be in H 1 . **6.3.3** **Learning guarantees** Here, we present general learning guarantees for hypothesis sets based on PDS kernels, which hold in particular for SVMs combined with PDS kernels. The following theorem gives a general bound on the empirical Rademacher complexity of kernel-based hypotheses with bounded norm, that is a hypothesis set of the form H = _{h \u2208_ H : _\u2225h\u2225_ H _\u2264_ \u039b _}_, for some \u039b _\u2265_ 0, where H is the RKHS associated to a kernel _K_ . By the reproducing property, any _h \u2208_ H is of the form _x \ufffd\u2192\u27e8h, K_ ( _x, \u00b7_ ) _\u27e9_ = _\u27e8h,_ \u03a6( _x_ ) _\u27e9_ with",
    "chunk_id": "foundations_machine_learning_115"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2225h\u2225_ H _\u2264_ \u039b, where \u03a6 is a feature mapping associated to _K_, that is of the form _x \ufffd\u2192\u27e8_ **w** _,_ \u03a6( _x_ ) _\u27e9_ with _\u2225_ **w** _\u2225_ H _\u2264_ \u039b. **118** **Chapter 6** **Kernel Methods** **Theorem 6.12 (Rademacher complexity of kernel-based hypotheses)** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS kernel and let_ \u03a6: X _\u2192_ H _be a feature mapping associated to K. Let S \u2286_ _{x_ : _K_ ( _x, x_ ) _\u2264_ _r_ [2] _} be a sample of size m, and let_ H = _{x \ufffd\u2192\u27e8_ **w** _,_ \u03a6( _x_ ) _\u27e9_ : _\u2225_ **w** _\u2225_ H _\u2264_ \u039b _}_ _for some_ \u039b _\u2265_ 0 _. Then_ \ufffd ~~\ufffd~~ Tr[ **K** ] R _S_ (H) _\u2264_ [\u039b] _\u2264_ _m_ \ufffd _r_ [2] \u039b [2] (6.16) _m_ _[.]_ Proof: The proof steps are as follows: _m_ \ufffd \ufffd _\u03c3_ _i_ \u03a6( _x_ _i_ )\ufffd [\ufffd] _i_ =1 \ufffd R _S_ (H) = [1] _m_ [E] _**\u03c3**_ sup \ufffd _\u2225_ **w** _\u2225\u2264_ \u039b **w** _,_ \ufffd _m_ \ufffd _i_ =1 _\u03c3_ _i_ \u03a6( _x_ _i_ )\ufffd\ufffd\ufffd H \ufffd (Cauchy-Schwarz, eq. case) = [\u039b] _m_ [E] _**\u03c3**_ \ufffd\ufffd\ufffd\ufffd H 1 _/_ 2 (Jensen\u2019s ineq.) \ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd \ufffd _[m]_ _K_ ( _x_ _i_ _, x_ _i_ )\ufffd [\ufffd] [1] _[/]_ [2] _i_ =1 _m_ \ufffd 2 \ufffd _\u03c3_ _i_ \u03a6( _x_ _i_ )\ufffd\ufffd\ufffd H _i_ =1 _\u2264_ [\u039b] _m_ = [\u039b] _m_ = [\u039b] _m_ E _**\u03c3**_ \ufffd E _**\u03c3**_ \ufffd E _**\u03c3**_ \ufffd \ufffd \ufffd _[m]_ _\u2225_ \u03a6( _x_ _i_ ) _\u2225_ H [2] \ufffd [\ufffd] [1] _[/]_ [2] ( _i \u0338_ = _j \u21d2_ E _**\u03c3**_ [[] _[\u03c3]_ _[i]_ _[\u03c3]_ _[j]_ [] = 0)] _i_ =1 ~~\ufffd~~ Tr[ **K** ] = [\u039b] _\u2264_ _m_ \ufffd _r_ [2] \u039b [2] _m_ _[.]_ The initial equality holds by definition of the empirical Rademacher complexity (definition 3.1). The first inequality is due to the Cauchy-Schwarz inequality and _\u2225_ **w** _\u2225_ H _\u2264_ \u039b. The following inequality results from Jensen\u2019s inequality (theorem B.20) applied to the concave function _[\u221a]_ _\u00b7_ . The subsequent equality is a consequence of E _**\u03c3**_ [ _\u03c3_ _i_ _\u03c3_ _j_ ] = E _**\u03c3**_ [ _\u03c3_ _i_ ] E _**\u03c3**_ [ _\u03c3_ _j_ ] = 0 for _i \u0338_ = _j_, since the Rademacher variables _\u03c3_ _i_ and _\u03c3_ _j_ are independent. The statement of the theorem then follows by noting that Tr[ **K** ] _\u2264_ _mr_ [2] . The theorem indicates that the trace of the kernel matrix is an important quantity for controlling the complexity of hypothesis sets based on kernels. Observe that by the Khintchine-Kahane inequality (D.24), the empirical Rademacher complexity R\ufffd _S_ (H) = [\u039b] [E] _**[\u03c3]**_ [[] _[\u2225]_ [\ufffd] _i_ _[m]_ =1 _[\u03c3]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [)] _[\u2225]_ [H] [] can also be lower bounded by] 1 \u039b _[\u221a]_ Tr[ **K** ], which R\ufffd _S_ (H) = _m_ [\u039b] [E] _**[\u03c3]**_ [[] _[\u2225]_ [\ufffd] _i_ _[m]_ =1 _[\u03c3]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [)] _[\u2225]_ [H] [] can also be lower bounded by] ~~_\u221a_~~ 12 \u039b _[\u221a]_",
    "chunk_id": "foundations_machine_learning_116"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_m_ Tr[ **K** ], which only differs from the upper bound found by the constant ~~_\u221a_~~ 12 [. Also, note that if] _m_ [\u039b] [E] _**[\u03c3]**_ [[] _[\u2225]_ [\ufffd] _i_ _[m]_ =1 _[\u03c3]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [)] _[\u2225]_ [H] [] can also be lower bounded by] ~~_\u221a_~~ 1 \u039b _[\u221a]_ 2 only differs from the upper bound found by the constant ~~_\u221a_~~ 2 [. Also, note that if] _K_ ( _x, x_ ) _\u2264_ _r_ [2] for all _x \u2208_ X, then the inequalities 6.16 hold for all samples _S_ . The bound of theorem 6.12 or the inequalities 6.16 can be plugged into any of the Rademacher complexity generalization bounds presented in the previous chapters. In particular, in combination with theorem 5.8, they lead directly to the following margin bound similar to that of corollary 5.11. **6.4** **Negative definite symmetric kernels** **119** **Corollary 6.13 (Margin bounds for kernel-based hypotheses)** _Let K_ : X _\u00d7_ X _\u2192_ R _be a_ _PDS kernel with r_ [2] = sup _x\u2208_ X _K_ ( _x, x_ ) _. Let_ \u03a6: X _\u2192_ H _be a feature mapping asso-_ _ciated to K and let_ H = _{_ **x** _\ufffd\u2192_ **w** _\u00b7_ \u03a6( _x_ ): _\u2225_ **w** _\u2225_ H _\u2264_ \u039b _} for some_ \u039b _\u2265_ 0 _. Fix \u03c1 >_ 0 _._ _Then, for any \u03b4 >_ 0 _, each of the following statements holds with probability at least_ 1 _\u2212_ _\u03b4 for any h \u2208_ H _:_ ~~\ufffd~~ log [1] _\u03b4_ (6.17) 2 _m_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 2 _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 2 \ufffd _r_ [2] \u039b [2] _/\u03c1_ [2] + _m_ _r_ [2] \u039b [2] _/\u03c1_ [2] ~~\ufffd~~ + 3 _m_ Tr[ **K** ]\u039b [2] _/\u03c1_ [2] \ufffd log [2] _\u03b4_ (6.18) 2 _m_ _[.]_ **6.4** **Negative definite symmetric kernels** Often in practice, a natural distance or metric is available for the learning task considered. This metric could be used to define a similarity measure. As an example, Gaussian kernels have the form exp( _\u2212d_ [2] ), where _d_ is a metric for the input vector space. Several natural questions arise such as: what other PDS kernels can we construct from a metric in a Hilbert space? What technical condition should _d_ satisfy to guarantee that exp( _\u2212d_ [2] ) is PDS? A natural mathematical definition that helps address these questions is that of _negative definite symmetric (NDS) kernels_ . **Definition 6.14 (Negative definite symmetric (NDS) kernels )** _A kernel K_ : X _\u00d7_ X _\u2192_ R _is said to be_ negative-definite symmetric (NDS) _if it is symmetric and if for all_ _{x_ 1 _, . . ., x_ _m_ _} \u2286_ X _and_ **c** _\u2208_ R _[m][\u00d7]_ [1] _with_ **1** _[\u22a4]_ **c** = 0 _, the following holds:_ **c** _[\u22a4]_ **Kc** _\u2264_ 0 _._ Clearly, if _K_ is PDS, then _\u2212K_ is NDS, but the converse does not hold in general. The following gives a standard example of an NDS kernel. **Example 6.15 (Squared distance",
    "chunk_id": "foundations_machine_learning_117"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\u2014 NDS kernel)** The squared distance ( _x, x_ _[\u2032]_ ) _\ufffd\u2192\u2225x_ _[\u2032]_ _\u2212_ _x\u2225_ [2] in R _[N]_ defines an NDS kernel. Indeed, let **c** _\u2208_ R _[m][\u00d7]_ [1] with [\ufffd] _[m]_ _i_ =1 _[c]_ _[i]_ [ = 0. Then,] **120** **Chapter 6** **Kernel Methods** for any _{x_ 1 _, . . ., x_ _m_ _} \u2286_ X, we can write _m_ \ufffd _c_ _i_ _c_ _j_ _||_ **x** _i_ _\u2212_ **x** _j_ _||_ [2] = _i,j_ =1 = = _\u2264_ _m_ \ufffd _c_ _i_ _c_ _j_ ( _\u2225_ **x** _i_ _\u2225_ [2] + _\u2225_ **x** _j_ _\u2225_ [2] _\u2212_ 2 **x** _i_ _\u00b7_ **x** _j_ ) _i,j_ =1 _m_ \ufffd _c_ _i_ _c_ _j_ ( _\u2225_ **x** _i_ _\u2225_ [2] + _\u2225_ **x** _j_ _\u2225_ [2] ) _i,j_ =1 _m_ \ufffd _c_ _i_ _c_ _j_ ( _\u2225_ **x** _i_ _\u2225_ [2] + _\u2225_ **x** _j_ _\u2225_ [2] ) _\u2212_ 2 _i,j_ =1 _m_ \ufffd _c_ _i_ **x** _i_ _\u00b7_ _i_ =1 _m_ \ufffd _c_ _j_ **x** _j_ _j_ =1 _m_ _m_ \ufffd _c_ _i_ _c_ _j_ ( _\u2225_ **x** _i_ _\u2225_ [2] + _\u2225_ **x** _j_ _\u2225_ [2] ) _\u2212_ 2\ufffd\ufffd \ufffd _i,j_ =1 _i_ =1 _m_ \ufffd 2 \ufffd _c_ _i_ **x** _i_ \ufffd\ufffd _i_ =1 \ufffd _c_ _j_ _\u2225_ **x** _j_ _\u2225_ [2] [\ufffd] = 0 _._ _j_ =1 \ufffd _[m]_ _c_ _j_ \ufffd\ufffd \ufffd _[m]_ _j_ =1 _i_ =1 \ufffd _[m]_ _c_ _i_ \ufffd\ufffd \ufffd _[m]_ _i_ =1 =1 = \ufffd \ufffd _[m]_ \ufffd _[m]_ _c_ _i_ ( _\u2225_ **x** _i_ _\u2225_ [2] [\ufffd] + \ufffd \ufffd _[m]_ _i_ =1 _i_ =1 The next theorems show connections between NDS and PDS kernels. These results provide another series of tools for designing PDS kernels. **Theorem 6.16** _Let K_ _[\u2032]_ _be defined for any x_ 0 _by_ _K_ _[\u2032]_ ( _x, x_ _[\u2032]_ ) = _K_ ( _x, x_ 0 ) + _K_ ( _x_ _[\u2032]_ _, x_ 0 ) _\u2212_ _K_ ( _x, x_ _[\u2032]_ ) _\u2212_ _K_ ( _x_ 0 _, x_ 0 ) _for all x, x_ _[\u2032]_ _\u2208_ X _. Then K is NDS iff K_ _[\u2032]_ _is PDS._ Proof: Assume that _K_ _[\u2032]_ is PDS and define _K_ such that for any _x_ 0 we have _K_ ( _x, x_ _[\u2032]_ ) = _K_ ( _x, x_ 0 )+ _K_ ( _x_ 0 _, x_ _[\u2032]_ ) _\u2212K_ ( _x_ 0 _, x_ 0 ) _\u2212K_ _[\u2032]_ ( _x, x_ _[\u2032]_ ). Then for any **c** _\u2208_ R _[m]_ such that **c** _[\u22a4]_ **1** = 0 and any set of points ( _x_ 1 _, . . ., x_ _m_ ) _\u2208_ X _[m]_ we have _m_ \ufffd \ufffd _[m]_ _c_ _i_ _K_ ( _x_ _i_ _, x_ 0 )\ufffd\ufffd \ufffd _[m]_ _i_ =1 =1 _m_ \ufffd _c_ _i_ _c_ _j_ _K_ ( _x_ _i_ _, x_ _j_ ) = \ufffd \ufffd _[m]_ _i,j_ =1 _i_ =1 \ufffd _[m]_ _c_ _j_ \ufffd + \ufffd \ufffd _[m]_ _j_ =1 _i_ =1 \ufffd _[m]_ _c_ _i_ \ufffd\ufffd \ufffd _[m]_ _i_ =1 =1 \ufffd _c_ _j_ _K_",
    "chunk_id": "foundations_machine_learning_118"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "( _x_ 0 _, x_ _j_ )\ufffd _j_ =1 _m_ \ufffd _c_ _i_ _c_ _j_ _K_ _[\u2032]_ ( _x_ _i_ _, x_ _j_ ) _\u2264_ 0 _._ _i,j_ =1 2 _\u2212_ \ufffd \ufffd _[m]_ _c_ _i_ \ufffd _K_ ( _x_ 0 _, x_ 0 ) _\u2212_ _i_ =1 _m_ \ufffd _c_ _i_ _c_ _j_ _K_ _[\u2032]_ ( _x_ _i_ _, x_ _j_ ) = _\u2212_ _i,j_ =1 which proves _K_ is NDS. Now, assume _K_ is NDS and define _K_ _[\u2032]_ for any _x_ 0 as above. Then, for any **c** _\u2208_ R _[m]_, we can define _c_ 0 = _\u2212_ **c** _[\u22a4]_ **1** and the following holds by the NDS property for any points ( _x_ 1 _, . . ., x_ _m_ ) _\u2208_ X _[m]_ as well as _x_ 0 defined previously: [\ufffd] _[m]_ _i,j_ =0 _[c]_ _[i]_ _[c]_ _[j]_ _[K]_ [(] _[x]_ _[i]_ _[, x]_ _[j]_ [)] _[ \u2264]_ [0. This] implies that \ufffd \ufffd _[m]_ \ufffd _[m]_ _c_ _i_ _K_ ( _x_ _i_ _, x_ 0 )\ufffd\ufffd \ufffd _[m]_ _i_ =0 =0 \ufffd _[m]_ _c_ _j_ \ufffd + \ufffd \ufffd _[m]_ _j_ =0 _i_ =0 \ufffd _[m]_ _c_ _i_ \ufffd\ufffd \ufffd _[m]_ _i_ =0 =0 \ufffd _c_ _j_ _K_ ( _x_ 0 _, x_ _j_ )\ufffd _j_ =0 2 _\u2212_ \ufffd \ufffd _[m]_ _c_ _i_ \ufffd _K_ ( _x_ 0 _, x_ 0 ) _\u2212_ _i_ =0 _m_ \ufffd _c_ _i_ _c_ _j_ _K_ _[\u2032]_ ( _x_ _i_ _, x_ _j_ ) = _\u2212_ _i,j_ =0 _m_ \ufffd _c_ _i_ _c_ _j_ _K_ _[\u2032]_ ( _x_ _i_ _, x_ _j_ ) _\u2264_ 0 _,_ _i,j_ =0 **6.5** **Sequence kernels** **121** which implies 2 [\ufffd] _[m]_ _i,j_ =1 _[c]_ _[i]_ _[c]_ _[j]_ _[K]_ _[\u2032]_ [(] _[x]_ _[i]_ _[, x]_ _[j]_ [)] _[ \u2265\u2212]_ [2] _[c]_ [0] \ufffd _mi_ =0 _[c]_ _[i]_ _[K]_ _[\u2032]_ [(] _[x]_ _[i]_ _[, x]_ [0] [) +] _[ c]_ 0 [2] _[K]_ _[\u2032]_ [(] _[x]_ [0] _[, x]_ [0] [) = 0.] The equality holds since _\u2200x \u2208_ X _, K_ _[\u2032]_ ( _x, x_ 0 ) = 0. This theorem is useful in showing other connections, such the following theorems, which are left as exercises (see exercises 6.17 and 6.18). **Theorem 6.17** _Let K_ : X _\u00d7_ X _\u2192_ R _be a symmetric kernel._ _Then, K is NDS iff_ exp( _\u2212tK_ ) _is a PDS kernel for all t >_ 0 _._ The theorem provides another proof that Gaussian kernels are PDS: as seen earlier (Example 6.15), the squared distance ( _x, x_ _[\u2032]_ ) _\ufffd\u2192\u2225x \u2212_ _x_ _[\u2032]_ _\u2225_ [2] in R _[N]_ is NDS, thus ( _x, x_ _[\u2032]_ ) _\ufffd\u2192_ exp( _\u2212t||x \u2212_ _x_ _[\u2032]_ _||_ [2] ) is PDS for all _t >_ 0. **Theorem 6.18** _Let K_ : X _\u00d7_ X _\u2192_ R _be an NDS kernel such that for all x, x_ _[\u2032]_ _\u2208_ X _, K_ ( _x, x_ _[\u2032]_ ) = 0 _iff x_ = _x_ _[\u2032]_ _. Then, there exists a Hilbert space_ H _and a mapping_ \u03a6: X _\u2192_ H _such that for",
    "chunk_id": "foundations_machine_learning_119"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "all x, x_ _[\u2032]_ _\u2208_ X _,_ _K_ ( _x, x_ _[\u2032]_ ) = _\u2225_ \u03a6( _x_ ) _\u2212_ \u03a6( _x_ _[\u2032]_ ) _\u2225_ [2] _._ _Thus, under the hypothesis of the theorem,_ _\u221a_ _K defines a metric._ This theorem can be used to show that the kernel ( _x, x_ _[\u2032]_ ) _\ufffd\u2192_ exp( _\u2212|x \u2212_ _x_ _[\u2032]_ _|_ _[p]_ ) in R is not PDS for _p >_ 2. Otherwise, for any _t >_ 0, _{x_ 1 _, . . ., x_ _m_ _} \u2286_ X and **c** _\u2208_ R _[m][\u00d7]_ [1], we would have: _m_ _m_ \ufffd _c_ _i_ _c_ _j_ _e_ _[\u2212][t][|][x]_ _[i]_ _[\u2212][x]_ _[j]_ _[|]_ _[p]_ = \ufffd _c_ _i_ _c_ _j_ _e_ _[\u2212|][t]_ [1] _[/p]_ _[x]_ _[i]_ _[\u2212][t]_ [1] _[/p]_ _[x]_ _[j]_ _[|]_ _[p]_ _\u2265_ 0 _._ \ufffd _c_ _i_ _c_ _j_ _e_ _[\u2212][t][|][x]_ _[i]_ _[\u2212][x]_ _[j]_ _[|]_ _[p]_ = _i,j_ =1 _m_ \ufffd \ufffd _c_ _i_ _c_ _j_ _e_ _[\u2212|][t]_ [1] _[/p]_ _[x]_ _[i]_ _[\u2212][t]_ [1] _[/p]_ _[x]_ _[j]_ _[|]_ _[p]_ _\u2265_ 0 _._ _i,j_ =1 This would imply that ( _x, x_ _[\u2032]_ ) _\ufffd\u2192|x \u2212_ _x_ _[\u2032]_ _|_ _[p]_ is NDS for _p >_ 2, which can be proven (via theorem 6.18) not to be valid. **6.5** **Sequence kernels** The examples given in the previous sections, including the commonly used polynomial or Gaussian kernels, were all for PDS kernels over vector spaces. In many learning tasks found in practice, the input space X is not a vector space. The examples to classify in practice could be protein sequences, images, graphs, parse trees, finite automata, or other discrete structures which may not be directly given as vectors. PDS kernels provide a method for extending algorithms such as SVMs originally designed for a vectorial space to the classification of such objects. But, how can we define PDS kernels for these structures? This section will focus on the specific case of _sequence kernels_, that is, kernels for sequences or strings. PDS kernels can be defined for other discrete structures in somewhat similar ways. Sequence kernels are particularly relevant to learning algorithms applied to computational biology or natural language processing, which are both important applications. **122** **Chapter 6** **Kernel Methods** How can we define PDS kernels for sequences, which are similarity measures for sequences? One idea consists of declaring two sequences, e.g., two documents or two biosequences, as similar when they share common substrings or subsequences. One example could be the kernel between two sequences defined by the sum of the product of the counts of their common substrings. But which substrings should be used in that definition? Most likely, we would need some flexibility in the definition of the matching substrings. For computational biology applications, for example, the match could be imperfect. Thus, we may need to consider some number of mismatches, possibly gaps, or wildcards. More generally, we might need to allow various substitutions and might wish to assign different weights to common substrings to emphasize some matching substrings and deemphasize others. As can be seen from this discussion, there are many different possibilities and",
    "chunk_id": "foundations_machine_learning_120"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "we need a general framework for defining such kernels. In the following, we will introduce a general framework for sequence kernels, _rational kernels_, which will include all the kernels considered in this discussion. We will also describe a general and efficient algorithm for their computation and will illustrate them with some examples. The definition of these kernels relies on that of _weighted transducers_ . Thus, we start with the definition of these devices as well as some relevant algorithms. **6.5.1** **Weighted transducers** Sequence kernels can be effectively represented and computed using _weighted trans-_ _ducers_ . In the following definition, let \u03a3 denote a finite input alphabet, \u2206a finite output alphabet, and _\u03f5_ the _empty string_ or null label, whose concatenation with any string leaves it unchanged. **Definition 6.19** _A_ weighted transducer _T is a 7-tuple T_ = (\u03a3 _,_ \u2206 _, Q, I, F, E, \u03c1_ ) _where_ \u03a3 _is a finite input alphabet,_ \u2206 _a finite output alphabet, Q is a finite set of states,_ _I \u2286_ _Q the set of initial states, F \u2286_ _Q the set of final states, E a finite multiset of_ _transitions elements of Q \u00d7_ (\u03a3 _\u222a{\u03f5}_ ) _\u00d7_ (\u2206 _\u222a{\u03f5}_ ) _\u00d7_ R _\u00d7 Q, and \u03c1_ : _F \u2192_ R _a final_ _weight function mapping F to_ R _. The_ size _of transducer T is the sum of its number_ _of states and transitions and is denoted by |T_ _|._ [7] Thus, weighted transducers are finite automata in which each transition is labeled with both an input and an output label and carries some real-valued weight. Figure 6.4 shows an example of a weighted finite-state transducer. In this figure, the input and output labels of a transition are separated by a colon delimiter, and the weight is indicated after the slash separator. The initial states are represented by 7 A multiset in the definition of the transitions is used to allow for the presence of several transitions from a state _p_ to a state _q_ with the same input and output label, and even the same weight, which may occur as a result of various operations. **6.5** **Sequence kernels** **123** **Image:** [No caption returned] **Figure 6.4** Example of weighted transducer. a bold circle and final states by double circles. The final weight _\u03c1_ [ _q_ ] at a final state _q_ is displayed after the slash. The input label of a path _\u03c0_ is a string element of \u03a3 _[\u2217]_ obtained by concatenating input labels along _\u03c0_ . Similarly, the output label of a path _\u03c0_ is obtained by concatenating output labels along _\u03c0_ . A path from an initial state to a final state is an _accepting path_ . The weight of an accepting path is obtained by multiplying the weights of its constituent transitions and the weight of the final state of the path. A weighted transducer defines a mapping from \u03a3 _[\u2217]_ _\u00d7_ \u2206 _[\u2217]_ to R. The weight associated by a weighted transducer _T_ to a pair of strings ( _x, y_ ) _\u2208_ \u03a3 _[\u2217]_ _\u00d7_ \u2206 _[\u2217]_",
    "chunk_id": "foundations_machine_learning_121"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "is denoted by _T_ ( _x, y_ ) and is obtained by summing the weights of all accepting paths with input label _x_ and output label _y_ . For example, the transducer of figure 6.4 associates to the pair ( _aab, baa_ ) the weight 3 _\u00d7_ 1 _\u00d7_ 4 _\u00d7_ 2 + 3 _\u00d7_ 2 _\u00d7_ 3 _\u00d7_ 2, since there is a path with input label _aab_ and output label _baa_ and weight 3 _\u00d7_ 1 _\u00d7_ 4 _\u00d7_ 2, and another one with weight 3 _\u00d7_ 2 _\u00d7_ 3 _\u00d7_ 2. The sum of the weights of all accepting paths of an acyclic transducer, that is a transducer _T_ with no cycle, can be computed in linear time, that is _O_ ( _|T_ _|_ ), using a general _shortest-distance_ or forward-backward algorithm. These are simple algorithms, but a detailed description would require too much of a digression from the main topic of this chapter. **Composition** An important operation for weighted transducers is _composition_, which can be used to combine two or more weighted transducers to form more complex weighted transducers. As we shall see, this operation is useful for the creation and computation of sequence kernels. Its definition follows that of composition of relations. Given two weighted transducers _T_ 1 = (\u03a3 _,_ \u2206 _, Q_ 1 _, I_ 1 _, F_ 1 _, E_ 1 _, \u03c1_ 1 ) and _T_ 2 = (\u2206 _,_ \u2126 _, Q_ 2 _, I_ 2 _, F_ 2 _, E_ 2 _, \u03c1_ 2 ), the result of the composition of _T_ 1 and _T_ 2 is a **124** **Chapter 6** **Kernel Methods** weighted transducer denoted by _T_ 1 _\u25e6_ _T_ 2 and defined for all _x \u2208_ \u03a3 _[\u2217]_ and _y \u2208_ \u2126 _[\u2217]_ by ( _T_ 1 _\u25e6_ _T_ 2 )( _x, y_ ) = \ufffd _T_ 1 ( _x, z_ ) _\u00b7 T_ 2 ( _z, y_ ) _,_ (6.19) _z\u2208_ \u2206 _[\u2217]_ where the sum runs over all strings _z_ over the alphabet \u2206. Thus, composition is similar to matrix multiplication with infinite matrices. There exists a general and efficient algorithm to compute the composition of two weighted transducers. In the absence of _\u03f5_ s on the input side of _T_ 1 or the output side of _T_ 2, the states of _T_ 1 _\u25e6_ _T_ 2 = (\u03a3 _,_ \u2206 _, Q, I, F, E, \u03c1_ ) can be identified with pairs made of a state of _T_ 1 and a state of _T_ 2, _Q \u2286_ _Q_ 1 _\u00d7 Q_ 2 . Initial states are those obtained by pairing initial states of the original transducers, _I_ = _I_ 1 _\u00d7 I_ 2, and similarly final states are defined by _F_ = _Q \u2229_ ( _F_ 1 _\u00d7 F_ 2 ). The final weight at a state ( _q_ 1 _, q_ 2 ) _\u2208_ _F_ 1 _\u00d7 F_ 2 is _\u03c1_ ( _q_ ) = _\u03c1_ 1 ( _q_ 1 ) _\u03c1_ 2 ( _q_ 2 ), that is the product of the final weights",
    "chunk_id": "foundations_machine_learning_122"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "at _q_ 1 and _q_ 2 . Transitions are obtained by matching a transition of _T_ 1 with one of _T_ 2 from appropriate transitions of _T_ 1 and _T_ 2 : _E_ = \ufffd ( _q_ 1 _,a,b,w_ 1 _,q_ 2 ) _\u2208E_ 1 ( _q_ 1 _[\u2032]_ _[,b,c,w]_ [2] _[,q]_ 2 _[\u2032]_ [)] _[\u2208][E]_ [2] ( _q_ 1 _, q_ 1 _[\u2032]_ [)] _[, a, c, w]_ [1] _[\u2297]_ _[w]_ [2] _[,]_ [ (] _[q]_ [2] _[, q]_ 2 _[\u2032]_ [)] _._ \ufffd\ufffd \ufffd\ufffd Here, _\u228e_ denotes the standard join operation of multisets as in _{_ 1 _,_ 2 _} \u228e{_ 1 _,_ 3 _}_ = _{_ 1 _,_ 1 _,_ 2 _,_ 3 _}_, to preserve the multiplicity of the transitions. In the worst case, all transitions of _T_ 1 leaving a state _q_ 1 match all those of _T_ 2 leaving state _q_ 1 _[\u2032]_ [, thus the space and time complexity of composition is quadratic:] _O_ ( _|T_ 1 _||T_ 2 _|_ ). In practice, such cases are rare and composition is very efficient. Figure 6.5 illustrates the algorithm in a particular case. As illustrated by figure 6.6, when _T_ 1 admits output _\u03f5_ labels or _T_ 2 input _\u03f5_ labels, the algorithm just described may create redundant _\u03f5_ -paths, which would lead to an incorrect result. The weight of the matching paths of the original transducers would be counted _p_ times, where _p_ is the number of redundant paths in the result of composition. To avoid with this problem, all but one _\u03f5_ -path must be filtered out of the composite transducer. Figure 6.6 indicates in boldface one possible choice for that path, which in this case is the shortest. Remarkably, that filtering mechanism itself can be encoded as a finite-state transducer _F_ (figure 6.6b). To apply that filter, we need to first augment _T_ 1 and _T_ 2 with auxiliary symbols that make the semantics of _\u03f5_ explicit: let _T_ [\u02dc] 1 ( _T_ [\u02dc] 2 ) be the weighted transducer obtained from _T_ 1 (respectively _T_ 2 ) by replacing the output (respectively input) _\u03f5_ labels with _\u03f5_ 2 (respectively _\u03f5_ 1 ) as illustrated by figure 6.6. Thus, matching with the symbol _\u03f5_ 1 corresponds to remaining at the same state of _T_ 1 and taking a transition of _T_ 2 with input _\u03f5_ . _\u03f5_ 2 can be described in a symmetric way. The filter transducer _F_ disallows a matching ( _\u03f5_ 2 _, \u03f5_ 2 ) immediately after ( _\u03f5_ 1 _, \u03f5_ 1 ) since this can be done **6.5** **Sequence kernels** **125** **Image:** [No caption returned] **Image:** [No caption returned] (a) (b) **Image:** [No caption returned] (c) **Figure 6.5** (a) Weighted transducer _T_ 1 . (b) Weighted transducer _T_ 2 . (c) Result of composition of _T_ 1 and _T_ 2, _T_ 1 _\u25e6_ _T_ 2 . Some states might be constructed during the execution of the algorithm that are not _co-accessible_, that is, they do not admit a path to a final state, e.g., (3 _,_ 2). Such",
    "chunk_id": "foundations_machine_learning_123"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "states and the related transitions (in red) can be removed by a trimming (or connection) algorithm in linear time. instead via ( _\u03f5_ 2 _, \u03f5_ 1 ). By symmetry, it also disallows a matching ( _\u03f5_ 1 _, \u03f5_ 1 ) immediately after ( _\u03f5_ 2 _, \u03f5_ 2 ). In the same way, a matching ( _\u03f5_ 1 _, \u03f5_ 1 ) immediately followed by ( _\u03f5_ 2 _, \u03f5_ 1 ) is not permitted by the filter _F_ since a path via the matchings ( _\u03f5_ 2 _, \u03f5_ 1 )( _\u03f5_ 1 _, \u03f5_ 1 ) is possible. Similarly, ( _\u03f5_ 2 _, \u03f5_ 2 )( _\u03f5_ 2 _, \u03f5_ 1 ) is ruled out. It is not hard to verify that the filter transducer _F_ is precisely a finite automaton over pairs accepting the complement of the language _L_ = _\u03c3_ _[\u2217]_ (( _\u03f5_ 1 _, \u03f5_ 1 )( _\u03f5_ 2 _, \u03f5_ 2 ) + ( _\u03f5_ 2 _, \u03f5_ 2 )( _\u03f5_ 1 _, \u03f5_ 1 ) + ( _\u03f5_ 1 _, \u03f5_ 1 )( _\u03f5_ 2 _, \u03f5_ 1 ) + ( _\u03f5_ 2 _, \u03f5_ 2 )( _\u03f5_ 2 _, \u03f5_ 1 )) _\u03c3_ _[\u2217]_ _,_ where _\u03c3_ = _{_ ( _\u03f5_ 1 _, \u03f5_ 1 ) _,_ ( _\u03f5_ 2 _, \u03f5_ 2 ) _,_ ( _\u03f5_ 2 _, \u03f5_ 1 ) _, x}_ . Thus, the filter _F_ guarantees that exactly one _\u03f5_ -path is allowed in the composition of each _\u03f5_ sequences. To obtain the correct result of composition, it suffices then to use the _\u03f5_ -free composition algorithm already described and compute _T_ \u02dc 1 _\u25e6_ _F \u25e6_ _T_ \u02dc 2 _._ (6.20) **126** **Chapter 6** **Kernel Methods** **Image:** [No caption returned] **Figure 6.6** Redundant _\u03f5_ -paths in composition. All transition and final weights are equal to one. (a) A straightforward generalization of the _\u03f5_ -free case would generate all the paths from (1 _,_ 1) to (3 _,_ 2) when composing _T_ 1 and _T_ 2 and produce an incorrect results in non-idempotent semirings. (b) Filter transducer _F_ . The shorthand _x_ is used to represent an element of \u03a3. Indeed, the two compositions in _T_ [\u02dc] 1 _\u25e6_ _F \u25e6_ _T_ [\u02dc] 2 no longer involve _\u03f5_ s. Since the size of the filter transducer _F_ is constant, the complexity of general composition is the same as that of _\u03f5_ -free composition, that is _O_ ( _|T_ 1 _||T_ 2 _|_ ). In practice, the augmented transducers _T_ [\u02dc] 1 and _T_ [\u02dc] 2 are not explicitly constructed, instead the presence of the auxiliary symbols is simulated. Further filter optimizations help limit the number of non-coaccessible states created, for example, by examining more carefully the case of states with only outgoing non- _\u03f5_ -transitions or only outgoing _\u03f5_ -transitions. **6.5.2** **Rational kernels** The following establishes a general framework for the definition of sequence kernels. **Definition 6.20 (Rational kernels)** _A kernel K_ : \u03a3 _[\u2217]_ _\u00d7_ \u03a3 _[\u2217]_ _\u2192_ R _is said to be_ rational _if it coincides",
    "chunk_id": "foundations_machine_learning_124"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "with the mapping defined by some weighted transducer U_ _: \u2200x, y \u2208_ \u03a3 _[\u2217]_ _, K_ ( _x, y_ ) = _U_ ( _x, y_ ) _._ **6.5** **Sequence kernels** **127** Note that we could have instead adopted a more general definition: instead of using weighted transducers, we could have used more powerful sequence mappings such as _algebraic transductions_, which are the functional counterparts of contextfree languages, or even more powerful ones. However, an essential need for kernels is an efficient computation, and more complex definitions would lead to substantially more costly computational complexities for kernel computation. For rational kernels, there exists a general and efficient computation algorithm. **Computation** We will assume that the transducer _U_ defining a rational kernel _K_ does not admit any _\u03f5_ -cycle with non-zero weight, otherwise the kernel value is infinite for all pairs. For any sequence _x_, let _T_ _x_ denote a weighted transducer with just one accepting path whose input and output labels are both _x_ and its weight equal to one. _T_ _x_ can be straightforwardly constructed from _x_ in linear time _O_ ( _|x|_ ). Then, for any _x, y \u2208_ \u03a3 _[\u2217]_, _U_ ( _x, y_ ) can be computed by the following two steps: 1. Compute _V_ = _T_ _x_ _\u25e6U_ _\u25e6T_ _y_ using the composition algorithm in time _O_ ( _|U_ _||T_ _x_ _||T_ _y_ _|_ ). 2. Compute the sum of the weights of all accepting paths of _V_ using a general shortest-distance algorithm in time _O_ ( _|V |_ ). By definition of composition, _V_ is a weighted transducer whose accepting paths are precisely those accepting paths of _U_ that have input label _x_ and output label _y_ . The second step computes the sum of the weights of these paths, that is, exactly _U_ ( _x, y_ ). Since _U_ admits no _\u03f5_ -cycle, _V_ is acyclic, and this step can be performed in linear time. The overall complexity of the algorithm for computing _U_ ( _x, y_ ) is then in _O_ ( _|U_ _||T_ _x_ _||T_ _y_ _|_ ). Since _U_ is fixed for a rational kernel _K_ and _|T_ _x_ _|_ = _O_ ( _|x|_ ) for any _x_, this shows that the kernel values can be obtained in quadratic time _O_ ( _|x||y|_ ). For some specific weighted transducers _U_, the computation can be more efficient, for example in _O_ ( _|x|_ + _|y|_ ) (see exercise 6.20). **PDS rational kernels** For any transducer _T_, let _T_ _[\u2212]_ [1] denote the _inverse_ of _T_, that is the transducer obtained from _T_ by swapping the input and output labels of every transition. For all _x, y_, we have _T_ _[\u2212]_ [1] ( _x, y_ ) = _T_ ( _y, x_ ). The following theorem gives a general method for constructing a PDS rational kernel from an arbitrary weighted transducer. **Theorem 6.21** _For any weighted transducer T_ = (\u03a3 _,_ \u2206 _, Q, I, F, E, \u03c1_ ) _, the function_ _K_ = _T \u25e6_ _T_ _[\u2212]_ [1] _is a PDS rational kernel._ Proof:",
    "chunk_id": "foundations_machine_learning_125"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "By definition of composition and the inverse operation, for all _x, y \u2208_ \u03a3 _[\u2217]_, _K_ ( _x, y_ ) = \ufffd _T_ ( _x, z_ ) _T_ ( _y, z_ ) _._ _z\u2208_ \u2206 _[\u2217]_ **128** **Chapter 6** **Kernel Methods** b:\u03b5/1 b:\u03b5/1 b:\u03b5/1 b:\u03b5/1 b:\u03b5/\u03bb **Image:** [No caption returned] **Image:** [No caption returned] (a) (b) **Figure 6.7** (a) Transducer _T_ bigram defining the bigram kernel _T_ bigram _\u25e6T_ bigram _[\u2212]_ [1] [for \u03a3 =] _[ {][a, b][}]_ [. (b) Transducer] _T_ gappy ~~b~~ igram defining the gappy bigram kernel _T_ gappy bigram _\u25e6_ _T_ gappy _[\u2212]_ [1] ~~b~~ igram [with gap penalty] _\u03bb \u2208_ (0 _,_ 1). _K_ is the pointwise limit of the kernel sequence ( _K_ _n_ ) _n\u2265_ 0 defined by: _\u2200n \u2208_ N _, \u2200x, y \u2208_ \u03a3 _[\u2217]_ _,_ _K_ _n_ ( _x, y_ ) = \ufffd _T_ ( _x, z_ ) _T_ ( _y, z_ ) _,_ _|z|\u2264n_ where the sum runs over all sequences in \u2206 _[\u2217]_ of length at most _n_ . _K_ _n_ is PDS since its corresponding kernel matrix **K** _n_ for any sample ( _x_ 1 _, . . ., x_ _m_ ) is SPSD. This can be see form the fact that **K** _n_ can be written as **K** _n_ = **AA** _[\u22a4]_ with **A** = ( _K_ _n_ ( _x_ _i_ _, z_ _j_ )) _i\u2208_ [ _m_ ] _,j\u2208_ [ _N_ ], where _z_ 1 _, . . ., z_ _N_ is some arbitrary enumeration of the set of strings in \u03a3 _[\u2217]_ with length at most _n_ . Thus, _K_ is PDS as the pointwise limit of the sequence of PDS kernels ( _K_ _n_ ) _n\u2208_ N . The sequence kernels commonly used in computational biology, natural language processing, computer vision, and other applications are all special instances of rational kernels of the form _T \u25e6_ _T_ _[\u2212]_ [1] . All of these kernels can be computed efficiently using the same general algorithm for the computational of rational kernels presented in the previous paragraph. Since the transducer _U_ = _T \u25e6_ _T_ _[\u2212]_ [1] defining such PDS rational kernels has a specific form, there are different options for the computation of the composition _T_ _x_ _\u25e6_ _U \u25e6_ _T_ _y_ : _\u2022_ compute _U_ = _T \u25e6_ _T_ _[\u2212]_ [1] first, then _V_ = _T_ _x_ _\u25e6_ _U \u25e6_ _T_ _y_ ; _\u2022_ compute _V_ 1 = _T_ _x_ _\u25e6_ _T_ and _V_ 2 = _T_ _y_ _\u25e6_ _T_ first, then _V_ = _V_ 1 _\u25e6_ _V_ 2 _[\u2212]_ [1] ; _\u2022_ compute first _V_ 1 = _T_ _x_ _\u25e6_ _T_, then _V_ 2 = _V_ 1 _\u25e6_ _T_ _[\u2212]_ [1], then _V_ = _V_ 2 _\u25e6_ _T_ _y_, or the similar series of operations with _x_ and _y_ permuted. All of these methods lead to the same result after computation of the sum of the weights of all accepting paths, and they all have the same worst-case complexity. However, in practice, due to the sparsity of intermediate compositions, there may be substantial differences between",
    "chunk_id": "foundations_machine_learning_126"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "their time and space computational costs. An alternative method based on an _n-way composition_ can further lead to significantly more efficient computations. **6.5** **Sequence kernels** **129** **Example 6.22 (Bigram and gappy bigram sequence kernels)** Figure 6.7a shows a weighted transducer _T_ bigram defining a common sequence kernel, the _bigram sequence kernel_, for the specific case of an alphabet reduced to \u03a3 = _{a, b}_ . The bigram kernel associates to any two sequences _x_ and _y_ the sum of the product of the counts of all bigrams in _x_ and _y_ . For any sequence _x \u2208_ \u03a3 _[\u2217]_ and any bigram _z \u2208{aa, ab, ba, bb}_, _T_ bigram ( _x, z_ ) is exactly the number of occurrences of the bigram _z_ in _x_ . Thus, by definition of composition and the inverse operation, _T_ bigram _\u25e6_ _T_ bigram _[\u2212]_ [1] [computes] exactly the bigram kernel. Figure 6.7b shows a weighted transducer _T_ gappy ~~b~~ igram defining the so-called _gappy_ _bigram kernel_ . The gappy bigram kernel associates to any two sequences _x_ and _y_ the sum of the product of the counts of all gappy bigrams in _x_ and _y_ penalized by the length of their _gaps_ . Gappy bigrams are sequences of the form _aua_, _aub_, _bua_, or _bub_, where _u \u2208_ \u03a3 _[\u2217]_ is called the gap. The count of a gappy bigram is multiplied by _\u03bb_ _[|][u][|]_ for some fixed _\u03bb \u2208_ (0 _,_ 1) so that gappy bigrams with longer gaps contribute less to the definition of the similarity measure. While this definition could appear to be somewhat complex, figure 6.7 shows that _T_ gappy ~~b~~ igram can be straightforwardly derived from _T_ bigram . The graphical representation of rational kernels helps understanding or modifying their definition. **Counting transducers** The definition of most sequence kernels is based on the counts of some common patterns appearing in the sequences. In the examples just examined, these were bigrams or gappy bigrams. There exists a simple and general method for constructing a weighted transducer counting the number of occurrences of patterns and using them to define PDS rational kernels. Let X be a finite automaton representing the set of patterns to count. In the case of bigram kernels with \u03a3 = _{a, b}_, X would be an automaton accepting exactly the set of strings _{aa, ab, ba, bb}_ . Then, the weighted transducer of figure 6.8 can be used to compute exactly the number of occurrences of each pattern accepted by X. **Theorem 6.23** _For any x \u2208_ \u03a3 _[\u2217]_ _and any sequence z accepted by_ X _, T_ _count_ ( _x, z_ ) _is the_ _number of occurrences of z in x._ Proof: Let _x \u2208_ \u03a3 _[\u2217]_ be an arbitrary sequence and let _z_ be a sequence accepted by X. Since all accepting paths of _T_ count have weight one, _T_ count ( _x, z_ ) is equal to the number of accepting paths in _T_ count with input label _x_ and output _z_ . Now, an accepting path _\u03c0_ in _T_ count with input _x_ and",
    "chunk_id": "foundations_machine_learning_127"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "output _z_ can be decomposed as _\u03c0_ = _\u03c0_ 0 _\u03c0_ 01 _\u03c0_ 1, where _\u03c0_ 0 is a path through the loops of state 0 with input label some prefix _x_ 0 of _x_ and output label _\u03f5_, _\u03c0_ 01 an accepting path from 0 to 1 with input and output labels equal to _z_, and _\u03c0_ 1 a path through the self-loops of state 1 with input label a suffix _x_ 1 of _x_ and output _\u03f5_ . Thus, the number of such paths is exactly **130** **Chapter 6** **Kernel Methods** b:\u03b5/1 b:\u03b5/1 **Image:** [No caption returned] **Figure 6.8** Counting transducer _T_ count for \u03a3 = _{a, b}_ . The \u201ctransition\u201d _X_ : _X/_ 1 stands for the weighted transducer created from the automaton X by adding to each transition an output label identical to the existing label, and by making all transition and final weights equal to one. the number of distinct ways in which we can write sequence _x_ as _x_ = _x_ 0 _zx_ 1, which is exactly the number of occurrences of _z_ in _x_ . The theorem provides a very general method for constructing PDS rational kernels _T_ count _\u25e6_ _T_ count _[\u2212]_ [1] [that are based on counts of some patterns that can be defined via] a finite automaton, or equivalently a regular expression. Figure 6.8 shows the transducer for the case of an input alphabet reduced to \u03a3 = _{a, b}_ . The general case can be obtained straightforwardly by augmenting states 0 and 1 with other self-loops using other symbols than _a_ and _b_ . In practice, a lazy evaluation can be used to avoid the explicit creation of these transitions for all alphabet symbols and instead creating them on-demand based on the symbols found in the input sequence _x_ . Finally, one can assign different weights to the patterns counted to emphasize or deemphasize some, as in the case of gappy bigrams. This can be done simply by changing the transitions weight or final weights of the automaton X used in the definition of _T_ count . **6.6** **Approximate kernel feature maps** In the previous sections, we have seen the benefits that kernel methods can provide by implicitly and efficiently mapping a learning problem from the input space X to a richer feature space H. One potential drawback when using kernel methods, is that the kernel function needs to be evaluated on all pairs of points in the training set. If this set contains a very large number of instances, then the _O_ ( _m_ [2] ) cost in memory and _O_ ( _m_ [2] _C_ _K_ ) cost in computation, where _C_ _K_ is the cost of a single kernel function evaluation, may be prohibitive. Another consideration is the cost of making predictions with a trained model. Evaluating the kernelized function _h_ ( _x_ ) = [\ufffd] _[m]_ _i_ =1 _[\u03b1]_ _[i]_ _[K]_ [(] _[x]_ _[i]_ _[, x]_ [) +] _[ b]_ [ requires] _[ O]_ [(] _[m]_ [) storage and] _[ O]_ [(] _[mC]_ _[K]_ [) computation cost] (the",
    "chunk_id": "foundations_machine_learning_128"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "exact amount of storage and number of operations depends on the number of support vectors). Note that if we use explicit feature vectors **x** _\u2208_ R _[N]_, then the primal formulation of the SVM problem can be used for training. The primal formulation incurs only an _O_ ( _Nm_ ) storage cost and evaluation requires only _O_ ( _N_ ) storage and computation **6.6** **Approximate kernel feature maps** **131** **Table 6.1** Examples of normalized shift-invariant kernels (defined over **x** _,_ **x** _[\u2032]_ _\u2208_ R _[N]_ ) and their corresponding densities (defined over _**\u03c9**_ _\u2208_ R _[N]_ ). _G_ ( **x** _\u2212_ **x** _[\u2032]_ ) _p_ ( _**\u03c9**_ ) Gaussian exp \ufffd _\u2212_ _[\u2225]_ **[x]** _[\u2212]_ 2 **[x]** _[\u2032]_ _[\u2225]_ [2] _D_ 2 exp \ufffd _\u2212_ _[\u2225]_ _**[\u03c9]**_ 2 _[\u2225]_ [2] **[x]** _[\u2032]_ _[\u2225]_ [2] _\u2212D_ 2 \ufffd (2 _\u03c0_ ) 2 2 \ufffd _N_ 1 Laplacian exp \ufffd _\u2212\u2225_ **x** _\u2212_ **x** _[\u2032]_ _\u2225_ 1 \ufffd \ufffd _i_ =1 _\u03c0_ (1+ _\u03c9_ _i_ [2] [)] _N_ 2 Cauchy \ufffd _i_ =1 1+( _x_ _i_ _\u2212x_ _[\u2032]_ _i_ [)] [2] exp \ufffd _\u2212\u2225_ _**\u03c9**_ _\u2225_ 1 \ufffd time: _h_ ( **x** ) = **w** _\u00b7_ **x** + _b_ . However, these observations are only useful if _N < m_, which is likely not the case when considering the explicit feature maps \u03a6( _x_ ) induced by a kernel function. For example, given an input feature space of dimension _N_, the dimension of the kernel feature map for a polynomial kernel of degree _d_ is _O_ ( _N_ _[d]_ ). In the case of Gaussian kernels the explicit feature map dimension is infinite. So clearly using explicit kernel feature maps in general is not possible and again emphasizes that using kernel functions to compute inner products implicitly is crucial. In this section we show that a compromise is possible by constructing _approximate_ _kernel feature maps_ . These are feature maps with a user-specified dimension _D_, \u03a8( _x_ ) _\u2208_ R _[D]_, which guarantee \u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ ) _\u2248_ _K_ ( _x, x_ _[\u2032]_ ) when using a sufficiently large dimension _D_ . To begin, we state a classical result from the field of harmonic analysis. **Theorem 6.24 (Bochner\u2019s theorem)** _A continuous kernel of the form K_ ( _x, x_ _[\u2032]_ ) = _G_ ( _x_ _\u2212_ _x_ _[\u2032]_ ) _defined over a locally compact set_ X _is positive definite if and only if G is the_ _Fourier transform of a non-negative measure. That is,_ _G_ ( _x_ ) = _p_ ( _\u03c9_ ) _e_ _[i\u03c9][\u00b7][x]_ _d\u03c9,_ \ufffd X _where p is a non-negative measure._ Kernels of the form _K_ ( _x, x_ _[\u2032]_ ) = _G_ ( _x \u2212_ _x_ _[\u2032]_ ) are called _shift-invariant kernels_ . Note that if the kernel is scaled such that _G_ (0) = 1, then _p_ is in fact a probability distribution. Several examples of such kernels and their corresponding distributions are displayed in table 6.1. The next proposition provides a simplified expression in the case of real-valued kernels. **Proposition 6.25** _Let K be",
    "chunk_id": "foundations_machine_learning_129"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "a continuous real-valued shift-invariant kernel and let p_ _denote its corresponding non-negative measure as in theorem 6.24. Furthermore,_ _assume that for all x \u2208_ X _we have K_ ( _x, x_ ) = 1 _so that p is a probability distribution._ _Then, the following identity holds:_ E _\u03c9\u223cp_ _\u22a4_ \ufffd\ufffd cos( _\u03c9 \u00b7 x_ ) _,_ sin( _\u03c9 \u00b7 x_ )\ufffd \ufffd cos( _\u03c9 \u00b7 x_ _[\u2032]_ ) _,_ sin( _\u03c9 \u00b7 x_ _[\u2032]_ )\ufffd [\ufffd] = _K_ ( _x, x_ _[\u2032]_ ) _._ **132** **Chapter 6** **Kernel Methods** Proof: First, since both _K_ and _p_ are real-valued, it suffices to consider only the real portion of _e_ _[ix]_ when invoking theorem 6.24. Thus, using Re[ _e_ _[ix]_ ] = Re[cos( _x_ ) + _i_ sin( _x_ )] = cos( _x_ ), we have _K_ ( _x, x_ _[\u2032]_ ) = Re[ _K_ ( _x, x_ _[\u2032]_ )] = _p_ ( _\u03c9_ ) cos( _\u03c9 \u00b7_ ( _x \u2212_ _x_ _[\u2032]_ )) _d\u03c9 ._ \ufffd X Next, by the standard trigonometric identity cos( _a\u2212b_ ) = cos( _a_ ) cos( _b_ )+sin( _a_ ) sin( _b_ ), we have _p_ ( _\u03c9_ ) cos( _\u03c9 \u00b7_ ( _x \u2212_ _x_ _[\u2032]_ )) _d\u03c9_ \ufffd X = _p_ ( _\u03c9_ )\ufffd cos( _\u03c9 \u00b7 x_ ) cos( _\u03c9 \u00b7 x_ _[\u2032]_ ) + sin( _\u03c9 \u00b7 x_ ) sin( _\u03c9 \u00b7 x_ _[\u2032]_ )\ufffd _d\u03c9_ \ufffd X = E _\u03c9\u223cp_ _\u22a4_ \ufffd\ufffd cos( _\u03c9 \u00b7 x_ ) _,_ sin( _\u03c9 \u00b7 x_ )\ufffd \ufffd cos( _\u03c9 \u00b7 x_ _[\u2032]_ ) _,_ sin( _\u03c9 \u00b7 x_ _[\u2032]_ )\ufffd [\ufffd] _,_ which completes the proof of the proposition. This proposition provides the motivation for a very simple method for generating for any _D \u2265_ 1, an approximate kernel map \u03a8 _\u2208_ R [2] _[D]_, defined for all _x \u2208_ X by _\u22a4_ cos( _\u03c9_ 1 _\u00b7 x_ ) _,_ sin( _\u03c9_ 1 _\u00b7 x_ ) _, . . .,_ cos( _\u03c9_ _D_ _\u00b7 x_ ) _,_ sin( _\u03c9_ _D_ _\u00b7 x_ ) _,_ (6.21) \ufffd \ufffd \u03a8( _x_ ) = \ufffd 1 _D_ where _\u03c9_ _i_ s, _i_ = 1 _, . . ., D_, are sampled i.i.d. according to the measure _p_ over X corresponding to kernel _K_ considered. Thus, \u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ ) = [1] _D_ _D_ \ufffd _i_ =1 _\u22a4_ cos( _\u03c9_ _i_ _\u00b7 x_ ) _,_ sin( _\u03c9_ _i_ _\u00b7 x_ ) cos( _\u03c9_ _i_ _\u00b7 x_ _[\u2032]_ ) _,_ sin( _\u03c9_ _i_ _\u00b7 x_ _[\u2032]_ ) \ufffd \ufffd \ufffd \ufffd is the empirical analog of the expectation computed in proposition 6.25. The following theorem shows that this empirical estimate converges uniformly over all points in a compact domain X as _D_ grows. **Lemma 6.26** _Let K be a continuously differentiable kernel function that satisfies the_ _conditions of proposition 6.25 and has associated measure p. Furthermore, assume_ X _is compact and let N denote its dimension, R denote the radius of the Euclidean_ _ball containing_ X _, and \u03c3_ _p_ [2] [=][ E] _[\u03c9][\u223c][p]_ [[] _[\u2225][\u03c9][\u2225]_ [2]",
    "chunk_id": "foundations_machine_learning_130"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[]] _[ <][ \u221e][. Then, for]_ [ \u03a8] _[ \u2208]_ [R] _[D]_ _[ as defined in]_ (6.21) _, the following holds for any_ 0 _< r \u2264_ 2 _R and \u03f5 >_ 0 _:_ P sup \ufffd _x,x_ _[\u2032]_ _\u2208_ X \ufffd\ufffd\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _\u2032_ ) _\u2212_ _K_ ( _x, x_ _\u2032_ )\ufffd\ufffd _\u2265_ _\u03f5_ \ufffd _\u2264_ 2 _N_ (2 _R, r_ ) exp \ufffd _\u2212_ _[D\u03f5]_ 8 [2] + [4] _[r\u03c3]_ _[p]_ _._ \ufffd _\u03f5_ _Where the probability is with respect to the draws of \u03c9 \u223c_ _p and N_ ( _R, r_ ) _denotes the_ _minimal number of balls of radius r needed to cover a ball of radius R._ Proof: Define Z = _{z_ : _z_ = _x \u2212_ _x_ _[\u2032]_ _, x, x_ _[\u2032]_ _\u2208_ X _}_ and note that Z is contained in a ball of radius at most 2 _R_ . Z is a closed set since X is closed and thus Z is a compact set. For convenience, define _B_ = _N_ (2 _R, r_ ) the number of balls of radius _r_ needed **6.6** **Approximate kernel feature maps** **133** to cover Z and let _z_ _j_, for _j \u2208_ [ _B_ ], denote the center of the covering balls. Thus, for any _z \u2208_ Z there exists a _j_ such that _z_ = _z_ _j_ + _\u03b4_ where _|\u03b4| < r_ . Next, define _S_ ( _z_ ) = \u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ ) _\u2212_ _K_ ( _x, x_ _[\u2032]_ ), where _z_ = _x \u2212_ _x_ _[\u2032]_ . Since _S_ is continuously differentiable over the compact set Z, it is _L_ -Lipschitz with _L_ = sup _z\u2208_ Z _\u2225\u2207S_ ( _z_ ) _\u2225_ . Note that if _L <_ 2 _\u03f5r_ [and for all] _[ j][ \u2208]_ [[] _[B]_ [] we have] _[ |][S]_ [(] _[z]_ _[j]_ [)] _[|][ <]_ 2 _\u03f5_ [,] then the following inequality holds for all _z_ = _z_ _j_ + _\u03b4 \u2208_ Z: _|S_ ( _z_ ) _|_ = _|S_ ( _z_ _j_ + _\u03b4_ ) _| \u2264_ _L|z_ _j_ _\u2212_ ( _z_ _j_ + _\u03b4_ ) _|_ + _|S_ ( _z_ _j_ ) _| \u2264_ _rL_ + _[\u03f5]_ (6.22) 2 _[< \u03f5 .]_ The remainder of this proof bounds the probability of the events _L \u2265_ 2 _\u03f5r_ [and] _|S_ ( _z_ _j_ ) _| \u2265_ 2 _[\u03f5]_ [. Note, all following probabilities and expectations are with respect to] the random variables _\u03c9_ 1 _, . . ., \u03c9_ _D_ . To bound the probability of the first event, we use proposition 6.25 and the linearity of expectation, which implies the key fact E[ _\u2207_ (\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ ))] = _\u2207K_ ( _x, x_ _[\u2032]_ ). We proceed with the following series of inequalities: E[ _L_ [2] ] = E sup _\u2225\u2207S_ ( _z_ ) _\u2225_ [2] \ufffd _z\u2208_ Z \ufffd = E sup _\u2225\u2207_ (\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ )) _\u2212\u2207K_ ( _x, x_ _[\u2032]_ ) _\u2225_ [2] \ufffd _x,x_ _[\u2032]_ _\u2208_ X \ufffd",
    "chunk_id": "foundations_machine_learning_131"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2264_ 2 E sup _\u2225\u2207_ (\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ )) _\u2225_ [2] + 2 sup _\u2225\u2207K_ ( _x, x_ _[\u2032]_ ) _\u2225_ [2] \ufffd _x,x_ _[\u2032]_ _\u2208_ X \ufffd _x,x_ _[\u2032]_ _\u2208_ X = 2 E sup _\u2225\u2207_ (\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ )) _\u2225_ [2] + 2 sup _\u2225_ E[ _\u2207_ (\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ ))] _\u2225_ [2] \ufffd _x,x_ _[\u2032]_ _\u2208_ X \ufffd _x,x_ _[\u2032]_ _\u2208_ X _\u2264_ 4 E sup _\u2225\u2207_ (\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ )) _\u2225_ [2] _,_ \ufffd _x,x_ _[\u2032]_ _\u2208_ X \ufffd where the first inequality holds due to the the inequality _\u2225a_ + _b\u2225_ [2] _\u2264_ 2 _\u2225a\u2225_ [2] + 2 _\u2225b\u2225_ [2] (which follows from Jensen\u2019s inequality) and the subadditivity of the supremum function. The second inequality also holds by Jensen\u2019s inequality (applied twice) and again the subadditivity of supremum function. Furthermore, using a sumdifference trigonometric identity and computing the gradient with respect to _z_ = _x \u2212_ _x_ _[\u2032]_, yield the following for any _x, x_ _[\u2032]_ _\u2208_ X: 1 _\u2207_ (\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ )) = _\u2207_ \ufffd _D_ 1 = _\u2207_ \ufffd _D_ 1 _\u2207_ (\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ )) = _\u2207_ \ufffd _D_ _D_ \ufffd cos( _\u03c9_ _i_ _\u00b7 x_ ) cos( _\u03c9_ _i_ _\u00b7 x_ _[\u2032]_ ) + sin( _\u03c9_ _i_ _\u00b7 x_ ) sin( _\u03c9_ _i_ _\u00b7 x_ _[\u2032]_ ) _i_ =1 \ufffd _D_ 1 = _\u2207_ \ufffd _D_ _D_ \ufffd \ufffd _i_ =1 cos( _\u03c9_ _i_ _\u00b7_ ( _x \u2212_ _x_ _[\u2032]_ ))\ufffd = _D_ [1] _D_ \ufffd _\u03c9_ _i_ sin( _\u03c9_ _i_ _\u00b7_ ( _x \u2212_ _x_ _[\u2032]_ )) _._ _i_ =1 **134** **Chapter 6** **Kernel Methods** Combining the two previous results gives 2 [\ufffd] \ufffd _\u03c9_ _i_ sin( _\u03c9_ _i_ _\u00b7_ ( _x \u2212_ _x_ _[\u2032]_ )) _i_ =1 \ufffd\ufffd\ufffd\ufffd E[ _L_ [2] ] _\u2264_ 4 E sup \ufffd _x,x_ _[\u2032]_ _\u2208_ X 1 \ufffd\ufffd\ufffd\ufffd _D_ _D_ \ufffd _D_ 2 [\ufffd] \ufffd _\u2225\u03c9_ _i_ _\u2225_ \ufffd _i_ =1 _\u2264_ 4 E _\u03c9_ 1 _,...,\u03c9_ _N_ _\u2264_ 4 E _\u03c9_ 1 _,...,\u03c9_ _N_ 1 \ufffd\ufffd _D_ 1 \ufffd _D_ _D_ \ufffd _\u2225\u03c9_ _i_ _\u2225_ [2] = 4 E _\u03c9_ \ufffd _\u2225\u03c9\u2225_ [2] [\ufffd] = 4 _\u03c3_ _p_ [2] _[,]_ _i_ =1 \ufffd _\u00b7_ which follows from the triangle inequality, _|_ sin( ) _| \u2264_ 1, Jensen\u2019s inequality and the fact that the _\u03c9_ _i_ s are drawn i.i.d. derive the final expression. Thus, we can bound the probability of the first event via Markov\u2019s inequality: _\u2264_ 4 _r\u03c3_ _p_ \ufffd \ufffd _\u03f5_ _\u2264_ 4 _r\u03c3_ _p_ \ufffd \ufffd _\u03f5_ 2 _._ (6.23) \ufffd P \ufffd _L \u2265_ 2 _[\u03f5]_ _r_ P \ufffd _L \u2265_ 2 _[\u03f5]_ _r_ To bound the probability of the second event, note that, by definition, _S_ ( _z_ ) is a sum of _D_ i.i.d. variables, each bounded in absolute value by _D_ 2 [(since, for all] _[ x]_ and _x_ _[\u2032]_, we have _|K_ ( _x, x_ _[\u2032]_ ) _| \u2264_ 1 and _|_",
    "chunk_id": "foundations_machine_learning_132"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ _[\u2032]_ ) _| \u2264_ 1), and E[ _S_ ( _z_ )] = 0. Thus, by Hoeffding\u2019s inequality and the union bound, we can write 2 P _\u2203j \u2208_ [ _B_ ]: _|S_ ( _z_ _j_ ) _| \u2265_ _[\u03f5]_ \ufffd 2 _\u2264_ \ufffd _B_ \ufffd \ufffd P \ufffd _|S_ ( _z_ _j_ ) _| \u2265_ 2 _[\u03f5]_ _i_ =1 _\u2264_ 2 _B_ exp _\u2212_ _[D\u03f5]_ [2] \ufffd \ufffd 8 _\u2264_ 2 _B_ exp _\u2212_ _[D\u03f5]_ [2] \ufffd \ufffd 8 _._ (6.24) \ufffd Finally, combining (6.22), (6.23), (6.24), and the definition of _B_ we have P sup _|S_ ( _z_ ) _| \u2265_ _\u03f5_ _\u2264_ 2 _N_ (2 _R, r_ ) exp _\u2212_ _[D\u03f5]_ [2] \ufffd _z\u2208Z_ \ufffd \ufffd 8 + 4 _r\u03c3_ _p_ \ufffd \ufffd _\u03f5_ 2 _,_ \ufffd which completes the lemma. A key factor in the bound of the lemma is the covering number _N_ (2 _R, r_ ), which strongly depends on the dimension of the space _N_ . In the following lemma, we make this dependency explicit for one especially simple case, although similar arguments hold for more general scenarios as well. **Lemma 6.27** _Let_ X _\u2282_ R _[N]_ _be a compact and let R denote the radius of the smallest_ _enclosing ball. Then, the following inequality holds:_ 3 _R_ _N_ ( _R, r_ ) _\u2264_ \ufffd _r_ _N_ _._ \ufffd Proof: First, by using the volume of balls in R _[N]_ we already see that _R_ _[N]_ _/_ ( _r/_ 3) _[N]_ = (3 _R/r_ ) _[N]_ is a trivial upper bound on the number of balls of radius _r/_ 3 that can be packed into a ball of radius _R_ without intersecting. Now, consider a maximal packing of at most (3 _R/r_ ) _[N]_ balls of radius _r/_ 3 into the ball of radius _R_ . Every **6.7** **Chapter notes** **135** point in the ball of radius _R_ is at distance at most _r_ from the center of at least one of the packing balls. If this were not true, we would be able to fit another ball into the packing, thereby contradicting the assumption that it is a maximal packing. Thus, if we grow the radius of the at most (3 _R/r_ ) _[N]_ balls to _r_, they will then provide a (not necessarily minimal) cover of the ball of radius _R_ . Finally, by combining the two previous lemmas, we can present an explicit finite sample approximation bound. **Theorem 6.28** _Let K be a continuously differentiable kernel function that satisfies the_ _conditions of proposition 6.25 and has associated measure p. Furthermore, assume_ _\u03c3_ _p_ [2] [=][ E] _[\u03c9][\u223c][p]_ [[] _[\u2225][\u03c9][\u2225]_ [2] []] _[ <][ \u221e]_ _[and]_ [ X] _[ \u2282]_ [R] _[N]_ _[. Let][ R][ denote the radius of the Euclidean ball]_ _containing_ X _. Then, for_ \u03a8 _\u2208_ R _[D]_ _as defined in_ (6.21) _and any_ 0 _< \u03f5 \u2264_ 32 _R\u03c3_ _p_ _, the_ _following holds_ _._ \ufffd 2 _D\u03f5_ [2] exp _\u2212_ \ufffd \ufffd 4( _N_ + 2) P sup \ufffd _x,x_ _[\u2032]_ _\u2208_ X",
    "chunk_id": "foundations_machine_learning_133"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2032_ _\u2032_ 48 _R\u03c3_ _p_ \ufffd\ufffd\u03a8( _x_ ) _\u00b7_ \u03a8( _x_ ) _\u2212_ _K_ ( _x, x_ )\ufffd\ufffd _\u2265_ _\u03f5_ _\u2264_ \ufffd \ufffd _\u03f5_ Proof: We use lemma 6.27 in conjunction with lemma 6.26 with the following choice of _r_ : 2 _N_ +2 _,_ \ufffd _r_ = 2(6 _R_ ) _[N]_ exp( _\u2212_ _[D\u03f5]_ 8 [2] 4 _\u03c3_ _p_ 2 \ufffd \ufffd _\u03f5_ \ufffd exp( _\u2212_ 8 [)] \ufffd 4 _\u03c3\u03f5_ _p_ \ufffd 2 _\u03c3\u03f5_ _p_ \ufffd 2 which results in the following expression P sup _|S_ ( _z_ ) _| \u2265_ _\u03f5_ _\u2264_ 4 24 _R\u03c3_ _p_ \ufffd _z\u2208Z_ \ufffd \ufffd _\u03f5_ \ufffd _N_ [2] _[N]_ _N_ +2 _D\u03f5_ [2] exp _\u2212_ \ufffd 4( _N_ 4( _N_ + 2) _._ \ufffd Since 32 _R\u03c3_ _p_ _/\u03f5 \u2265_ 1, the exponent _N_ 2 _N_ +2 [can be replaced by 2, which completes the] proof. The previous theorem provides the guarantee that a good estimate of the kernel function can be found, with high probability, by sampling a finite number of coordinates _D_ . In particular, for an absolute error of at most _\u03f5_ it suffices to sample _D_ = _O_ _N_ _R\u03c3_ _p_ coordinates. \ufffd _\u03f5_ [2] [ log] \ufffd _\u03f5_ \ufffd\ufffd **6.7** **Chapter notes** The mathematical theory of PDS kernels in a general setting originated with the fundamental work of Mercer [1909] who also proved the equivalence of a condition similar to that of theorem 6.2 for continuous kernels with the PDS property. The connection between PDS and NDS kernels, in particular theorems 6.18 and 6.17, are due to Schoenberg [1938]. A systematic treatment of the theory of reproducing kernel Hilbert spaces was presented in a long and elegant paper by Aronszajn [1950]. For an excellent mathematical presentation of PDS kernels and positive definite **136** **Chapter 6** **Kernel Methods** functions we refer the reader to Berg, Christensen, and Ressel [1984], which is also the source of several of the exercises given in this chapter. The fact that SVMs could be extended by using PDS kernels was pointed out by Boser, Guyon, and Vapnik [1992]. The idea of kernel methods has been since then widely adopted in machine learning and applied in a variety of different tasks and settings. The following two books are in fact specifically devoted to the study of kernel methods: Sch\u00a8olkopf and Smola [2002] and Shawe-Taylor and Cristianini [2004]. The classical representer theorem is due to Kimeldorf and Wahba [1971]. A generalization to non-quadratic cost functions was stated by Wahba [1990]. The general form presented in this chapter was given by Sch\u00a8olkopf, Herbrich, Smola, and Williamson [2000]. Rational kernels were introduced by Cortes, Haffner, and Mohri [2004]. A general class of kernels, _convolution kernels_, was earlier introduced by Haussler [1999]. The convolution kernels for sequences described by Haussler [1999], as well as the pairHMM string kernels described by Watkins [1999], are special instances of rational kernels. Rational kernels can be straightforwardly extended to define kernels for finite automata and even weighted automata [Cortes et al., 2004]. Cortes, Mohri, and Rostamizadeh [2008b] study the problem of _learning_ rational",
    "chunk_id": "foundations_machine_learning_134"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "kernels such as those based on counting transducers. The composition of weighted transducers and the filter transducers in the presence of _\u03f5_ -paths are described in Pereira and Riley [1997], Mohri, Pereira, and Riley [2005], and Mohri [2009]. Composition can be further generalized to the _N_ _-way_ _composition_ of weighted transducers [Allauzen and Mohri, 2009]. _N_ -way composition of three or more transducers can substantially speed up computation, in particular for PDS rational kernels of the form _T \u25e6_ _T_ _[\u2212]_ [1] . A generic _shortest-distance_ _algorithm_ which can be used with a large class of semirings and arbitrary queue disciplines is described by Mohri [2002]. A specific instance of that algorithm can be used to compute the sum of the weights of all paths as needed for the computation of rational kernels after composition. For a study of the class of languages linearly separable with rational kernels, see Cortes, Kontorovich, and Mohri [2007a]. The use of cosine-based approximate kernel feature maps was introduced by Rahimi and Recht [2007], as were the corresponding uniform convergence bounds, though their proofs were not complete. Sriperumbudur and Szab\u00b4o [2015] gave an improved approximation bound that reduces the dependence on the radius of the data from _O_ ( _R_ [2] ) to only _O_ (log( _R_ )). Bochner\u2019s theorem, which plays a central role in deriving an approximate map, is a classical result of harmonic analysis (for example, see Rudin [1990]). The general form of the theorem is due to Weil [1965], while Solomon Bochner recognized its importance to harmonic analysis. **6.8** **Exercises** **137** **6.8** **Exercises** 6.1 Let _K_ : X _\u00d7_ X _\u2192_ R be a PDS kernel, and let _\u03b1_ : X _\u2192_ R be a positive function. _K_ ( _x,y_ ) Show that the kernel _K_ _[\u2032]_ defined for all _x, y \u2208_ X by _K_ _[\u2032]_ ( _x, y_ ) = _\u03b1_ ( _x_ ) _\u03b1_ ( _y_ ) [is a PDS] kernel. 6.2 Show that the following kernels _K_ are PDS: (a) _K_ ( _x, y_ ) = cos( _x \u2212_ _y_ ) over R _\u00d7_ R. (b) _K_ ( _x, y_ ) = cos( _x_ [2] _\u2212_ _y_ [2] ) over R _\u00d7_ R. (c) For all integers _n >_ 0 _, K_ ( **x** _,_ **y** ) = [\ufffd] _[N]_ _i_ =1 [cos] _[n]_ [(] _[x]_ _i_ [2] _[\u2212]_ _[y]_ _i_ [2] [) over][ R] _[N]_ _[ \u00d7]_ [ R] _[N]_ [.] (d) _K_ ( _x, y_ ) = ( _x_ + _y_ ) _[\u2212]_ [1] over (0 _,_ + _\u221e_ ) _\u00d7_ (0 _,_ + _\u221e_ ). (e) _K_ ( **x** _,_ **x** _[\u2032]_ ) = cos \u2220( **x** _,_ **x** _[\u2032]_ ) over R _[n]_ _\u00d7_ R _[n]_, where \u2220( **x** _,_ **x** _[\u2032]_ ) is the angle between **x** and **x** _[\u2032]_ . (f) _\u2200\u03bb >_ 0 _, K_ ( _x, x_ _[\u2032]_ ) = exp \ufffd _\u2212_ _\u03bb_ [sin( _x_ _[\u2032]_ _\u2212_ _x_ )] [2] [\ufffd] over R _\u00d7_ R. ( _Hint_ : rewrite [sin( _x_ _[\u2032]_ _\u2212_ _x_ )] [2] as the square of",
    "chunk_id": "foundations_machine_learning_135"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the norm of the difference of two vectors.) (g) _\u2200\u03c3 >_ 0 _, K_ ( _x, y_ ) = _e_ _[\u2212]_ _[\u2225]_ **[x]** _[\u2212]_ _\u03c3_ **[y]** _[\u2225]_ over R _[N]_ _\u00d7_ R _[N]_ . ( _Hint_ : you could show that _K_ is the normalized kernel of a kernel _K_ _[\u2032]_ and show that _K_ _[\u2032]_ is PDS using the following equality: _\u2225_ **x** _\u2212_ **y** _\u2225_ = + _\u221e_ 1 _\u2212e_ _[\u2212][t][\u2225]_ **[x]** _[\u2212]_ **[y]** _[\u2225]_ [2] [1] 2 [)] \ufffd 0 _t_ 23 1 2\u0393( [1] 3 _dt_ valid for all **x** _,_ **y** .) _t_ 2 (h) _K_ ( _x, y_ ) = min( _x, y_ ) _\u2212_ _xy_ over [0 _,_ 1] _\u00d7_ [0 _,_ 1]. 1 ( _Hint_ : you could consider the two integrals \ufffd 0 [1] _[t][\u2208]_ [[0] _[,x]_ []] [1] _[t][\u2208]_ [[0] _[,y]_ []] _[dt]_ [ and] \ufffd 01 [1] _[t][\u2208]_ [[] _[x,]_ [1]] [1] _[t][\u2208]_ [[] _[y,]_ [1]] _[dt]_ [.)] (i) _K_ ( _x, x_ _[\u2032]_ ) = _\u221a_ 1 _\u2212_ 1( **x** _\u00b7_ **x** _[\u2032]_ ) [over] **[ x]** _[,]_ **[ x]** _[\u2032]_ _[ \u2208]_ [X][ =] _[ {]_ **[x]** _[ \u2208]_ [R] _[N]_ [ :] _[ \u2225]_ **[x]** _[\u2225]_ [2] _[ <]_ [ 1] _[}]_ [.] ( _Hint_ : one approach is to find an explicit expression of a feature mapping \u03a6 by considering the Taylor expansion of the kernel function.) (j) _\u2200\u03c3 >_ 0 _, K_ ( _x, y_ ) = 1 1+ _[\u2225][x][\u2212][y][\u2225]_ [2] _[\u2212][y][\u2225]_ [2] over R _[N]_ _\u00d7_ R _[N]_ . _\u03c3_ [2] + _\u221e_ ( _Hint_ : the function _x \ufffd\u2192_ \ufffd 0 _e_ _[\u2212][sx]_ _e_ _[\u2212][s]_ _ds_ defined for all _x \u2265_ 0 could be useful for the proof.) (k) _\u2200\u03c3 >_ 0 _, K_ ( _x, y_ ) = exp \ufffd _Ni_ =1 [min] _\u03c3_ [2] [(] _[|][x]_ _[i]_ _[|][,][|][y]_ _[i]_ _[|]_ [)] \ufffd over R _[N]_ _\u00d7_ R _[N]_ . \ufffd + _\u221e_ ( _Hint_ : the function ( _x_ 0 _, y_ 0 ) _\ufffd\u2192_ \ufffd 0 1 _t\u2208_ [0 _,|x_ 0 _|_ ] 1 _t\u2208_ [0 _,|y_ 0 _|_ ] _dt_ defined over R _\u00d7_ R could be useful for the proof.) **138** **Chapter 6** **Kernel Methods** 6.3 Graph kernel. Let _G_ = (V _,_ E) be an undirected graph with vertex set V and edge set E. V could represent a set of documents or biosequences and _E_ the set of connections between them. Let _w_ [ _e_ ] _\u2208_ R denote the weight assigned to edge _e \u2208_ E. The weight of a path is the product of the weights of its constituent edges. Show that the kernel _K_ over V _\u00d7_ V where _K_ ( _p, q_ ) is the sum of the weights of all paths of length two between _p_ and _q_ is PDS ( _Hint_ : you could introduce the matrix _W_ = ( _W_ _pq_ ), where _W_ _pq_ = 0 when there is no edge between _p_ and _q_, _W_ _pq_ equal to the weight of the edge between _p_ and _q_ otherwise).",
    "chunk_id": "foundations_machine_learning_136"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "6.4 Symmetric difference kernel. Let X be a finite set. Show that the kernel _K_ defined over 2 [X], the set of subsets of X, by _\u2200_ A _,_ B _\u2208_ 2 [X] _, K_ (A _,_ B) = exp _\u2212_ [1] _,_ \ufffd 2 _[|]_ [A][\u2206][B] _[|]_ \ufffd where A\u2206B is the symmetric difference of A and B is PDS ( _Hint_ : you could use the fact that _K_ is the result of the normalization of a kernel function _K_ _[\u2032]_ ). 6.5 Set kernel. Let X be a finite set. Let _K_ 0 be a PDS kernel over X, show that _K_ _[\u2032]_ defined by _\u2200_ A _,_ B _\u2208_ 2 [X] _, K_ _[\u2032]_ (A _,_ B) = \ufffd _K_ 0 ( _x, x_ _[\u2032]_ ) _x\u2208_ A _,x_ _[\u2032]_ _\u2208_ B is a PDS kernel. 6.6 Show that the following kernels _K_ are NDS: (a) _K_ ( _x, y_ ) = [sin( _x \u2212_ _y_ )] [2] over R _\u00d7_ R. (b) _K_ ( _x, y_ ) = log( _x_ + _y_ ) over (0 _,_ + _\u221e_ ) _\u00d7_ (0 _,_ + _\u221e_ ). 6.7 Define a _difference kernel_ as _K_ ( _x, x_ _[\u2032]_ ) = _|x \u2212_ _x_ _[\u2032]_ _|_ for _x, x_ _[\u2032]_ _\u2208_ R. Show that this kernel is not positive definite symmetric (PDS). 6.8 Is the kernel _K_ defined over R _[n]_ _\u00d7_ R _[n]_ by _K_ ( **x** _,_ **y** ) = _\u2225_ **x** _\u2212_ **y** _\u2225_ [3] _[/]_ [2] PDS? Is it NDS? 6.9 Let H be a Hilbert space with the corresponding dot product _\u27e8\u00b7, \u00b7\u27e9_ . Show that the kernel _K_ defined over H _\u00d7_ H by _K_ ( _x, y_ ) = 1 _\u2212\u27e8x, y\u27e9_ is negative definite. 6.10 For any _p >_ 0, let _K_ _p_ be the kernel defined over R + _\u00d7_ R + by _K_ _p_ ( _x, y_ ) = _e_ _[\u2212]_ [(] _[x]_ [+] _[y]_ [)] _[p]_ _._ (6.25) Show that _K_ _p_ is positive definite symmetric (PDS) iff _p \u2264_ 1. ( _Hint_ : you can use the fact that if _K_ is NDS, then for any 0 _< \u03b1 \u2264_ 1, _K_ _[\u03b1]_ is also NDS.) **6.8** **Exercises** **139** 6.11 Explicit mappings. (a) Denote a data set _x_ 1 _, . . ., x_ _m_ and a kernel _K_ ( _x_ _i_ _, x_ _j_ ) with a Gram matrix **K** . Assuming **K** is positive semidefinite, then give a map \u03a6( _\u00b7_ ) such that _K_ ( _x_ _i_ _, x_ _j_ ) = _\u27e8_ \u03a6( _x_ _i_ ) _,_ \u03a6( _x_ _j_ ) _\u27e9_ . (b) Show the converse of the previous statement, i.e., if there exists a mapping \u03a6( _x_ ) from input space to some Hilbert space, then the corresponding matrix **K** is positive semidefinite. 6.12 Explicit polynomial kernel mapping. Let _K_ be a polynomial kernel of degree _d_, i.e., _K_ : R _[N]_ _\u00d7_ R _[N]_ _\u2192_ R, _K_ ( **x** _,_ **x** _[\u2032]_ ) = ( **x** _\u00b7_ **x** _[\u2032]_ + _c_",
    "chunk_id": "foundations_machine_learning_137"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ") _[d]_, with _c >_ 0, Show that the dimension of the feature space associated to _K_ is _N_ + _d_ _._ (6.26) \ufffd _d_ \ufffd Write _K_ in terms of kernels _k_ _i_ : ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ ( **x** _\u00b7_ **x** _[\u2032]_ ) _[i]_, _i \u2208{_ 0 _, . . ., d}_ . What is the weight assigned to each _k_ _i_ in that expression? How does it vary as a function of _c_ ? 6.13 High-dimensional mapping. Let \u03a6: X _\u2192_ H be a feature mapping such that the dimension _N_ of H is very large and let _K_ : X _\u00d7_ X _\u2192_ R be a PDS kernel defined by _K_ ( _x, x_ _[\u2032]_ ) = E _i\u223c_ D \ufffd[\u03a6( _x_ )] _i_ [\u03a6( _x_ _[\u2032]_ )] _i_ \ufffd _,_ (6.27) where [\u03a6( _x_ )] _i_ is the _i_ th component of \u03a6( _x_ ) (and similarly for \u03a6 _[\u2032]_ ( _x_ )) and where D is a distribution over the indices _i_ . We shall assume that _|_ [\u03a6( _x_ )] _i_ _| \u2264_ _R_ for all _x \u2208_ X and _i \u2208_ [ _N_ ]. Suppose that the only method available to compute _K_ ( _x, x_ _[\u2032]_ ) involved direct computation of the inner product (6.27), which would require _O_ ( _N_ ) time. Alternatively, an approximation can be computed based on random selection of a subset _I_ of the _N_ components of \u03a6( _x_ ) and \u03a6( _x_ _[\u2032]_ ) according to D, that is: _K_ _[\u2032]_ ( _x, x_ _[\u2032]_ ) = [1] \ufffd D( _i_ )[\u03a6( _x_ )] _i_ [\u03a6( _x_ _[\u2032]_ )] _i_ _,_ (6.28) _n_ \ufffd \ufffd D( _i_ )[\u03a6( _x_ )] _i_ [\u03a6( _x_ _[\u2032]_ )] _i_ _,_ (6.28) _i\u2208I_ where _|I|_ = _n_ . (a) Fix _x_ and _x_ _[\u2032]_ in X. Prove that P _I\u223c_ D _[n]_ [[] _[|][K]_ [(] _[x, x]_ _[\u2032]_ [)] _[ \u2212]_ _[K]_ _[\u2032]_ [(] _[x, x]_ _[\u2032]_ [)] _[|][ > \u03f5]_ []] _[ \u2264]_ [2] _[e]_ ( _Hint_ : use McDiarmid\u2019s inequality). _\u2212n\u03f5_ [2] 2 _r_ [2] _._ (6.29) **140** **Chapter 6** **Kernel Methods** (b) Let **K** and **K** _[\u2032]_ be the kernel matrices associated to _K_ and _K_ _[\u2032]_ . Show that _r_ [2] for any _\u03f5, \u03b4 >_ 0, for _n >_ _\u03f5_ [2] [ log] _[ m]_ [(] _[m]_ _\u03b4_ [+][1][)], with probability at least 1 _\u2212_ _\u03b4_, _|_ **K** _[\u2032]_ _ij_ _[\u2212]_ **[K]** _[ij]_ _[| \u2264]_ _[\u03f5]_ [ for all] _[ i, j][ \u2208]_ [[] _[m]_ [].] 6.14 Classifier based kernel. Let _S_ be a training sample of size _m_ . Assume that _S_ has been generated according to some probability distribution D( _x, y_ ), where ( _x, y_ ) _\u2208_ X _\u00d7 {\u2212_ 1 _,_ +1 _}_ . (a) Define the Bayes classifier _h_ _[\u2217]_ : X _\u2192{\u2212_ 1 _,_ +1 _}_ . Show that the kernel _K_ _[\u2217]_ defined by _K_ _[\u2217]_ ( _x, x_ _[\u2032]_ ) = _h_ _[\u2217]_ ( _x_ ) _h_ _[\u2217]_ ( _x_ _[\u2032]_ ) for any",
    "chunk_id": "foundations_machine_learning_138"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_x, x_ _[\u2032]_ _\u2208_ X is positive definite symmetric. What is the dimension of the natural feature space associated to _K_ _[\u2217]_ ? (b) Give the expression of the solution obtained using SVMs with this kernel. What is the number of support vectors? What is the value of the margin? What is the generalization error of the solution obtained? Under what condition are the data linearly separable? (c) Let _h_ : X _\u2192_ R be an arbitrary real-valued function. Under what condition on _h_ is the kernel _K_ defined by _K_ ( _x, x_ _[\u2032]_ ) = _h_ ( _x_ ) _h_ ( _x_ _[\u2032]_ ), _x, x_ _[\u2032]_ _\u2208_ X, positive definite symmetric? 6.15 Image classification kernel. For _\u03b1 \u2265_ 0, the kernel _K_ _\u03b1_ : ( **x** _,_ **x** _[\u2032]_ ) _\ufffd\u2192_ _N_ \ufffd min( _|x_ _k_ _|_ _[\u03b1]_ _, |x_ _[\u2032]_ _k_ _[|]_ _[\u03b1]_ [)] (6.30) _k_ =1 over R _[N]_ _\u00d7_ R _[N]_ is used in image classification. Show that _K_ _\u03b1_ is PDS for all _\u03b1 \u2265_ 0. To do so, proceed as follows. + _\u221e_ (a) Use the fact that ( _f, g_ ) _\ufffd\u2192_ \ufffd _t_ =0 _[f]_ [(] _[t]_ [)] _[g]_ [(] _[t]_ [)] _[dt]_ [ is an inner product over the set] of measurable functions over [0 _,_ + _\u221e_ ) to show that ( _x, x_ _[\u2032]_ ) _\ufffd\u2192_ min( _x, x_ _[\u2032]_ ) is a PDS kernel. ( _Hint_ : associate an indicator function to _x_ and another one to _x_ _[\u2032]_ .) (b) Use the result from (a) to first show that _K_ 1 is PDS and similarly that _K_ _\u03b1_ with other values of _\u03b1_ is also PDS. 6.16 Fraud detection. To prevent fraud, a credit-card company decides to contact Professor Villebanque and provides him with a random list of several thousand fraudulent and non-fraudulent _events_ . There are many different types of events, e.g., transactions of various amounts, changes of address or card-holder **6.8** **Exercises** **141** information, or requests for a new card. Professor Villebanque decides to use SVMs with an appropriate kernel to help predict fraudulent events accurately. It is difficult for Professor Villebanque to define relevant features for such a diverse set of events. However, the risk department of his company has created a complicated method to estimate a probability P[ _U_ ] for any event _U_ . Thus, Professor Villebanque decides to make use of that information and comes up with the following kernel defined over all pairs of events ( _U, V_ ): _K_ ( _U, V_ ) = P[ _U \u2227_ _V_ ] _\u2212_ P[ _U_ ] P[ _V_ ] _._ (6.31) Help Professor Villebanque show that his kernel is positive definite symmetric. 6.17 Relationship between NDS and PDS kernels. Prove the statement of theorem 6.17. ( _Hint_ : Use the fact that if _K_ is PDS then exp( _K_ ) is also PDS, along with theorem 6.16.) 6.18 Metrics and Kernels. Let X be a non-empty set and _K_ : X _\u00d7_ X _\u2192_ R be a negative definite symmetric kernel such",
    "chunk_id": "foundations_machine_learning_139"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "that _K_ ( _x, x_ ) = 0 for all _x \u2208_ X. (a) Show that there exists a Hilbert space H and a mapping \u03a6( _x_ ) from X to H such that: _K_ ( _x, y_ ) = _||_ \u03a6( _x_ ) _\u2212_ \u03a6( _x_ _[\u2032]_ ) _||_ [2] _._ Assume that _K_ ( _x, x_ _[\u2032]_ ) = 0 _\u21d2_ _x_ = _x_ _[\u2032]_ . Use theorem 6.16 to show that _\u221aK_ defines a metric on X. (b) Use this result to prove that the kernel _K_ ( _x, y_ ) = exp( _\u2212|x \u2212_ _x_ _[\u2032]_ _|_ _[p]_ ), _x, x_ _[\u2032]_ _\u2208_ R, is not positive definite for _p >_ 2. (c) The kernel _K_ ( _x, x_ _[\u2032]_ ) = tanh( _a_ ( _x \u00b7 x_ _[\u2032]_ ) + _b_ ) was shown to be equivalent to a two-layer neural network when combined with SVMs. Show that _K_ is not positive definite if _a <_ 0 or _b <_ 0. What can you conclude about the corresponding neural network when _a <_ 0 or _b <_ 0? 6.19 Sequence kernels. Let X = _{a, c, g, t}_ . To classify DNA sequences using SVMs, we wish to define a kernel between sequences defined over X. We are given a finite set I _\u2282_ X _[\u2217]_ of non-coding regions (introns). For _x \u2208_ X _[\u2217]_, denote by _|x|_ the length of _x_ and by _F_ ( _x_ ) the set of factors of _x_, i.e., the set of subsequences of _x_ with contiguous symbols. For any two strings _x, y \u2208_ X _[\u2217]_ define _K_ ( _x, y_ ) by _K_ ( _x, y_ ) = \ufffd _\u03c1_ _[|][z][|]_ _,_ (6.32) _z \u2208_ ( _F_ ( _x_ ) _\u2229F_ ( _y_ )) _\u2212_ I where _\u03c1 \u2265_ 1 is a real number. **142** **Chapter 6** **Kernel Methods** (a) Show that _K_ is a rational kernel and that it is positive definite symmetric. (b) Give the time and space complexity of the computation of _K_ ( _x, y_ ) with respect to the size _s_ of a minimal automaton representing X _[\u2217]_ _\u2212_ I. (c) Long common factors between _x_ and _y_ of length greater than or equal to _n_ are likely to be important coding regions (exons). Modify the kernel _K_ to assign weight _\u03c1_ _[|]_ 2 _[z][|]_ to _z_ when _|z| \u2265_ _n_, _\u03c1_ _[|]_ 1 _[z][|]_ otherwise, where 1 _\u2264_ _\u03c1_ 1 _\u226a_ _\u03c1_ 2 . Show that the resulting kernel is still positive definite symmetric. 6.20 _n_ -gram kernel. Show that for all _n \u2265_ 1, and any _n_ -gram kernel _K_ _n_, _K_ _n_ ( _x, y_ ) can be computed in linear time _O_ ( _|x|_ + _|y|_ ), for all _x, y \u2208_ \u03a3 _[\u2217]_ assuming _n_ and the alphabet size are constants. 6.21 Mercer\u2019s condition. Let X _\u2282_ R _[N]_ be a compact set and _K_ : X _\u00d7_ X _\u2192_ R a continuous kernel function. Prove that if _K_ verifies Mercer\u2019s condition (theorem 6.2), then it is PDS. (",
    "chunk_id": "foundations_machine_learning_140"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_Hint_ : assume that _K_ is not PDS and consider a set _{x_ 1 _, . . ., x_ _m_ _} \u2286_ X and a column-vector _c \u2208_ R _[m][\u00d7]_ [1] such that [\ufffd] _[m]_ _i,j_ =1 _[c]_ _[i]_ _[c]_ _[j]_ _[K]_ [(] _[x]_ _[i]_ _[, x]_ _[j]_ [)] _<_ 0.) 6.22 Anomaly detection. For this problem, consider a Hilbert space H with associated feature map \u03a6: X _\u2192_ H and kernel _K_ ( _x, x_ _[\u2032]_ ) = \u03a6( _x_ ) _\u00b7_ \u03a6( _x_ _[\u2032]_ ). (a) First, let us consider finding the smallest enclosing sphere for a given sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ). Let **c** _\u2208_ H denote the center of the sphere and let _r >_ 0 be its radius, then clearly the following optimization problem searches for the smallest enclosing sphere: min _r>_ 0 _,_ **c** _\u2208_ H _[r]_ [2] subject to: _\u2200i \u2208_ [ _m_ ] _, \u2225_ \u03a6( _x_ _i_ ) _\u2212_ **c** _\u2225_ [2] _\u2264_ _r_ [2] _._ Show how to derive the equivalent dual optimization _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _K_ ( _x_ _i_ _, x_ _j_ ) _i,j_ =1 max _**\u03b1**_ _m_ \ufffd _\u03b1_ _i_ _K_ ( _x_ _i_ _, x_ _i_ ) _\u2212_ _i_ =1 subject to: _**\u03b1**_ _\u2265_ **0** _\u2227_ _m_ \ufffd _\u03b1_ _i_ = 1 _,_ _i_ =1 **6.8** **Exercises** **143** and prove that the optimal solution satisfies **c** = [\ufffd] _i_ _[\u03b1]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [). In other words] the location of the sphere only depends on points _x_ _i_ with non-zero coefficients _\u03b1_ _i_ . These points are analogous to the support vectors of SVM. (b) Consider the hypothesis class H = _{x \ufffd\u2192_ _r_ [2] _\u2212\u2225_ \u03a6( _x_ ) _\u2212_ **c** _\u2225_ [2] : _\u2225_ **c** _\u2225\u2264_ \u039b _,_ 0 _< r \u2264_ _R} ._ A hypothesis _h \u2208_ H can be used to detect anomalies in data, where _h_ ( _x_ ) _\u2265_ 0 indicates a non-anomalous point and _h_ ( _x_ ) _<_ 0 indicates an anomaly. Show that if sup _x_ _\u2225_ \u03a6( _x_ ) _\u2225\u2264_ _M_, then the solution to the optimization problem in part (a) is found in the hypothesis set H with \u039b _\u2264_ _M_ and _R \u2264_ 2 _M_ . (c) Let D denote the distribution of non-outlier points define the associated expected loss _R_ ( _h_ ) = E _x\u223c_ D [1 _h_ ( _x_ ) _<_ 0 ] and empirical margin loss _R_ [\ufffd] _S,\u03c1_ ( _h_ ) = \ufffd _mi_ =1 _m_ 1 [\u03a6] _[\u03c1]_ [(] _[h]_ [(] _[x]_ _[i]_ [))] _[ \u2264]_ [\ufffd] _i_ _[m]_ =1 _m_ 1 [1] _[h]_ [(] _[x]_ _[i]_ [)] _[<\u03c1]_ [. These losses measure errors caused] by _false-positive_ predictions, i.e. errors caused by incorrectly labeling a point anomalous. i. Show that the empirical Rademacher complexity for the hypothesis class H from part (b) can be upper bound as follows: \ufffd R _S_ (H) _\u2264_ _[R]_ [2] [ + \u039b] [2] Tr[ **K** ] _,_ + \u039b ~~\ufffd~~ _\u221am_ where **K** is",
    "chunk_id": "foundations_machine_learning_141"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the kernel matrix constructed with the sample. ii. Prove that with probability at least 1 _\u2212\u03b4_, the following holds for all _h \u2208_ H and _\u03c1 \u2208_ (0 _,_ 1]: \ufffd log [4] _\u03b4_ 2 _m_ _[.]_ _R_ 2 + \u039b 2 \ufffd _\u221am_ + \u039b\ufffd \ufffd _R_ 2 + \u039b _m_ 2 Tr[ **K** ] + \ufffd _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [4] _\u03c1_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [4] \ufffd log log 2 _\u03c1_ 2 + 3 _m_ (d) Just as in the case of soft-margin SVM, we can also define a soft-margin objective for the smallest enclosing sphere that allows us tune the sensitivity to outliers in the training set by adjusting a regularization parameter _C_ : min _r>_ 0 _,_ **c** _\u2208_ H _,\u03be_ _[r]_ [2] [ +] _[ C]_ _m_ \ufffd _\u03be_ _i_ _i_ =1 subject to: _\u2200i \u2208_ [ _m_ ] _, \u2225_ \u03a6( _x_ _i_ ) _\u2212_ **c** _\u2225_ [2] _\u2264_ _r_ [2] + _\u03be_ _i_ _\u2227_ _\u03be_ _i_ _\u2265_ 0 _._ **144** **Chapter 6** **Kernel Methods** Show that the equivalent dual formulation of this problem is _m_ \ufffd max _**\u03b1**_ _m_ \ufffd _\u03b1_ _i_ _K_ ( _x_ _i_ _, x_ _i_ ) _\u2212_ _i_ =1 _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ _K_ ( _x_ _i_ _, x_ _j_ ) _i,j_ =1 subject to: **0** _\u2264_ _**\u03b1**_ _\u2264_ _C_ **1** _\u2227_ _m_ \ufffd _\u03b1_ _i_ = 1 _,_ _i_ =1 and that at the optimum we have **c** = [\ufffd] _[m]_ _i_ =1 _[\u03b1]_ _[i]_ [\u03a6(] _[x]_ _[i]_ [).] # 7 Boosting _Ensemble methods_ are general techniques in machine learning for combining several predictors to create a more accurate one. This chapter studies an important family of ensemble methods known as _boosting_, and more specifically the _AdaBoost_ algorithm. This algorithm has been shown to be very effective in practice in some scenarios and is based on a rich theoretical analysis. We first introduce AdaBoost, show how it can rapidly reduce the empirical error as a function of the number of rounds of boosting, and point out its relationship with some known algorithms. Next, we present a theoretical analysis of the generalization properties of AdaBoost based on the VC-dimension of its hypothesis set and then based on the notion of margin. The margin theory developed in this context can be applied to other similar ensemble algorithms. A game-theoretic interpretation of AdaBoost further helps analyzing its properties and revealing the equivalence between the weak learning assumption and a separability condition. We end with a discussion of AdaBoost\u2019s benefits and drawbacks. **7.1** **Introduction** It is often difficult, for a non-trivial learning task, to directly devise an accurate algorithm satisfying the strong PAC-learning requirements of chapter 2. But, there can be more hope for finding simple predictors guaranteed only to perform slightly better than random. The following gives a formal definition of such _weak learners_ . As in the PAC-learning chapter, we let _n_ be a number such that the computational cost of representing any",
    "chunk_id": "foundations_machine_learning_142"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "element _x \u2208_ X is at most _O_ ( _n_ ) and denote by size( _c_ ) the maximal cost of the computational representation of _c \u2208_ C. **Definition 7.1 (Weak learning)** _A concept class_ C _is said to be_ weakly PAC-learnable _if there exists an algorithm A, \u03b3 >_ 0 _, and a polynomial function poly_ ( _\u00b7, \u00b7, \u00b7_ ) _such_ _that for any \u03b4 >_ 0 _, for all distributions_ D _on_ X _and for any target concept c \u2208_ C _,_ **146** **Chapter 7** **Boosting** AdaBoost( _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ ))) 1 **for** _i \u2190_ 1 **to** _m_ **do** 2 D 1 ( _i_ ) _\u2190_ _m_ [1] 3 **for** _t \u2190_ 1 **to** _T_ **do** 4 _h_ _t_ _\u2190_ base classifier in H with small error _\u03f5_ _t_ = P _i\u223c_ D _t_ \ufffd _h_ _t_ ( _x_ _i_ ) _\u0338_ = _y_ _i_ \ufffd [1] 2 [log] [1] _[\u2212]_ _\u03f5_ _[\u03f5]_ _[t]_ 5 _\u03b1_ _t_ _\u2190_ [1] _\u03f5_ _t_ 6 _Z_ _t_ _\u2190_ 2\ufffd _\u03f5_ _t_ (1 _\u2212_ _\u03f5_ _t_ )\ufffd [1] 2 7 **for** _i \u2190_ 1 **to** _m_ **do** 2 _\u25b7_ normalization factor 8 D _t_ +1 ( _i_ ) _\u2190_ [D] _[t]_ [(] _[i]_ [)][ ex][p(] _[\u2212]_ _Z_ _[\u03b1]_ _t_ _[t]_ _[y]_ _[i]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [))] 9 _f \u2190_ [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ 10 **return** _f_ **Figure 7.1** AdaBoost algorithm for a base classifier set H _\u2286{\u2212_ 1 _,_ +1 _}_ [X] . _the following holds for any sample size m \u2265_ _poly_ (1 _/\u03b4, n, size_ ( _c_ )) _:_ P _S\u223c_ D _[m]_ _R_ ( _h_ _S_ ) _\u2264_ [1] _\u2265_ 1 _\u2212_ _\u03b4,_ (7.1) \ufffd 2 _[\u2212]_ _[\u03b3]_ \ufffd _where h_ _S_ _is the hypothesis returned by algorithm A when trained on sample S._ _When such an algorithm A exists, it is called a_ weak learning algorithm _for_ C _or_ _a_ weak learner _. The hypotheses returned by a weak learning algorithm are called_ base classifiers _._ The key idea behind boosting techniques is to use a weak learning algorithm to build a _strong learner_, that is, an accurate PAC-learning algorithm. To do so, boosting techniques use an ensemble method: they combine different base classifiers returned by a weak learner to create a more accurate predictor. But which base classifiers should be used and how should they be combined? The next section addresses these questions by describing in detail one of the most prevalent and successful boosting algorithms, AdaBoost. **7.2** **AdaBoost** We denote by H the hypothesis set out of which the base classifiers are selected, which we will sometimes refer to as the _base classifier set_ . Figure 7.1 gives the **7.2** **AdaBoost** **147** _h_ _t_ _\u2208_ H is selected that minimizes the error on the training sample weighted by the |Col1|Col2|decision<br>boundary<br>updated<br>weights<br>t = 1 t = 2 t = 3|Col4|Col5|Col6|Col7|Col8| |---|---|---|---|---|---|---|---| |(a)<br>\u03b11 + \u03b12 + \u03b13 =<br>(b)<br>Figure 7.2<br>Example of AdaBoost with axis-aligned hyperplanes as base classifiers. (a)",
    "chunk_id": "foundations_machine_learning_143"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "The top row shows<br>decision boundaries at each boosting round. The bottom row shows how weights are updated at<br>ach round, with incorrectly (resp., correctly) points given increased (resp., decreased) weights. (b)<br>Visualization of final classifier, constructed as a non-negative linear combination of base classifiers.<br>pseudocode of AdaBoost in the case where the base classifiers are functions mapping<br>rom X to {\u22121, +1}, thus H \u2286{\u22121, +1}X.<br>The algorithm takes as input a labeled sample S = ((x, y ), . . ., (x, y )), with<br>1 1 m m<br>x, y ) \u2208X\u00d7{\u22121, +1} for all i \u2208[m], and maintains a distribution over the indices<br>i i<br>{1, . . ., m}. Initially (lines 1-2), the distribution is uniform (D ). At each round<br>1<br>of boosting, that is each iteration t \u2208 [T] of the loop 3\u20138, a new base classifier|(a)<br>\u03b11 + \u03b12 + \u03b13 =<br>(b)<br>Figure 7.2<br>Example of AdaBoost with axis-aligned hyperplanes as base classifiers. (a) The top row shows<br>decision boundaries at each boosting round. The bottom row shows how weights are updated at<br>ach round, with incorrectly (resp., correctly) points given increased (resp., decreased) weights. (b)<br>Visualization of final classifier, constructed as a non-negative linear combination of base classifiers.<br>pseudocode of AdaBoost in the case where the base classifiers are functions mapping<br>rom X to {\u22121, +1}, thus H \u2286{\u22121, +1}X.<br>The algorithm takes as input a labeled sample S = ((x, y ), . . ., (x, y )), with<br>1 1 m m<br>x, y ) \u2208X\u00d7{\u22121, +1} for all i \u2208[m], and maintains a distribution over the indices<br>i i<br>{1, . . ., m}. Initially (lines 1-2), the distribution is uniform (D ). At each round<br>1<br>of boosting, that is each iteration t \u2208 [T] of the loop 3\u20138, a new base classifier|(a)|(a)|(a)|(a)|(a)|(a)| |(a)<br>\u03b11 + \u03b12 + \u03b13 =<br>(b)<br>Figure 7.2<br>Example of AdaBoost with axis-aligned hyperplanes as base classifiers. (a) The top row shows<br>decision boundaries at each boosting round. The bottom row shows how weights are updated at<br>ach round, with incorrectly (resp., correctly) points given increased (resp., decreased) weights. (b)<br>Visualization of final classifier, constructed as a non-negative linear combination of base classifiers.<br>pseudocode of AdaBoost in the case where the base classifiers are functions mapping<br>rom X to {\u22121, +1}, thus H \u2286{\u22121, +1}X.<br>The algorithm takes as input a labeled sample S = ((x, y ), . . ., (x, y )), with<br>1 1 m m<br>x, y ) \u2208X\u00d7{\u22121, +1} for all i \u2208[m], and maintains a distribution over the indices<br>i i<br>{1, . . ., m}. Initially (lines 1-2), the distribution is uniform (D ). At each round<br>1<br>of boosting, that is each iteration t \u2208 [T] of the loop 3\u20138, a new base classifier|||||||| |(a)<br>\u03b11 + \u03b12 + \u03b13 =<br>(b)<br>Figure 7.2<br>Example of AdaBoost with axis-aligned hyperplanes as base classifiers. (a) The top row shows<br>decision boundaries at each boosting round. The bottom row shows how weights are updated at<br>ach round, with incorrectly (resp., correctly) points given increased (resp., decreased) weights. (b)<br>Visualization of final classifier, constructed as a non-negative linear combination of base classifiers.<br>pseudocode of AdaBoost in the case where the base classifiers are functions mapping<br>rom X to {\u22121, +1}, thus H \u2286{\u22121, +1}X.<br>The algorithm takes as input a labeled sample",
    "chunk_id": "foundations_machine_learning_144"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "S = ((x, y ), . . ., (x, y )), with<br>1 1 m m<br>x, y ) \u2208X\u00d7{\u22121, +1} for all i \u2208[m], and maintains a distribution over the indices<br>i i<br>{1, . . ., m}. Initially (lines 1-2), the distribution is uniform (D ). At each round<br>1<br>of boosting, that is each iteration t \u2208 [T] of the loop 3\u20138, a new base classifier|||||||| |(a)<br>\u03b11 + \u03b12 + \u03b13 =<br>(b)<br>Figure 7.2<br>Example of AdaBoost with axis-aligned hyperplanes as base classifiers. (a) The top row shows<br>decision boundaries at each boosting round. The bottom row shows how weights are updated at<br>ach round, with incorrectly (resp., correctly) points given increased (resp., decreased) weights. (b)<br>Visualization of final classifier, constructed as a non-negative linear combination of base classifiers.<br>pseudocode of AdaBoost in the case where the base classifiers are functions mapping<br>rom X to {\u22121, +1}, thus H \u2286{\u22121, +1}X.<br>The algorithm takes as input a labeled sample S = ((x, y ), . . ., (x, y )), with<br>1 1 m m<br>x, y ) \u2208X\u00d7{\u22121, +1} for all i \u2208[m], and maintains a distribution over the indices<br>i i<br>{1, . . ., m}. Initially (lines 1-2), the distribution is uniform (D ). At each round<br>1<br>of boosting, that is each iteration t \u2208 [T] of the loop 3\u20138, a new base classifier|||||||| distribution D _t_ : _\u0338_ _m_ \ufffd D _t_ ( _i_ )1 _h_ ( _x_ _i_ )= _\u0338_ _y_ _i_ _._ _i_ =1 _h_ _t_ _\u2208_ argmin _h\u2208_ H _i\u223c_ P D _t_ _\u0338_ \ufffd _h_ ( _x_ _i_ ) _\u0338_ = _y_ _i_ \ufffd = argmin _\u0338_ _h\u2208_ H **148** **Chapter 7** **Boosting** _Z_ _t_ is simply a normalization factor to ensure that the weights D _t_ +1 ( _i_ ) sum to one. The precise reason for the definition of the coefficient _\u03b1_ _t_ will become clear later. 1 For now, observe that if _\u03f5_ _t_, the error of the base classifier, is less than 2 [, then] 1 _\u2212\u03f5_ _t_ _\u03f5_ _t_ _>_ 1 and _\u03b1_ _t_ is positive ( _\u03b1_ _t_ _>_ 0). Thus, the new distribution D _t_ +1 is defined from D _t_ by substantially increasing the weight on _i_ if point _x_ _i_ is incorrectly classified ( _y_ _i_ _h_ _t_ ( _x_ _i_ ) _<_ 0), and, on the contrary, decreasing it if _x_ _i_ is correctly classified. This has the effect of focusing more on the points incorrectly classified at the next round of boosting, less on those correctly classified by _h_ _t_ . After _T_ rounds of boosting, the classifier returned by AdaBoost is based on the sign of function _f_, which is a non-negative linear combination of the base classifiers _h_ _t_ . The weight _\u03b1_ _t_ assigned to _h_ _t_ in that sum is a logarithmic function of the ratio of the accuracy 1 _\u2212_ _\u03f5_ _t_ and error _\u03f5_ _t_ of _h_ _t_ . Thus, more accurate base classifiers are assigned a larger weight in that sum. Figure 7.2 illustrates the AdaBoost algorithm. The size of the points represents the distribution weight assigned to them at each boosting",
    "chunk_id": "foundations_machine_learning_145"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "round. For any _t \u2208_ [ _T_ ], we will denote by _f_ _t_ the linear combination of the base classifiers after _t_ rounds of boosting: _f_ _t_ = [\ufffd] _[t]_ _s_ =1 _[\u03b1]_ _[s]_ _[h]_ _[s]_ [. In particular, we have] _[ f]_ _[T]_ [ =] _[ f]_ [. The] distribution D _t_ +1 can be expressed in terms of _f_ _t_ and the normalization factors _Z_ _s_, _s \u2208_ [ _t_ ], as follows: _\u2200i \u2208_ [ _m_ ] _,_ D _t_ +1 ( _i_ ) = _[e]_ _[\u2212][y]_ _[i]_ _[f]_ _[t]_ [(] _[x]_ _[i]_ [)] _._ (7.2) _m_ [\ufffd] _[t]_ _s_ =1 _[Z]_ _[s]_ We will make use of this identity several times in the proofs of the following sections. It can be shown straightforwardly by repeatedly expanding the definition of the distribution over the point _x_ _i_ : D _t_ +1 ( _i_ ) = [D] _[t]_ [(] _[i]_ [)] _[e]_ _[\u2212][\u03b1]_ _[t]_ _[y]_ _[i]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)] _Z_ _t\u2212_ 1 _Z_ _t_ _[\u03b1]_ _[t]_ _[y]_ _[i]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)] = [D] _[t][\u2212]_ [1] [(] _[i]_ [)] _[e]_ _[\u2212][\u03b1]_ _[t][\u2212]_ [1] _[y]_ _[i]_ _[h]_ _[t][\u2212]_ [1] [(] _[x]_ _[i]_ [)] _[e]_ _[\u2212][\u03b1]_ _[t]_ _[y]_ _[i]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)] _Z_ _t_ _Z_ _t\u2212_ 1 _Z_ _t_ \ufffd _ts_ =1 _[\u03b1]_ _[s]_ _[h]_ _[s]_ [(] _[x]_ _[i]_ [)] = _[e]_ _[\u2212][y]_ _m_ _[i]_ [\ufffd] _[t]_ _s_ =1 _[Z]_ _[s]_ _._ The AdaBoost algorithm can be generalized in several ways: _\u2022_ Instead of a hypothesis with minimal weighted error, _h_ _t_ can be more generally the base classifier returned by a weak learning algorithm trained on D _t_ ; _\u2022_ The range of the base classifiers could be [ _\u2212_ 1 _,_ +1], or more generally a bounded subset of R. The coefficients _\u03b1_ _t_ can then be different and may not even admit a closed form. In general, they are chosen to minimize an upper bound on the empirical error, as discussed in the next section. Of course, in that general case, the hypotheses _h_ _t_ are not binary _classifiers_, but their sign could define the label, and their magnitude could be interpreted as a measure of confidence. **7.2** **AdaBoost** **149** In rest of this chapter, the range of the base classifiers in H will be assumed to be included in [ _\u2212_ 1 _,_ +1]. We now further analyze the properties of AdaBoost and discuss its typical use in practice. **7.2.1** **Bound on the empirical error** We first show that the empirical error of AdaBoost decreases exponentially fast as a function of the number of rounds of boosting. **Theorem 7.2** _The empirical error of the classifier returned by AdaBoost verifies:_ \ufffd _R_ _S_ ( _f_ ) _\u2264_ exp _\u2212_ 2 \ufffd _T_ \ufffd _t_ =1 1 2 [\ufffd] _._ (7.3) \ufffd 2 _[\u2212]_ _[\u03f5]_ _[t]_ \ufffd _Furthermore, if for all t \u2208_ [ _T_ ] _, \u03b3 \u2264_ \ufffd 12 _[\u2212]_ _[\u03f5]_ _[t]_ \ufffd _, then_ \ufffd _R_ _S_ ( _f_ ) _\u2264_ exp( _\u2212_ 2 _\u03b3_ [2] _T_ )",
    "chunk_id": "foundations_machine_learning_146"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_._ (7.4) Proof: Using the general inequality 1 _u\u2264_ 0 _\u2264_ exp( _\u2212u_ ) valid for all _u \u2208_ R and identity 7.2, we can write: _m_ _m_ _T_ \ufffd _Z_ _t_ _t_ =1 D _T_ +1 ( _i_ ) = \ufffd _T_ \ufffd _Z_ _t_ _._ _t_ =1 \ufffd _R_ _S_ ( _f_ ) = [1] _m_ _m_ \ufffd \ufffd 1 _y_ _i_ _f_ ( _x_ _i_ ) _\u2264_ 0 _\u2264_ _m_ [1] _i_ =1 _m_ \ufffd \ufffd _e_ _[\u2212][y]_ _[i]_ _[f]_ [(] _[x]_ _[i]_ [)] = _m_ [1] _i_ =1 _m_ \ufffd _i_ =1 _m_ \ufffd Since for all _t \u2208_ [ _T_ ], _Z_ _t_ is a normalization factor, it can be expressed in terms of _\u03f5_ _t_ by: \ufffd D _t_ ( _i_ ) _e_ _[\u2212][\u03b1]_ _[t]_ + \ufffd _i_ : _y_ _i_ _h_ _t_ ( _x_ _i_ )=+1 _i_ : _y_ _i_ _h_ _t_ ( _x_ _i_ \ufffd D _t_ ( _i_ ) _e_ _[\u03b1]_ _[t]_ _i_ : _y_ _i_ _h_ _t_ ( _x_ _i_ )= _\u2212_ 1 _Z_ _t_ = _m_ \ufffd \ufffd D _t_ ( _i_ ) _e_ _[\u2212][\u03b1]_ _[t]_ _[y]_ _[i]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)] = \ufffd _i_ =1 _i_ : _h_ _x_ = (1 _\u2212_ _\u03f5_ _t_ ) _e_ _[\u2212][\u03b1]_ _[t]_ + _\u03f5_ _t_ _e_ _[\u03b1]_ _[t]_ _\u03f5_ _t_ = (1 _\u2212_ _\u03f5_ _t_ ) + _\u03f5_ _t_ ~~\ufffd~~ 1 _\u2212_ _\u03f5_ _t_ ~~\ufffd~~ 1 _\u2212_ _\u03f5_ _t_ = 2\ufffd _\u03f5_ _t_ 1 _\u2212_ _\u03f5_ _t_ _\u03f5_ _t_ (1 _\u2212_ _\u03f5_ _t_ ) _._ Thus, the product of the normalization factors can be expressed and upper bounded as follows: _T_ \ufffd _Z_ _t_ = _t_ =1 _T_ \ufffd \ufffd 2 ~~\ufffd~~ _t_ =1 1 2 [\ufffd] 2 _[\u2212]_ _[\u03f5]_ _[t]_ \ufffd _T_ \ufffd \ufffd exp \ufffd _\u2212_ 2\ufffd 12 _t_ =1 _\u03f5_ _t_ (1 _\u2212_ _\u03f5_ _t_ ) = _T_ \ufffd _t_ =1 ~~\ufffd~~ 1 _\u2212_ 4 1 \ufffd 2 12 _[\u2212]_ _[\u03f5]_ _[t]_ \ufffd 2 _\u2264_ = exp _\u2212_ 2 \ufffd _T_ \ufffd _t_ =1 1 2 [\ufffd] \ufffd 2 _[\u2212]_ _[\u03f5]_ _[t]_ \ufffd _,_ where the inequality follows from the inequality 1 _\u2212_ _x \u2264_ _e_ _[\u2212][x]_ valid for all _x \u2208_ R. \u25a1 Note that the value of _\u03b3_, which is known as the _edge_, and the accuracy of the base classifiers do not need to be known to the algorithm. The algorithm adapts to their **150** **Chapter 7** **Boosting** **Figure 7.3** **Image:** [No caption returned] Visualization of the zero-one loss (blue) and the convex and differentiable upper bound on the zero-one loss (red) that is optimized by AdaBoost. accuracy and defines a solution based on these values. This is the source of the extended name of AdaBoost: _adaptive boosting_ . The proof of theorem 7.2 reveals several other important properties. First, observe that _\u03b1_ _t_ is the minimizer of the function _\u03d5_ : _\u03b1 \ufffd\u2192_ (1 _\u2212_ _\u03f5_ _t_ ) _e_ _[\u2212][\u03b1]_ + _\u03f5_ _t_ _e_ _[\u03b1]_ . Indeed, _\u03d5_ is convex and differentiable, and setting its derivative to zero yields: _\u03d5_ _[\u2032]_ ( _\u03b1_ ) =",
    "chunk_id": "foundations_machine_learning_147"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2212_ (1 _\u2212_ _\u03f5_ _t_ ) _e_ _[\u2212][\u03b1]_ + _\u03f5_ _t_ _e_ _[\u03b1]_ = 0 _\u21d4_ (1 _\u2212_ _\u03f5_ _t_ ) _e_ _[\u2212][\u03b1]_ = _\u03f5_ _t_ _e_ _[\u03b1]_ _\u21d4_ _\u03b1_ = [1] [1] [1] _[ \u2212]_ _[\u03f5]_ _[t]_ 2 [log] _\u03f5_ _._ (7.5) _\u03f5_ _t_ Thus, _\u03b1_ _t_ is chosen to minimize _Z_ _t_ = _\u03d5_ ( _\u03b1_ _t_ ) and, in light of the bound _R_ [\ufffd] _S_ ( _f_ ) _\u2264_ \ufffd _Tt_ =1 _[Z]_ _[t]_ [ shown in the proof, these coefficients are selected to minimize an upper] bound on the empirical error. In fact, for base classifiers whose range is [ _\u2212_ 1 _,_ +1] or R, _\u03b1_ _t_ can be chosen in a similar fashion to minimize _Z_ _t_, and this is the way AdaBoost is extended to these more general cases. Observe also that the equality (1 _\u2212_ _\u03f5_ _t_ ) _e_ _[\u2212][\u03b1]_ _[t]_ = _\u03f5_ _t_ _e_ _[\u03b1]_ _[t]_ just shown in (7.5) implies that at each iteration, AdaBoost assigns equal distribution mass to correctly and incorrectly classified instances, since (1 _\u2212_ _\u03f5_ _t_ ) _e_ _[\u2212][\u03b1]_ _[t]_ is the total distribution assigned to correctly classified points and _\u03f5_ _t_ _e_ _[\u03b1]_ _[t]_ that of incorrectly classified ones. This may seem to contradict the fact that AdaBoost increases the weights of incorrectly classified points and decreases that of others, but there is in fact no inconsistency: the reason is that there are always fewer incorrectly classified points, since the base classifier\u2019s accuracy is better than random. **7.2.2** **Relationship with coordinate descent** AdaBoost was originally designed to address the theoretical question of whether a weak learning algorithm could be used to derive a strong learning one. Here, **7.2** **AdaBoost** **151** we will show that it coincides in fact with a very simple algorithm, which consists of applying a general coordinate descent technique to a convex and differentiable objective function. For simplicity, in this section, we assume that the base classifier set H is finite, with cardinality _N_ : H = _{h_ 1 _, . . ., h_ _N_ _}_ . An ensemble function _f_ such as the one returned by AdaBoost can then be written as _f_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [, with \u00af] _[\u03b1]_ _[j]_ _[ \u2265]_ [0. Given a] labeled sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )), let _F_ be the objective function defined for all \u00af _**\u03b1**_ = (\u00af _\u03b1_ 1 _, . . .,_ \u00af _\u03b1_ _N_ ) _\u2208_ R _[N]_ by _m_ _F_ (\u00af _**\u03b1**_ ) = [1] _m_ _m_ \ufffd \ufffd _e_ _[\u2212][y]_ _[i]_ _[f]_ [(] _[x]_ _[i]_ [)] = _m_ [1] _i_ =1 _m_ \ufffd _e_ _[\u2212][y]_ _[i]_ \ufffd _Nj_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] _._ (7.6) _i_ =1 Since the exponential loss _u \ufffd\u2192_ _e_ _[\u2212][u]_ is an upper bound on the zero-one loss _u \ufffd\u2192_ 1 _u\u2264_ 0 (see figure 7.3), _F_ is an upper bound on the empirical error: _m_ \ufffd",
    "chunk_id": "foundations_machine_learning_148"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_R_ _S_ ( _f_ ) = [1] _m_ _m_ \ufffd \ufffd 1 _y_ _i_ _f_ ( _x_ _i_ ) _\u2264_ 0 _\u2264_ _m_ [1] _i_ =1 _m_ \ufffd _e_ _[\u2212][y]_ _[i]_ _[f]_ [(] _[x]_ _[i]_ [)] _._ (7.7) _i_ =1 _F_ is a convex function of \u00af _**\u03b1**_ since it is a sum of convex functions, each obtained by composition of the (convex) exponential function with an affine function of \u00af _**\u03b1**_ . _F_ is also differentiable since the exponential function is differentiable. We will show that _F_ is the objective function minimized by AdaBoost. Different convex optimization techniques can be used to minimize _F_ . Here, we will use a variant of the coordinate descent technique. Coordinate descent is applied over _T_ rounds. Let \u00af _**\u03b1**_ 0 = **0** and let \u00af _**\u03b1**_ _t_ denote the parameter vector at the end of iteration _t_ . At each round _t \u2208_ [ _T_ ], a direction **e** _k_ corresponding to the _k_ th coordinate of \u00af _**\u03b1**_ in R _[N]_ is selected, as well as a step size _\u03b7_ along that direction. \u00af _**\u03b1**_ _t_ is obtained from \u00af _**\u03b1**_ _t\u2212_ 1 according to the update \u00af _**\u03b1**_ _t_ = \u00af _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_, where _\u03b7_ is the step size chosen along the direction **e** _k_ . Observe that if we denote by \u00af _g_ _t_ the ensemble function defined by _**\u03b1**_ \u00af _t_, that is \u00af _g_ _t_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[t,j]_ _[h]_ _[j]_ [, then the coordinate descent update coincides with] the update \u00af _g_ _t_ = \u00af _g_ _t\u2212_ 1 + _\u03b7h_ _k_, which is also the AdaBoost update. Thus, since both algorithms start with \u00af _g_ 0 = 0, to show that AdaBoost coincides with coordinate descent applied to _F_, it suffices to show at every iteration _t_, coordinate descent selects the same base hypothesis _h_ _k_ and step _\u03b7_ as AdaBoost. We will assume by induction that this holds up to iteration _t\u2212_ 1, which implies the equality \u00af _g_ _t\u2212_ 1 = _f_ _t\u2212_ 1, and will show then that it also holds at iteration _t_ . The variant of coordinate descent we consider here consists of selecting, at each iteration, the maximum descent direction, that is the direction **e** _k_ along which the derivative of _F_ is the largest in absolute value, and of selecting the best step along that direction, that is of choosing _\u03b7_ to minimize _F_ (\u00af _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_ ). To give the expressions of the direction and the step at each iteration, we first introduce similar **152** **Chapter 7** **Boosting** quantities to those appearing in the analysis of the boosting algorithm. For any _t \u2208_ [ _T_ ], we define a distribution D [\u00af] _t_ over the indices _{_ 1 _, . . ., m}_ as follows: \u00af \ufffd _Nj_ =1 _[\u03b1]_ [\u00af] _[t][\u2212]_ [1] _[,j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] D _t_ ( _i_ ) = _[e]_ _[\u2212][y]_ _[i]_ \u00af _Z_ \u00af _t_ _,_ _[\u03b1]_ [\u00af]",
    "chunk_id": "foundations_machine_learning_149"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[t][\u2212]_ [1] _[,j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] \u00af = _[e]_ _[\u2212][y]_ _[i]_ _[g]_ [\u00af] \u00af _[t][\u2212]_ [1] [(] _[x]_ _[i]_ [)] _Z_ _t_ _Z_ _t_ where _Z_ [\u00af] _t_ is the normalization factor _Z_ [\u00af] _t_ = [\ufffd] _[m]_ _i_ =1 _[e]_ _[\u2212][y]_ _[i]_ \ufffd _Nj_ =1 _[\u03b1]_ [\u00af] _[t][\u2212]_ [1] _[,j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] . Observe that, since \u00af _g_ _t\u2212_ 1 = _f_ _t\u2212_ 1, D [\u00af] _t_ coincides with D _t_ . We also define for any base hypothesis _h_ _j_, _j \u2208_ [ _N_ ], its expected error \u00af _\u03f5_ _t,j_ with respect to the distribution D [\u00af] _t_ : _\u03f5_ \u00af _t,j_ = E _i\u223c_ D [\u00af] _t_ \ufffd1 _y_ _i_ _h_ _j_ ( _x_ _i_ ) _\u2264_ 0 \ufffd _._ The directional derivative of _F_ at \u00af _**\u03b1**_ _t\u2212_ 1 along **e** _k_ is denoted by _F_ _[\u2032]_ (\u00af _**\u03b1**_ _t\u2212_ 1 _,_ **e** _k_ ) and defined by \u00af \u00af _F_ ( _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_ ) _\u2212_ _F_ ( _**\u03b1**_ _t\u2212_ 1 ) _F_ _[\u2032]_ (\u00af _**\u03b1**_ _t\u2212_ 1 _,_ **e** _k_ ) = lim _\u03b7\u2192_ 0 _._ _\u03b7_ Since _F_ (\u00af _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_ ) = [\ufffd] _[m]_ _i_ =1 _[e]_ _[\u2212][y]_ _[i]_ \ufffd _Nj_ =1 _[\u03b1]_ [\u00af] _[t][\u2212]_ [1] _[,j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] _[\u2212][\u03b7y]_ _[i]_ _[h]_ _[k]_ [(] _[x]_ _[i]_ [)], the directional derivative along **e** _k_ can be expressed as follows: _F_ _[\u2032]_ (\u00af _**\u03b1**_ _t\u2212_ 1 _,_ **e** _k_ ) = _\u2212_ [1] _m_ = _\u2212_ [1] _m_ _m_ \ufffd _y_ _i_ _h_ _k_ ( _x_ _i_ ) _e_ _[\u2212][y]_ _[i]_ \ufffd _Nj_ =1 _[\u03b1]_ [\u00af] _[t][\u2212]_ [1] _[,j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] _i_ =1 _m_ \ufffd _y_ _i_ _h_ _k_ ( _x_ _i_ ) D [\u00af] _t_ ( _i_ ) _Z_ [\u00af] _t_ _i_ =1 _m_ \ufffd \ufffd _i_ =1 _Z_ \u00af _t_ _m_ \ufffd _m_ \ufffd D\u00af _t_ ( _i_ )1 _y_ _i_ _h_ _k_ ( _x_ _i_ )= _\u2212_ 1 _i_ =1 = _\u2212_ \ufffd D\u00af _t_ ( _i_ )1 _y_ _i_ _h_ _k_ ( _x_ _i_ )=+1 _\u2212_ _i_ =1 \u00af \u00af _Z_ \u00af _t_ = _\u2212_ (1 _\u2212_ _\u03f5_ _t,k_ ) _\u2212_ _\u03f5_ _t,k_ \ufffd \ufffd _m_ _m_ _[.]_ _Z_ \u00af _t_ 2\u00af _\u03f5_ _t,k_ _\u2212_ 1 _Z_ \u00af _t_ _m_ [=] \ufffd \ufffd _m_ Since _Zm_ \u00af _t_ [does not depend on] _[ k]_ [, the maximum descent direction] _[ k]_ [ is the one mini-] mizing \u00af _\u03f5_ _t,k_ . Thus, the hypothesis _h_ _k_ selected by coordinate descent at iteration _t_ is the one with the smallest expected error on the sample _S_, where the expectation is taken with respect to D [\u00af] _t_ = D _t_ . This matches exactly the choice made by AdaBoost at the _t_ th round. The step size _\u03b7_ is selected to minimize the function along the direction **e** _k_ chosen: argmin _\u03b7_ _F_ (\u00af _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_ ). Since _F_ (\u00af _**\u03b1**_ _t\u2212_ 1",
    "chunk_id": "foundations_machine_learning_150"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "+ _\u03b7_ **e** _k_ ) is a convex function of _\u03b7_, to find the **7.2** **AdaBoost** **153** **Figure 7.4** **Image:** [No caption returned] Examples of several convex upper bounds on the zero-one loss. minimum, it suffices to set its derivative to zero: \u00af _dF_ ( _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_ ) = 0 _\u21d4\u2212_ _d\u03b7_ _\u21d4\u2212_ _\u21d4\u2212_ _m_ \ufffd _y_ _i_ _h_ _k_ ( _x_ _i_ ) _e_ _[\u2212][y]_ _[i]_ \ufffd _Nj_ =1 _[\u03b1]_ [\u00af] _[t][\u2212]_ [1] _[,j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] _e_ _[\u2212][\u03b7y]_ _[i]_ _[h]_ _[k]_ [(] _[x]_ _[i]_ [)] = 0 _i_ =1 _m_ \ufffd _y_ _i_ _h_ _k_ ( _x_ _i_ ) D [\u00af] _t_ ( _i_ ) _Z_ [\u00af] _t_ _e_ _[\u2212][\u03b7y]_ _[i]_ _[h]_ _[k]_ [(] _[x]_ _[i]_ [)] = 0 _i_ =1 _m_ \ufffd _y_ _i_ _h_ _k_ ( _x_ _i_ ) D [\u00af] _t_ ( _i_ ) _e_ _[\u2212][\u03b7y]_ _[i]_ _[h]_ _[k]_ [(] _[x]_ _[i]_ [)] = 0 _i_ =1 \u00af \u00af _\u21d4\u2212_ \ufffd(1 _\u2212_ _\u03f5_ _t,k_ ) _e_ _[\u2212][\u03b7]_ _\u2212_ _\u03f5_ _t,k_ _e_ _[\u03b7]_ [\ufffd] = 0 [1] 2 [log] [1] _[ \u2212]_ _\u03f5_ \u00af _[\u03f5]_ [\u00af] _[t][,][k]_ _\u21d4_ _\u03b7_ = [1] _\u03f5_ \u00af _t,k_ _._ This proves that the step size chosen by coordinate descent coincides with the weight _\u03b1_ _t_ assigned by AdaBoost to the classifier chosen in the _t_ th round. Thus, coordinate descent applied to exponential objective _F_ precisely coincides with AdaBoost and _F_ can be viewed as the objective function that AdaBoost seeks to minimize. In light of this relationship, one may wish to consider similar applications of coordinate descent to other convex and differentiable functions of \u00af _**\u03b1**_ upper-bounding the zero-one loss. In particular, the _logistic loss x \ufffd\u2192_ log 2 (1 + _e_ _[\u2212][x]_ ) is convex and differentiable and upper bounds the zero-one loss. Figure 7.4 shows other examples of alternative convex loss functions upper-bounding the zero-one loss. Using the logistic loss, instead of the exponential loss used by AdaBoost, leads to an objective that coincides with _logistic regression_ . **154** **Chapter 7** **Boosting** **7.2.3** **Practical use** Here, we briefly describe the standard practical use of AdaBoost. An important requirement for the algorithm is the choice of the base classifiers or that of the weak learner. The family of base classifiers typically used with AdaBoost in practice is that of _decision trees_, which are equivalent to hierarchical partitions of the space (see chapter 9, section 9.3.3). Among decision trees, those of depth one, also known as _stumps_, are by far the most frequently used base classifiers. Boosting stumps are threshold functions associated to a single feature. Thus, a stump corresponds to a single axis-aligned partition of the space, as illustrated in figure 7.2. If the data is in R _[N]_, we can associate a stump to each of the _N_ components. Thus, to determine the stump with the minimal weighted error at each round of boosting, the best component and the best threshold for each component must be computed. To do so, we can first presort each component in _O_ ( _m_ log",
    "chunk_id": "foundations_machine_learning_151"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_m_ ) time with a total computational cost of _O_ ( _mN_ log _m_ ). For a given component, there are only _m_ + 1 possible distinct thresholds, since two thresholds between the same consecutive component values are equivalent. To find the best threshold at each round of boosting, all of these possible _m_ + 1 values can be compared, which can be done in _O_ ( _m_ ) time. Thus, the total computational complexity of the algorithm for _T_ rounds of boosting is _O_ ( _mN_ log _m_ + _mNT_ ). Observe, however, that while boosting stumps are widely used in combination with AdaBoost and can perform well in practice, the algorithm that returns the stump with the minimal (weighted) empirical error is _not a weak learner_ (see definition 7.1)! Consider, for example, the simple XOR example with four data points lying in R [2] (see figure 6.3a), where points in the second and fourth quadrants are labeled positively and those in the first and third quadrants negatively. Then, no decision stump can achieve an accuracy better than [1] 2 [.] **7.3** **Theoretical results** In this section we present a theoretical analysis of the generalization properties of AdaBoost. **7.3.1** **VC-dimension-based analysis** We start with an analysis of AdaBoost based on the VC-dimension of its hypothesis set. The family of functions _F_ _T_ out of which AdaBoost selects its output after _T_ rounds of boosting is \ufffd \ufffd _._ (7.8) _F_ _T_ = \ufffd sgn _T_ \ufffd _\u03b1_ _t_ _h_ _t_ \ufffd _t_ =1 : _\u03b1_ _t_ _\u2265_ 0 _, h_ _t_ _\u2208_ H _, t \u2208_ [ _T_ ] **Image:** [No caption returned] **Figure 7.5** training error goes to zero after about 5 rounds of boosting ( _T \u2248_ 5), yet the test error continues to decrease for larger values of _T_ . The VC-dimension of _F_ _T_ can be bounded as follows in terms of the VC-dimension _d_ of the family of base hypothesis H (exercise 7.1): VCdim( _F_ _T_ ) _\u2264_ 2( _d_ + 1)( _T_ + 1) log 2 (( _T_ + 1) _e_ ) _._ (7.9) The upper bound grows as _O_ ( _dT_ log _T_ ), thus, the bound suggests that AdaBoost could overfit for large values of _T_, and indeed this can occur. However, in many cases, it has been observed empirically that the generalization error of AdaBoost decreases as a function of the number of rounds of boosting _T_, as illustrated in figure 7.5! How can these empirical results be explained? The following sections present a margin-based analysis in support of AdaBoost that can serve as a theoretical explanation for these empirical observations. **7.3.2** _L_ 1 **-geometric margin** In chapter 5, we introduced the definition of confidence margin and presented a series of general learning bounds based on that notion which, in particular, provided strong learning guarantees for SVMs. Here, we will similarly derive general learning bounds based on that same notion of confidence margin for ensemble methods, which we will use, in particular, to derive learning guarantees for AdaBoost. Recall that the confidence",
    "chunk_id": "foundations_machine_learning_152"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "margin of a real-valued function _f_ at a point _x_ labeled with _y_ is the quantity _yf_ ( _x_ ). For SVMs, we also defined the notion of geometric margin which, in the separable case, is a lower bound on the confidence margin of a linear hypothesis with a normalized weighted vector **w**, _\u2225_ **w** _\u2225_ 2 = 1. Here, we will also define a notion of geometric margin for linear hypotheses with a norm-1 constraint, such as the ensemble hypotheses returned by AdaBoost, and similarly relate that notion to that of confidence margin. This will also serve as an opportunity for us to point out the connection between several concepts and terminology used in the context of SVMs and those used in the context of boosting. **156** **Chapter 7** **Boosting** **Image:** [No caption returned] **Image:** [No caption returned] Norm || \u00b7 || 2 . Norm || \u00b7 || \u221e . **Figure 7.6** Maximum margin hyperplanes for norm-2 and norm- _\u221e_ . First note that a function _f_ = [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [ that is a linear combination of base] hypotheses _h_ 1 _, . . ., h_ _T_ can be equivalently expressed as an inner product _f_ = _**\u03b1**_ _\u00b7_ **h**, where _**\u03b1**_ = ( _\u03b1_ 1 _, . . ., \u03b1_ _T_ ) _[\u22a4]_ and **h** = [ _h_ 1 _, . . ., h_ _T_ ] _[\u22a4]_ . This makes the similarity between the linear hypotheses considered in this chapter and those of chapter 5 and chapter 6 evident: the vector of base hypothesis values **h** ( _x_ ) can be viewed as a feature vector associated to _x_, which was denoted by **\u03a6** ( _x_ ) in previous chapters, and _**\u03b1**_ is the weight vector that was denoted by **w** . For ensemble linear combinations such as those returned by AdaBoost, additionally, the weight vector is non-negative: _**\u03b1**_ _\u2265_ 0. Next, we introduce a notion of geometric margin for such ensemble functions which differs from the one introduced for SVMs only by the norm-1 used instead of norm-2, using the notation just introduced. **Definition 7.3 (** _L_ 1 **-geometric margin)** _The L_ 1 -geometric margin _\u03c1_ _f_ ( _x_ ) _of a linear func-_ _tion f_ = [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ _[ with]_ _**[ \u03b1]**_ _[ \u0338]_ [= 0] _[ at a point][ x][ \u2208]_ [X] _[ is defined by]_ _[f]_ [(] _[x]_ [)] _[|]_ = _[|]_ [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ [)] _[|]_ _\u2225_ _**\u03b1**_ _\u2225_ 1 _\u2225_ _**\u03b1**_ _\u2225_ 1 _\u03c1_ _f_ ( _x_ ) = _[|][f]_ [(] _[x]_ [)] _[|]_ = _\u2225_ _**\u03b1**_ _\u2225_ 1 \ufffd\ufffd _**\u03b1**_ _\u00b7_ **h** ( _x_ )\ufffd\ufffd _._ (7.10) _\u2225_ _**\u03b1**_ _\u2225_ 1 _The L_ 1 -margin _of f over a sample S_ = ( _x_ 1 _, . . ., x_ _m_ ) _is its minimum margin at the_ _points in that sample:_ _\u03c1_ _f_ = min _i\u2208_ [ _m_ ] _[\u03c1]_ _[f]_ [(] _[x]_ _[i]_ [) = min] _i\u2208_ [ _m_ ] \ufffd\ufffd _**\u03b1**_",
    "chunk_id": "foundations_machine_learning_153"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u00b7_ **h** ( _x_ _i_ )\ufffd\ufffd _._ (7.11) _\u2225_ _**\u03b1**_ _\u2225_ 1 This definition of geometric margin differs from definition 5.1 given in the context of the SVM algorithm only by the norm used for the weight vector: _L_ 1 -norm here, _L_ 2 -norm in definition 5.1. To distinguish them in the discussion that follows, let _\u03c1_ 1 ( _x_ ) denote the _L_ 1 -margin and _\u03c1_ 2 ( _x_ ) the _L_ 2 -margin at point _x_ (definition 5.1): _\u03c1_ 1 ( _x_ ) = _[|]_ _**[\u03b1]**_ _[ \u00b7]_ **[ h]** [(] _[x]_ [)] _[|]_ _[ \u00b7]_ **[ h]** [(] _[x]_ [)] _[|]_ and _\u03c1_ 2 ( _x_ ) = _[|]_ _**[\u03b1]**_ _[ \u00b7]_ **[ h]** [(] _[x]_ [)] _[|]_ _\u2225_ _**\u03b1**_ _\u2225_ 1 _\u2225_ _**\u03b1**_ _\u2225_ 2 _._ _\u2225_ _**\u03b1**_ _\u2225_ 2 **7.3** **Theoretical results** **157** _\u03c1_ 2 ( _x_ ) is then the norm-2 distance of the vector **h** ( _x_ ) to the hyperplane of equation _**\u03b1**_ _\u00b7_ **x** = 0 in R _[T]_ . Similarly, _\u03c1_ 1 ( _x_ ) is the norm- _\u221e_ distance of **h** ( _x_ ) to that hyperplane. This geometric difference is illustrated by figure 7.6. [8] We will denote by \u00af _f_ _f_ _f_ = _T_ = \ufffd _t_ =1 _[\u03b1]_ _[t]_ _\u2225_ _**\u03b1**_ _\u2225_ 1 the normalized version of the function _f_ returned by AdaBoost. Note that if a point _x_ with label _y_ is correctly classified by _f_ (or _f_ [\u00af] ), then the confidence margin of _f_ [\u00af] at _x_ coincides with the _L_ 1 -geometric margin of _f_ : _yf_ [\u00af] ( _x_ ) = _[yf]_ _\u2225_ _**\u03b1**_ [(] _\u2225_ _[x]_ 1 [)] [=] _[ \u03c1]_ _[f]_ [(] _[x]_ [). Observe] that, since the coefficients _\u03b1_ _t_ are non-negative, _\u03c1_ _f_ ( _x_ ) is then a convex combination of the base hypothesis values _h_ _t_ ( _x_ ). In particular, if the base hypotheses _h_ _t_ take values in [ _\u2212_ 1 _,_ +1], then _\u03c1_ _f_ ( _x_ ) is in [ _\u2212_ 1 _,_ +1]. **7.3.3** **Margin-based analysis** To analyze the generalization properties of AdaBoost, we start by examining the Rademacher complexity of convex linear ensembles. For any hypothesis set H of real-valued functions, we denote by conv(H) its convex hull defined by _p_ conv(H) = \ufffd _\u00b5_ _k_ _h_ _k_ : _p \u2265_ 1 _, \u2200k \u2208_ [ _p_ ] _, \u00b5_ _k_ _\u2265_ 0 _, h_ _k_ _\u2208_ H _,_ \ufffd _k_ =1 _p_ \ufffd _\u00b5_ _k_ _\u2264_ 1 _._ (7.12) _k_ =1 \ufffd The following lemma shows that, remarkably, the empirical Rademacher complexity of conv(H), which in general is a strictly larger set including H, coincides with that of H. **Lemma 7.4** _Let_ H _be a set of functions mapping from_ X _to_ R _. Then, for any sample_ _S, we have_ \ufffd R _S_ \ufffd conv(H)\ufffd = R [\ufffd] _S_ (H) _._ 8 More generally, for _p, q \u2265_ 1, _p_ and _q conjugate_, that is 1 _p_ 1 [+] [1] _q_ [1] _[|]_ _**[\u03b1]**_ _[\u00b7]_ **[h]** [(]",
    "chunk_id": "foundations_machine_learning_154"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[x]_ [)] _[|]_ _q_ [= 1,] _\u2225_ _**\u03b1**_ _\u2225_ _\u2225_ _**\u03b1**_ _\u2225_ _p_ is the norm- _q_ distance of **h** ( _x_ ) to the hyperplane of equation _**\u03b1**_ _\u00b7_ **h** ( _x_ ) = 0. **158** **Chapter 7** **Boosting** Proof: The proof follows from a straightforward series of equalities: \ufffd R _S_ \ufffd conv(H)\ufffd = _m_ [1] [E] _\u03c3_ _m_ \ufffd _\u03c3_ _i_ _i_ =1 _m_ \ufffd _p_ \ufffd _\u00b5_ _k_ _h_ _k_ ( _x_ _i_ ) _k_ =1 \ufffd _m_ \ufffd _\u03c3_ _i_ _h_ _k_ ( _x_ _i_ ) _i_ =1 \ufffd = [1] _m_ [E] _\u03c3_ sup \ufffd _h_ 1 _,...,h_ _p_ _\u2208_ H _,_ _**\u00b5**_ _\u2265_ 0 _,\u2225_ _**\u00b5**_ _\u2225_ 1 _\u2264_ 1 sup sup \ufffd _h_ 1 _,...,h_ _p_ _\u2208_ H _**\u00b5**_ _\u2265_ 0 _,\u2225_ _**\u00b5**_ _\u2225_ 1 _\u2264_ 1 _p_ \ufffd _\u00b5_ _k_ _k_ =1 _p_ \ufffd _m_ \ufffd _\u03c3_ _i_ _h_ _k_ ( _x_ _i_ ) _i_ =1 \ufffd = [1] _m_ [E] _\u03c3_ sup max \ufffd _h_ 1 _,...,h_ _p_ _\u2208_ H _k\u2208_ [ _p_ ] _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ ) = R [\ufffd] _S_ (H) _,_ _i_ =1 \ufffd = [1] _m_ [E] _\u03c3_ sup \ufffd _h\u2208_ H where the third equality follows the definition of the dual norm (see section A.1.2) or the observation that the maximizing vector _**\u00b5**_ for a convex combination of _p_ terms is the one placing all the weight on the largest term. This theorem can be used directly in combination with theorem 5.8 to derive the following Rademacher complexity generalization bound for convex combination ensembles of hypotheses. Recall that _R_ [\ufffd] _S,\u03c1_ ( _h_ ) denotes the empirical margin loss with margin _\u03c1_ . **Corollary 7.5 (Ensemble Rademacher margin bound)** _Let_ H _denote a set of real-valued_ _functions. Fix \u03c1 >_ 0 _. Then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, each of_ _the following holds for all h \u2208_ conv(H) _:_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] \ufffdH\ufffd + _\u03c1_ [R] _[m]_ \ufffd log [1] _\u03b4_ (7.13) 2 _m_ \ufffd _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] R _S_ \ufffdH\ufffd + 3 _\u03c1_ \ufffd log [2] _\u03b4_ (7.14) 2 _m_ _[.]_ Using corollary 3.8 and corollary 3.18 to bound the Rademacher complexity in terms of the VC-dimension yields immediately the following VC-dimension-based generalization bounds for convex combination ensembles of hypotheses. **Corollary 7.6 (Ensemble VC-Dimension margin bound)** _Let_ H _be a family of functions_ _taking values in {_ +1 _, \u2212_ 1 _} with VC-dimension d. Fix \u03c1 >_ 0 _. Then, for any \u03b4 >_ 0 _,_ _with probability at least_ 1 _\u2212_ _\u03b4, the following holds for all h \u2208_ conv(H) _:_ \ufffd log [1] _\u03b4_ (7.15) 2 _m_ _[.]_ log [1] _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] _\u03c1_ ~~\ufffd~~ 2 _d_ log _[em]_ _d_ + _m_ These bounds can be generalized to hold uniformly for all _\u03c1 \u2208_ (0 _,_ 1], at the price",
    "chunk_id": "foundations_machine_learning_155"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "of an additional term of the form \ufffd log lo _m_ g 2 _\u03b4_ 2 as in theorem 5.9. They cannot be **7.3** **Theoretical results** **159** directly applied to the function _f_ returned by AdaBoost, since it is not a convex combination of base hypotheses, but they can be applied to its normalized version, _f_ \u00af = \ufffd _Tt_ _\u2225_ =1 _**\u03b1**_ _\u2225_ _[\u03b1]_ 1 _[t]_ _[h]_ _[t]_ _\u2208_ conv(H). Notice that from the point of view of binary classifica tion, _f_ and _f_ [\u00af] are equivalent since sgn( _f_ ) = sgn \ufffd _\u2225_ _**\u03b1**_ _f\u2225_ 1 \ufffd, thus _R_ ( _f_ ) = _R_ \ufffd _\u2225_ _**\u03b1**_ _f\u2225_ 1 \ufffd _,_ but their empirical margin losses are distinct. Let _f_ = [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [ denote the function defining the classifier returned by Ad-] aBoost after _T_ rounds of boosting when trained on sample _S_ . Then, in view of (7.13), for any _\u03b4 >_ 0, the following holds with probability at least 1 _\u2212_ _\u03b4_ : \u00af _R_ ( _f_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ \ufffd _f_ \ufffd + [2] \ufffdH\ufffd + _\u03c1_ [R] _[m]_ ~~\ufffd~~ log [1] _\u03b4_ (7.16) 2 _m_ _[.]_ Similar bounds can be derived from (7.14) and (7.15). Remarkably, the number of rounds of boosting _T_ does not appear in the generalization bound (7.16). The bound depends only on the confidence margin _\u03c1_, the sample size _m_, and the Rademacher complexity of the family of base classifiers H\u00af. Thus, the bound guarantees an effective generalization if the margin loss _R_ _\u03c1_ \ufffd _f_ \ufffd is small for a relatively large _\u03c1_ . Recall that the margin loss can be upper bounded by the fraction of the points _x_ labeled with _y_ in the training sample with confidence margin at most _\u03c1_, that is _yf_ ( _x_ ) _\u2225_ _**\u03b1**_ _\u2225_ 1 _[\u2264]_ _[\u03c1]_ [ (see (5.38)). With our definition of] _[ L]_ [1] [-margin, this can also be written] as follows: \ufffd \u00af _[y]_ _[i]_ _[\u03c1]_ _[f]_ [(] _[x]_ _[i]_ [)] _[ \u2264]_ _[\u03c1][}|]_ _R_ _S,\u03c1_ \ufffd _f_ \ufffd _\u2264_ _[|{][i][ \u2208]_ [[] _[m]_ []] [:] _._ (7.17) _m_ Additionally, the following theorem provides a bound on the empirical margin loss, which decreases with _T_ under conditions discussed later. **Theorem 7.7** _Let f_ = [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ _[ denote the function returned by AdaBoost after][ T]_ _rounds of boosting and assume for all t \u2208_ [ _T_ ] _that \u03f5_ _t_ _<_ [1] 2 _[, which implies][ \u03b1]_ _[t]_ _[ >]_ [ 0] _[.]_ _Then, for any \u03c1 >_ 0 _, the following holds:_ _T_ \ufffd _R_ _S,\u03c1_ ( \u00af _f_ ) _\u2264_ 2 _[T]_ \ufffd _t_ =1 \ufffd _\u03f5_ [1] _t_ _[\u2212][\u03c1]_ (1 _\u2212_ _\u03f5_ _t_ ) [1+] _[\u03c1]_ _._ Proof: Using the general inequality 1 _u\u2264_ 0 _\u2264_ exp( _\u2212u_ ) valid for all _u \u2208_ R, identity 7.2, _e_ _[\u2212][yif]_ [(] _[xi]_ [)] that is D _t_ +1 ( _i_ ) = _[T]_ _[ Z]_ _[t]_ [ = 2] \ufffd _\u03f5_ _t_",
    "chunk_id": "foundations_machine_learning_156"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(1 _\u2212_ _\u03f5_ _t_ ) from the proof of _me_ _[\u2212]_ [\ufffd] _[T]_ _t_ =1 _[xi]_ _[Z]_ _[t]_ [, the equality] _[ Z]_ _[t]_ [ = 2] \ufffd _\u03f5_ _t_ (1 _\u2212_ _\u03f5_ _t_ ) from the proof of **160** **Chapter 7** **Boosting** [1] [1] _[\u2212][\u03f5]_ _[t]_ 2 [log(] _\u03f5_ theorem 7.2, and the definition of _\u03b1_ _t_ = [1] _[\u2212][\u03f5]_ _[t]_ _\u03f5_ _t_ [) in AdaBoost, we can write:] _m_ 1 _m_ _m_ \ufffd \ufffd 1 _y_ _i_ _f_ ( _x_ _i_ ) _\u2212\u03c1\u2225_ _**\u03b1**_ _\u2225_ 1 _\u2264_ 0 _\u2264_ _m_ [1] _i_ =1 _m_ \ufffd exp( _\u2212y_ _i_ _f_ ( _x_ _i_ ) + _\u03c1\u2225_ _**\u03b1**_ _\u2225_ 1 ) _i_ =1 _T_ \ufffd _Z_ _t_ _t_ =1 = [1] _m_ _m_ _e_ _[\u03c1][\u2225]_ _**[\u03b1]**_ _[\u2225]_ [1] _m_ \ufffd _i_ =1 \ufffd D _T_ +1 ( _i_ ) \ufffd _T_ = _e_ _[\u03c1][\u2225]_ _**[\u03b1]**_ _[\u2225]_ [1] \ufffd _Z_ _t_ = _e_ _[\u03c1]_ [ \ufffd] _t_ =1 _T_ _t_ _[\u2032]_ _[ \u03b1]_ _t_ _[\u2032]_ \ufffd _Z_ _t_ _t_ =1 _T_ = 2 _[T]_ \ufffd _t_ =1 \ufffd\ufffd 1 _\u2212\u03f5_ _t_ _\u03f5_ _t_ \ufffd _\u03c1_ \ufffd _\u03f5_ _t_ (1 _\u2212_ _\u03f5_ _t_ ) _,_ which concludes the proof. Moreover, if for all _t \u2208_ [ _T_ ] we have _\u03b3 \u2264_ ( 2 [1] _[\u2212]_ _[\u03f5]_ _[t]_ [) and] _[ \u03c1][ \u2264]_ [2] _[\u03b3]_ [, then the expression] 4 _\u03f5_ [1] _t_ _[\u2212][\u03c1]_ (1 _\u2212_ _\u03f5_ _t_ ) [1+] _[\u03c1]_ is maximized at _\u03f5_ _t_ = 12 _[\u2212]_ _[\u03b3]_ [.] [9] Thus, the upper bound on the empirical margin loss can then be bounded by \ufffd 2 _R_ _S,\u03c1_ ( \u00af _f_ ) _\u2264_ (1 _\u2212_ 2 _\u03b3_ ) [1] _[\u2212][\u03c1]_ (1 + 2 _\u03b3_ ) [1+] _[\u03c1]_ [\ufffd] _[T]_ _._ (7.18) \ufffd Observe that (1 _\u2212_ 2 _\u03b3_ ) [1] _[\u2212][\u03c1]_ (1 + 2 _\u03b3_ ) [1+] _[\u03c1]_ = (1 _\u2212_ 4 _\u03b3_ [2] )\ufffd 11 _\u2212_ +22 _\u03b3\u03b3_ \ufffd _\u03c1_ . This is an increasing function of _\u03c1_ since we have \ufffd 11 _\u2212_ +22 _\u03b3_ \ufffd _>_ 1 as a consequence of _\u03b3 >_ 0. Thus, if _\u03c1 < \u03b3_, Observe that (1 _\u2212_ 2 _\u03b3_ ) [1] _[\u2212][\u03c1]_ (1 + 2 _\u03b3_ ) [1+] _[\u03c1]_ = (1 _\u2212_ 4 _\u03b3_ [2] )\ufffd 11 _\u2212_ +22 _\u03b3_ function of _\u03c1_ since we have \ufffd 11 _\u2212_ +22 _\u03b3\u03b3_ \ufffd _>_ 1 as a consequence of _\u03b3 >_ 0. Thus, if _\u03c1 < \u03b3_, it can be strictly upper bounded as follows (1 _\u2212_ 2 _\u03b3_ ) [1] _[\u2212][\u03c1]_ (1 + 2 _\u03b3_ ) [1+] _[\u03c1]_ _<_ (1 _\u2212_ 2 _\u03b3_ ) [1] _[\u2212][\u03b3]_ (1 + 2 _\u03b3_ ) [1+] _[\u03b3]_ _._ The function _\u03b3 \ufffd\u2192_ (1 _\u2212_ 2 _\u03b3_ ) [1] _[\u2212][\u03b3]_ (1 + 2 _\u03b3_ ) [1+] _[\u03b3]_ is strictly upper bounded by 1 over the interval (0 _,_ 1 _/_ 2), thus, if _\u03c1 < \u03b3_, then (1 _\u2212_ 2 _\u03b3_ ) [1] _[\u2212][\u03c1]_ (1+2 _\u03b3_ ) [1+] _[\u03c1]_ _<_ 1 and the right-hand side of (7.18) decreases exponentially with _T_ . Since the condition _\u03c1 \u226b_ _O_",
    "chunk_id": "foundations_machine_learning_157"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(1 _/_ _[\u221a]_ _m_ ) is necessary in order for the given margin bounds to converge, this places a condition of _\u03b3 \u226b_ _O_ (1 _/_ _[\u221a]_ _m_ ~~)~~ on the edge value. In practice, the error _\u03f5_ _t_ of the base classifier at round _t_ may increase as a function of _t_ . Informally, this is because boosting presses the weak learner to concentrate on instances that are harder and harder to classify, for which even the best base classifier could not achieve an error significantly better than random. If _\u03f5_ _t_ becomes close to [1] 2 [relatively fast as a function of] _[ t]_ [, then the] bound of theorem 7.7 becomes uninformative. 9 The differential of _f_ : _\u03f5 \ufffd\u2192_ log[ _\u03f5_ 1 _\u2212\u03c1_ (1 _\u2212_ _\u03f5_ ) 1+ _\u03c1_ ] = (1 _\u2212_ _\u03c1_ ) log _\u03f5_ + (1 + _\u03c1_ ) log(1 _\u2212_ _\u03f5_ ) over the interval 2 _[\u2212]_ _[\u03c1]_ 2 _[\u03c1]_ 2 [),] (0 _,_ 1) is given by _f_ _[\u2032]_ ( _\u03f5_ ) = [1] _[\u2212][\u03c1]_ _[\u2212][\u03c1]_ _\u2212_ [1][+] _[\u03c1]_ _\u03f5_ 1 _\u2212\u03f5_ [1][+] _[\u03c1]_ ( [1] 2 1 _\u2212\u03f5_ [= 2] [1] 2 _[\u2212]_ _[\u03c1]_ 2 2 _\u03f5_ (1 _[\u2212]_ _\u2212_ _[\u03c1]_ 2 [)] _e_ _[\u2212]_ ) _[\u03f5]_ . Thus, _f_ is an increasing function over (0 _,_ [1] 2 which implies that it is increasing over (0 _,_ [1] 2 [.] 2 _[\u2212]_ _[\u03b3]_ [) when] _[ \u03b3][ \u2265]_ _[\u03c1]_ 2 **7.3** **Theoretical results** **161** The analysis and discussion that precede show that if AdaBoost admits a positive edge ( _\u03b3 >_ 0), then, for _\u03c1 < \u03b3_, the empirical margin loss _R_ [\ufffd] _S,\u03c1_ ( _f_ [\u00af] ) becomes zero for _T_ sufficiently large (it decreases exponentially fast). Thus, AdaBoost achieves an _L_ 1 -geometric margin of _\u03b3_ over the training sample. In section 7.3.5, we will see that the edge _\u03b3_ is positive if and only if the training sample is separable. In that case, the edge can be chosen to be as large as half the maximum _L_ 1 -geometric margin _\u03c1_ max that can be achieved on the sample: _\u03b3_ = _[\u03c1]_ [max] 2 [. Thus, for a separable data] set, AdaBoost can asymptotically achieve a geometric margin that is at least half the maximum geometric margin, _[\u03c1]_ [max] 2 [.] This analysis can serve as a theoretical explanation of the empirical observation that, in some tasks, the generalization error decreases as a function of _T_ even after the error on the training sample is zero: the geometric margin continues to increase when the training sample is separable. In (7.16), for the ensemble function _f_ determined by AdaBoost after _T_ rounds, as _T_ increases, _\u03c1_ can be chosen as a larger quantity for which the first term on the right-hand side vanishes ( _R_ [\ufffd] _S,\u03c1_ ( _f_ [\u00af] ) = 0) while the second term becomes more favorable since it decreases as [1] _\u03c1_ [.] But, does AdaBoost achieve the maximum _L_ 1 -geometric margin _\u03c1_ max ? No. It has been shown that AdaBoost may converge,",
    "chunk_id": "foundations_machine_learning_158"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "for a linearly separable sample, to a geometric margin that is significantly smaller than the maximum margin (e.g., [1] 3 instead of [3] 8 [).] **7.3.4** **Margin maximization** In view of these results, several algorithms have been devised with the explicit goal of maximizing the _L_ 1 -geometric margin. These algorithms correspond to different methods for solving a linear program (LP). By definition of the _L_ 1 -margin, the maximum margin for a linearly separable sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) is given by _y_ _i_ \ufffd _**\u03b1**_ _\u00b7_ **h** ( _x_ _i_ )\ufffd _\u03c1_ = max min _._ (7.19) _**\u03b1**_ _i\u2208_ [ _m_ ] _\u2225_ _**\u03b1**_ _\u2225_ 1 By definition of the maximization, the optimization problem can be written as: max _\u03c1_ _**\u03b1**_ \ufffd _**\u03b1**_ _\u00b7_ **h** ( _x_ _i_ )\ufffd subject to: _[y]_ _[i]_ _\u2265_ _\u03c1, \u2200i \u2208_ [ _m_ ] _._ _\u2225_ _**\u03b1**_ _\u2225_ 1 Since _**[\u03b1]**_ _\u2225_ _[\u00b7]_ **[h]** _**\u03b1**_ [(] _\u2225_ _[x]_ 1 _[i]_ [)] is invariant to the scaling of _**\u03b1**_, we can restrict ourselves to _\u2225_ _**\u03b1**_ _\u2225_ 1 = 1. Further seeking a non-negative _**\u03b1**_ as in the case of AdaBoost leads to the following **162** **Chapter 7** **Boosting** optimization: max _\u03c1_ _**\u03b1**_ subject to: _y_ _i_ \ufffd _**\u03b1**_ _\u00b7_ **h** ( _x_ _i_ )\ufffd _\u2265_ _\u03c1, \u2200i \u2208_ [ _m_ ]; _T_ \ufffd _\u03b1_ _t_ = 1 _\u2227_ \ufffd _\u03b1_ _t_ _\u2265_ 0 _, \u2200t_ \ufffd _t_ =1 \ufffd \ufffd _\u03b1_ _t_ = 1 _t_ =1 _\u2227_ _\u03b1_ _t_ _\u2265_ 0 _, \u2200t \u2208_ [ _T_ ] _._ \ufffd \ufffd This is a linear program (LP), that is, a convex optimization problem with a linear objective function and linear constraints. There are several different methods for solving relative large LPs in practice, using the simplex method, interior-point methods, or a variety of special-purpose solutions. Note that the solution of this algorithm differs from the margin-maximization defining SVMs in the separable case only by the definition of the geometric margin used ( _L_ 1 versus _L_ 2 ) and the non-negativity constraint on the weight vector. Figure 7.6 illustrates the margin-maximizing hyperplanes found using these two distinct margin definitions in a simple case. The left figure shows the SVM solution, where the distance to the closest points to the hyperplane is measured with respect to the norm _\u2225\u00b7 \u2225_ 2 . The right figure shows the solution for the _L_ 1 -margin, where the distance to the closest points to the hyperplane is measured with respect to the norm _\u2225\u00b7 \u2225_ _\u221e_ . By definition, the solution of the LP just described admits an _L_ 1 -margin that is larger or equal to that of the AdaBoost solution. However, empirical results do not show a systematic benefit for the solution of the LP. In fact, it appears that in many cases, AdaBoost outperforms that algorithm. The margin theory described does not seem sufficient to explain that performance. **7.3.5** **Game-theoretic interpretation** In this section, we show that AdaBoost admits a",
    "chunk_id": "foundations_machine_learning_159"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "natural game-theoretic interpretation. The application of von Neumann\u2019s theorem then helps us relate the maximum margin and the optimal edge and clarify the connection of AdaBoost\u2019s weak-learning assumption with the notion of _L_ 1 -margin. We first introduce the definition of the edge of a base classifier for a particular distribution. **Definition 7.8** _The_ edge of a base classifier _h_ _t_ for a distribution D _over the training_ _sample S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _is defined by_ _\u03b3_ _t_ (D) = [1] 2 [1] [1] 2 _[\u2212]_ _[\u03f5]_ _[t]_ [ =] 2 _m_ \ufffd _y_ _i_ _h_ _t_ ( _x_ _i_ )D( _i_ ) _._ (7.20) _i_ =1 _AdaBoost\u2019s weak learning condition_ can now be formulated as follows: there exists _\u03b3 >_ 0 such that for any distribution D over the training sample and any base **7.3** **Theoretical results** **163** **Table 7.1** The loss matrix for the standard rock-paper-scissors game. rock paper scissors rock 0 +1 -1 paper -1 0 +1 scissors +1 -1 0 classifier _h_ _t_, the following holds: _\u03b3_ _t_ (D) _\u2265_ _\u03b3._ (7.21) This condition is required for the analysis of theorem 7.2 and the non-negativity of the coefficients _\u03b1_ _t_ . We will frame boosting as a two-person zero-sum game. **Definition 7.9 (Zero-sum game)** _A_ finite two-person zero-sum game _consists of a_ loss matrix **M** _\u2208_ R _[m][\u00d7][n]_ _, where m is the number of possible_ actions _(or_ pure strategies _)_ _for the row player and n the number of possible actions for the column player. The_ _entry M_ _ij_ _is the loss for the row player (or equivalently the payoff for the column_ _payer) when the row player takes action i and the column player takes action j._ [10] An example of a loss matrix for the familiar \u201crock-paper-scissors\u201d game is shown in table 7.1. **Definition 7.10 (Mixed strategy)** _A_ mixed strategy _for the row player is a distribution_ _p over the m possible row actions; a_ mixed strategy _for the column player is a_ _distribution q over the n possible column actions. The_ expected loss _for the row_ _player (expected payoff for the column player) with respect to the mixed strategies p_ _and q is_ _n_ \ufffd _p_ _i_ _M_ _ij_ _q_ _j_ = **p** _[\u22a4]_ **Mq** _._ _j_ =1 _i\u223c_ E _p_ [ _M_ _ij_ ] = _j\u223cq_ _m_ \ufffd _i_ =1 The following is a fundamental result in game theory proven in chapter 8. **Theorem 7.11 (Von Neumann\u2019s minimax theorem)** _For any finite two-person zero-sum_ _game defined by the matrix_ **M** _, the following equality holds:_ min **p** _[\u22a4]_ **Mq** = max min (7.22) **p** [max] **q** **q** **p** **[p]** _[\u22a4]_ **[Mq]** _[ .]_ The common value in (7.22) is called the _value of the game_ . The theorem states that for any two-person zero-sum game, there exists a mixed strategy for each player 10 To be consistent with the results discussed in other chapters, we consider the loss matrix as opposed to the payoff matrix (its opposite).",
    "chunk_id": "foundations_machine_learning_160"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**164** **Chapter 7** **Boosting** such that the expected loss for one is the same as the expected payoff for the other, both of which are equal to the value of the game. Note that, given the row player\u2019s strategy, the column player can choose a pure strategy optimizes their payoff. That is, the column player can choose the single strategy corresponding the largest coordinate of the vector **p** _[\u22a4]_ **M** . A similar comment applies to the reverse. Thus, an alternative and equivalent form of the minimax theorem is min **p** [max] _j\u2208_ [ _n_ ] **[p]** _[\u22a4]_ **[Me]** _[j]_ [ = max] **q** _i_ min _\u2208_ [ _m_ ] **[e]** _i_ _[\u22a4]_ **[Mq]** _[,]_ (7.23) where **e** _i_ denotes the _i_ th unit vector. We can now view AdaBoost as a zero-sum game, where an action of the row player is the selection of a training instance _x_ _i_, _i \u2208_ [ _m_ ], and an action of the column player the selection of a base learner _h_ _t_, _t \u2208_ [ _T_ ]. A mixed strategy for the row player is thus a distribution D over the training points\u2019 indices [ _m_ ]. A mixed strategy for the column player is a distribution over the based classifiers\u2019 indices [ _T_ ]. This can be defined from a non-negative vector _**\u03b1**_ _\u2265_ 0: the weight assigned to _t \u2208_ [ _T_ ] is _\u03b1_ _t_ _/\u2225_ _**\u03b1**_ _\u2225_ 1 . The loss matrix **M** _\u2208{\u2212_ 1 _,_ +1 _}_ _[m][\u00d7][T]_ for AdaBoost is defined by _M_ _it_ = _y_ _i_ _h_ _t_ ( _x_ _i_ ) for all ( _i, t_ ) _\u2208_ [ _m_ ] _\u00d7_ [ _T_ ]. By von Neumann\u2019s theorem (7.23), the following holds: _T_ \ufffd _t_ =1 min D _\u2208D_ _t_ [max] _\u2208_ [ _T_ ] _m_ \ufffd D( _i_ ) _y_ _i_ _h_ _t_ ( _x_ _i_ ) = max _**\u03b1**_ _\u2265_ 0 _i_ [min] _\u2208_ [ _m_ ] _i_ =1 _\u03b1_ _t_ _y_ _i_ _h_ _t_ ( _x_ _i_ ) _,_ (7.24) _\u2225_ _**\u03b1**_ _\u2225_ 1 where _D_ denotes the set of all distributions over the training sample. Let _\u03c1_ _**\u03b1**_ ( _x_ ) denote the margin of point _x_ for the classifier defined by _f_ = [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [. The result] can be rewritten as follows in terms of the margins and edges: 2 _\u03b3_ _[\u2217]_ = 2 min min (7.25) D _t_ [max] _\u2208_ [ _T_ ] _[\u03b3]_ _[t]_ [(][D][) = max] _**\u03b1**_ _i\u2208_ [ _m_ ] _[\u03c1]_ _**[\u03b1]**_ [(] _[x]_ _[i]_ [) =] _[ \u03c1]_ _[\u2217]_ _[,]_ where _\u03c1_ _[\u2217]_ is the maximum margin of a classifier and _\u03b3_ _[\u2217]_ the best possible edge. This result has several implications. First, it shows that the weak learning condition ( _\u03b3_ _[\u2217]_ _>_ 0) implies _\u03c1_ _[\u2217]_ _>_ 0 and thus the existence of a classifier with positive margin, which motivates the search for a non-zero margin. AdaBoost can be viewed as an algorithm seeking to achieve such a non-zero margin, though, as discussed earlier, AdaBoost does",
    "chunk_id": "foundations_machine_learning_161"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "not always achieve an optimal margin and is thus suboptimal in that respect. Furthermore, we see that the \u201cweak learning\u201d assumption, which originally appeared to be the weakest condition one could require for an algorithm (that of performing better than random), is in fact a strong condition: it implies that the training sample is linearly separable with margin 2 _\u03b3_ _[\u2217]_ _>_ 0. Linear separability often does not hold for the data sets found in practice. **7.4** _L_ 1 **-regularization** **165** **7.4** _**L**_ **1** **-regularization** In practice, the training sample may not be linearly separable and AdaBoost may not admit a positive edge, in which case the weak learning condition does not hold. It may also be that AdaBoost does admit a positive edge but with _\u03b3_ very small. In such cases, running AdaBoost may result in large total mixture weights for some base classifiers _h_ _j_ . This can be because the algorithm increasingly concentrates on a few examples that are hard to classify and whose weights keep growing. Only a few base classifiers might achieve the best performance for those examples and the algorithm keeps selecting them, thereby increasing their total mixture weights. These base classifiers with relatively large total mixture weight end up dominating in an ensemble _f_ and therefore solely dictating the classification decision. The performance of the resulting ensemble is typically poor since it almost entirely hinges on that of a few base classifiers. There are several methods for avoiding such situations. One consists of limiting the number of rounds of boosting _T_, which is also known as _early-stopping_ . Another one consists of controlling the magnitude of the mixture weights. This can be done by augmenting the objective function of AdaBoost with a regularization term based on a norm of the vector of mixture weights. Using a norm-1 regularization leads to an algorithm that we will refer to as _L_ 1 _-regularized AdaBoost_ . Given a labeled sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )), the objective function _G_ minimized by _L_ 1 regularized AdaBoost is defined for all \u00af _**\u03b1**_ = (\u00af _\u03b1_ 1 _, . . .,_ \u00af _\u03b1_ _N_ ) _\u2208_ R _[N]_ by _m_ _G_ (\u00af _**\u03b1**_ ) = [1] _m_ _m_ \ufffd \u00af \ufffd _e_ _[\u2212][y]_ _[i]_ _[f]_ [(] _[x]_ _[i]_ [)] + _\u03bb\u2225_ _**\u03b1**_ _\u2225_ 1 = _m_ [1] _i_ =1 _m_ _N_ \u00af \ufffd _e_ _[\u2212][y]_ _[i]_ \ufffd _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] + _\u03bb\u2225_ _**\u03b1**_ _\u2225_ 1 _,_ (7.26) _i_ =1 _\u03b1_ where, as for AdaBoost,\u00af _j_ _\u2265_ 0. The objective function _f_ is an ensemble function defined by _G_ is a convex function of \u00af _**\u03b1**_ as the sum of the convex _f_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [, with] objective of AdaBoost and the norm-1 of \u00af _**\u03b1**_ . _L_ 1 -regularized AdaBoost consists of applying coordinate-descent to the objective function _G_ . We now show that the algorithm can be directed",
    "chunk_id": "foundations_machine_learning_162"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "derived from the margin-based guarantee for ensemble methods of Corollary 7.5 or Corollary 7.6. Thus, in that way, _L_ 1 -regularized AdaBoost benefits from a more favorable and natural theoretical guarantee than AdaBoost. By the generalization of Corollary 7.5 to a uniform convergence bound over _\u03c1_, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, the following holds for all ensemble functions _f_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [ with] _[ \u2225]_ _**[\u03b1]**_ [\u00af] _[\u2225]_ [1] _[ \u2264]_ [1 and all] _[ \u03c1][ \u2208]_ [(0] _[,]_ [ 1]:] \ufffd log [2] _\u03b4_ (7.27) 2 _m_ _[.]_ \ufffdH\ufffd + _\u03c1_ [R] _[m]_ _R_ ( _f_ ) _\u2264_ [1] _m_ _m_ \ufffd \ufffd 1 _f_ ( _x_ _i_ ) _\u2264\u03c1_ + [2] _i_ =1 _\u03c1_ \ufffd log log 2 _\u03c1_ 2 + _m_ **166** **Chapter 7** **Boosting** The inequality also trivially holds for _\u03c1 >_ 1 since, in that case, the first term on the right-hand side of the bound is equal to one. Indeed, in that case, by H\u00a8older\u2019s _\u2225_ inequality, for any _**\u03b1**_ \u00af _\u2225_ 1 _\u2264_ 1 _< \u03c1_ . _x \u2208_ X, we have _f_ ( _x_ ) = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[i]_ _[h]_ _[j]_ [(] _[x]_ [)] _[ \u2264\u2225]_ _**[\u03b1]**_ [\u00af] _[\u2225]_ [1] [ max] _[j][\u2208]_ [[] _[N]_ []] _[ |][h]_ _[j]_ [(] _[x]_ [)] _[| \u2264]_ Now, in view of the general upper bound 1 _u\u2264_ 0 _\u2264_ _e_ _[\u2212][u]_ valid for all _u \u2208_ R, with probability at least 1 _\u2212_ _\u03b4_, the following holds for all _f_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [ with] _[ \u2225]_ _**[\u03b1]**_ [\u00af] _[\u2225]_ [1] _[ \u2264]_ [1] and all _\u03c1 >_ 0: \ufffd log [2] _\u03b4_ (7.28) 2 _m_ _[.]_ \ufffdH\ufffd + _\u03c1_ [R] _[m]_ ~~\ufffd~~ _R_ ( _f_ ) _\u2264_ [1] _m_ _m_ \ufffd \ufffd _e_ [1] _[\u2212]_ _[f]_ [(] _\u03c1_ _[x][i]_ [)] _i_ =1 _[x][i]_ [)] _\u03c1_ + [2] log log 2 _\u03c1_ 2 + _m_ Since for any _\u03c1 >_ 0, _f/\u03c1_ admits the same generalization error as _f_, with probability at least 1 _\u2212_ _\u03c1_, the following inequality holds for all _f_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [ with] _[ \u2225]_ _**[\u03b1]**_ [\u00af] _[\u2225]_ [1] _[ \u2264]_ [1] _[/\u03c1]_ and all _\u03c1 >_ 0: ~~\ufffd~~ log [2] _\u03b4_ (7.29) 2 _m_ _[.]_ \ufffdH\ufffd + _\u03c1_ [R] _[m]_ _R_ ( _f_ ) _\u2264_ [1] _m_ _m_ \ufffd _e_ [1] _[\u2212][f]_ [(] _[x]_ _[i]_ [)] + [2] \ufffd _i_ =1 _\u03c1_ ~~\ufffd~~ log log 2 _\u03c1_ 2 + _m_ This inequality can be used to derive an algorithm that selects \u00af _**\u03b1**_ and _\u03c1 >_ 0 to minimize the right-hand side. The minimization with respect to _\u03c1_ does not lead to a convex optimization and depends on theoretical constant factors affecting the second and third terms, which may not be optimal. Thus, instead, _\u03c1_ is left as a free parameter of the algorithm, typically determined via cross-validation. Now, since only the first term of the right-hand side",
    "chunk_id": "foundations_machine_learning_163"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "depends on \u00af _**\u03b1**_, the bound suggests selecting \u00af _**\u03b1**_ as the solution of the following optimization problem: _m_ _m_ \ufffd \ufffd _e_ _[\u2212][f]_ [(] _[x]_ _[i]_ [)] = _m_ [1] _i_ =1 min _\u2225_ _**\u03b1**_ \u00af _\u2225_ 1 _\u2264_ _\u03c1_ [1] 1 _m_ _m_ \ufffd _e_ _[\u2212]_ [\ufffd] _j_ _[N]_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] _._ (7.30) _i_ =1 Introducing a Lagrange variable _\u03bb \u2265_ 0, the optimization problem can be written equivalently as _m_ \ufffd _e_ _[\u2212]_ [\ufffd] _j_ _[N]_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [)] + _\u03bb\u2225_ _**\u03b1**_ \u00af _\u2225_ 1 _._ (7.31) _i_ =1 min _\u2225_ _**\u03b1**_ \u00af _\u2225_ 1 _\u2264_ _\u03c1_ [1] 1 _m_ Since for any choice of _\u03c1_ in the constraint of (7.30) there exists an equivalent dual variable _\u03bb_ in the formulation (7.31) that achieves the same optimal \u00af _**\u03b1**_, _\u03bb \u2265_ 0 can be freely selected via cross-validation. The resulting objective function therefore precisely coincides with that of _L_ 1 -regularized AdaBoost. **7.5** **Discussion** **167** **7.5** **Discussion** AdaBoost offers several advantages: it is simple, its implementation is straightforward, and the time complexity of each round of boosting as a function of the sample size is rather favorable. As already discussed, when using decision stumps, the time complexity of each round of boosting is in _O_ ( _mN_ ). Of course, if the dimension of the feature space _N_ is very large, then the algorithm could become in fact quite slow. AdaBoost additionally benefits from a rich theoretical analysis. Nevertheless, there are still many theoretical questions related to the algorithm. For example, as we saw, the algorithm in fact does not maximize the margin, and yet algorithms that do maximize the margin do not always outperform it. This suggests that perhaps a finer analysis based on a notion different from that of minimal margin could shed more light on the properties of the algorithm. A minor drawback of the algorithm is the need to select the parameter _T_ and the base classifier set. The choice of the number of rounds of boosting _T_ (stopping criterion) is crucial to the performance of the algorithm. As suggested by the VCdimension analysis, larger values of _T_ can lead to overfitting. In practice, _T_ is typically determined via cross-validation. The choice of the base classifiers is also crucial. The complexity of the family of base classifiers H appeared in all the bounds presented and it is important to control it in order to guarantee generalization. On the other hand, insufficiently complex hypothesis sets could lead to low margins. Probably the most serious disadvantage of AdaBoost is its performance in the presence of noise, at least in some tasks. The distribution weight assigned to examples that are harder to classify substantially increases with the number of rounds of boosting, by the nature of the algorithm. These examples may end up dominating the selection of the base classifiers, which, with a large enough number of rounds, will play a detrimental role in the definition of the linear combination defined by AdaBoost.",
    "chunk_id": "foundations_machine_learning_164"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Several solutions have been proposed to address these issues. One consists of using a \u201cless aggressive\u201d objective function than the exponential function of AdaBoost, such as the logistic loss, to penalize less incorrectly classified points. Another solution is based on a regularization, e.g., the _L_ 1 -regularized AdaBoost described in the previous section. An empirical study of AdaBoost has shown that uniform noise severely damages its accuracy. This has also been corroborated by recent theoretical results showing that boosting algorithms based on convex potentials do not tolerate even low levels of random noise. Moreover, these issues have been shown to persist even when using _L_ 1 -regularization or early stopping. However, the uniform noise model used in those experiments or analysis is rather unrealistic and seems unlikely to appear **168** **Chapter 7** **Boosting** in practice. The model assumes that a label corruption with some fixed probability affects all instances uniformly. Clearly, the performance of any algorithm should degrade in the presence of such noise. Empirical results suggest, however, that the performance of AdaBoost tends to degrade more than that of other algorithms for this uniform noise model. Finally, notice that the behavior of AdaBoost in the presence of noise can be used, in fact, as a useful feature for detecting _outliers_, that is, examples that are incorrectly labeled or that are hard to classify. Examples with large weights after a certain number of rounds of boosting can be identified as outliers. **7.6** **Chapter notes** The question of whether a weak learning algorithm could be _boosted_ to derive a strong learning algorithm was first posed by Kearns and Valiant [1988, 1994], who also gave a negative proof of this result for a distribution-dependent setting. The first positive proof of this result in a distribution-independent setting was given by Schapire [1990], and later by Freund [1990]. These early boosting algorithms, boosting by filtering [Schapire, 1990] or boosting by majority [Freund, 1990, 1995] were not practical. The AdaBoost algorithm introduced by Freund and Schapire [1997] solved several of these practical issues. Freund and Schapire [1997] further gave a detailed presentation and analysis of the algorithm including the bound on its empirical error, a VC-dimension analysis, and its applications to multi-class classification and regression. Early experiments with AdaBoost were carried out by Drucker, Schapire, and Simard [1993], who gave the first implementation in OCR with weak learners based on neural networks and Drucker and Cortes [1995], who reported the empirical performance of AdaBoost combined with decision trees, in particular decision stumps. The fact that AdaBoost coincides with coordinate descent applied to an exponential objective function was later shown by Duffy and Helmbold [1999], Mason et al. [1999], and Friedman [2000]. Friedman, Hastie, and Tibshirani [2000] also gave an interpretation of boosting in terms of additive models. They also pointed out the close connections between AdaBoost and logistic regression, in particular the fact that their objective functions have a similar behavior near zero or the fact that their expectation admit the same minimizer, and derived an alternative boosting algorithm, LogitBoost, based on the logistic loss. Lafferty [1999]",
    "chunk_id": "foundations_machine_learning_165"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "showed how an incremental family of algorithms, including LogitBoost, can be derived from Bregman divergences and designed to closely approximate AdaBoost when varying a parameter. Kivinen and Warmuth [1999] gave an equivalent view of AdaBoost as an entropy projection. They showed that the distribution over the sample found **7.6** **Chapter notes** **169** by Adaboost at each round is approximately the solution to the problem of finding the closest distribution to the one at the previous round, subject to the constraint that it be orthogonal to the vector of errors of the current base hypotheses. Here, closeness is measured by a Bregman divergence, which, for AdaBoost is the unnormalized relative entropy. Collins, Schapire, and Singer [2002] later showed that boosting and logistic regression were special instances of a common framework based on Bregman divergences and used that to give the first convergence proof of AdaBoost. Another direct relationship between AdaBoost and logistic regression is given by Lebanon and Lafferty [2001] who showed that the two algorithms minimize the same extended relative entropy objective function subject to the same feature constraints, except from an additional normalization constraint for logistic regression. A margin-based analysis of AdaBoost was first presented by Schapire, Freund, Bartlett, and Lee [1997], including theorem 7.7 which gives a bound on the empirical margin loss. Our presentation is based on the elegant derivation of margin bounds by Koltchinskii and Panchenko [2002] using the notion of Rademacher complexity. Rudin et al. [2004] gave an example showing that, in general, AdaBoost does not maximize the _L_ 1 -margin. R\u00a8atsch and Warmuth [2002] provided asymptotic lower bounds for the margin achieved by AdaBoost under some conditions. The _L_ 1 margin maximization based on an LP is due to Grove and Schuurmans [1998]. R\u00a8atsch, Onoda, and M\u00a8uller [2001] suggested a modification of that algorithm using a soft-margin instead and pointed out its connections with SVMs. The gametheoretic interpretation of boosting and the application of von Neumann\u2019s minimax theorem [von Neumann, 1928] in that context were pointed out by Freund and Schapire [1996, 1999b]; see also Grove and Schuurmans [1998] and Breiman [1999]. The _L_ 1 -regularized AdaBoost algorithm described in Section 7.4 is presented and analyzed by R\u00a8atsch, Mika, and Warmuth [2001]. Cortes, Mohri, and Syed [2014] introduced a new boosting algorithm, _DeepBoost_, which they proved to benefit from finer learning guarantees, including favorable ones even when using as base classifier set relatively rich families, for example a family of very deep decision trees, or other similarly complex families. In DeepBoost, the decisions in each iteration of which classifier to add to the ensemble and which weight to assign to that classifier, depend on the (data-dependent) complexity of the sub-family to which the classifier belongs. Cortes, Mohri, and Syed [2014] further showed that empirically DeepBoost achieves a better performance than AdaBoost, Logistic Regression, and their _L_ 1 regularized variants. Both AdaBoost and _L_ 1 -regularized AdaBoost can be viewed as special instances of DeepBoost. Dietterich [2000] provided extensive empirical evidence for the fact that uniform noise can severely damage the accuracy of AdaBoost. This has been",
    "chunk_id": "foundations_machine_learning_166"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "reported by **170** **Chapter 7** **Boosting** a number of other authors since then. Long and Servedio [2010] further recently showed the failure of boosting algorithms based on convex potentials to tolerate random noise, even with _L_ 1 -regularization or early stopping. There are several excellent surveys and tutorials related to boosting [Schapire, 2003, Meir and R\u00a8atsch, 2002, Meir and R\u00a8atsch, 2003], including the recent book of Schapire and Freund [2012] fully dedicated to this topic, with an extensive list of references and a detailed presentation. **7.7** **Exercises** 7.1 VC-dimension of the hypothesis set of AdaBoost. Prove the upper bound on the VC-dimension of the hypothesis set _F_ _T_ of AdaBoost after _T_ rounds of boosting, as stated in equation (7.9). 7.2 Alternative objective functions. This problem studies boosting-type algorithms defined with objective functions different from that of AdaBoost. We assume that the training data are given as _m_ labeled examples ( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ ) _\u2208_ X _\u00d7 {\u2212_ 1 _,_ +1 _}_ . We further assume that \u03a6 is a strictly increasing convex and differentiable function over R such that: _\u2200x \u2265_ 0 _,_ \u03a6( _x_ ) _\u2265_ 1 and _\u2200x <_ 0 _,_ \u03a6( _x_ ) _>_ 0. (a) Consider the loss function _L_ ( _\u03b1_ ) = [\ufffd] _[m]_ _i_ =1 [\u03a6(] _[\u2212][y]_ _[i]_ _[f]_ [(] _[x]_ _[i]_ [)) where] _[ f]_ [ is a linear] combination of base classifiers, i.e., _f_ = [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [ (as in AdaBoost). Derive] a new boosting algorithm using the objective function _L_ . In particular, characterize the best base classifier _h_ _u_ to select at each round of boosting if we use coordinate descent. (b) Consider the following functions: (1) zero-one loss \u03a6 1 ( _\u2212u_ ) = 1 _u\u2264_ 0 ; (2) least squared loss \u03a6 2 ( _\u2212u_ ) = (1 _\u2212_ _u_ ) [2] ; (3) SVM loss \u03a6 3 ( _\u2212u_ ) = max _{_ 0 _,_ 1 _\u2212_ _u}_ ; and (4) logistic loss \u03a6 4 ( _\u2212u_ ) = log(1 + _e_ _[\u2212][u]_ ). Which functions satisfy the assumptions on \u03a6 stated earlier in this problem? (c) For each loss function satisfying these assumptions, derive the corresponding boosting algorithm. How do the algorithm(s) differ from AdaBoost? 7.3 Update guarantee. Assume that the main weak learner assumption of AdaBoost holds. Let _h_ _t_ be the base learner selected at round _t_ . Show that the base learner _h_ _t_ +1 selected at round _t_ + 1 must be different from _h_ _t_ . 7.4 Weighted instances. Let the training sample be _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )). Suppose we wish to penalize differently errors made on _x_ _i_ versus _x_ _j_ . To do that, we associate some non-negative importance weight _w_ _i_ to each point _x_ _i_ and define **7.7** **Exercises** **171** the objective function _F_ ( _**\u03b1**_ ) = [\ufffd] _[m]_ _i_",
    "chunk_id": "foundations_machine_learning_167"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "=1 _[w]_ _[i]_ _[e]_ _[\u2212][y]_ _[i]_ _[f]_ [(] _[x]_ _[i]_ [)] [, where] _[ f]_ [ =][ \ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [. Show] that this function is convex and differentiable and use it to derive a boostingtype algorithm. 7.5 Define the unnormalized correlation of two vectors **x** and **x** _[\u2032]_ as the inner product between these vectors. Prove that the distribution vector (D _t_ +1 (1) _, . . .,_ D _t_ +1 ( _m_ )) defined by AdaBoost and the vector of components _y_ _i_ _h_ _t_ ( _x_ _i_ ) are uncorrelated. 7.6 Fix _\u03f5 \u2208_ (0 _,_ 1 _/_ 2). Let the training sample be defined by _m_ points in the plane with _[m]_ _[m]_ _[m]_ 4 [negative points all at coordinate (1] _[,]_ [ 1), another] 4 with _[m]_ _[m]_ 4 [negative points all at coordinate (1] _[,]_ [ 1), another] 4 [negative points all] at coordinate ( _\u2212_ 1 _, \u2212_ 1), _m_ (14 _\u2212\u03f5_ ) positive points all at coordinate (1 _, \u2212_ 1), and _m_ (14+ _\u03f5_ ) positive points all at coordinate ( _\u2212_ 1 _,_ +1). Describe the behavior of AdaBoost when run on this sample using boosting stumps. What solution does the algorithm return after _T_ rounds? 7.7 Noise-tolerant AdaBoost. AdaBoost may be significantly overfitting in the presence of noise, in part due to the high penalization of misclassified examples. To reduce this effect, one could use instead the following objective function: _F_ = _m_ \ufffd _G_ ( _\u2212y_ _i_ _f_ ( _x_ _i_ )) _,_ (7.32) _i_ =1 where _G_ is the function defined on R by _G_ ( _x_ ) = _e_ _[x]_ if _x \u2264_ 0 (7.33) _x_ + 1 otherwise _._ \ufffd (a) Show that the function _G_ is convex and differentiable. (b) Use _F_ and greedy coordinate descent to derive an algorithm similar to Ad aBoost. (c) Compare the reduction of the empirical error rate of this algorithm with that of AdaBoost. 7.8 Simplified AdaBoost. Suppose we simplify AdaBoost by setting the parameter _\u03b1_ _t_ to a fixed value _\u03b1_ _t_ = _\u03b1 >_ 0, independent of the boosting round _t_ . (a) Let _\u03b3_ be such that ( [1] 2 _[\u2212]_ _[\u03f5]_ _[t]_ [)] _[ \u2265]_ _[\u03b3 >]_ [ 0. Find the best value of] _[ \u03b1]_ [ as a function] of _\u03b3_ by analyzing the empirical error. (b) For this value of _\u03b1_, does the algorithm assign the same probability mass to correctly classified and misclassified examples at each round? If not, which set is assigned a higher probability mass? **172** **Chapter 7** **Boosting** AdaBoost( **M** _, t_ max ) 1 _\u03bb_ 1 _,j_ _\u2190_ 0 for _i_ = 1 _, . . ., m_ 2 **for** _t \u2190_ 1 **to** _t_ max **do** exp( _\u2212_ ( **M** _**\u03bb**_ _t_ ) _i_ ) 3 _d_ _t,i_ _\u2190_ \ufffd _mk_ =1 [exp(] _[\u2212]_ [(] **[M]** _**[\u03bb]**_ _[t]_ [)] _[k]_ [)] [ for] _[ i]_ [ = 1] _[, . . ., m]_ 4 _j_ _t_ _\u2190_ argmax _j_ ( **d** _[\u22a4]_ _t_",
    "chunk_id": "foundations_machine_learning_168"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**[M]** [)] _[j]_ 5 _r_ _t_ _\u2190_ ( **d** _[\u22a4]_ _t_ **[M]** [)] _[j]_ _t_ 6 _\u03b1_ _t_ _\u2190_ [1] [log] \ufffd 1+ _r_ _t_ \ufffd [1] 2 [log] \ufffd 11+ _\u2212rr_ _t_ 1+ _r_ _t_ 1 _\u2212r_ _t_ \ufffd 7 _**\u03bb**_ _t_ +1 _\u2190_ _**\u03bb**_ _t_ + _\u03b1_ _t_ **e** _jt_ _,_ where **e** _jt_ is 1 in position _j_ _t_ and 0 elsewhere _._ 8 **return** _**\u03bb**_ _t_ max _\u2225_ _**\u03bb**_ _t_ max _\u2225_ 1 **Figure 7.7** AdaBoost defined with respect to a matrix **M**, which encodes the accuracy of each weak classifier on each training point. (c) Using the previous value of _\u03b1_, give a bound on the empirical error of the algorithm that depends only on _\u03b3_ and the number of rounds of boosting _T_ . (d) Using the previous bound, show that for _T >_ [lo] 2 [g] _\u03b3_ _[ m]_ [2] [, the resulting hypothesis] is consistent with the sample of size _m_ . (e) Let _s_ be the VC-dimension of the base learners used. Give a bound on the generalization error of the consistent hypothesis obtained after _T_ = \ufffd lo2g _\u03b3 m_ [2] \ufffd+ generalization error of the consistent hypothesis obtained after _T_ = \ufffd lo2g _\u03b3 m_ [2] \ufffd+ 1 rounds of boosting. ( _Hint_ : Use the fact that the VC-dimension of the family of functions _{_ sgn( [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [) :] _[ \u03b1]_ _[t]_ _[ \u2208]_ [R] _[}]_ [ is bounded by 2(] _[s]_ [+1)] _[T]_ [ log] 2 [(] _[eT]_ [)).] Suppose now that _\u03b3_ varies with _m_ . Based on the bound derived, what can be said if _\u03b3_ ( _m_ ) = _O_ (\ufffd lo _m_ g _m_ [)?)] log _m_ _m_ g _m_ [)?)] 7.9 AdaBoost example. In this exercise we consider a concrete example that consists of eight training points and eight weak classifiers. (a) Define an _m \u00d7 n_ matrix **M** where **M** _ij_ = _y_ _i_ _h_ _j_ ( **x** _i_ ), i.e., **M** _ij_ = +1 if training example _i_ is classified correctly by weak classifier _h_ _j_, and _\u2212_ 1 otherwise. Let **d** _t_ _,_ _**\u03bb**_ _t_ _\u2208_ R _[n]_, _\u2225_ **d** _t_ _\u2225_ 1 = 1 and _d_ _t,i_ (respectively _\u03bb_ _t,i_ ) equal the _i_ _[th]_ component of **d** _t_ (respectively _**\u03bb**_ _t_ ). Now, consider AdaBoost as described in figure 7.7 **7.7** **Exercises** **173** and define **M** as below with eight training points and eight weak classifiers. \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 **M** = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed _\u2212_ 1 1 1 1 1 _\u2212_ 1 _\u2212_ 1 1 _\u2212_ 1 1 1 _\u2212_ 1 _\u2212_ 1 1 1 1 1 _\u2212_ 1 1 1 1 _\u2212_ 1 1 1 1 _\u2212_ 1 1 1 _\u2212_ 1 1 1 1 1 _\u2212_ 1 1 _\u2212_ 1 1 1 1 _\u2212_ 1 1 1 _\u2212_ 1 1 1 1 1 _\u2212_",
    "chunk_id": "foundations_machine_learning_169"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "1 1 1 _\u2212_ 1 1 1 1 _\u2212_ 1 1 1 1 1 1 _\u2212_ 1 _\u2212_ 1 1 _\u2212_ 1 Assume that we start with the following initial distribution over the datapoints: 8 _\u221a_ 5 _,_ [3] _[ \u2212]_ 8 _\u221a_ [1] 6 _[,]_ [ 1] 6 5 _\u2212_ 1 8 _,_ _\u22a4_ 5 _\u2212_ 1 8 _,_ 0\ufffd 3 _\u2212_ _\u221a_ **d** 1 = \ufffd 8 8 5 _,_ [1] 6 6 _[,]_ [ 1] 6 6 _[,]_ _\u221a_ _\u221a_ Compute the first few steps of the AdaBoost algorithm using **M**, **d** 1, and _t_ _max_ = 7. What weak classifier is picked at each round of boosting? Do you notice any pattern? (b) What is the _L_ 1 norm margin produced by AdaBoost for this example? (c) Instead of using AdaBoost, imagine we combined our classifiers using the following coefficients: [2 _,_ 3 _,_ 4 _,_ 1 _,_ 2 _,_ 2 _,_ 1 _,_ 1] _\u00d7_ 161 [. What is the margin in this] case? Does AdaBoost maximize the margin? 7.10 Boosting in the presence of unknown labels. Consider the following variant of the classification problem where, in addition to the positive and negative labels +1 and _\u2212_ 1, points may be labeled with 0. This can correspond to cases where the true label of a point is unknown, a situation that often arises in practice, or more generally to the fact that the learning algorithm incurs no loss for predicting _\u2212_ 1 or +1 for such a point. Let X be the input space and let Y = _{\u2212_ 1 _,_ 0 _,_ +1 _}_ . As in standard binary classification, the loss of _f_ : X _\u2192_ R on a pair ( _x, y_ ) _\u2208_ X _\u00d7_ Y is defined by 1 _yf_ ( _x_ ) _<_ 0 . Consider a sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_ and a hypothesis set _H_ of base functions taking values in _{\u2212_ 1 _,_ 0 _,_ +1 _}_ . For a base hypothesis _h_ _t_ _\u2208_ H and a distribution D _t_ over indices _i \u2208_ [ _m_ ], define _\u03f5_ _[s]_ _t_ [for] _[ s][ \u2208{\u2212]_ [1] _[,]_ [ 0] _[,]_ [ +1] _[}]_ [ by] _\u03f5_ _[s]_ _t_ [=][ E] _[i][\u223c]_ [D] _t_ [[1] _y_ _i_ _h_ _t_ ( _x_ _i_ )= _s_ [].] **174** **Chapter 7** **Boosting** (a) Derive a boosting-style algorithm for this setting in terms of _\u03f5_ _[s]_ _t_ [s, using the] same objective function as that of AdaBoost. You should carefully justify the definition of the algorithm. (b) What is the weak-learning assumption in this setting? (c) Write the full pseudocode of the algorithm. (d) Give an upper bound on the training error of the algorithm as a function of the number of rounds of boosting and _\u03f5_ _[s]_ _t_ [s.] 7.11 HingeBoost. As discussed in the chapter, AdaBoost can be viewed as coordinate descent applied to an exponential objective function. Here,",
    "chunk_id": "foundations_machine_learning_170"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "we consider an alternative ensemble method algorithm, HingeBoost, that consists of applying coordinate descent to an objective function based on the hinge loss. Consider the function _F_ defined for all _**\u03b1**_ _\u2208_ R _[N]_ by \uf8f6 _,_ (7.34) \uf8f8 \uf8eb\uf8ed0 _,_ 1 _\u2212_ _y_ _i_ _N_ \ufffd \ufffd _\u03b1_ _j_ _h_ _j_ ( _x_ _i_ ) _j_ =1 _F_ ( _**\u03b1**_ ) = _m_ max \ufffd _i_ =1 where the _h_ _j_ s are base classifiers belonging to a hypothesis set _H_ of functions taking values _\u2212_ 1 or +1. (a) Show that _F_ is convex and admits a right- and left-derivative along any direction. (b) For any _j \u2208_ [ _N_ ], let _**e**_ _j_ denote the direction corresponding to the base hypothesis _h_ _j_ . Let _**\u03b1**_ _t_ denote the vector of coefficients _\u03b1_ _t,j_, _j \u2208_ [ _N_ ] obtained after _t \u2265_ 0 iterations of coordinate descent and _f_ _t_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ _[t,j]_ _[h]_ _[j]_ [ the predic-] tor obtained after _t_ iterations. Give the expression of the right-derivative _F_ + _[\u2032]_ [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [) and the left-derivative] _F_ _\u2212_ _[\u2032]_ [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [) after] _[ t][ \u2212]_ [1 iterations in terms of] _[ f]_ _[t][\u2212]_ [1] [.] (c) For any _j \u2208_ [ _N_ ], define the maximum directional derivative _\u03b4F_ ( _**\u03b1**_ _t\u2212_ 1 _,_ _**e**_ _j_ ) at _**\u03b1**_ _t\u2212_ 1 as follows: _\u03b4F_ ( _**\u03b1**_ _t\u2212_ 1 _,_ _**e**_ _j_ ) = \uf8f1\uf8f4\uf8f40 _F_ _[\u2032]_ \uf8f2 + [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] 0 if _F_ _\u2212_ _[\u2032]_ [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] _[ \u2264]_ [0] _[ \u2264]_ _[F]_ _[ \u2032]_ + [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] _F_ + _[\u2032]_ [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] if _F_ _\u2212_ _[\u2032]_ [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] _[ \u2264]_ _[F]_ _[ \u2032]_ + [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] _[ \u2264]_ [0] _F_ _\u2212_ _[\u2032]_ [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] if 0 _\u2264_ _F_ _\u2212_ _[\u2032]_ [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] _[ \u2264]_ _[F]_ _[ \u2032]_ + [(] _**[\u03b1]**_ _[t][\u2212]_ [1] _[,]_ _**[ e]**_ _[j]_ [)] _[.]_ \uf8f4\uf8f4\uf8f3 The direction _**e**_ _j_ considered by the coordinate descent considered here is the one maximizing _|\u03b4F_ ( _**\u03b1**_ _t\u2212_ 1 _,_ _**e**_ _j_ ) _|_ . Once the best direction _j_ is selected, the **7.7** **Exercises** **175** step _\u03b7_ can be determined by minimizing _F_ ( _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ _**e**_ _j_ ) using a grid search. Give the pseudocode of HingeBoost. 7.12 Empirical margin loss boosting. As discussed in the chapter, AdaBoost can be viewed as coordinate descent applied to a convex upper bound on the empirical error. Here, we consider an algorithm seeking to minimize the empirical margin loss. For any 0 _\u2264_ _\u03c1 <_ 1 let _R_ [\ufffd] _S,\u03c1_ ( _f_ ) = _m_ 1 \ufffd _mi_ =1 [1]",
    "chunk_id": "foundations_machine_learning_171"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[y]_ _i_ _[f]_ [(] _[x]_ _i_ [)] _[\u2264][\u03c1]_ [ denote the empirical] margin loss of a function _f_ of the form _f_ = \ufffd _Tt_ =1 _T_ _[\u03b1]_ _[t]_ _[h]_ _[t]_ for a labeled sample \ufffd _t_ =1 _[\u03b1]_ _[t]_ _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )). (a) Show that _R_ [\ufffd] _S,\u03c1_ ( _f_ ) can be upper bounded as follows: _T_ \ufffd _\u03b1_ _t_ _t_ =1 _T_ \ufffd \ufffd _\u2212y_ _i_ \ufffd _T_ \ufffd _\u03b1_ _t_ _h_ _t_ ( _x_ _i_ ) + _\u03c1_ _t_ =1 _T_ \ufffd \ufffd _R_ _S,\u03c1_ ( _f_ ) _\u2264_ [1] _m_ _m_ \ufffd exp _i_ =1 _._ (b) For any _\u03c1 >_ 0, let _G_ _\u03c1_ be the objective function defined for all _**\u03b1**_ _\u2265_ 0 by _N_ \ufffd _\u03b1_ _j_ _j_ =1 _N_ \ufffd \uf8f6 \uf8f8 _,_ \uf8f6 \uf8eb\uf8ed _\u2212y_ _i_ _N_ \ufffd _\u03b1_ _j_ _h_ _j_ ( _x_ _i_ ) + _\u03c1_ _j_ =1 _N_ \ufffd _G_ _\u03c1_ ( _**\u03b1**_ ) = [1] _m_ _m_ \ufffd exp _i_ =1 with _h_ _j_ _\u2208_ _H_ for all _j \u2208_ [ _N_ ], with the notation used in class in the boosting lecture. Show that _G_ _\u03c1_ is convex and differentiable. (c) Derive a boosting-style algorithm _A_ _\u03c1_ by applying (maximum) coordinate descent to _G_ _\u03c1_ . You should justify in detail the derivation of the algorithm, in particular the choice of the base classifier selected at each round and that of the step. Compare both to their counterparts in AdaBoost. (d) What is the equivalent of the weak learning assumption for _A_ _\u03c1_ ( _Hint_ : use non-negativity of the step value)? (e) Give the full pseudocode of the algorithm _A_ _\u03c1_ . What can you say about the _A_ 0 algorithm? (f) Provide a bound on _R_ [\ufffd] _S,\u03c1_ ( _f_ ). i. Prove the upper bound _R_ [\ufffd] _S,\u03c1_ ( _f_ ) _\u2264_ exp _Tt_ =1 _[\u03b1]_ _[t]_ _[\u03c1]_ _Tt_ =1 _[Z]_ _[t]_ [, where the] \ufffd\ufffd \ufffd\ufffd normalization factors _Z_ _t_ are defined as in the case of AdaBoost (with _\u03b1_ _t_ the step chosen by _A_ _\u03c1_ at round _t_ ). ii. Give the expression of _Z_ _t_ as a function of _\u03c1_ and _\u03f5_ _t_, where _\u03f5_ _t_ is the weighted error of the hypothesis found by _A_ _\u03c1_ at round _t_ (defined in the same way **176** **Chapter 7** **Boosting** as for AdaBoost in class). Use that to prove the following upper bound \ufffd 1+ _\u03c1_ _R_ _S,\u03c1_ ( _f_ ) _\u2264_ _u_ 2 \ufffd _T_ _[\u03c1]_ _T_ 2 \ufffd \ufffd + _\u03c1_ 2 + _u_ _[\u2212]_ [1] _[\u2212]_ 2 _[\u03c1]_ \ufffd _\u03f5_ [1] _t_ _[\u2212][\u03c1]_ (1 _\u2212_ _\u03f5_ _t_ ) [1+] _[\u03c1]_ _,_ _t_ =1 where _u_ = [1] _[\u2212][\u03c1]_ 1+ _\u03c1_ [.] iii. Assume that for all _t \u2208_ [ _T_ ], 1 _\u2212_ 2 _\u03c1_ _\u2212_ _\u03f5_ _t_ _> \u03b3 >_ 0. Use the result of the previous question to show that \ufffd _R_ _S,\u03c1_ ( _f_ ) _\u2264_ exp _\u2212_",
    "chunk_id": "foundations_machine_learning_172"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[2] _[\u03b3]_ [2] _[T]_ \ufffd 1 _\u2212_ _\u03c1_ [2] _._ \ufffd ( _Hint_ : you can use without proof the following identity: 1+ _\u03c1_ _u_ 2 \ufffd _[\u03c1]_ 2 \ufffd [\ufffd] + _\u03c1_ 2 + _u_ _[\u2212]_ [1] _[\u2212]_ 2 _[\u03c1]_ _\u03f5_ [1] _t_ _[\u2212][\u03c1]_ (1 _\u2212_ _\u03f5_ _t_ ) [1+] _[\u03c1]_ _\u2264_ 1 _\u2212_ 2 \ufffd 1 _\u2212_ 2 _\u03c1_ _\u2212_ _\u03f5_ _t_ \ufffd 2 1 _\u2212_ _\u03c1_ [2] _,_ _[\u2212]_ 2 _[\u03c1]_ _\u2212_ _\u03f5_ _t_ _>_ 0.) Show that for _T \u2265_ [(][lo][g] _[ m]_ 2 [)(] [2] [1] _[\u2212][\u03c1]_ [2] [)] valid for [1] _[\u2212][\u03c1]_ valid for _[\u2212]_ 2 _[\u03c1]_ _\u2212_ _\u03f5_ _t_ _>_ 0.) Show that for _T \u2265_ 2 _\u03b3_ [2] _[\u2212][\u03c1]_, all points of the training data have margin at least _\u03c1_ . # 8 On-Line Learning This chapter presents an introduction to on-line learning, an important area with a rich literature and multiple connections with game theory and optimization that is increasingly influencing the theoretical and algorithmic advances in machine learning. In addition to the intriguing novel learning theory questions that they raise, on-line learning algorithms are particularly attractive in modern applications since they provide an efficient solution for large-scale problems. These algorithms process one sample at a time with an update per iteration that is often computationally cheap and easy to implement. As a result, they are typically significantly more efficient both in time and space and more practical than batch algorithms, when processing modern data sets of several million or billion points. They are also typically easy to implement. Moreover, on-line algorithms do not require any distributional assumption; their analysis assumes an adversarial scenario. This makes them applicable in a variety of scenarios where the sample points are not drawn i.i.d. or according to a fixed distribution. We first introduce the general scenario of on-line learning, then present and analyze several key algorithms for on-line learning with expert advice, including the deterministic and randomized weighted majority algorithms for the zero-one loss and an extension of these algorithms for convex losses. We also describe and analyze two standard on-line algorithms for linear classification, the Perceptron and Winnow algorithms, as well as some extensions. While on-line learning algorithms are designed for an adversarial scenario, they can be used, under some assumptions, to derive accurate predictors for a distributional scenario. We derive learning guarantees for this on-line to batch conversion. Finally, we briefly point out the connection of on-line learning with game theory by describing its use to derive a simple proof of von Neumann\u2019s minimax theorem. **178** **Chapter 8** **On-Line Learning** **8.1** **Introduction** The learning framework for on-line algorithms is in stark contrast to the PAC learning or stochastic models discussed up to this point. First, instead of learning from a training set and then testing on a test set, the on-line learning scenario mixes the training and test phases. Second, PAC learning follows the key assumption that the distribution over data points is fixed over time, both for training and test points, and that points are sampled in an i.i.d. fashion. Under this assumption,",
    "chunk_id": "foundations_machine_learning_173"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the natural goal is to learn a hypothesis with a small expected loss or generalization error. In contrast, with on-line learning, _no distributional assumption_ is made, and thus there is no notion of generalization. Instead, the performance of on-line learning algorithms is measured using a _mistake model_ and the notion of _regret_ . To derive guarantees in this model, theoretical analyses are based on a worst-case or adversarial assumption. The general on-line setting involves _T_ rounds. At the _t_ th round, the algorithm receives an instance _x_ _t_ _\u2208_ X and makes a prediction \ufffd _y_ _t_ _\u2208_ Y. It then receives the true label _y_ _t_ _\u2208_ Y and incurs a loss _L_ ( _y_ \ufffd _t_ _, y_ _t_ ), where _L_ : Y _\u00d7_ Y _\u2192_ R + is a loss function. More generally, the prediction domain for the algorithm may be Y _[\u2032]_ = _\u0338_ Y and the loss function defined over Y _[\u2032]_ _\u00d7_ Y. For classification problems, we often have Y = _{_ 0 _,_ 1 _}_ and _L_ ( _y, y_ _[\u2032]_ ) = _|y_ _[\u2032]_ _\u2212_ _y|_, while for regression Y _\u2286_ R and typically _L_ ( _y, y_ _[\u2032]_ ) = ( _y_ _[\u2032]_ _\u2212_ _y_ ) [2] . The objective in the on-line setting is to minimize the cumulative loss: [\ufffd] _[T]_ _t_ =1 _[L]_ [(] _[y]_ [\ufffd] _[t]_ _[, y]_ _[t]_ [) over] _[ T]_ [ rounds.] **8.2** **Prediction with expert advice** We first discuss the setting of online learning with expert advice, and the associated notion of _regret_ . In this setting, at the _t_ th round, in addition to receiving _x_ _t_ _\u2208_ X, the algorithm also receives _advice y_ _t,i_ _\u2208_ Y, _i \u2208_ [ _N_ ], from _N_ experts. Following the general framework of on-line algorithms, it then makes a prediction, receives the true label, and incurs a loss. After _T_ rounds, the algorithm has incurred a cumulative loss. The objective in this setting is to minimize the _regret R_ _T_, also called _external regret_, which compares the cumulative loss of the algorithm to that of the best expert in hindsight after _T_ rounds: _\u0338_ _T_ \ufffd _L_ ( _y_ \ufffd _t,i_ _, y_ _t_ ) _._ (8.1) _t_ =1 _\u0338_ _R_ _T_ = _\u0338_ _T_ _N_ \ufffd _L_ ( _y_ \ufffd _t_ _, y_ _t_ ) _\u2212_ min _i_ =1 _t_ =1 **8.2** **Prediction with expert advice** **179** ## ? **Image:** [No caption returned] **Image:** [No caption returned] **Image:** [No caption returned] wunderground.com bbc.com weather.com cnn.com algorithm **Figure 8.1** **Image:** [No caption returned] Weather forecast: an example of a prediction problem based on expert advice. This problem arises in a variety of different domains and applications. Figure 8.1 illustrates the problem of predicting the weather using several forecasting sources as experts. **8.2.1** **Mistake bounds and Halving algorithm** Here, we assume that the loss function is the standard zero-one loss used in classification. To analyze the expert advice setting, we first consider the realizable case, that is the setting where at least one of the experts makes",
    "chunk_id": "foundations_machine_learning_174"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "no errors. As such, we discuss the _mistake bound model_, which asks the simple question \u201cHow many mistakes before we learn a particular concept?\u201d Since we are in the realizable case, after some number of rounds _T_, we will learn the concept and no longer make errors in subsequent rounds. For any fixed concept _c_, we define the maximum number of mistakes a learning algorithm _A_ makes as _M_ _A_ ( _c_ ) = max (8.2) _x_ 1 _,...,x_ _T_ _[|]_ [mistakes(] _[A][, c]_ [)] _[|][.]_ Further, for any concept in a concept class C, the maximum number of mistakes a learning algorithm makes is _M_ _A_ (C) = max _c\u2208_ C _[M]_ _[A]_ [(] _[c]_ [)] _[.]_ (8.3) Our goal in this setting is to derive _mistake bounds_, that is, a bound _M_ on _M_ _A_ (C). We will first do this for the Halving algorithm, an elegant and simple algorithm for which we can guarantee surprisingly favorable mistake bounds. At each round, the Halving algorithm makes its prediction by taking the majority vote over all _active_ experts. After any incorrect prediction, it deactivates all experts that gave faulty advice. Initially, all experts are active, and by the time the algorithm has converged to the correct concept, the active set contains only those experts that are consistent with the target concept. The pseudocode for this algorithm is shown in figure 8.2. We also present straightforward mistake bounds in theorems 8.1 and 8.2, where the former deals with finite hypothesis sets and the latter relates mistake bounds to VC-dimension. Note that the hypothesis complexity term in theorem 8.1 is identical to the corresponding complexity term in the PAC model bound of theorem 2.5. **180** **Chapter 8** **On-Line Learning** Halving(H) 1 H 1 _\u2190_ H 2 **for** _t \u2190_ 1 **to** _T_ **do** 3 Receive( _x_ _t_ ) 4 _y_ \ufffd _t_ _\u2190_ MajorityVote(H _t_ _, x_ _t_ ) 5 Receive( _y_ _t_ ) \ufffd 6 **if** ( _y_ _t_ _\u0338_ = _y_ _t_ ) **then** 7 H _t_ +1 _\u2190{c \u2208_ H _t_ : _c_ ( _x_ _t_ ) = _y_ _t_ _}_ 8 **else** H _t_ +1 _\u2190_ H _t_ 9 **return** H _T_ +1 **Figure 8.2** Halving algorithm. **Theorem 8.1** _Let_ H _be a finite hypothesis set. Then_ _M_ Halving (H) _\u2264_ log 2 _|_ H _|._ (8.4) Proof: Since at each round the algorithm makes predictions using majority vote from the active set, at each mistake, the active set is reduced by at least half. Hence, after log 2 _|_ H _|_ mistakes, there can only remain one active hypothesis, and since we are in the realizable case, this hypothesis must coincide with the target concept. **Theorem 8.2** _Let opt_ (H) _be the optimal mistake bound for_ H _. Then,_ VCdim(H) _\u2264_ _opt_ (H) _\u2264_ _M_ Halving (H) _\u2264_ log 2 _|_ H _|._ (8.5) Proof: The second inequality is true by definition and the third inequality holds based on theorem 8.1. To prove the first inequality, we let _d_ = VCdim(H). Then there exists a shattered set of",
    "chunk_id": "foundations_machine_learning_175"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_d_ points, for which we can form a complete binary tree of the mistakes with height _d_, and we can choose labels at each round of learning to ensure that _d_ mistakes are made. Note that this adversarial argument is valid since the on-line setting makes no statistical assumptions about the data. **8.2** **Prediction with expert advice** **181** Weighted-Majority( _N_ ) 1 **for** _i \u2190_ 1 **to** _N_ **do** 2 _w_ 1 _,i_ _\u2190_ 1 3 **for** _t \u2190_ 1 **to** _T_ **do** 4 Receive( _x_ _t_ ) 5 **if** [\ufffd] _i_ : _y_ _t,i_ =1 _[w]_ _[t,i]_ _[ \u2265]_ [\ufffd] _i_ : _y_ _t,i_ =0 _[w]_ _[t,i]_ **[ then]** 6 _y_ \ufffd _t_ _\u2190_ 1 7 **else** \ufffd _y_ _t_ _\u2190_ 0 8 Receive( _y_ _t_ ) \ufffd 9 **if** ( _y_ _t_ _\u0338_ = _y_ _t_ ) **then** 10 **for** _i \u2190_ 1 **to** _N_ **do** 11 **if** ( _y_ _t,i_ _\u0338_ = _y_ _t_ ) **then** 12 _w_ _t_ +1 _,i_ _\u2190_ _\u03b2w_ _t,i_ 13 **else** _w_ _t_ +1 _,i_ _\u2190_ _w_ _t,i_ 14 **return w** _T_ +1 **Figure 8.3** Weighted majority algorithm, _y_ _t_ _, y_ _t,i_ _\u2208{_ 0 _,_ 1 _}_ . **8.2.2** **Weighted majority algorithm** In the previous section, we focused on the realizable setting in which the Halving algorithm simply discarded experts after a single mistake. We now move to the nonrealizable setting and use a more general and less extreme algorithm, the Weighted Majority (WM) algorithm, that _weights_ the importance of experts as a function of their mistake rate. The WM algorithm begins with uniform weights over all _N_ experts. At each round, it generates predictions using a weighted majority vote. After receiving the true label, the algorithm then reduces the weight of each incorrect expert by a factor of _\u03b2 \u2208_ [0 _,_ 1). Note that this algorithm reduces to the Halving algorithm when _\u03b2_ = 0. The pseudocode for the WM algorithm is shown in figure 8.3. Since we are not in the realizable setting, the mistake bounds of theorem 8.1 cannot apply. However, the following theorem presents a bound on the number of mistakes _m_ _T_ made by the WM algorithm after _T \u2265_ 1 rounds of on-line learning as a function of the number of mistakes made by the best expert, that is the expert **182** **Chapter 8** **On-Line Learning** who achieves the smallest number of mistakes for the sequence _y_ 1 _, . . ., y_ _T_ . Let us emphasize that this is the best expert _in hindsight_ . **Theorem 8.3** _Fix \u03b2 \u2208_ (0 _,_ 1) _. Let m_ _T_ _be the number of mistakes made by algorithm_ _WM after T \u2265_ 1 _rounds, and m_ _[\u2217]_ _T_ _[be the number of mistakes made by the best of]_ _the N experts. Then, the following inequality holds:_ log _N_ + _m_ _[\u2217]_ _T_ [log] _\u03b2_ [1] _m_ _T_ _\u2264_ log 1+2 _\u03b2_ _._ (8.6) Proof: To prove this theorem, we first introduce a potential function. We then derive upper and lower bounds for this function, and combine them",
    "chunk_id": "foundations_machine_learning_176"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "to obtain our result. This potential function method is a general proof technique that we will use throughout this chapter. For any _t \u2265_ 1, we define our potential function as _W_ _t_ = [\ufffd] _[N]_ _i_ =1 _[w]_ _[t,i]_ [. Since predic-] tions are generated using weighted majority vote, if the algorithm makes an error at round _t_, this implies that 1 + _\u03b2_ _W_ _t_ +1 _\u2264_ \ufffd1 _/_ 2 + (1 _/_ 2) _\u03b2_ \ufffd _W_ _t_ = \ufffd 2 _W_ _t_ _._ (8.7) \ufffd Since _W_ 1 = _N_ and _m_ _T_ mistakes are made after _T_ rounds, we thus have the following upper bound: 1 + _\u03b2_ _W_ _T_ _\u2264_ \ufffd 2 _m_ _T_ _N._ (8.8) \ufffd Next, since the weights are all non-negative, it is clear that for any expert _i_, _W_ _T_ _\u2265_ _w_ _T,i_ = _\u03b2_ _[m]_ _[T,i]_, where _m_ _T,i_ is the number of mistakes made by the _i_ th expert after _T_ rounds. Applying this lower bound to the best expert and combining it with the upper bound in (8.8) gives us: _\u03b2_ _[m]_ _T_ _[\u2217]_ _\u2264_ _W_ _T_ _\u2264_ 1 + _\u03b2_ \ufffd 2 _m_ _T_ _N_ \ufffd 1 + _\u03b2_ _\u21d2_ _m_ _[\u2217]_ _T_ [log] _[ \u03b2][ \u2264]_ [log] _[ N]_ [ +] _[ m]_ _[T]_ [log] \ufffd 2 \ufffd 2 _\u21d2_ _m_ _T_ log \ufffd 1 + _\u03b2_ _\u2264_ log _N_ + _m_ _[\u2217]_ _T_ [log] [1] \ufffd _\u03b2_ _[,]_ which concludes the proof. Thus, the theorem guarantees a bound of the following form for algorithm WM: _m_ _T_ _\u2264_ _O_ (log _N_ ) + constant _\u00d7 |_ mistakes of best expert _|._ Since the first term varies only logarithmically as a function of _N_, the theorem guarantees that the number of mistakes is roughly a constant times that of the best expert in hindsight. This is a remarkable result, especially because it requires no **8.2** **Prediction with expert advice** **183** assumption about the sequence of points and labels generated. In particular, the sequence could be chosen adversarially. In the realizable case where _m_ _[\u2217]_ _T_ [= 0, the] bound reduces to _m_ _T_ _\u2264_ _O_ (log _N_ ) as for the Halving algorithm. **8.2.3** **Randomized weighted majority algorithm** In spite of the guarantees just discussed, the WM algorithm admits a drawback that affects all deterministic algorithms in the case of the zero-one loss: no deterministic algorithm can achieve a regret _R_ _T_ = _o_ ( _T_ ) over all sequences. Clearly, for any deterministic algorithm _A_ and any _t \u2208_ [ _T_ ], we can adversarially select _y_ _t_ to be 1 if the algorithm predicts 0, and choose it to be 0 otherwise. Thus, _A_ errs at every point of such a sequence and its cumulative mistake is _m_ _T_ = _T_ . Assume for example that _N_ = 2 and that one expert always predicts 0, the other one always 1. The error of the best expert over that sequence (and in fact any sequence of that length) is then at most _m_ _[\u2217]_ _T_ _[\u2264]_",
    "chunk_id": "foundations_machine_learning_177"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[T/]_ [2. Thus, for that sequence, we have] _R_ _T_ = _m_ _T_ _\u2212_ _m_ _[\u2217]_ _T_ _[\u2265]_ _[T/]_ [2] _[,]_ which shows that _R_ _T_ = _o_ ( _T_ ) cannot be achieved in general. Note that this does not contradict the bound proven in the previous section, since for any _\u03b2 \u2208_ (0 _,_ 1), log _\u03b2_ [1] log 1+2 _\u03b2_ _[\u2265]_ [2. As we shall see in the next section, this negative result does not hold] for any loss that is convex with respect to one of its arguments. But for the zero-one loss, this leads us to consider randomized algorithms instead. In the randomized scenario of on-line learning, we assume that a set A = _{_ 1 _, . . ., N_ _}_ of _N_ actions is available. At each round _t \u2208_ [ _T_ ], an on-line algorithm _A_ selects a distribution **p** _t_ over the set of actions, receives a loss vector **l** _t_, whose _i_ th component _l_ _t,i_ _\u2208_ [0 _,_ 1] is the loss associated with action _i_, and incurs the expected loss _L_ _t_ = [\ufffd] _[N]_ _i_ =1 _[p]_ _[t,i]_ _[ l]_ _[t,i]_ [. The total loss incurred by the algorithm over] _[ T]_ [ rounds] is _L_ _T_ = [\ufffd] _[T]_ _t_ =1 _[L]_ _[t]_ [. The total loss associated to action] _[ i]_ [ is] _[ L]_ _[T,i]_ [ =][ \ufffd] _[T]_ _t_ =1 _[l]_ _[t,i]_ [. The] minimal loss of a single action is denoted by _L_ [min] _T_ = min _i\u2208_ A _L_ _T,i_ . The regret _R_ _T_ of the algorithm after _T_ rounds is then typically defined by the difference of the loss of the algorithm and that of the best single action: [11] _R_ _T_ = _L_ _T_ _\u2212L_ [min] _T_ _._ Here, we consider specifically the case of zero-one losses and assume that _l_ _t,i_ _\u2208{_ 0 _,_ 1 _}_ for all _t \u2208_ [ _T_ ] and _i \u2208_ A. 11 Alternative definitions of the regret with comparison classes different from the set of single actions can be considered. **184** **Chapter 8** **On-Line Learning** Randomized-Weighted-Majority ( _N_ ) 1 **for** _i \u2190_ 1 **to** _N_ **do** 2 _w_ 1 _,i_ _\u2190_ 1 3 _p_ 1 _,i_ _\u2190_ 1 _/N_ 4 **for** _t \u2190_ 1 **to** _T_ **do** 5 Receive( **l** _t_ ) 6 **for** _i \u2190_ 1 **to** _N_ **do** 7 **if** ( _l_ _t,i_ = 1) **then** 8 _w_ _t_ +1 _,i_ _\u2190_ _\u03b2w_ _t,i_ 9 **else** _w_ _t_ +1 _,i_ _\u2190_ _w_ _t,i_ 10 _W_ _t_ +1 _\u2190_ [\ufffd] _[N]_ _i_ =1 _[w]_ _[t]_ [+1] _[,i]_ 11 **for** _i \u2190_ 1 **to** _N_ **do** 12 _p_ _t_ +1 _,i_ _\u2190_ _w_ _t_ +1 _,i_ _/W_ _t_ +1 13 **return w** _T_ +1 **Figure 8.4** Randomized weighted majority algorithm. The WM algorithm admits a straightforward randomized version, the randomized weighted majority (RWM) algorithm. The pseudocode of this algorithm is given in figure 8.4. The algorithm updates the weight _w_ _t,i_ of expert _i_ as in the case of the WM algorithm",
    "chunk_id": "foundations_machine_learning_178"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "by multiplying it by _\u03b2_ . The following theorem gives a strong guarantee on the regret _R_ _T_ of the RWM algorithm, showing that it is in _O_ ( _[\u221a]_ _T_ log _N_ ). **Theorem 8.4** _Fix \u03b2 \u2208_ [1 _/_ 2 _,_ 1) _. Then, for any T \u2265_ 1 _, the loss of algorithm RWM on_ _any sequence can be bounded as follows:_ _L_ _T_ _\u2264_ [lo][g] _[ N]_ _T_ _._ (8.9) 1 _\u2212_ _\u03b2_ [+ (2] _[ \u2212]_ _[\u03b2]_ [)] _[L]_ [min] _In particular, for \u03b2_ = max _{_ 1 _/_ 2 _,_ 1 _\u2212_ ~~\ufffd~~ (log _N_ ) _/T_ _}, the loss can be bounded as:_ _L_ _T_ _\u2264L_ [min] _T_ + 2\ufffd _T_ log _N._ (8.10) Proof: As in the proof of theorem 8.3, we derive upper and lower bounds for the potential function _W_ _t_ = [\ufffd] _[N]_ _i_ =1 _[w]_ _[t,i]_ [,] _[ t][ \u2208]_ [[] _[T]_ [], and combine these bounds to obtain] **8.2** **Prediction with expert advice** **185** the result. By definition of the algorithm, for any _t \u2208_ [ _T_ ], _W_ _t_ +1 can be expressed as follows in terms of _W_ _t_ : _W_ _t_ +1 = \ufffd \ufffd _w_ _t,i_ + _\u03b2_ \ufffd _i_ : _l_ _t,i_ =0 _i_ : _l_ _t.i_ \ufffd _w_ _t,i_ = _W_ _t_ + ( _\u03b2 \u2212_ 1) \ufffd _i_ : _l_ _t.i_ =1 _i_ : _l_ _w_ _t,i_ _i_ : _l_ _t,i_ =1 = _W_ _t_ + ( _\u03b2 \u2212_ 1) _W_ _t_ \ufffd _p_ _t,i_ _i_ : _l_ _t,i_ =1 = _W_ _t_ + ( _\u03b2 \u2212_ 1) _W_ _t_ _L_ _t_ = _W_ _t_ (1 _\u2212_ (1 _\u2212_ _\u03b2_ ) _L_ _t_ ) _._ Thus, since _W_ 1 = _N_, it follows that _W_ _T_ +1 = _N_ [\ufffd] _[T]_ _t_ =1 [(1] _[ \u2212]_ [(1] _[ \u2212]_ _[\u03b2]_ [)] _[L]_ _[t]_ [). On the other] hand, the following lower bound clearly holds: _W_ _T_ +1 _\u2265_ max _i\u2208_ [ _N_ ] _w_ _T_ +1 _,i_ = _\u03b2_ _[L]_ _T_ [min] . This leads to the following inequality and series of derivations after taking the log and using the inequalities log(1 _\u2212_ _x_ ) _\u2264\u2212x_ valid for all _x <_ 1, and _\u2212_ log(1 _\u2212_ _x_ ) _\u2264_ _x_ + _x_ [2] valid for all _x \u2208_ [0 _,_ 1 _/_ 2]: _T_ \ufffd log(1 _\u2212_ (1 _\u2212_ _\u03b2_ ) _L_ _t_ ) _t_ =1 _\u03b2_ _[L]_ _T_ [min] _\u2264_ _N_ _T_ \ufffd(1 _\u2212_ (1 _\u2212_ _\u03b2_ ) _L_ _t_ ) = _\u21d2L_ [min] _T_ log _\u03b2 \u2264_ log _N_ + _t_ =1 = _\u21d2L_ [min] _T_ log _\u03b2 \u2264_ log _N \u2212_ (1 _\u2212_ _\u03b2_ ) _T_ \ufffd _L_ _t_ _t_ =1 = _\u21d2L_ [min] _T_ log _\u03b2 \u2264_ log _N \u2212_ (1 _\u2212_ _\u03b2_ ) _L_ _T_ [lo][g] _[ N]_ _[\u03b2]_ 1 _\u2212_ _\u03b2_ _[\u2212]_ 1 [lo] _\u2212_ [g] _\u03b2_ = _\u21d2L_ _T_ _\u2264_ [lo][g] _[ N]_ 1 _\u2212_ _\u03b2_ _[L]_ _T_ [min] [lo][g] _[ N]_ 1 _\u2212_ _\u03b2_ _[\u2212]_ [lo][g(][1] _[ \u2212]_ 1 _\u2212_ [(][1] _\u03b2_ _[ \u2212]_",
    "chunk_id": "foundations_machine_learning_179"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u03b2]_ [))] = _\u21d2L_ _T_ _\u2264_ [lo][g] _[ N]_ 1 _\u2212_ _\u03b2_ _L_ [min] _T_ = _\u21d2L_ _T_ _\u2264_ [lo] 1 _\u2212_ [g] _[ N]_ _\u03b2_ [+ (2] _[ \u2212]_ _[\u03b2]_ [)] _[L]_ _T_ [min] _._ This shows the first statement. Since _L_ [min] _T_ _\u2264_ _T_, this also implies _L_ _T_ _\u2264_ [lo][g] _[ N]_ _T_ _._ (8.11) 1 _\u2212_ _\u03b2_ [+ (1] _[ \u2212]_ _[\u03b2]_ [)] _[T]_ [ +] _[ L]_ [min] Differentiating the upper bound with respect to _\u03b2_ and setting it to zero gives (1lo _\u2212_ g _\u03b2 N_ ) [2] _[ \u2212]_ _[T]_ [ = 0, that is] _[ \u03b2]_ [ = 1] _[ \u2212]_ \ufffd(log _N_ ) _/T <_ 1. Thus, if 1 _\u2212_ \ufffd(log _N_ ) _/T \u2265_ 1 _/_ 2, (log _N_ ) _/T <_ 1. Thus, if 1 _\u2212_ \ufffd (1 _\u2212_ g _\u03b2_ ) [2] _[ \u2212]_ _[T]_ [ = 0, that is] _[ \u03b2]_ [ = 1] _[ \u2212]_ \ufffd(log _N_ ) _/T <_ 1. Thus, if 1 _\u2212_ \ufffd(log _N_ ) _/T \u2265_ 1 _/_ 2, _\u03b2_ 0 = 1 _\u2212_ \ufffd(log _N_ ) _/T_ is the minimizing value of _\u03b2_, otherwise the boundary value _\u03b2_ 0 = 1 _\u2212_ \ufffd(log _N_ ) _/T_ is the minimizing value of _\u03b2_, otherwise the boundary value _\u03b2_ 0 = 1 _/_ 2 is the optimal value. The second statement follows by replacing _\u03b2_ with _\u03b2_ 0 in (8.11). The bound (8.10) assumes that the algorithm additionally receives as a parameter the number of rounds _T_ . As we shall see in the next section, however, there exists a general _doubling trick_ that can be used to relax this requirement at the price of a **186** **Chapter 8** **On-Line Learning** small constant factor increase. Inequality 8.10 can be written directly in terms of the regret _R_ _T_ of the RWM algorithm: _R_ _T_ _\u2264_ 2\ufffd _T_ log _N._ (8.12) Thus, for _N_ constant, the regret verifies _R_ _T_ = _O_ ( _\u221a_ Thus, for _N_ constant, the regret verifies _R_ _T_ = _O_ ( _\u221aT_ ) and the _average regret_ or _regret per round R_ _T_ _/T_ decreases as _O_ (1 _/\u221aT_ ). These results are optimal, as shown by the following theorem. **Theorem 8.5** _Let N_ = 2 _. There exists a stochastic sequence of losses for which the_ _regret of any on-line learning algorithm verifies_ E[ _R_ _T_ ] _\u2265_ ~~\ufffd~~ _T/_ 8 _._ Proof: For any _t \u2208_ [ _T_ ], let the vector of losses **l** _t_ take the values **l** 01 = (0 _,_ 1) _[\u22a4]_ and **l** 10 = (1 _,_ 0) _[\u22a4]_ with equal probability. Then, the expected loss of any randomized algorithm _A_ is **Theorem 8.5** _Let N_ = 2 _. There exists a stochastic sequence of losses for which the_ _regret of any on-line learning algorithm verifies_ E[ _R_ _T_ ] _\u2265_ ~~\ufffd~~ _T/_ 8 _._ _T_ \ufffd _t_ =1 _T_ E[ _L_ _T_ ] = E \ufffd \ufffd **p** _t_ _\u00b7_ **l** _t_ \ufffd = _t_ =1 _T_ \ufffd **p** _t_ _\u00b7_ E[",
    "chunk_id": "foundations_machine_learning_180"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**l** _t_ ] = _t_ =1 1 2 _[p]_ _[t,]_ [1] [ + 1] 2 [(1] _[ \u2212]_ _[p]_ _[t,]_ [1] [) =] _[ T/]_ [2] _[,]_ where we denoted by **p** _t_ the distribution selected by _A_ at round _t_ . By definition, _L_ [min] _T_ can be written as follows: _L_ [min] _T_ = min _{L_ _T,_ 1 _, L_ _T,_ 2 _}_ = [1] 2 [(] _[L]_ _[T,]_ [1] [ +] _[ L]_ _[T,]_ [2] _[ \u2212|L]_ _[T,]_ [1] _[ \u2212L]_ _[T,]_ [2] _[|]_ [) =] _[ T/]_ [2] _[ \u2212|L]_ _[T,]_ [1] _[ \u2212]_ _[T/]_ [2] _[|][,]_ using the fact that _L_ _T,_ 1 + _L_ _T,_ 2 = _T_ . Thus, the expected regret of _A_ is E[ _R_ _T_ ] = E[ _L_ _T_ ] _\u2212_ E[ _L_ [min] _T_ ] = E[ _|L_ _T,_ 1 _\u2212_ _T/_ 2 _|_ ] _._ Let _\u03c3_ _t_, _t \u2208_ [ _T_ ], denote Rademacher variables taking values in _{\u2212_ 1 _,_ +1 _}_, then _L_ _T,_ 1 can be rewritten as _L_ _T,_ 1 = [\ufffd] _[T]_ _t_ =1 1+2 _\u03c3_ _t_ = _T/_ 2 + [1] 2 \ufffd _Tt_ =1 _[\u03c3]_ _[t]_ [.] Thus, introducing can be rewritten as _L_ _T,_ 1 = [\ufffd] _[T]_ _t_ =1 1+2 _\u03c3_ _t_ = _T/_ 2 + [1] 2 \ufffd _Tt_ =1 _[\u03c3]_ _[t]_ [.] Thus, introducing scalars _x_ _t_ = 1 _/_ 2, _t \u2208_ [ _T_ ], by the Khintchine-Kahane inequality, (D.24) we have: 2 _\u03c3_ _t_ = _T/_ 2 + [1] 2 ~~\ufffd~~ \ufffd \ufffd 2 \ufffd [1] \ufffd _x_ [2] _t_ [=] ~~\ufffd~~ _t_ =1 E[ _R_ _T_ ] = E _|_ \ufffd _T_ \ufffd _\u03c3_ _t_ _x_ _t_ _|_ \ufffd _\u2265_ _t_ =1 _T_ \ufffd _T/_ 8 _,_ which concludes the proof. More generally, for _T \u2265_ _N_, a lower bound of _R_ _T_ = \u2126( _[\u221a]_ _T_ log _N_ ) can be proven for the regret of any algorithm. **8.2.4** **Exponential weighted average algorithm** The WM algorithm can be extended to other loss functions _L_ taking values in [0 _,_ 1]. The Exponential Weighted Average algorithm presented here can be viewed as that extension for the case where _L_ is convex in its first argument. Note that this algorithm is deterministic and yet, as we shall see, admits a very favorable regret **8.2** **Prediction with expert advice** **187** Exponential-Weighted-Average ( _N_ ) 1 **for** _i \u2190_ 1 **to** _N_ **do** 2 _w_ 1 _,i_ _\u2190_ 1 3 **for** _t \u2190_ 1 **to** _T_ **do** 4 Receive( _x_ _t_ ) 5 _y_ \ufffd _t_ _\u2190_ \ufffd _Ni_ =1 _N_ _[w]_ _[t][,][i]_ _[y]_ _[t][,][i]_ \ufffd _i_ =1 _[w]_ _[t,i]_ 6 Receive( _y_ _t_ ) 7 **for** _i \u2190_ 1 **to** _N_ **do** 8 _w_ _t_ +1 _,i_ _\u2190_ _w_ _t,i_ _e_ _[\u2212][\u03b7L]_ [(] _[y]_ [\ufffd] _[t,i]_ _[,y]_ _[t]_ [)] 9 **return w** _T_ +1 **Figure 8.5** Exponential weighted average, _L_ ( _y_ \ufffd _t,i_ _, y_ _t_ ) _\u2208_ [0 _,_ 1]. guarantee. Figure 8.5 gives its pseudocode. At round _t \u2208_ [ _T_ ], the",
    "chunk_id": "foundations_machine_learning_181"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "algorithm\u2019s prediction is _N_ \ufffd \ufffd _i_ =1 _[w]_ _[t][,][i]_ _[y]_ _[t][,][i]_ _y_ _t_ = \ufffd _Ni_ =1 _[w]_ _[t,i]_ _,_ (8.13) where _y_ _t,i_ is the prediction by expert _i_ and _w_ _t,i_ the weight assigned by the algorithm to that expert. Initially, all weights are set to one. The algorithm then updates the weights at the end of round _t_ according to the following rule: _w_ _t_ +1 _,i_ _\u2190_ _w_ _t,i_ _e_ _[\u2212][\u03b7L]_ [(] _[y]_ [\ufffd] _[t,i]_ _[,y]_ _[t]_ [)] = _e_ _[\u2212][\u03b7L]_ _[t,i]_ _,_ (8.14) where _L_ _t,i_ is the total loss incurred by expert _i_ after _t_ rounds. Note that this algorithm, as well as the others presented in this chapter, are simple, since they do not require keeping track of the losses incurred by each expert at all previous rounds but only of their cumulative performance. Furthermore, this property is also computationally advantageous. The following theorem presents a regret bound for this algorithm. **Theorem 8.6** _Assume that the loss function L is convex in its first argument and_ _takes values in_ [0 _,_ 1] _. Then, for any \u03b7 >_ 0 _and any sequence y_ 1 _, . . ., y_ _T_ _\u2208_ Y _, the_ _regret of the Exponential Weighted Average algorithm after T rounds satisfies_ _R_ _T_ _\u2264_ [lo][g] _[ N]_ (8.15) 8 _[.]_ _[ N]_ + _[\u03b7][T]_ _\u03b7_ 8 **188** **Chapter 8** **On-Line Learning** _In particular, for \u03b7_ = \ufffd 8 log _N/T_ _, the regret is bounded as_ _R_ _T_ _\u2264_ \ufffd( _T/_ 2) log _N._ (8.16) Proof: We apply the same potential function analysis as in previous proofs but using as potential \u03a6 _t_ = log [\ufffd] _[N]_ _i_ =1 _[w]_ _[t,i]_ [,] _[ t][ \u2208]_ [[] _[T]_ []. Let] **[ p]** _[t]_ [ denote the distribution over] _{_ the difference of two consecutive potential values:1 _, . . ., N_ _}_ with _p_ _t,i_ = \ufffd _Ni_ _w_ =1 _t,_ _[w]_ _i_ _[t,i]_ [ . To derive an upper bound on \u03a6] _[t]_ [, we first examine] _N_ \u03a6 _t_ +1 _\u2212_ \u03a6 _t_ = log \ufffd _i_ =1 _[w]_ _[t][,]_ _N_ _[i]_ _[ e]_ _[\u2212][\u03b7L]_ [(] _[y]_ [\ufffd] _[t,i]_ _[,y]_ _[t]_ [)] = log \ufffd E \ufffd _,_ \ufffd _i_ =1 _[w]_ _[t,i]_ **p** _t_ [[] _[e]_ _[\u03b7X]_ []] \ufffd with _X_ = _\u2212L_ ( _y_ _t,i_ _, y_ _t_ ) _\u2208_ [ _\u2212_ 1 _,_ 0]. To upper bound the expression appearing in the right-hand side, we apply Hoeffding\u2019s lemma (lemma D.1) to the centered random variable _X \u2212_ E **p** _t_ [ _X_ ], then Jensen\u2019s inequality (theorem B.20) using the convexity of _L_ with respect to its first argument: \u03a6 _t_ +1 _\u2212_ \u03a6 _t_ = log \ufffd E **p** _t_ \ufffd _e_ _[\u03b7]_ [(] _[X][\u2212]_ [E][[] _[X]_ [])+] _[\u03b7]_ [ E][[] _[X]_ []] [\ufffd\ufffd] _\u2264_ _[\u03b7]_ [2] (Hoeffding\u2019s lemma) 8 _[\u2212]_ _[\u03b7]_ [ E] **p** _t_ [[] _[L]_ [(] _[y]_ [\ufffd] _[t,i]_ _[, y]_ _[t]_ [)]] [2] _[\u03b7]_ [2] 8 [+] _[ \u03b7]_ [ E] **p** _t_ [[] _[X]_ [] =] 8 _\u2264\u2212\u03b7L_ \ufffd E",
    "chunk_id": "foundations_machine_learning_182"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd + _[\u03b7]_ [2] (convexity of first arg. of _L_ ) **p** _t_ [[] _[y]_ [\ufffd] _[t,i]_ []] _[, y]_ _[t]_ 8 \ufffd = _\u2212\u03b7L_ ( _y_ _t_ _, y_ _t_ ) + _[\u03b7]_ [2] 8 _[.]_ Summing up these inequalities yields the following upper bound: _T_ \ufffd (8.17) 8 _[.]_ \u03a6 _T_ +1 _\u2212_ \u03a6 1 _\u2264\u2212\u03b7_ \ufffd _L_ ( _y_ \ufffd _t_ _, y_ _t_ ) + _[\u03b7]_ [2] 8 _[T]_ _t_ =1 We obtain a lower bound for the same quantity as follows: _N_ _N_ _N_ \u03a6 _T_ +1 _\u2212_ \u03a6 1 = log \ufffd _e_ _[\u2212][\u03b7L]_ _[T,i]_ _\u2212_ log _N \u2265_ log max _i_ =1 _[e]_ _[\u2212][\u03b7L]_ _[T,i]_ _[\u2212]_ [log] _[ N]_ [ =] _[ \u2212][\u03b7]_ min _i_ =1 _[L]_ _[T,i]_ _[\u2212]_ [log] _[ N.]_ _i_ =1 Combining the upper and lower bounds yields: _N_ _\u2212_ _\u03b7_ min _i_ =1 _[L]_ _[T,i]_ _[ \u2212]_ [log] _[ N][ \u2264\u2212][\u03b7]_ \ufffd _L_ ( _y_ \ufffd _t_ _, y_ _t_ ) + _[\u03b7]_ [2] 8 _[T]_ _t_ =1 _T_ \ufffd 8 _[ N]_ + _[\u03b7][T]_ _\u03b7_ 8 8 _[,]_ = _\u21d2_ _T_ \ufffd _N_ \ufffd _t_ =1 _L_ ( _y_ \ufffd _t_ _, y_ _t_ ) _\u2212_ min _i_ =1 _[L]_ _[T,i]_ _[ \u2264]_ [lo][g] _\u03b7_ _[ N]_ and concludes the proof. **8.2** **Prediction with expert advice** **189** The optimal choice of _\u03b7_ in theorem 8.6 requires knowledge of the horizon _T_, which is an apparent disadvantage of this analysis. However, we can use a standard _doubling_ _trick_ to eliminate this requirement, at the price of a small constant factor. This consists of dividing time into periods [2 _[k]_ _,_ 2 _[k]_ [+1] _\u2212_ 1] of length 2 _[k]_ with _k_ = 0 _, . . ., n_ and _T \u2265_ 2 _[n]_ _\u2212_ 1, and then choosing _\u03b7_ _k_ = ~~\ufffd~~ 8 log _N_ _T \u2265_ 2 _[n]_ _\u2212_ 1, and then choosing _\u03b7_ _k_ = 2g _[k]_ in each period. The following theorem presents a regret bound when using the doubling trick to select _\u03b7_ . A more general method consists of interpreting _\u03b7_ as a function of time, i.e., _\u03b7_ _t_ = \ufffd(8 log _N_ ) _/t_, method consists of interpreting _\u03b7_ as a function of time, i.e., _\u03b7_ _t_ = \ufffd(8 log _N_ ) _/t_, which can lead to a further constant factor improvement over the regret bound of the following theorem. **Theorem 8.7** _Assume that the loss function L is convex in its first argument and_ _takes values in_ [0 _,_ 1] _. Then, for any T \u2265_ 1 _and any sequence y_ 1 _, . . ., y_ _T_ _\u2208_ Y _, the_ _regret of the Exponential Weighted Average algorithm after T rounds is bounded as_ _follows:_ ( _T/_ 2) log _N_ + \ufffdlog _N/_ 2 _._ (8.18) _R_ _T_ _\u2264_ 2 _\u221a_ 2 _\u2212_ _\u221a_ 2 _\u2212_ 1 \ufffd Proof: Let _T \u2265_ 1 and let I _k_ = [2 _[k]_ _,_ 2 _[k]_ [+1] _\u2212_ 1], for _k \u2208_ [0 _, n_ ], with _n_ = _\u230a_ log( _T_ + 1) _\u230b_ .",
    "chunk_id": "foundations_machine_learning_183"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Let _L_ I _k_ denote the loss incurred in the interval I _k_ . By theorem 8.6 (8.16), for any _k \u2208{_ 0 _, . . ., n}_, we have _N_ _L_ I _k_ _\u2212_ min _i_ =1 _[L]_ [I] _[k]_ _[,i]_ _[ \u2264]_ \ufffd2 _[k]_ _/_ 2 log _N._ (8.19) Thus, we can bound the total loss incurred by the algorithm after _T_ rounds as: _n_ \ufffd _k_ =0 _n_ \ufffd _k_ =0 _L_ _T_ = _n_ \ufffd _L_ I _k_ _\u2264_ _k_ =0 _N_ min _i_ =1 _[L]_ [I] _[k]_ _[,i]_ [ +] ~~\ufffd~~ 2 _[k]_ (log _N_ ) _/_ 2 _N_ _\u2264_ min ~~\ufffd~~ (log _N_ ) _/_ 2 _\u00b7_ _i_ =1 _[L]_ _[T,i]_ [ +] _n_ _k_ \ufffd 2 2 _,_ (8.20) _k_ =0 where the second inequality follows from the super-additivity of min, that is min _i_ _X_ _i_ + min _i_ _Y_ _i_ _\u2264_ min _i_ ( _X_ _i_ + _Y_ _i_ ) for any sequences ( _X_ _i_ ) _i_ and ( _Y_ _i_ ) _i_, which implies \ufffd _nk_ =0 [min] _i_ _[N]_ =1 _[L]_ [I] _k_ _[,i]_ _[\u2264]_ [min] _[N]_ _i_ =1 \ufffd _nk_ =0 _[L]_ [I] _k_ _[,i]_ [. The geometric sum appearing in the right-] hand side of (8.20) can be expressed as follows: _n_ _k_ \ufffd 2 2 = [2] [(] _[n]_ [+1)] _[/]_ [2] _[ \u2212]_ [1] _k_ =0 _\u221a_ 2 _\u2212_ 1 _n_ \ufffd _\u2264_ 2 _\u2212_ 1 _T_ + 1 _\u2212_ 1 _\u221a_ 2 _\u2212_ 1 2( _\u221a_ _T_ + 1) _\u2212_ 1 _\u221a_ 2 _\u2212_ 1 _\u2264_ 2 _\u2212_ 1 _\u221a_ 2 _\u221a_ _\u221a_ 2 _\u221a_ _\u221a_ = 2 _\u2212_ 1 _\u221a_ 2 _T_ _\u221a_ 2 _\u2212_ 2 _\u2212_ 1 [+ 1] _[.]_ Plugging back into (8.20) and rearranging terms yields (8.18). The _O_ ( _\u221aT_ ) dependency on _T_ presented in this bound cannot be improved for general loss functions. **190** **Chapter 8** **On-Line Learning** Perceptron( **w** 0 ) 1 **w** 1 _\u2190_ **w** 0 _\u25b7_ typically **w** 0 = **0** 2 **for** _t \u2190_ 1 **to** _T_ **do** 3 Receive( **x** _t_ ) 4 _y_ \ufffd _t_ _\u2190_ sgn( **w** _t_ _\u00b7_ **x** _t_ ) 5 Receive( _y_ _t_ ) \ufffd 6 **if** ( _y_ _t_ _\u0338_ = _y_ _t_ ) **then** 7 **w** _t_ +1 _\u2190_ **w** _t_ + _y_ _t_ **x** _t_ _\u25b7_ more generally _\u03b7y_ _t_ **x** _t_ _, \u03b7 >_ 0 _._ 8 **else w** _t_ +1 _\u2190_ **w** _t_ 9 **return w** _T_ +1 **Figure 8.6** Perceptron algorithm. **8.3** **Linear classification** This section presents two well-known on-line learning algorithms for linear classification: the Perceptron and Winnow algorithms. **8.3.1** **Perceptron algorithm** The Perceptron algorithm is one of the earliest machine learning algorithms. It is an on-line linear classification algorithm. Thus, it learns a decision function based on a hyperplane by processing training points one at a time. Figure 8.6 gives its pseudocode. The algorithm maintains a weight vector **w** _t_ _\u2208_ R _[N]_ defining the hyperplane learned, starting with an arbitrary vector **w** 0 . At each round _t \u2208_",
    "chunk_id": "foundations_machine_learning_184"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[ _T_ ], it predicts the label of the point **x** _t_ _\u2208_ R _[N]_ received, using the current vector **w** _t_ (line 4). When the prediction made does not match the correct label (lines 6-7), it updates **w** _t_ by adding _y_ _t_ **x** _t_ . More generally, when a learning rate _\u03b7 >_ 0 is used, the vector added is _\u03b7y_ _t_ **x** _t_ . This update can be partially motivated by examining the inner product of the current weight vector with _y_ _t_ **x** _t_, whose sign determines the classification of **x** _t_ . Just before an update, **x** _t_ is misclassified and thus _y_ _t_ **w** _t_ _\u00b7_ **x** _t_ is negative; afterward, _y_ _t_ **w** _t_ +1 _\u00b7_ **x** _t_ = _y_ _t_ **w** _t_ _\u00b7_ **x** _t_ + _\u03b7\u2225_ **x** _t_ _\u2225_ [2], thus, the update corrects the weight vector in the direction of making the inner product _y_ _t_ **w** _t_ _\u00b7_ **x** _t_ positive by augmenting it with the quantity _\u03b7\u2225_ **x** _t_ _\u2225_ [2] _>_ 0. **8.3** **Linear classification** **191** **Image:** [No caption returned] **Figure 8.7** An example path followed by the iterative stochastic gradient descent technique. Each inner contour indicates a region of lower elevation. The Perceptron algorithm can be shown in fact to seek a weight vector **w** minimizing an objective function _F_ precisely based on the quantities ( _\u2212y_ _t_ **w** _\u00b7_ **x** _t_ ), _t \u2208_ [ _T_ ]. Since ( _\u2212y_ _t_ **w** _\u00b7_ **x** _t_ ) is positive when **x** _t_ is misclassified by **w**, _F_ is defined for all **w** _\u2208_ R _[N]_ by _F_ ( **w** ) = [1] _T_ _T_ \ufffd _t_ =1 max \ufffd0 _, \u2212y_ _t_ ( **w** _\u00b7_ **x** _t_ )\ufffd = **x** _\u223c_ E D [\ufffd] [ _F_ [\ufffd] ( **w** _,_ **x** )] _,_ (8.21) where _F_ [\ufffd] ( **w** _,_ **x** ) = max \ufffd0 _, \u2212f_ ( **x** )( **w** _\u00b7_ **x** )\ufffd with _f_ ( **x** ) denoting the label of **x**, and D [\ufffd] is the empirical distribution associated with the sample ( **x** 1 _, . . .,_ **x** _T_ ). For any _t \u2208_ [ _T_ ], **w** _\ufffd\u2192\u2212y_ _t_ ( **w** _\u00b7_ **x** _t_ ) is linear and thus convex. Since the max operator preserves convexity, this shows that _F_ is convex. However, _F_ is not differentiable. Nevertheless, the Perceptron algorithm coincides with the application of the _stochastic_ _subgradient descent_ technique to _F_ . The stochastic (or on-line) subgradient descent technique examines one point **x** _t_ at a time. Note, the function _F_ [\ufffd] ( _\u00b7,_ **x** _t_ ) is non-differentiable for any **w** _t_ where **w** _t_ _\u00b7_ **x** _t_ = 0. In such a case any subgradient of _F_ [\ufffd], i.e. any vector in the convex hull of **0** and _\u2212y_ _t_ **x** _t_, may be used for the update step (see B.4.1). Choosing the subgradient _\u2212y_ _t_ **x** _t_, we arrive at the following general update for each point **x** _t_ : **w** _t_ +1 _\u2190_ **w** _t_ _\u2212_",
    "chunk_id": "foundations_machine_learning_185"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u03b7\u2207_ **w** _F_ [\ufffd] ( **w** _t_ _,_ **x** _t_ ) if **w** _t_ _\u00b7_ **x** _t_ _\u0338_ = 0 (8.22) \ufffd **w** _t_ + _\u03b7y_ _t_ **x** _t_ otherwise _,_ where _\u03b7 >_ 0 is a learning rate parameter. Figure 8.7 illustrates an example path the gradient descent follows. In the specific case we are considering, **w** _\ufffd\u2192_ _F_ [\ufffd] ( **w** _,_ **x** _t_ ) is differentiable at any **w** such that _y_ _t_ ( **w** _\u00b7_ **x** _t_ ) _\u0338_ = 0 with _\u2207_ **w** _F_ [\ufffd] ( **w** _,_ **x** _t_ ) = _\u2212y_ **x** _t_ if _y_ _t_ ( **w** _\u00b7_ **x** _t_ ) _<_ 0 and _\u2207_ **w** _F_ [\ufffd] ( **w** _,_ **x** _t_ ) = 0 if _y_ _t_ ( **w** _\u00b7_ **x** _t_ ) _>_ 0. Thus, the stochastic gradient **192** **Chapter 8** **On-Line Learning** descent update becomes **w** _t_ +1 _\u2190_ **w** _t_ + _\u03b7y_ _t_ **x** _t_ if _y_ _t_ ( **w** _t_ _\u00b7_ **x** _t_ ) _\u2264_ 0; (8.23) \ufffd **w** _t_ if _y_ _t_ ( **w** _t_ _\u00b7_ **x** _t_ ) _>_ 0 _,_ which coincides exactly with the update of the Perceptron algorithm. The following theorem gives a margin-based upper bound on the number of mistakes or updates made by the Perceptron algorithm when processing a sequence of _T_ points that can be linearly separated by a hyperplane with margin _\u03c1 >_ 0. **Theorem 8.8** _Let_ **x** 1 _, . . .,_ **x** _T_ _\u2208_ R _[N]_ _be a sequence of T points with \u2225_ **x** _t_ _\u2225\u2264_ _r for all_ _t \u2208_ [ _T_ ] _, for some r >_ 0 _. Assume that there exist \u03c1 >_ 0 _and_ **v** _\u2208_ R _[N]_ _such that_ _for all t \u2208_ [ _T_ ] _, \u03c1 \u2264_ _[y]_ _[t]_ [(] _\u2225_ **[v]** **v** _[\u00b7]_ _\u2225_ **[x]** _[t]_ [)] _. Then, the number of updates made by the Perceptron_ _algorithm when processing_ **x** 1 _, . . .,_ **x** _T_ _is bounded by r_ [2] _/\u03c1_ [2] _._ Proof: Let I be the subset of the _T_ rounds at which there is an update, and let _M_ be the total number of updates, i.e., _|_ I _|_ = _M_ . Summing up the assumption inequalities yields: [\ufffd] _M\u03c1 \u2264_ **[v]** _[ \u00b7]_ \ufffd _y_ _t_ **x** _t_ \ufffd\ufffd\ufffd (Cauchy-Schwarz inequality ) _t\u2208_ I _t\u2208_ I _[y]_ _[t]_ **[x]** _[t]_ _\u2264_ _\u2225_ **v** _\u2225_ \ufffd\ufffd\ufffd \ufffd = \ufffd\ufffd\ufffd \ufffd( **w** _t_ +1 _\u2212_ **w** _t_ )\ufffd\ufffd\ufffd (definition of updates) _t\u2208_ I = _\u2225_ **w** _T_ +1 _\u2225_ (telescoping sum, **w** 0 = 0) = _\u2225_ **w** _t_ +1 _\u2225_ [2] _\u2212\u2225_ **w** _t_ _\u2225_ [2] (telescoping sum, **w** 0 = 0) \ufffd\ufffd _t\u2208_ I = _\u2225_ **w** _t_ + _y_ _t_ **x** _t_ _\u2225_ [2] _\u2212\u2225_ **w** _t_ _\u2225_ [2] (definition of updates) \ufffd\ufffd _t\u2208_ I ~~\ufffd~~ = \ufffd\ufffd \ufffd \ufffd 2 _y_ _t_ **w** _t_ _\u00b7_ **x** _t_ _t\u2208_ I ~~\ufffd~~ _\u2264_ ~~\ufffd~~ \ufffd 0 ~~\ufffd~~ + _\u2225_ **x** _t_ _\u2225_ [2] _Mr_ [2] _._ _\u2264_ _\u2225_ **x** _t_",
    "chunk_id": "foundations_machine_learning_186"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2225_ [2] _\u2264_ _\u221a_ \ufffd\ufffd _t\u2208_ I Comparing the left- and right-hand sides gives _\u221a_ _M \u2264_ _r/\u03c1_, that is, _M \u2264_ _r_ [2] _/\u03c1_ [2] . \u25a1 **8.3** **Linear classification** **193** By definition of the algorithm, the weight vector **w** _T_ after processing _T_ points is a linear combination of the vectors **x** _t_ at which an update was made: **w** _T_ = [\ufffd] _t\u2208_ I _[y]_ _[t]_ **[x]** _[t]_ [.] Thus, as in the case of SVMs, these vectors can be referred to as _support vectors_ for the Perceptron algorithm. The bound of theorem 8.8 is remarkable, since it depends only on the normalized margin _\u03c1/r_ and not on the dimension _N_ of the space. This bound can be shown to be tight, that is the number of updates can be equal to _r_ [2] _/\u03c1_ [2] in some instances (see exercise 8.3 to show the upper bound is tight). The theorem required no assumption about the sequence of points **x** 1 _, . . .,_ **x** _T_ . A standard setting for the application of the Perceptron algorithm is one where a finite sample _S_ of size _m < T_ is available and where the algorithm makes multiple passes over these _m_ points. The result of the theorem implies that when _S_ is linearly separable, the Perceptron algorithm converges after a finite number of updates and thus passes. For a small margin _\u03c1_, the convergence of the algorithm can be quite slow, however. In fact, for some samples, regardless of the order in which the points in _S_ are processed, the number of updates made by the algorithm is in \u2126(2 _[N]_ ) (see exercise 8.1). Of course, if _S_ is not linearly separable, the Perceptron algorithm does not converge. In practice, it is stopped after some number of passes over _S_ . There are many variants of the standard Perceptron algorithm which are used in practice and have been theoretically analyzed. One notable example is the _voted_ _Perceptron algorithm_, which predicts according to the rule sgn \ufffd( [\ufffd] _t\u2208_ I _[c]_ _[t]_ **[w]** _[t]_ [)] _[ \u00b7]_ **[ x]** \ufffd, where _c_ _t_ is a weight proportional to the number of iterations that **w** _t_ survives, i.e., the number of iterations between **w** _t_ and **w** _t_ +1 . For the following theorem, we consider the case where the Perceptron algorithm is trained via multiple passes till convergence over a finite sample that is linearly separable. In view of theorem 8.8, convergence occurs after a finite number of updates. For a linearly separable sample _S_, we denote by _r_ _S_ the radius of the smallest origin-centered sphere containing all points in _S_ and by _\u03c1_ _S_ the largest margin of a separating hyperplane for _S_ . We also denote by _M_ ( _S_ ) the number of updates made by the algorithm after training over _S_ . **Theorem 8.9** _Assume that the data is linearly separable. Let h_ _S_ _be the hypothesis_ _returned by the Perceptron algorithm after training over a sample S of size m_ _drawn according",
    "chunk_id": "foundations_machine_learning_187"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "to some distribution_ D _. Then, the expected error of h_ _S_ _is bounded_ _as follows:_ _._ \ufffd E E _S\u223c_ D _[m]_ [[] _[R]_ [(] _[h]_ _[S]_ [)]] _[ \u2264]_ _S\u223c_ D _[m]_ [+1] min \ufffd _M_ ( _S_ ) _, r_ _S_ [2] _[/][\u03c1]_ [2] _S_ \ufffd \ufffd _m_ + 1 _m_ + 1 Proof: Let _S_ be a linearly separable sample of size _m_ + 1 drawn i.i.d. according to D and let **x** be a point in _S_ . If _h_ _S\u2212{_ **x** _}_ misclassifies **x**, then **x** must be a support vector for _h_ _S_ . Thus, the leave-one-out error of the Perceptron algorithm on sample **194** **Chapter 8** **On-Line Learning** _S_ is at most _[M]_ _m_ [(] +1 _[S]_ [)] [. The result then follows lemma 5.3, which relates the expected] leave-one-out error to the expected error, along with the upper bound on _M_ ( _S_ ) given by theorem 8.8. This result can be compared with a similar one given for the SVM algorithm (with no offset) in the following theorem, which is an extension of theorem 5.4. We denote by _N_ SV ( _S_ ) the number of support vectors that define the hypothesis _h_ _S_ returned by SVMs when trained on a sample _S_ . **Theorem 8.10** _Assume that the data is linearly separable. Let h_ _S_ _be the hypothesis_ _returned by SVMs used with no offset (b_ = 0 _) after training over a sample S of_ _size m drawn according to some distribution_ D _. Then, the expected error of h_ _S_ _is_ _bounded as follows:_ E E _S\u223c_ D _[m]_ [[] _[R]_ [(] _[h]_ _[S]_ [)]] _[ \u2264]_ _S\u223c_ D _[m]_ [+1] min \ufffd _N_ _SV_ ( _S_ ) _, r_ _S_ [2] _[/][\u03c1]_ [2] _S_ \ufffd \ufffd _m_ + 1 _._ \ufffd Proof: The fact that the expected error can be upper bounded by the average fraction of support vectors ( _N_ SV ( _S_ ) _/_ ( _m_ + 1)) was already shown by theorem 5.4. Thus, it suffices to show that it is also upper bounded by the expected value of ( _r_ _S_ [2] _[/\u03c1]_ [2] _S_ [)] _[/]_ [(] _[m]_ [ + 1). To do so, we will bound the leave-one-out error of the SVM] algorithm for a sample _S_ of size _m_ + 1 by ( _r_ _S_ [2] _[/\u03c1]_ [2] _S_ [)] _[/]_ [(] _[m]_ [ + 1). The result will then] follow by lemma 5.3, which relates the expected leave-one-out error to the expected error. Let _S_ = ( **x** 1 _, . . .,_ **x** _m_ +1 ) be a linearly separable sample drawn i.i.d. according to D and let **x** be a point in _S_ that is misclassified by _h_ _S\u2212{_ **x** _}_ . We will analyze the case where **x** = **x** _m_ +1, the analysis of other cases is similar. We denote by _S_ _[\u2032]_ the sample ( **x** 1 _, . . .,_ **x** _m_ ). For any _q \u2208_ [ _m_ + 1], let _G_ _q_",
    "chunk_id": "foundations_machine_learning_188"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "denote the function defined over R _[q]_ by _G_ _q_ : _**\u03b1**_ _\ufffd\u2192_ \ufffd _qi_ =1 _[\u03b1]_ _[i]_ _[ \u2212]_ [1] 2 \ufffd _qi,j_ =1 _[\u03b1]_ _[i]_ _[\u03b1]_ _[j]_ _[y]_ _[i]_ _[y]_ _[j]_ [(] **[x]** _[i]_ _[ \u00b7]_ **[ x]** _[j]_ [). Then,] _[ G]_ _[m]_ [+1] [ is the objective function of the] dual optimization problem for SVMs associated to the sample _S_ and _G_ _m_ the one for the sample _S_ _[\u2032]_ . Let _**\u03b1**_ _\u2208_ R _[m]_ [+1] denote a solution of the dual SVM problem max _**\u03b1**_ _\u2265_ 0 _G_ _m_ +1 ( _**\u03b1**_ ) and _**\u03b1**_ _[\u2032]_ _\u2208_ R _[m]_ [+1] the vector such that ( _\u03b1_ 1 _[\u2032]_ _[, . . ., \u03b1]_ _m_ _[\u2032]_ [)] _[\u22a4]_ _[\u2208]_ [R] _[m]_ [ is a] solution of max _**\u03b1**_ _\u2265_ 0 _G_ _m_ ( _**\u03b1**_ ) and _\u03b1_ _m_ _[\u2032]_ +1 [= 0. Let] **[ e]** _[m]_ [+1] [denote the (] _[m]_ [ + 1)th unit] vector in R _[m]_ [+1] . By definition of _**\u03b1**_ and _**\u03b1**_ _[\u2032]_ as maximizers, max _\u03b2\u2265_ 0 _G_ _m_ +1 ( _**\u03b1**_ _[\u2032]_ + _\u03b2_ **e** _m_ +1 ) _\u2264_ _G_ _m_ +1 ( _**\u03b1**_ ) and _G_ _m_ +1 ( _**\u03b1**_ _\u2212_ _\u03b1_ _m_ +1 **e** _m_ +1 ) _\u2264_ _G_ _m_ ( _**\u03b1**_ _[\u2032]_ ). Thus, the quantity _A_ = _G_ _m_ +1 ( _**\u03b1**_ ) _\u2212_ _G_ _m_ ( _**\u03b1**_ _[\u2032]_ ) admits the following lower and upper bounds: max _\u03b2\u2265_ 0 _[G]_ _[m]_ [+1] [(] _**[\u03b1]**_ _[\u2032]_ [ +] _[ \u03b2]_ **[e]** _[m]_ [+1] [)] _[ \u2212]_ _[G]_ _[m]_ [(] _**[\u03b1]**_ _[\u2032]_ [)] _[ \u2264]_ _[A][ \u2264]_ _[G]_ _[m]_ [+1] [(] _**[\u03b1]**_ [)] _[ \u2212]_ _[G]_ _[m]_ [+1] [(] _**[\u03b1]**_ _[ \u2212]_ _[\u03b1]_ _[m]_ [+1] **[e]** _[m]_ [+1] [)] _[.]_ Let **w** = [\ufffd] _[m]_ _i_ =1 [+1] _[y]_ _[i]_ _[\u03b1]_ _[i]_ **[x]** _[i]_ [ denote the weight vector returned by SVMs for the sample] _S_ . Since _h_ _S_ _\u2032_ misclassifies **x** _m_ +1, **x** _m_ +1 must be a support vector for _h_ _S_, thus **8.3** **Linear classification** **195** _y_ _m_ +1 **w** _\u00b7_ **x** _m_ +1 = 1. In view of that, the upper bound can be rewritten as follows: _G_ _m_ +1 ( _**\u03b1**_ ) _\u2212_ _G_ _m_ +1 ( _**\u03b1**_ _\u2212_ _\u03b1_ _m_ +1 **e** _m_ +1 ) _m_ +1 _[\u2225]_ **[x]** _[m]_ [+1] _[\u2225]_ [2] 2 _[\u03b1]_ [2] = _\u03b1_ _m_ +1 _\u2212_ _m_ +1 \ufffd \ufffd ( _y_ _i_ _\u03b1_ _i_ **x** _i_ ) _\u00b7_ ( _y_ _m_ +1 _\u03b1_ _m_ +1 **x** _m_ +1 ) + [1] 2 _i_ =1 = _\u03b1_ _m_ +1 (1 _\u2212_ _y_ _m_ +1 **w** _\u00b7_ **x** _m_ +1 ) + [1] _m_ +1 _[\u2225]_ **[x]** _[m]_ [+1] _[\u2225]_ [2] 2 _[\u03b1]_ [2] = [1] 2 _[\u03b1]_ _m_ [2] +1 _[\u2225]_ **[x]** _[m]_ [+1] _[\u2225]_ [2] _[.]_ Similarly, let **w** _[\u2032]_ = [\ufffd] _[m]_ _i_ =1 _[y]_ _[i]_ _[\u03b1]_ _i_ _[\u2032]_ **[x]** _[i]_ [. Then, for any] _[ \u03b2][ \u2265]_ [0, the quantity maximized in] the lower bound can be written as _G_ _m_",
    "chunk_id": "foundations_machine_learning_189"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "+1 ( _**\u03b1**_ _[\u2032]_ + _\u03b2_ **e** _m_ +1 ) _\u2212_ _G_ _m_ ( _**\u03b1**_ _[\u2032]_ ) = _\u03b2_ \ufffd1 _\u2212_ _y_ _m_ +1 ( **w** _[\u2032]_ + _\u03b2y_ _m_ +1 **x** _m_ +1 ) _\u00b7_ **x** _m_ +1 \ufffd + [1] 2 _[\u03b2]_ [2] _[\u2225]_ **[x]** _[m]_ [+1] _[\u2225]_ [2] = _\u03b2_ (1 _\u2212_ _y_ _m_ +1 **w** _[\u2032]_ _\u00b7_ **x** _m_ +1 ) _\u2212_ [1] 2 _[\u03b2]_ [2] _[\u2225]_ **[x]** _[m]_ [+1] _[\u2225]_ [2] _[.]_ The right-hand side is maximized for the following value of _\u03b2_ : 1 _\u2212y_ _m_ +1 **w** _[\u2032]_ _\u00b7_ **x** _m_ +1 [2] [1] (1 _\u2212y_ _m_ +1 **w** _[\u2032]_ _\u00b7_ **x** _m_ +1 ) [2] 2 **x** [2] _\u2225_ **x** _m_ +1 _\u2225_ [2] . Plugging in this value in the right-hand side gives [1] _\u2225_ + **x** _m_ 1 +1 _\u00b7\u2225_ [2] _m_ +1 . Thus, [1] (1 _\u2212_ _y_ _m_ +1 **w** _[\u2032]_ _\u00b7_ **x** _m_ +1 ) [2] 2 **x** [2] _A \u2265_ [1] +1 **w** _\u00b7_ **x** _m_ +1 ) 1 _\u2265_ _\u2225_ **x** _m_ +1 _\u2225_ [2] 2 _\u2225_ **x** _m_ +1 _\u2225_ [2] _[,]_ using the fact that _y_ _m_ +1 **w** _[\u2032]_ _\u00b7_ **x** _m_ +1 _<_ 0, since **x** _m_ +1 is misclassified by **w** _[\u2032]_ . Comparing this lower bound on _A_ with the upper bound previously derived leads to 2 _\u2225_ **x** _m_ 1 +1 _\u2225_ [2] _[ \u2264]_ 1 2 _[\u03b1]_ _m_ [2] +1 _[\u2225]_ **[x]** _[m]_ [+1] _[\u2225]_ [2] [, that is] 1 _\u03b1_ _m_ +1 _\u2265_ _\u2225_ **x** _m_ +1 _\u2225_ [2] _[ \u2265]_ _r_ [1] _._ _r_ _S_ [2] The analysis carried out in the case **x** = **x** _m_ +1 holds similarly for any **x** _i_ in _S_ that is misclassified by _h_ _S\u2212{_ **x** _i_ _}_ . Let I denote the set of such indices _i_ . Then, we can write: \ufffd _\u03b1_ _i_ _\u2265_ _[|]_ [I] [2] _[|]_ _._ \ufffd _i\u2208_ I _\u03b1_ _i_ _\u2265_ _r_ _[|]_ [I] _S_ [2] _[|]_ _._ _r_ _S_ [2] By (5.19), the following simple expression holds for the margin: [\ufffd] _[m]_ _i_ =1 [+1] _[\u03b1]_ _[i]_ [ = 1] _[/\u03c1]_ _S_ [2] [.] Using this identity leads to _|_ I _| \u2264_ _r_ _S_ [2] \ufffd _\u03b1_ _i_ _\u2264_ _r_ _S_ [2] _i\u2208_ I _m_ +1 \ufffd _\u03b1_ _i_ = _[r]_ _S_ [2] _._ _i_ =1 _\u03c1_ [2] _S_ **196** **Chapter 8** **On-Line Learning** Since by definition _|_ I _|_ is the total number of leave-one-out errors, this concludes the proof. Thus, the guarantees given by theorem 8.9 and theorem 8.10 in the separable case have a similar form. These bounds do not seem sufficient to distinguish the effectiveness of the SVM and Perceptron algorithms. Note, however, that while the same margin quantity _\u03c1_ _S_ appears in both bounds, the radius _r_ _S_ can be replaced by a finer quantity that is different for the two algorithms: in both cases, instead of the radius of the sphere containing all sample points, _r_ _S_ can be replaced by the radius of the",
    "chunk_id": "foundations_machine_learning_190"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "sphere containing the support vectors, as can be seen straightforwardly from the proof of the theorems. Thus, the position of the support vectors in the case of SVMs can provide a more favorable guarantee than that of the support vectors (update vectors) for the Perceptron algorithm. Finally, the guarantees given by these theorems are somewhat weak. These are not high probability bounds, they hold only for the expected error of the hypotheses returned by the algorithms and in particular provide no information about the variance of their error. The following two theorems give bounds on the number of updates or mistakes made by the Perceptron algorithm in the more general scenario of a non-linearly separable sample in terms of the _\u03c1_ -Hinge losses of an arbitrary weight vector **v** . **Theorem 8.11** _Let_ I _denote the set of indices t \u2208_ [ _T_ ] _at which the Perceptron algo-_ _rithm makes an update when processing a sequence_ **x** 1 _, . . .,_ **x** _T_ _with \u2225_ **x** _t_ _\u2225\u2264_ _r for_ _some r >_ 0 _. Then, the number of updates M_ = _|_ I _| made by the algorithm can be_ _bounded as follows:_ 2 \uf8f9 \uf8fb _\u2264_ inf _\u03c1>_ 0 _,\u2225v\u2225_ 2 _\u2264_ 1 2 _,_ \ufffd _M \u2264_ inf _\u03c1>_ 0 _,\u2225_ **v** _\u2225_ 2 _\u2264_ 1 \uf8ee _\u03c1r_ [+] \ufffd \uf8f0 _r_ [2] _\u03c1_ [2] [ + 4] _[\u2225]_ **[l]** _[\u03c1]_ _[\u2225]_ [1] 2 \ufffd _\u03c1r_ [+] \ufffd _\u2225_ **l** _\u03c1_ _\u2225_ 1 _where_ **l** _\u03c1_ = ( _l_ _t_ ) _t\u2208_ I _with l_ _t_ = max \ufffd0 _,_ 1 _\u2212_ _[y]_ _[t]_ [(] **[v]** _\u03c1_ _[\u00b7]_ **[x]** _[t]_ [)] \ufffd _._ Proof: Fix _\u03c1 >_ 0 and **v** with _\u2225_ **v** _\u2225_ 2 = 1. By definition of _l_ _t_, for any _t_, we have 1 _\u2212_ _[y]_ _[t]_ [(] **[v]** _\u03c1_ _[\u00b7]_ **[x]** _[t]_ [)] _\u2264_ _l_ _t_ . Summing up these inequalities over all _t \u2208_ I yields _M \u2264_ \ufffd \ufffd _l_ _t_ + \ufffd _t\u2208_ I _t\u2208_ I _y_ _t_ ( **v** _\u00b7_ **x** _t_ ) _\u03c1_ _y_ _t_ ( **v** _\u00b7_ **x** _t_ ) _\u2264\u2225_ **l** _\u03c1_ _\u2225_ 1 + _\u03c1_ _t\u2208_ I _Mr_ [2] _,_ (8.24) _\u03c1_ = _\u2225_ **l** _\u03c1_ _\u2225_ 1 + \ufffd _t\u2208_ I _\u221a_ where the last inequality holds by the bound shown in the proof of the separable case (theorem 8.8): **v** _\u00b7_ [\ufffd] _t\u2208_ I _[y]_ _[t]_ **[x]** _[t]_ _\u2264_ _\u221aMr_ [2] . Now, solving the resulting second-degree _\u2225_ _t\u2208_ **v** I _\u2225_ _[y]_ _[t]_ **[x]** _[t]_ _\u2264_ _\u221a_ _Mr_ [2] . Now, solving the resulting second-degree ~~_\u221a_~~ inequality _M \u2264\u2225_ **l** _\u03c1_ _\u2225_ 1 + ~~_\u221a_~~ _Mr_ [2] _r_ _r_ [2] inequality _M \u2264\u2225_ **l** _\u03c1_ _\u2225_ 1 + _\u03c1_ gives _\u221aM \u2264_ [1] 2 \ufffd _\u03c1_ [+] \ufffd _\u03c1_ [2] [ + 4] _[\u2225]_ **[l]** _[\u03c1]_ _[\u2225]_ [1] \ufffd _,_ which proves the first inequality. The second inequality follows from the sub-additivity of the square-root function. _Mr_ [2] _M \u2264_ [1] [1] 2 \ufffd _r_ _Mr\u03c1_ [2] gives _\u221a_ _\u03c1r_ [+]",
    "chunk_id": "foundations_machine_learning_191"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd **8.3** **Linear classification** **197** **Theorem 8.12** _Let_ I _denote the set of indices t \u2208_ [ _T_ ] _at which the Perceptron algo-_ _rithm makes an update when processing a sequence_ **x** 1 _, . . .,_ **x** _T_ _with \u2225_ **x** _t_ _\u2225\u2264_ _r for_ _some r >_ 0 _. Then, the number of updates M_ = _|_ I _| made by the algorithm can be_ _bounded as follows:_ 2 _,_ \ufffd _M \u2264_ inf _\u03c1>_ 0 _,\u2225v\u2225_ 2 _\u2264_ 1 _r_ \ufffd _\u03c1_ [+] _[ \u2225]_ **[l]** _[\u03c1]_ _[\u2225]_ [2] _where_ **l** _\u03c1_ = ( _l_ _t_ ) _t\u2208_ I _with l_ _t_ = max \ufffd0 _,_ 1 _\u2212_ _[y]_ _[t]_ [(] **[v]** _\u03c1_ _[\u00b7]_ **[x]** _[t]_ [)] \ufffd _._ Proof: Fix _\u03c1 >_ 0 and **v** with _\u2225_ **v** _\u2225_ 2 = 1. Starting with line (8.24) of theorem 8.11 and using _\u2225_ **l** _\u03c1_ _\u2225_ 1 _\u2264_ _\u221aM_ _\u2225_ **l** _\u03c1_ _\u2225_ 2, which holds by the Cauchy-Schwarz inequality, give _Mr_ [2] _\u2264_ _\u221a_ _\u03c1_ _M_ _\u2225_ **l** _\u03c1_ _\u2225_ 2 + _Mr_ [2] _._ _\u03c1_ _M \u2264\u2225_ **l** _\u03c1_ _\u2225_ 1 + _\u221a_ _\u221a_ This implies _\u221a_ ~~_\u221a_~~ _r_ [2] _M \u2264\u2225_ **l** _\u03c1_ _\u2225_ 2 + _\u03c1_ and proves the statement. These bounds strictly generalize the bounds given in the separable case (theorem 8.8) since in that case the vector **v** can be chosen to be that of a maximummargin hyperplane with no Hinge loss at any point. The main difference between the two bounds is the _L_ 1 -norm of the vector of Hinge losses in Theorem 8.11 versus the _L_ 2 -norm in Theorem 8.12. Note that, since the _L_ 2 -norm bound follows from upper bounding inequality (8.24), which is equivalent to the first inequality of Theorem 8.11, the first _L_ 1 -norm bound of Theorem 8.11 is always tighter than the _L_ 2 -norm bound of Theorem 8.12. The Perceptron algorithm can be generalized, as in the case of SVMs, to define a linear separation in a high-dimensional space. It admits an equivalent dual form, the dual Perceptron algorithm, which is presented in figure 8.8. The dual Perceptron algorithm maintains a vector _**\u03b1**_ _\u2208_ R _[T]_ of coefficients assigned to each point **x** _t_, _t \u2208_ [ _T_ ]. The label of a point **x** _t_ is predicted according to the rule sgn( **w** _\u00b7_ **x** _t_ ), where **w** = [\ufffd] _[T]_ _s_ =1 _[\u03b1]_ _[s]_ _[y]_ _[s]_ **[x]** _[s]_ [. The coefficient] _[ \u03b1]_ _[t]_ [ is incremented by one when this prediction] does not match the correct label. Thus, an update for **x** _t_ is equivalent to augmenting the weight vector **w** with _y_ _t_ **x** _t_, which shows that the dual algorithm matches exactly the standard Perceptron algorithm. The dual Perceptron algorithm can be written solely in terms of inner products between training instances. Thus, as in the case of SVMs, instead of the inner product between points in the input space, an arbitrary PDS kernel can be used, which leads to the kernel Perceptron algorithm detailed",
    "chunk_id": "foundations_machine_learning_192"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "in figure 8.9. The kernel Perceptron algorithm and its average variant, i.e., voted Perceptron with uniform weights _c_ _t_, are commonly used algorithms in a variety of applications. **198** **Chapter 8** **On-Line Learning** DualPerceptron( _**\u03b1**_ 0 ) 1 _**\u03b1**_ _\u2190_ _**\u03b1**_ 0 _\u25b7_ typically _**\u03b1**_ 0 = **0** 2 **for** _t \u2190_ 1 **to** _T_ **do** 3 Receive( **x** _t_ ) 4 _y_ \ufffd _t_ _\u2190_ sgn( [\ufffd] _[T]_ _s_ =1 _[\u03b1]_ _[s]_ _[y]_ _[s]_ [(] **[x]** _[s]_ _[ \u00b7]_ **[ x]** _[t]_ [))] 5 Receive( _y_ _t_ ) \ufffd 6 **if** ( _y_ _t_ _\u0338_ = _y_ _t_ ) **then** 7 _\u03b1_ _t_ _\u2190_ _\u03b1_ _t_ + 1 8 **else** _\u03b1_ _t_ _\u2190_ _\u03b1_ _t_ 9 **return** _**\u03b1**_ **Figure 8.8** Dual Perceptron algorithm. **8.3.2** **Winnow algorithm** This section presents an alternative on-line linear classification algorithm, the _Win-_ _now algorithm_ . Thus, it learns a weight vector defining a separating hyperplane by sequentially processing the training points. As suggested by the name, the algorithm is particularly well suited to cases where a relatively small number of dimensions or experts can be used to define an accurate weight vector. Many of the other dimensions may then be irrelevant. The Winnow algorithm is similar to the Perceptron algorithm, but, instead of the additive update of the weight vector in the Perceptron case, Winnow\u2019s update is multiplicative. The pseudocode of the algorithm is given in figure 8.10. The algorithm takes as input a learning parameter _\u03b7 >_ 0. It maintains a non-negative weight vector **w** _t_ with components summing to one ( _\u2225_ **w** _t_ _\u2225_ 1 = 1) starting with the uniform weight vector (line 1). At each round _t \u2208_ [ _T_ ], if the prediction does not match the correct label (line 6), each component _w_ _t,i_, _i \u2208_ [ _N_ ], is updated by multiplying it by exp( _\u03b7y_ _t_ _x_ _t,i_ ) and dividing by the normalization factor _Z_ _t_ to ensure that the weights sum to one (lines 7\u20139). Thus, if the label _y_ _t_ and _x_ _t,i_ share the same sign, then _w_ _t,i_ is increased, while, in the opposite case, it is significantly decreased. The Winnow algorithm is closely related to the WM algorithm: when **x** _t,i_ _\u2208_ _{\u2212_ 1 _,_ +1 _}_, sgn( **w** _t_ _\u00b7_ **x** _t_ ) coincides with the majority vote, since multiplying the weight of correct or incorrect experts by _e_ _[\u03b7]_ or _e_ _[\u2212][\u03b7]_ is equivalent to multiplying the weight **8.3** **Linear classification** **199** KernelPerceptron( _**\u03b1**_ 0 ) 1 _**\u03b1**_ _\u2190_ _**\u03b1**_ 0 _\u25b7_ typically _**\u03b1**_ 0 = **0** 2 **for** _t \u2190_ 1 **to** _T_ **do** 3 Receive( _x_ _t_ ) 4 _y_ \ufffd _t_ _\u2190_ sgn( [\ufffd] _[T]_ _s_ =1 _[\u03b1]_ _[s]_ _[y]_ _[s]_ _[K]_ [(] _[x]_ _[s]_ _[, x]_ _[t]_ [))] 5 Receive( _y_ _t_ ) \ufffd 6 **if** ( _y_ _t_ _\u0338_ = _y_ _t_ ) **then** 7 _\u03b1_ _t_ _\u2190_ _\u03b1_ _t_ + 1 8 **else** _\u03b1_ _t_ _\u2190_ _\u03b1_ _t_ 9 **return** _**\u03b1**_ **Figure 8.9** Kernel Perceptron algorithm for PDS kernel _K_ . of incorrect",
    "chunk_id": "foundations_machine_learning_193"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "ones by _\u03b2_ = _e_ _[\u2212]_ [2] _[\u03b7]_ . The multiplicative update rule of Winnow is of course also similar to that of AdaBoost. The following theorem gives a mistake bound for the Winnow algorithm in the separable case, which is similar in form to the bound of theorem 8.8 for the Perceptron algorithm. **Theorem 8.13** _Let_ **x** 1 _, . . .,_ **x** _T_ _\u2208_ R _[N]_ _be a sequence of T points with \u2225x_ _t_ _\u2225_ _\u221e_ _\u2264_ _r_ _\u221e_ _for_ _all t \u2208_ [ _T_ ] _, for some r_ _\u221e_ _>_ 0 _. Assume that there exist_ **v** _\u2208_ R _[N]_ _,_ **v** _\u2265_ 0 _, and \u03c1_ _\u221e_ _>_ 0 _such that for all t \u2208_ [ _T_ ] _, \u03c1_ _\u221e_ _\u2264_ _[y]_ _[t]_ _\u2225_ [(] **v** **[v]** _\u2225_ _[\u00b7]_ **[x]** 1 _[t]_ [)] _[. Then, for][ \u03b7]_ [ =] _[\u03c1]_ _r_ _\u221e_ [2] _[\u221e]_ _[, the number of updates]_ _made by the Winnow algorithm when processing_ **x** 1 _, . . .,_ **x** _T_ _is upper bounded by_ 2 ( _r_ _\u221e_ [2] _[/\u03c1]_ [2] _\u221e_ [) log] _[ N]_ _[.]_ Proof: Let I _\u2286_ [ _T_ ] be the set of iterations at which there is an update, and let _M_ be the total number of updates, i.e., _|_ I _|_ = _M_ . The potential function \u03a6 _t_, _t \u2208_ [ _T_ ], used for this proof is the relative entropy of the distribution defined by the normalized weights _v_ _i_ _/\u2225_ **v** _\u2225_ 1 _\u2265_ 0, _i \u2208_ [ _N_ ], and the one defined by the components of the weight vector _w_ _t,i_, _i \u2208_ [ _N_ ]: \u03a6 _t_ = _N_ \ufffd _i_ =1 _v_ _i_ log _[v]_ _[i]_ _[/][\u2225]_ **[v]** _[\u2225]_ [1] _._ _\u2225_ **v** _\u2225_ 1 _w_ _t,i_ To derive an upper bound on \u03a6 _t_, we analyze the difference of the potential functions at two consecutive rounds. For all _t \u2208_ I, this difference can be expressed and **200** **Chapter 8** **On-Line Learning** Winnow( _\u03b7_ ) 1 _w_ 1 _\u2190_ **1** _/N_ 2 **for** _t \u2190_ 1 **to** _T_ **do** 3 Receive( **x** _t_ ) 4 _y_ \ufffd _t_ _\u2190_ sgn( **w** _t_ _\u00b7_ **x** _t_ ) 5 Receive( _y_ _t_ ) \ufffd 6 **if** ( _y_ _t_ _\u0338_ = _y_ _t_ ) **then** 7 _Z_ _t_ _\u2190_ [\ufffd] _[N]_ _i_ =1 _[w]_ _[t,i]_ [ exp(] _[\u03b7y]_ _[t]_ _[x]_ _[t,i]_ [)] 8 **for** _i \u2190_ 1 **to** _N_ **do** [ex][p(] _[\u03b7y]_ _[t]_ _[x]_ _[t][,][i]_ [)] 9 _w_ _t_ +1 _,i_ _\u2190_ _[w]_ _[t][,][i]_ _Z_ _t_ 10 **else w** _t_ +1 _\u2190_ **w** _t_ 11 **return w** _T_ +1 **Figure 8.10** Winnow algorithm, with _y_ _t_ _\u2208{\u2212_ 1 _,_ +1 _}_ for all _t \u2208_ [ _T_ ]. bounded as follows: \u03a6 _t_ +1 _\u2212_ \u03a6 _t_ = = _N_ \ufffd _i_ =1 _N_ \ufffd _i_ =1 _v_ _i_ log _w_ _t,i_ _\u2225_ **v** _\u2225_ 1 _w_ _t_ +1 _,i_ _v_ _i_ log _Z_ _t_ _\u2225_ **v** _\u2225_ 1 exp( _\u03b7y_ _t_ _x_ _t,i_ ) = log _Z_",
    "chunk_id": "foundations_machine_learning_194"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_t_ _\u2212_ _\u03b7_ _N_ \ufffd _i_ =1 _v_ _i_ _y_ _t_ _x_ _t,i_ _\u2225_ **v** _\u2225_ 1 _\u2264_ log \ufffd \ufffd _[N]_ _w_ _t,i_ exp( _\u03b7y_ _t_ _x_ _t,i_ )\ufffd _\u2212_ _\u03b7\u03c1_ _\u221e_ _i_ =1 = log E _i\u223c_ **w** _t_ = log E _i\u223c_ **w** _t_ \ufffd exp( _\u03b7y_ _t_ _x_ _t,i_ )\ufffd _\u2212_ _\u03b7\u03c1_ _\u221e_ \ufffd exp( _\u03b7y_ _t_ _x_ _t,i_ _\u2212_ _\u03b7y_ _t_ **w** _t_ _\u00b7 x_ _t_ + _\u03b7y_ _t_ **w** _t_ _\u00b7 x_ _t_ )\ufffd _\u2212_ _\u03b7\u03c1_ _\u221e_ _\u2264_ log \ufffd exp( _\u03b7_ [2] (2 _r_ _\u221e_ ) [2] _/_ 8)\ufffd + _\u03b7y_ _t_ ( **w** _t_ _\u00b7 x_ _t_ ) \ufffd ~~\ufffd~~ \ufffd ~~\ufffd~~ _\u2264_ 0 _\u2264_ _\u03b7_ [2] _r_ _\u221e_ [2] _[/]_ [2] _[ \u2212]_ _[\u03b7\u03c1]_ _[\u221e]_ _[.]_ _\u2212\u03b7\u03c1_ _\u221e_ **8.4** **On-line to batch conversion** **201** The first inequality follows the definition of _\u03c1_ _\u221e_ . The subsequent equality rewrites the summation as an expectation over the distribution defined by **w** _t_ . The next inequality uses Hoeffding\u2019s lemma (lemma D.1) and the last one the fact that there has been an update at _t_, which implies _y_ _t_ ( **w** _t_ _\u00b7 x_ _t_ ) _\u2264_ 0. Summing up these inequalities over all _t \u2208_ I yields: \u03a6 _T_ +1 _\u2212_ \u03a6 1 _\u2264_ _M_ ( _\u03b7_ [2] _r_ _\u221e_ [2] _[/]_ [2] _[ \u2212]_ _[\u03b7\u03c1]_ _[\u221e]_ [)] _[.]_ Next, we derive a lower bound by noting that _N_ \ufffd _i_ =1 _v_ _i_ _v_ _i_ log _\u2264_ log _N ._ _\u2225_ **v** _\u2225_ 1 _\u2225_ **v** _\u2225_ 1 \u03a6 1 = _N_ \ufffd _i_ =1 _v_ _i_ log _[v]_ _[i]_ _[/][\u2225]_ **[v]** _[\u2225]_ [1] = log _N_ + _\u2225_ **v** _\u2225_ 1 1 _/N_ Additionally, since the relative entropy is always non-negative, we have \u03a6 _T_ +1 _\u2265_ 0. This yields the following lower bound: \u03a6 _T_ +1 _\u2212_ \u03a6 1 _\u2265_ 0 _\u2212_ log _N_ = _\u2212_ log _N ._ Combining the upper and lower bounds we see that _\u2212_ log _N \u2264_ _M_ ( _\u03b7_ [2] _r_ _\u221e_ [2] _[/]_ [2] _[ \u2212]_ _[\u03b7\u03c1]_ _[\u221e]_ [).] Setting _\u03b7_ = _[\u03c1]_ _r_ _\u221e_ [2] _[\u221e]_ [yields the statement of the theorem.] The margin-based mistake bounds of theorem 8.8 and theorem 8.13 for the Perceptron and Winnow algorithms have a similar form, but they are based on different norms. For both algorithms, the norm _\u2225\u00b7 \u2225_ _p_ used for the input vectors **x** _t_, _t \u2208_ [ _T_ ], is the dual of the norm _\u2225\u00b7 \u2225_ _q_ used for the margin vector **v**, that is _p_ and _q_ are conjugate: 1 _/p_ + 1 _/q_ = 1: in the case of the Perceptron algorithm _p_ = _q_ = 2, while for Winnow _p_ = _\u221e_ and _q_ = 1. These bounds imply different types of guarantees. The bound for Winnow is favorable when a sparse set of the experts _i \u2208_ [ _N_ ] can predict well. For example, if **v** = **e** 1 where **e** 1 is the unit vector along the first axis in R _[N]_ and if **x** _t_",
    "chunk_id": "foundations_machine_learning_195"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2208{\u2212_ 1 _,_ +1 _}_ _[N]_ for all _t_, then the upper bound on the number of mistakes given for Winnow by theorem 8.13 is only 2 log _N_, while the upper bound of theorem 8.8 for the Perceptron algorithm is _N_ . The guarantee for the Perceptron algorithm is more favorable in the opposite situation, where sparse solutions are not effective. **8.4** **On-line to batch conversion** The previous sections presented several algorithms for the scenario of on-line learning, including the Perceptron and Winnow algorithms, and analyzed their behavior within the mistake model, where no assumption is made about the way the training sequence is generated. Can these algorithms be used to derive hypotheses with small generalization error in the standard stochastic setting? How can the interme **202** **Chapter 8** **On-Line Learning** diate hypotheses they generate be combined to form an accurate predictor? These are the questions addressed in this section. Let H be a hypothesis of functions mapping X to Y _[\u2032]_, and let _L_ : Y _[\u2032]_ _\u00d7_ Y _\u2192_ R + be a bounded loss function, that is _L \u2264_ _M_ for some _M \u2265_ 0. We assume a standard supervised learning setting where a labeled sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _T_ _, y_ _T_ )) _\u2208_ (X _\u00d7_ Y) _[T]_ is drawn i.i.d. according to some fixed but unknown distribution D. The sample is sequentially processed by an on-line learning algorithm _A_ . The algorithm starts with an initial hypothesis _h_ 1 _\u2208_ H and generates a new hypothesis _h_ _t_ +1 _\u2208_ H, after processing pair ( _x_ _t_ _, y_ _t_ ), _t \u2208_ [ _m_ ]. The regret of the algorithm is defined as before by _T_ \ufffd _L_ ( _h_ ( _x_ _t_ ) _, y_ _t_ ) _._ (8.25) _t_ =1 _R_ _T_ = _T_ \ufffd _L_ ( _h_ _t_ ( _x_ _t_ ) _, y_ _t_ ) _\u2212_ _h_ min _\u2208_ H _t_ =1 The generalization error of a hypothesis _h \u2208_ H is its expected loss _R_ ( _h_ ) = E ( _x,y_ ) _\u223c_ D [ _L_ ( _h_ ( _x_ ) _, y_ )]. The following lemma gives a bound on the average of the generalization errors of the hypotheses generated by _A_ in terms of its average loss _T_ [1] \ufffd _Tt_ =1 _[L]_ [(] _[h]_ _[t]_ [(] _[x]_ _[t]_ [)] _[, y]_ _[t]_ [).] **Lemma 8.14** _Let S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _T_ _, y_ _T_ )) _\u2208_ (X _\u00d7_ Y) _[T]_ _be a labeled sample drawn_ _i.i.d. according to_ D _, L a loss bounded by M and h_ 1 _, . . ., h_ _T_ _the sequence of hy-_ _potheses generated by an on-line algorithm A sequentially processing S. Then, for_ _any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, the following holds:_ _T_ ~~\ufffd~~ 1 _T_ _T_ \ufffd \ufffd _R_ ( _h_ _t_ ) _\u2264_ _T_ [1] _t_ =1",
    "chunk_id": "foundations_machine_learning_196"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_T_ \ufffd _L_ ( _h_ _t_ ( _x_ _t_ ) _, y_ _t_ ) + _M_ _t_ =1 2 log [1] _\u03b4_ _._ (8.26) _T_ Proof: For any _t \u2208_ [ _T_ ], let _V_ _t_ be the random variable defined by _V_ _t_ = _R_ ( _h_ _t_ ) _\u2212_ _L_ ( _h_ _t_ ( _x_ _t_ ) _, y_ _t_ ). Observe that for any _t \u2208_ [ _T_ ], E[ _V_ _t_ _|x_ 1 _, . . ., x_ _t\u2212_ 1 ] = _R_ ( _h_ _t_ ) _\u2212_ E[ _L_ ( _h_ _t_ ( _x_ _t_ ) _, y_ _t_ ) _|h_ _t_ ] = _R_ ( _h_ _t_ ) _\u2212_ _R_ ( _h_ _t_ ) = 0 _._ Since the loss is bounded by _M_, _V_ _t_ takes values in the interval [ _\u2212M,_ + _M_ ] for all _t \u2208_ [ _T_ ]. Thus, by Azuma\u2019s inequality (theorem D.7), P[ _T_ [1] \ufffd _Tt_ =1 _[V]_ _[t]_ _[ \u2265]_ _[\u03f5]_ []] _[ \u2264]_ exp( _\u2212_ 2 _T\u03f5_ [2] _/_ (2 _M_ ) [2] )). Setting the right-hand side to be equal to _\u03b4 >_ 0 yields the statement of the lemma. When the loss function is convex with respect to its first argument, the lemma can be used to derive a bound on the generalization error of the average of the hypotheses generated by _A_, _T_ 1 \ufffd _Tt_ =1 _[h]_ _[t]_ [, in terms of the average loss of] _[ A]_ [ on] _[ S]_ [, or] in terms of the regret _R_ _T_ and the infimum error of hypotheses in H. **Theorem 8.15** _Let S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _T_ _, y_ _T_ )) _\u2208_ (X _\u00d7_ Y) _[T]_ _be a labeled sample drawn_ _i.i.d. according to_ D _, L a loss bounded by M and convex with respect to its first_ _argument, and h_ 1 _, . . ., h_ _T_ _the sequence of hypotheses generated by an on-line algo-_ _rithm A sequentially processing S. Then, for any \u03b4 >_ 0 _, with probability at least_ **8.4** **On-line to batch conversion** **203** 1 _\u2212_ _\u03b4, each of the following holds:_ ~~\ufffd~~ 2 log [1] _\u03b4_ (8.27) _T_ 1 _R_ \ufffd _T_ 1 _R_ \ufffd _T_ _T_ \ufffd _h_ _t_ _t_ =1 _T_ \ufffd _h_ _t_ _t_ =1 \ufffd _\u2264_ _h_ inf _\u2208_ H _[R]_ [(] _[h]_ [) +] _[ R]_ _T_ _[T]_ + 2 _M_ \ufffd _\u2264_ _h_ inf _\u2208_ H _[R]_ [(] _[h]_ [) +] _[ R]_ _T_ _[T]_ _\u2264_ [1] \ufffd _T_ _T_ \ufffd _L_ ( _h_ _t_ ( _x_ _t_ ) _, y_ _t_ ) + _M_ _t_ =1 _T_ \ufffd \ufffd 2 log [2] _\u03b4_ _._ (8.28) _T_ 2 log [2] Proof: By the convexity of _L_ with respect to its first argument, for any ( _x, y_ ) _\u2208_ X _\u00d7_ Y, we have _L_ ( [1] \ufffd _Tt_ =1 _[h]_ _[t]_ [(] _[x]_ [)] _[, y]_ [)] _[ \u2264]_ [1] \ufffd _Tt_ =1 _[L]_ [(] _[h]_ _[t]_",
    "chunk_id": "foundations_machine_learning_197"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[(] _[x]_ [)] _[, y]_ [). Taking the expectation gives] Y, we have _L_ ( _T_ [1] \ufffd _Tt_ =1 _[h]_ _[t]_ [(] _[x]_ [)] _[, y]_ [)] _[ \u2264]_ _T_ [1] \ufffd _Tt_ =1 _[L]_ [(] _[h]_ _[t]_ [(] _[x]_ [)] _[, y]_ [). Taking the expectation gives] _R_ ( [1] \ufffd _Tt_ =1 _[h]_ _[t]_ [)] _[ \u2264]_ 1 \ufffd _Tt_ =1 _[R]_ [(] _[h]_ _[t]_ [). The first inequality then follows by lemma 8.14.] _T_ [1] \ufffd _Tt_ =1 _[h]_ _[t]_ [(] _[x]_ [)] _[, y]_ [)] _[ \u2264]_ _T_ [1] _R_ ( _T_ [1] \ufffd _Tt_ =1 _[h]_ _[t]_ [)] _[ \u2264]_ _T_ 1 \ufffd _Tt_ =1 _[R]_ [(] _[h]_ _[t]_ [). The first inequality then follows by lemma 8.14.] Thus, by definition of the regret _R_ _T_, for any _\u03b4 >_ 0, the following holds with probability at least 1 _\u2212_ _\u03b4/_ 2: _\u2264_ [1] \ufffd _T_ \ufffd 1 _R_ \ufffd _T_ _T_ \ufffd _h_ _t_ _t_ =1 _T_ \ufffd _T_ \ufffd _L_ ( _h_ _t_ ( _x_ _t_ ) _, y_ _t_ ) + _M_ _t_ =1 2 log [2] _\u03b4_ _T_ 2 log [2] + _M_ _T_ \ufffd 1 _\u2264_ min _h\u2208_ H _T_ _T_ \ufffd \ufffd _L_ ( _h_ ( _x_ _t_ ) _, y_ _t_ ) + _[R]_ _T_ _[T]_ _t_ =1 2 log [2] _\u03b4_ _T_ _._ 2 log [2] By definition of inf _h\u2208_ H _R_ ( _h_ ), for any _\u03f5 >_ 0, there exists _h_ _[\u2217]_ _\u2208_ H with _R_ ( _h_ _[\u2217]_ ) _\u2264_ inf _h\u2208_ H _R_ ( _h_ ) + _\u03f5_ . By Hoeffding\u2019s inequality, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4/_ 2, _T_ 1 \ufffd _Tt_ =1 _[L]_ [(] _[h]_ _[\u2217]_ [(] _[x]_ _[t]_ [)] _[, y]_ _[t]_ [)] _[ \u2264]_ _[R]_ [(] _[h]_ _[\u2217]_ [) +] _[ M]_ \ufffd 2 log [2] 1 _\u2212_ _\u03b4/_ 2, _T_ \ufffd _t_ =1 _[L]_ [(] _[h]_ _[\u2217]_ [(] _[x]_ _[t]_ [)] _[, y]_ _[t]_ [)] _[ \u2264]_ _[R]_ [(] _[h]_ _[\u2217]_ [) +] _[ M]_ _T_ _\u03b4_ . Thus, for any _\u03f5 >_ 0, by the union bound, the following holds with probability at least 1 _\u2212_ _\u03b4_ : + _M_ _T_ 1 _R_ \ufffd _T_ _T_ \ufffd _h_ _t_ _t_ =1 _\u2264_ [1] \ufffd _T_ _T_ \ufffd \ufffd _L_ ( _h_ _[\u2217]_ ( _x_ _t_ ) _, y_ _t_ ) + _[R]_ _T_ _[T]_ _t_ =1 \ufffd 2 log [2] _\u03b4_ _T_ + _M_ _T_ _\u2264_ _R_ ( _h_ _[\u2217]_ ) + _M_ \ufffd 2 log [2] _\u03b4_ + _[R]_ _[T]_ _T_ _T_ ~~\ufffd~~ 2 log [2] _\u03b4_ _T_ 2 log [2] = _R_ ( _h_ _[\u2217]_ ) + _[R]_ _[T]_ + 2 _M_ _T_ = _R_ ( _h_ _[\u2217]_ ) + _[R]_ _[T]_ ~~\ufffd~~ 2 log [2] _\u03b4_ _T_ _\u2264_ _h_ inf _\u2208_ H _[R]_ [(] _[h]_ [) +] _[ \u03f5]_ [ +] _[ R]_ _T_ _[T]_ + 2 _M_ _\u2264_ _h_ inf _\u2208_ H _[R]_ [(] _[h]_ [) +] _[ \u03f5]_ [ +] _[ R]_ _T_ _[T]_ \ufffd 2",
    "chunk_id": "foundations_machine_learning_198"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "log [2] _\u03b4_ _T_ _._ 2 log [2] Since this inequality holds for all _\u03f5 >_ 0, it implies the second statement of the theorem. The theorem can be applied to a variety of on-line regret minimization algorithms, for example when _R_ _T_ _/T_ = _O_ (1 _/\u221aT_ ). In particular, we can apply the theorem to the exponential weighted average algorithm. Assuming that the loss _L_ is bounded **204** **Chapter 8** **On-Line Learning** by _M_ = 1 and that the number of rounds _T_ is known to the algorithm, we can use the regret bound of theorem 8.6. The doubling trick (used in theorem 8.7) can be used to derive a similar bound if _T_ is not known in advance. Thus, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, the following holds for the generalization error of the average of the hypotheses generated by exponential weighted average: 1 _R_ \ufffd _T_ _T_ \ufffd _h_ _t_ _t_ =1 \ufffd _\u2264_ _h_ inf _\u2208_ H _[R]_ [(] _[h]_ [) +] \ufffd log _N_ + 2 2 _T_ log _N_ \ufffd 2 log [2] _\u03b4_ _T_ _,_ where _N_ is the number of experts, or the dimension of the weight vectors. **8.5** **Game-theoretic connection** The existence of regret minimization algorithms can be used to give a simple proof of von Neumann\u2019s theorem. For any _m \u2265_ 1, we will denote by \u2206 _m_ the set of all distributions over _{_ 1 _, . . ., m}_, that is \u2206 _m_ = _{_ **p** _\u2208_ R _[m]_ : **p** _\u2265_ 0 _\u2227\u2225_ **p** _\u2225_ 1 = 1 _}_ . **Theorem 8.16 (Von Neumann\u2019s minimax theorem)** _Let m, n \u2265_ 1 _. Then, for any two-_ _person zero-sum game defined by matrix_ **M** _\u2208_ R _[m][\u00d7][n]_ _,_ min (8.29) **p** _\u2208_ \u2206 _m_ **q** [max] _\u2208_ \u2206 _n_ **[p]** _[\u22a4]_ **[Mq]** [ = max] **q** _\u2208_ \u2206 _n_ **p** [min] _\u2208_ \u2206 _m_ **[p]** _[\u22a4]_ **[Mq]** _[ .]_ Proof: The inequality max **q** min **p** **p** _[\u22a4]_ **Mq** _\u2264_ min **p** max **q** **p** _[\u22a4]_ **Mq** is straightforward, since by definition of min, for all **p** _\u2208_ \u2206 _m_ _,_ **q** _\u2208_ \u2206 _n_, we have min **p** **p** _[\u22a4]_ **Mq** _\u2264_ **p** _[\u22a4]_ **Mq** . Taking the maximum over **q** of both sides gives: max **q** min **p** **p** _[\u22a4]_ **Mq** _\u2264_ max **q** **p** _[\u22a4]_ **Mq** for all **p**, subsequently taking the minimum over **p** proves the inequality. [12] To show the reverse inequality, consider an on-line learning setting where at each round _t \u2208_ [ _T_ ], algorithm _A_ returns **p** _t_ and incurs loss **Mq** _t_ . We can assume that **q** _t_ is selected in the optimal adversarial way, that is **q** _t_ _\u2208_ argmax _q\u2208_ \u2206 _m_ **p** _t_ _[\u22a4]_ **[Mq]** [,] and that _A_ is a regret minimization algorithm, that is _R_ _T_ _/T \u2192_ 0, where _R_ _T_ = _T_ _T_ \ufffd _t_ =1 **[p]** _t_ _[\u22a4]_ **[Mq]** _[t]_ _[\u2212]_ [min] **[p]** _[\u2208]_ [\u2206] _m_ \ufffd _t_ =1 **[p]** _[\u22a4]_ **[Mq]** _[t]_ [. Then,",
    "chunk_id": "foundations_machine_learning_199"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the following holds:] _T_ _T_ _T_ \ufffd _\u22a4_ \ufffd **p** _t_ \ufffd **Mq** _\u2264_ _T_ [1] _t_ =1 min **p** _\u2208_ \u2206 _m_ **q** [max] _\u2208_ \u2206 _n_ **[p]** _[\u22a4]_ **[Mq]** _[ \u2264]_ [max] **q** \ufffd _T_ 1 _T_ \ufffd \ufffd max **q** **p** _[\u22a4]_ _t_ **[Mq]** [ =] _T_ [1] _t_ =1 _T_ \ufffd **p** _[\u22a4]_ _t_ **[Mq]** _[t]_ _[.]_ _t_ =1 12 More generally, the maxmin is always upper bounded by the minmax for any function or two arguments and any constraint sets, following the same proof. **8.6** **Chapter notes** **205** By definition of regret, the right-hand side can be expressed and bounded as follows: _T_ \ufffd \ufffd **p** _[\u22a4]_ **Mq** _t_ + _[R]_ _T_ _[T]_ _t_ =1 _[T]_ 1 = min _T_ **p** _\u2208_ \u2206 _m_ **[p]** _[\u22a4]_ **[M]** \ufffd _T_ _T_ _T_ 1 _T_ _T_ 1 \ufffd **p** _[\u22a4]_ _t_ **[Mq]** _[t]_ [= min] **p** _\u2208_ \u2206 _m_ _T_ _t_ =1 _T_ \ufffd \ufffd **q** _t_ \ufffd + _[R]_ _T_ _[T]_ _t_ =1 _\u2264_ max **q** _\u2208_ \u2206 _n_ **p** [min] _\u2208_ \u2206 _m_ **[p]** _[\u22a4]_ **[Mq]** [ +] _[ R]_ _T_ _[T]_ _[.]_ This implies that the following bound holds for the minmax for all _T \u2265_ 1: min **p** _\u2208_ \u2206 _m_ **q** [max] _\u2208_ \u2206 _n_ **[p]** _[\u22a4]_ **[Mq]** _[ \u2264]_ **q** [max] _\u2208_ \u2206 _n_ **p** [min] _\u2208_ \u2206 _m_ **[p]** _[\u22a4]_ **[Mq]** [ +] _[ R]_ _T_ _[T]_ Since lim _T \u2192_ + _\u221e_ _RT_ _T_ [= 0, this shows that min] **[p]** [ max] **[q]** **[ p]** _[\u22a4]_ **[Mq]** _[ \u2264]_ [max] **[q]** [ min] **[p]** **[ p]** _[\u22a4]_ **[Mq]** [.][\u25a1] **8.6** **Chapter notes** Algorithms for regret minimization were initiated with the pioneering work of Hannan [1957] who gave an algorithm whose regret decreases as _O_ ( _\u221aT_ ) as a function of _T_ but whose dependency on _N_ is linear. The weighted majority algorithm and the randomized weighted majority algorithm, whose regret is only logarithmic in _N_, are due to Littlestone and Warmuth [1989]. The exponential weighted average algorithm and its analysis, which can be viewed as an extension of the WM algorithm to convex non-zero-one losses is due to the same authors [Littlestone and Warmuth, 1989, 1994]. The analysis we presented follows Cesa-Bianchi [1999] and Cesa-Bianchi and Lugosi [2006]. The doubling trick technique appears in Vovk [1990] and Cesa-Bianchi et al. [1997]. The algorithm of exercise 8.7 and the analysis leading to a second-order bound on the regret are due to Cesa-Bianchi et al. [2005]. The lower bound presented in theorem 8.5 is from Blum and Mansour [2007]. While the regret bounds presented are logarithmic in the number of the experts _N_, when _N_ is exponential in the size of the input problem, the computational complexity of an expert algorithm could be exponential. For example, in the online shortest paths problem, _N_ is the number of paths between two vertices of a directed graph. However, several computationally efficient algorithms have been presented for broad classes of such problems by exploiting their structure [Takimoto and Warmuth, 2002, Kalai and Vempala, 2003,",
    "chunk_id": "foundations_machine_learning_200"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Zinkevich, 2003]. The notion of regret (or _external regret_ ) presented in this chapter can be generalized to that of _internal regret_ or even _swap regret_, by comparing the loss of the algorithm not just to that of the best expert in retrospect, but to that of any modification of the actions taken by the algorithm by replacing each occurrence of some specific action with another one (internal regret), or even replacing actions via an arbitrary mapping (swap regret) [Foster and Vohra, 1997, Hart and Mas-Colell, 2000, Lehrer, 2003]. Several algorithms for low internal regret have been given **206** **Chapter 8** **On-Line Learning** [Foster and Vohra, 1997, 1998, 1999, Hart and Mas-Colell, 2000, Cesa-Bianchi and Lugosi, 2001, Stoltz and Lugosi, 2003], including a conversion of low external regret to low swap regret by Blum and Mansour [2005]. The Perceptron algorithm was introduced by Rosenblatt [1958]. The algorithm raised a number of reactions, in particular by Minsky and Papert [1969], who objected that the algorithm could not be used to recognize the XOR function. Of course, the kernel Perceptron algorithm already given by Aizerman et al. [1964] could straightforwardly succeed to do so using second-degree polynomial kernels. The margin bound for the Perceptron algorithm was proven by Novikoff [1962] and is one of the first results in learning theory. We presented two extensions of Novikoff\u2019s result which hold in the more general non-separable case: Theorem 8.12 due to Freund and Schapire [1999a] and Theorem 8.11 due to Mohri and Rostamizadeh [2013]. Our proof of Theorem 8.12 is significantly more concise that the original proof given by Freund and Schapire [1999a] and shows that the bound of Theorem 8.11 is always tighter than that of Theorem 8.12. See [Mohri and Rostamizadeh, 2013] for other more general data-dependent upper bounds on the number of updates made by the Perceptron algorithm in the non-separable case. The leave-one-out analysis for SVMs is described by Vapnik [1998]. The Winnow algorithm was introduced by Littlestone [1987]. The analysis of the on-line to batch conversion and exercises 8.10 and 8.11 are from Cesa-Bianchi et al. [2001, 2004] (see also Littlestone [1989]). Von Neumann\u2019s minimax theorem admits a number of different generalizations. See Sion [1958] for a generalization to quasi-concave-convex functions semi-continuous in each argument and the references therein. The simple proof of von Neumann\u2019s theorem presented here is entirely based on learning-related techniques. A proof of a more general version using multiplicative updates was presented by Freund and Schapire [1999b]. On-line learning is a very broad and fast-growing research area in machine learning. The material presented in this chapter should be viewed only as an introduction to the topic, but the proofs and techniques presented should indicate the flavor of most results in this area. For a more comprehensive presentation of on-line learning and related game theory algorithms and techniques, the reader could consult the book of Cesa-Bianchi and Lugosi [2006]. **8.7** **Exercises** 8.1 Perceptron lower bound. Let _S_ be a labeled sample of _m_ points in R _[N]_ with _x_ _i_ = (( _\u2212_ 1) _[i]_ _, . .",
    "chunk_id": "foundations_machine_learning_201"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ".,_ ( _\u2212_ 1) _[i]_ _,_ ( _\u2212_ 1) _[i]_ [+1] ~~\ufffd~~ ~~\ufffd\ufffd~~ ~~\ufffd~~ _i_ first components _,_ 0 _, . . .,_ 0) and _y_ _i_ = ( _\u2212_ 1) _[i]_ [+1] _._ (8.30) **8.7** **Exercises** **207** On-line-SVM( **w** 0 ) 1 **w** 1 _\u2190_ **w** 0 _\u25b7_ typically **w** 0 = **0** 2 **for** _t \u2190_ 1 **to** _T_ **do** 3 Receive( **x** _t_ _, y_ _t_ ) 4 **if** _y_ _t_ ( **w** _t_ _\u00b7_ **x** _t_ ) _<_ 1 **then** 5 **w** _t_ +1 _\u2190_ **w** _t_ _\u2212_ _\u03b7_ ( **w** _t_ _\u2212_ _Cy_ _t_ **x** _t_ ) 6 **elseif** _y_ _t_ ( **w** _t_ _\u00b7_ **x** _t_ ) _>_ 1 **then** 7 **w** _t_ +1 _\u2190_ **w** _t_ _\u2212_ _\u03b7_ **w** _t_ 8 **else w** _t_ +1 _\u2190_ **w** _t_ 9 **return w** _T_ +1 **Figure 8.11** On-line SVM algorithm. Show that the Perceptron algorithm makes \u2126(2 _[N]_ ) updates before finding a separating hyperplane, regardless of the order in which it receives the points. 8.2 Generalized mistake bound. Theorem 8.8 presents a margin bound on the maximum number of updates for the Perceptron algorithm for the special case _\u03b7_ = 1. Consider now the general Perceptron update **w** _t_ +1 _\u2190_ **w** _t_ + _\u03b7y_ _t_ **x** _t_, where _\u03b7 >_ 0. Prove a bound on the maximum number of mistakes. How does _\u03b7_ affect the bound? 8.3 Sparse instances. Suppose each input vector **x** _t_, _t \u2208_ [ _T_ ], coincides with the _t_ th unit vector of R _[T]_ . How many updates are required for the Perceptron algorithm to converge? Show that the number of updates matches the margin bound of theorem 8.8. 8.4 Tightness of lower bound. Is the lower bound of theorem 8.5 tight? Explain why or show a counter-example. 8.5 On-line SVM algorithm. Consider the algorithm described in figure 8.11. Show that this algorithm corresponds to the stochastic gradient descent technique applied to the SVM problem (5.24) with hinge loss and no offset (i.e., fix _p_ = 1 and _b_ = 0). **208** **Chapter 8** **On-Line Learning** MarginPerceptron() 1 **w** 1 _\u2190_ **0** 2 **for** _t \u2190_ 1 **to** _T_ **do** 3 Receive( **x** _t_ ) 4 Receive( _y_ _t_ ) 5 **if** \ufffd( **w** _t_ = 0) **or** ( _[y]_ _[t]_ **[w]** **w** _[t]_ _[\u00b7]_ **[x]** _[t]_ _[\u03c1]_ 2 [)] \ufffd **[w]** _[t]_ _[\u00b7]_ **[x]** _[t]_ _<_ _[\u03c1]_ _\u2225_ **w** _t_ _\u2225_ 2 **then** 6 **w** _t_ +1 _\u2190_ **w** _t_ + _y_ _t_ **x** _t_ 7 **else w** _t_ +1 _\u2190_ **w** _t_ 8 **return w** _T_ +1 **Figure 8.12** Margin Perceptron algorithm. 8.6 Margin Perceptron. Given a training sample _S_ that is linearly separable with a maximum margin _\u03c1 >_ 0, theorem 8.8 states that the Perceptron algorithm run cyclically over _S_ is guaranteed to converge after at most _R_ [2] _/\u03c1_ [2] updates, where _R_ is the radius of the sphere containing the sample points. However, this theorem does not guarantee that the hyperplane solution of the Perceptron algorithm achieves a margin close to _\u03c1_ .",
    "chunk_id": "foundations_machine_learning_202"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Suppose we modify the Perceptron algorithm to ensure that the margin of the hyperplane solution is at least _\u03c1/_ 2. In particular, consider the algorithm described in figure 8.12. In this problem we show that this algorithm converges after at most 16 _R_ [2] _/\u03c1_ [2] updates. Let I denote the set of times _t \u2208_ [ _T_ ] at which the algorithm makes an update and let _M_ = _|_ I _|_ be the total number of updates. (a) Using an analysis similar to the one given for the Perceptron algorithm, show that _M\u03c1 \u2264\u2225_ **w** _T_ +1 _\u2225_ . Conclude that if _\u2225_ **w** _T_ +1 _\u2225_ _<_ [4] _[R]_ [2] [, then] _[ M <]_ [ 4] _[R]_ [2] _[/\u03c1]_ [2] [.] that _M\u03c1 \u2264\u2225_ **w** _T_ +1 _\u2225_ . Conclude that if _\u2225_ **w** _T_ +1 _\u2225_ _<_ [4] _[R]_ _\u03c1_ [, then] _[ M <]_ [ 4] _[R]_ [2] _[/\u03c1]_ [2] [.] (For the remainder of this problem, we will assume that _\u2225_ **w** _T_ +1 _\u2225\u2265_ [4] _[R]_ [2] [.)] _\u03c1_ [.)] (b) Show that for any _t \u2208_ I (including _t_ = 0), the following holds: _\u2225_ **w** _t_ +1 _\u2225_ [2] _\u2264_ ( _\u2225_ **w** _t_ _\u2225_ + _\u03c1/_ 2) [2] + _R_ [2] _._ (c) From (b), infer that for any _t \u2208_ I we have _R_ [2] _\u2225_ **w** _t_ +1 _\u2225\u2264\u2225_ **w** _t_ _\u2225_ + _\u03c1/_ 2 + _\u2225_ **w** _t_ _\u2225_ + _\u2225_ **w** _t_ +1 _\u2225_ + _\u03c1/_ 2 _[.]_ **8.7** **Exercises** **209** (d) Using the inequality from (c), show that for any _t \u2208_ I such that either _\u2225_ **w** _t_ _\u2225\u2265_ [4] _[R]_ [2] or _\u2225_ **w** _t_ +1 _\u2225\u2265_ [4] _[R]_ [2] [, we have] _[R]_ [2] _\u03c1_ or _\u2225_ **w** _t_ +1 _\u2225\u2265_ [4] _[R]_ _\u03c1_ [2] _\u03c1_ [, we have] _\u2225_ **w** _t_ +1 _\u2225\u2264\u2225_ **w** _t_ _\u2225_ + [3] 4 _[\u03c1.]_ (e) Show that _\u2225_ **w** 1 _\u2225\u2264_ _R \u2264_ 4 _R_ [2] _/\u03c1_ . Since by assumption we have _\u2225_ **w** _T_ +1 _\u2225\u2265_ [4] _[R]_ [2] _\u03c1_ [,] conclude that there must exist a largest time _t_ 0 _\u2208_ I such that _\u2225_ **w** _t_ 0 _\u2225\u2264_ [4] _[R]_ [2] _\u03c1_ and _\u2225_ **w** _t_ 0 +1 _\u2225\u2265_ [4] _[R]_ [2] [.] _\u03c1_ [.] (f) Show that _\u2225_ **w** _T_ +1 _\u2225\u2264\u2225_ **w** _t_ 0 _\u2225_ + [3] 4 _[M\u03c1]_ [. Conclude that] _[ M][ \u2264]_ [16] _[R]_ [2] _[/\u03c1]_ [2] [.] 8.7 Second-order regret bound. Consider the randomized algorithm that differs from the RWM algorithm only by the weight update, i.e., _w_ _t_ +1 _,i_ _\u2190_ (1 _\u2212_ (1 _\u2212\u03b2_ ) _l_ _t,i_ ) _w_ _t,i_, _t \u2208_ [ _T_ ], which is applied to all _i \u2208_ [ _N_ ] with 1 _/_ 2 _\u2264_ _\u03b2 <_ 1. This algorithm can be used in a more general setting than RWM since the losses _l_ _t,i_ are only assumed to be in [0 _,_ 1]. The objective of this problem is to show that a similar upper bound can be",
    "chunk_id": "foundations_machine_learning_203"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "shown for the regret. (a) Use the same potential _W_ _t_ as for the RWM algorithm and derive a simple upper bound for log _W_ _T_ +1 : log _W_ _T_ +1 _\u2264_ log _N \u2212_ (1 _\u2212_ _\u03b2_ ) _L_ _T_ _._ ( _Hint_ : Use the inequality log(1 _\u2212_ _x_ ) _\u2264\u2212x_ for _x \u2208_ [0 _,_ 1 _/_ 2].) (b) Prove the following lower bound for the potential for all _i \u2208_ [ _N_ ]: _T_ log _W_ _T_ +1 _\u2265\u2212_ (1 _\u2212_ _\u03b2_ ) _L_ _T,i_ _\u2212_ (1 _\u2212_ _\u03b2_ ) [2] \ufffd _l_ _t,i_ [2] _[.]_ _t_ =1 ( _Hint_ : Use the inequality log(1 _\u2212_ _x_ ) _\u2265\u2212x \u2212_ _x_ [2], which is valid for all _x \u2208_ [0 _,_ 1 _/_ 2].) (c) Use upper and lower bounds to derive the following regret bound for the algorithm: _R_ _T_ _\u2264_ 2 _[\u221a]_ _T_ log _N_ . 8.8 Polynomial weighted algorithm. The objective of this problem is to show how another regret minimization algorithm can be defined and studied. Let _L_ be a loss function convex in its first argument and taking values in [0 _, M_ ]. We will assume _N > e_ [2] and then for any expert _i \u2208_ [ _N_ ], we denote by _r_ _t,i_ the instantaneous regret of that expert at time _t \u2208_ [ _T_ ], _r_ _t,i_ = _L_ ( _y_ \ufffd _t_ _, y_ _t_ ) _\u2212_ _L_ ( _y_ _t,i_ _, y_ _t_ ), **210** **Chapter 8** **On-Line Learning** and by _R_ _t,i_ its cumulative regret up to time _t_ : _R_ _t,i_ = [\ufffd] _[t]_ _s_ =1 _[r]_ _[t,i]_ [. For conve-] nience, we also define _R_ 0 _,i_ = 0 for all _i \u2208_ [ _N_ ]. For any _x \u2208_ R, ( _x_ ) + denotes max( _x,_ 0), that is the positive part of _x_, and for **x** = ( _x_ 1 _, . . ., x_ _N_ ) _[\u22a4]_ _\u2208_ R _[N]_, ( **x** ) + = (( _x_ 1 ) + _, . . .,_ ( _x_ _N_ ) + ) _[\u22a4]_ . toLet \ufffd _y \u03b1 >_ _t_ = 2 and consider the algorithm that predicts at round \ufffd _ni_ =1 _n_ _[w]_ _[t][,][i]_ _[y]_ _[t][,][i]_ _[ w]_ _[t,i]_ [ defined based on the] _t \u2208_ _[ \u03b1]_ [ _T_ [th power of] ] according to \ufffd _y_ _t_ = \ufffd\ufffd _ni_ =1 _ni_ =1 _[w]_ _[t]_ _[w]_ _[,][i][t,i]_ _[y]_ _[t]_ [, with the weight] _[,][i]_ _[ w]_ _[t,i]_ [ defined based on the] _[ \u03b1]_ [th power of] the regret up to time ( _t \u2212_ 1): _w_ _t,i_ = ( _R_ _t\u2212_ 1 _,i_ ) _[\u03b1]_ + _[\u2212]_ [1] . The potential function we use to analyze the algorithm is based on the function \u03a6 defined over R _[N]_ by \u03a6: **x** _\ufffd\u2192\u2225_ ( **x** ) + _\u2225_ [2] _\u03b1_ [=] \ufffd\ufffd _Ni_ =1 [(] _[x]_ _[i]_ [)] _[\u03b1]_ + \ufffd _\u03b1_ [2] . _\u03b1_ . (a) Show that \u03a6 is twice differentiable over R _[N]_ _\u2212B_,",
    "chunk_id": "foundations_machine_learning_204"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "where _B_ is defined as follows: _B_ = _{_ **u** _\u2208_ R _[N]_ : ( **u** ) + = 0 _}._ (b) For any _t \u2208_ [ _T_ ], let **r** _t_ denote the vector of instantaneous regrets, **r** _t_ = ( _r_ _t,_ 1 _, . . ., r_ _t,N_ ) _[\u22a4]_, and similarly **R** _t_ = ( _R_ _t,_ 1 _, . . ., R_ _t,N_ ) _[\u22a4]_ . We define the potential function as \u03a6( **R** _t_ ) = _\u2225_ ( **R** _t_ ) + _\u2225_ [2] _\u03b1_ [. Compute] _[ \u2207]_ [\u03a6(] **[R]** _[t][\u2212]_ [1] [) for] **[ R]** _[t][\u2212]_ [1] _[\u0338\u2208]_ _[B]_ and show that _\u2207_ \u03a6( **R** _t\u2212_ 1 ) _\u00b7_ **r** _t_ _\u2264_ 0 ( _Hint_ : use the convexity of the loss with respect to the first argument). (c) Prove the inequality **r** _[\u22a4]_ [ _\u2207_ [2] \u03a6( **u** )] **r** _\u2264_ 2( _\u03b1 \u2212_ 1) _\u2225_ **r** _\u2225_ [2] _\u03b1_ [valid for all] **[ r]** _[ \u2208]_ [R] _[N]_ [ and] **u** _\u2208_ R _[N]_ _\u2212_ _B_ ( _Hint_ : write the Hessian _\u2207_ [2] \u03a6( **u** ) as a sum of a diagonal matrix and a positive semidefinite matrix multiplied by (2 _\u2212_ _\u03b1_ ). Also, use H\u00a8older\u2019s inequality generalizing Cauchy-Schwarz: for any _p >_ 1 and _q >_ 1 with _p_ 1 [+] [1] _q_ [= 1 and] **[ u]** _[,]_ **[ v]** _[ \u2208]_ [R] _[N]_ [,] _[ |]_ **[u]** _[ \u00b7]_ **[ v]** _[| \u2264\u2225]_ **[u]** _[\u2225]_ _[p]_ _[\u2225]_ **[v]** _[\u2225]_ _[q]_ [).] (d) Using the answers to the two previous questions and Taylor\u2019s formula, show that for all _t \u2265_ 1, \u03a6( **R** _t_ ) _\u2212_ \u03a6( **R** _t\u2212_ 1 ) _\u2264_ ( _\u03b1_ _\u2212_ 1) _\u2225_ **r** _t_ _\u2225_ [2] _\u03b1_ [, if] _[ \u03b3]_ **[R]** _[t][\u2212]_ [1] [+(1] _[\u2212]_ _[\u03b3]_ [)] **[R]** _[t]_ _[\u0338\u2208]_ _[B]_ for all _\u03b3 \u2208_ [0 _,_ 1]. (e) Suppose there exists _\u03b3 \u2208_ [0 _,_ 1] such that (1 _\u2212_ _\u03b3_ ) **R** _t\u2212_ 1 + _\u03b3_ **R** _t_ _\u2208_ _B_ . Show that \u03a6( **R** _t_ ) _\u2264_ ( _\u03b1 \u2212_ 1) _\u2225_ **r** _t_ _\u2225_ [2] _\u03b1_ [.] (f) Using the two previous questions, derive an upper bound on \u03a6( **R** _T_ ) expressed in terms of _T_, _N_, and _M_ . (g) Show that \u03a6( **R** _T_ ) admits as a lower bound the square of the regret _R_ _T_ of the algorithm. (h) Using the two previous questions give an upper bound on the regret _R_ _T_ . For what value of _\u03b1_ is the bound the most favorable? Give a simple expression of the upper bound on the regret for a suitable approximation of that optimal value. **8.7** **Exercises** **211** 8.9 General inequality. In this exercise we generalize the result of exercise 8.7 by using a more general inequality: log(1 _\u2212_ _x_ ) _\u2265\u2212x \u2212_ _[x]_ _\u03b1_ [2] [for some 0] _[ < \u03b1 <]_ [ 2.] (a) First prove that the inequality is true for _x \u2208_ [0 _,_ 1 _\u2212_ _[\u03b1]_ 2 []. What",
    "chunk_id": "foundations_machine_learning_205"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "does this] imply about the valid range of _\u03b2_ ? (b) Give a generalized version of the regret bound derived in exercise 8.7 in terms of _\u03b1_, which shows: [lo][g] _[ N]_ [1] _[ \u2212]_ _[\u03b2]_ 1 _\u2212_ _\u03b2_ [+] _\u03b1_ _R_ _T_ _\u2264_ [lo][g] _[ N]_ _T ._ _\u03b1_ What is the optimal choice of _\u03b2_ and the resulting bound in this case? (c) Explain how _\u03b1_ may act as a regularization parameter. What is the optimal choice of _\u03b1_ ? 8.10 On-line to batch \u2014 non-convex loss. The on-line to batch result of theorem 8.15 heavily relies on the fact that the loss is convex in order to provide a generalization guarantee for the uniformly averaged hypothesis _T_ [1] \ufffd _Ti_ =1 _[h]_ _[i]_ [. For general losses, instead of using the averaged] hypothesis we will use a different strategy and try to estimate the best single base hypothesis and show the expected loss of this hypothesis is bounded. Let _m_ _i_ denote the cumulative loss of hypothesis _h_ _i_ on the points ( _x_ _i_ _, . . ., x_ _T_ ), that is _m_ _i_ = [\ufffd] _[T]_ _t_ = _i_ _[L]_ [(] _[h]_ _[i]_ [(] _[x]_ _[t]_ [)] _[, y]_ _[t]_ [). Then we define the] _[ penalized risk estimate]_ [ of] hypothesis _h_ _i_ as, _m_ _i_ _T \u2212_ _i_ + 1 [+] _[ c]_ _[\u03b4]_ [(] _[T][ \u2212]_ _[i]_ [ + 1) where] _[ c]_ _[\u03b4]_ [(] _[x]_ [) =] ~~\ufffd~~ 1 2 _x_ [log] _[ T]_ [(] _[T]_ [ + 1] _\u03b4_ [)] _._ \ufffdThe term _c_ _\u03b4_ penalizes the empirical error when the test sample is small. Define _h_ = _h_ _i_ _\u2217_ where _i_ _[\u2217]_ = argmin _i_ _m_ _i_ _/_ ( _T \u2212_ _i_ + 1) + _c_ _\u03b4_ ( _T \u2212_ _i_ + 1). We will then show under the same conditions of theorem 8.15 (with _M_ = 1 for simplicity), but without requiring the convexity of _L_, that the following holds with probability at least 1 _\u2212_ _\u03b4_ : _R_ ( [\ufffd] _h_ ) _\u2264_ [1] _T_ _T_ \ufffd _L_ ( _h_ _i_ ( _x_ _i_ ) _, y_ _i_ ) + 6 _i_ =1 \ufffd 1 _._ (8.31) _T_ [log 2][(] _[T]_ [ + 1] _\u03b4_ [)] (a) Prove the following inequality: min _i\u2208_ [ _T_ ] [(] _[R]_ [(] _[h]_ _[i]_ [) + 2] _[c]_ _[\u03b4]_ [(] _[T][ \u2212]_ _[i]_ [ + 1))] _[ \u2264]_ _T_ [1] _T_ \ufffd _R_ ( _h_ _i_ ) + 4 _i_ =1 \ufffd 1 _T_ [log] _[ T]_ [ + 1] _\u03b4_ _._ **212** **Chapter 8** **On-Line Learning** (b) Use part (a) to show that with probability at least 1 _\u2212_ _\u03b4_, min _i\u2208_ [ _T_ ] [(] _[R]_ [(] _[h]_ _[i]_ [) + 2] _[c]_ _[\u03b4]_ [(] _[T][ \u2212]_ _[i]_ [ + 1))] ~~\ufffd~~ 1 _T_ [log] _[ T]_ [ + 1] _\u03b4_ _._ _<_ _T_ \ufffd _L_ ( _h_ _i_ ( _x_ _i_ ) _, y_ _i_ ) + _i_ =1 \ufffd 2 [1] _T_ [log] _\u03b4_ [+ 4]",
    "chunk_id": "foundations_machine_learning_206"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(c) By design, the definition of _c_ _\u03b4_ ensures that with probability at least 1 _\u2212_ _\u03b4_ _R_ ( [\ufffd] _h_ ) _\u2264_ min _i\u2208_ [ _T_ ] [(] _[R]_ [(] _[h]_ _[i]_ [) + 2] _[c]_ _[\u03b4]_ [(] _[T][ \u2212]_ _[i]_ [ + 1))] _[ .]_ Use this property to complete the proof of (8.31). 8.11 On-line to batch \u2014 kernel Perceptron margin bound. In this problem, we give a margin-based generalization guarantee for the kernel Perceptron algorithm. Let _h_ 1 _, . . ., h_ _T_ be the sequence of hypotheses generated by the kernel Perceptron algorithm and let [\ufffd] _h_ be defined as in exercise 8.10. Finally, let _L_ denote the \ufffdzero-one loss. We now wish to more precisely bound the generalization error of _h_ in this setting. (a) First, show that _T_ \ufffd _L_ ( _h_ _i_ ( _x_ _i_ ) _, y_ _i_ ) _\u2264_ _h\u2208_ H inf : _\u2225h\u2225\u2264_ 1 _i_ =1 _T_ \ufffd \ufffd max 0 _,_ 1 _\u2212_ _[y]_ _[i]_ _[h]_ [(] _[x]_ _[i]_ [)] _i_ =1 \ufffd _\u03c1_ _\u03c1_ _K_ ( _x_ _i_ _, x_ _i_ ) _,_ _i\u2208I_ \ufffd\ufffd + [1] \ufffd _\u03c1_ where _I_ is the set of indices where the kernel Perceptron makes an update and where _\u03b4_ and _\u03c1_ are defined as in theorem 8.12. (b) Now, use the result of exercise 8.10 to derive a generalization guarantee for\ufffd _h_ in the case of kernel Perceptron, which states that for any 0 _< \u03b4 \u2264_ 1, the following holds with probability at least 1 _\u2212_ _\u03b4_ : ~~\ufffd~~ _R_ ( [\ufffd] _h_ ) _\u2264_ inf _R_ \ufffd _S,\u03c1_ ( _h_ ) + [1] _h\u2208_ H: _\u2225h\u2225\u2264_ 1 _\u03c1T_ _K_ ( _x_ _i_ _, x_ _i_ ) + 6 _i\u2208I_ \ufffd\ufffd 1 _T_ [log 2][(] _[T]_ [ + 1] _\u03b4_ [)] _,_ _T_ [1] \ufffd _Ti_ =1 [max] \ufffd0 _,_ 1 _\u2212_ _[y]_ _[i]_ _[h]_ [(] _[x]_ _[i]_ [)] where _R_ [\ufffd] _S,\u03c1_ ( _h_ ) = _T_ [1] where _R_ _S,\u03c1_ ( _h_ ) = _T_ [1] \ufffd _i_ =1 [max] \ufffd0 _,_ 1 _\u2212_ _[y]_ _[i]_ _\u03c1_ _[x]_ _[i]_ \ufffd. Compare this result with the margin bounds for kernel-based hypotheses given by corollary 6.13. # 9 Multi-Class Classification The classification problems we examined in the previous chapters were all binary. However, in most real-world classification problems the number of classes is greater than two. The problem may consist of assigning a topic to a text document, a category to a speech utterance or a function to a biological sequence. In all of these tasks, the number of classes may be on the order of several hundred or more. In this chapter, we analyze the problem of multi-class classification. We first introduce the multi-class classification learning problem and discuss its multiple settings, and then derive generalization bounds for it using the notion of Rademacher complexity. Next, we describe and analyze a series of algorithms for tackling the multi-class classification problem. We will distinguish between two broad classes of algorithms: _uncombined algorithms_ that are specifically designed for the multiclass setting",
    "chunk_id": "foundations_machine_learning_207"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "such as multi-class SVMs, decision trees, or multi-class boosting, and _aggregated algorithms_ that are based on a reduction to binary classification and require training multiple binary classifiers. We will also briefly discuss the problem of structured prediction, which is a related problem arising in a variety of applications. **9.1** **Multi-class classification problem** Let X denote the input space and Y denote the output space, and let D be an unknown distribution over X according to which input points are drawn. We will distinguish between two cases: the _mono-label case_, where Y is a finite set of classes that we mark with numbers for convenience, Y = _{_ 1 _, . . ., k}_, and the _multi-label case_ where Y = _{\u2212_ 1 _,_ +1 _}_ _[k]_ . In the mono-label case, each example is labeled with a single class, while in the multi-label case it can be labeled with several. The latter can be illustrated by the case of text documents, which can be labeled with several different relevant topics, e.g., _sports_, _business_, and _society_ . The positive components of a vector in _{\u2212_ 1 _,_ +1 _}_ _[k]_ indicate the classes associated with an example. **214** **Chapter 9** **Multi-Class Classification** In either case, the learner receives a labeled sample _S_ = \ufffd( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )\ufffd _\u2208_ (X _\u00d7_ Y) _[m]_ with _x_ 1 _, . . ., x_ _m_ drawn i.i.d. according to D, and _y_ _i_ = _f_ ( _x_ _i_ ) for all _i \u2208_ [ _m_ ], where _f_ : X _\u2192_ Y is the target labeling function. Thus, we consider a deterministic scenario, which, as discussed in section 2.4.1, can be straightforwardly extended to a stochastic one that admits a distribution over X _\u00d7_ Y. Given a hypothesis set H of functions mapping X to Y, the multi-class classification problem consists of using the labeled sample _S_ to find a hypothesis _h \u2208_ H with small generalization error _R_ ( _h_ ) with respect to the target _f_ : _R_ ( _h_ ) = E mono-label case (9.1) _x\u223c_ D [[1] _[h]_ [(] _[x]_ [)] _[\u0338]_ [=] _[f]_ [(] _[x]_ [)] []] _\u0338_ _[\u0338]_ _R_ ( _h_ ) = _x\u223c_ E D _\u0338_ _[\u0338]_ _k_ \ufffd \ufffd 1 [ _h_ ( _x_ )] _l_ _\u0338_ =[ _f_ ( _x_ )] _l_ \ufffd multi-label case _._ (9.2) _l_ =1 _[\u0338]_ _\u0338_ The notion of _Hamming distance d_ _H_, that is, the number of corresponding components in two vectors that differ, can be used to give a common formulation for both errors: _R_ ( _h_ ) = E _d_ _H_ ( _h_ ( _x_ ) _, f_ ( _x_ )) _._ (9.3) _x\u223c_ D \ufffd \ufffd The empirical error of _h \u2208_ H is denoted by _R_ [\ufffd] _S_ ( _h_ ) and defined by _[\u0338]_ _\u0338_ \ufffd _R_ _S_ ( _h_ ) = [1] _m_ _[\u0338]_ _\u0338_ _m_ \ufffd _d_ _H_ ( _h_ ( _x_ _i_ ) _, y_ _i_ ) _._ (9.4) _i_",
    "chunk_id": "foundations_machine_learning_208"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "=1 _[\u0338]_ _\u0338_ Several issues, both computational and learning-related, often arise in the multiclass setting. Computationally, dealing with a large number of classes can be problematic. The number of classes _k_ directly enters the time complexity of the algorithms we will present. Even for a relatively small number of classes such as _k_ = 100 or _k_ = 1 _,_ 000, some techniques may become prohibitive to use in practice. This dependency is even more critical in the case where _k_ is very large or even infinite as in the case of some structured prediction problems. A learning-related issue that commonly appears in the multi-class setting is the existence of unbalanced classes. Some classes may be represented by less than 5 percent of the labeled sample, while others may dominate a very large fraction of the data. When separate binary classifiers are used to define the multi-class solution, we may need to train a classifier distinguishing between two classes with only a small representation in the training sample. This implies training on a small sample, with poor performance guarantees. Alternatively, when a large fraction of the training instances belong to one class, it may be tempting to propose a hypothesis always returning that class, since its generalization error as defined earlier is likely to be relatively low. However, this trivial solution is typically not the **9.2** **Generalization bounds** **215** one intended. Instead, the loss function may need to be reformulated by assigning different misclassification weights to each pair of classes. Another learning-related issue is the relationship between classes, which can be hierarchical. For example, in the case of document classification, the error of misclassifying a document dealing with _world politics_ as one dealing with _real estate_ should naturally be penalized more than the error of labeling a document with _sports_ instead of the more specific label _baseball_ . Thus, a more complex and more useful multi-class classification formulation would take into consideration the hierarchical relationships between classes and define the loss function in accordance with this hierarchy. More generally, there may be a graph relationship between classes as in the case of gene ontology in computational biology. The use of hierarchical relationships between classes leads to a richer and more complex multi-class classification problem. **9.2** **Generalization bounds** In this section, we present margin-based generalization bounds for multi-class classification in the mono-label case. In the binary setting, classifiers are often defined based on the sign of a scoring function. In the multi-class setting, a hypothesis is defined based on a scoring function _h_ : X _\u00d7_ Y _\u2192_ R. The label associated to point _x_ is the one resulting in the largest score _h_ ( _x, y_ ), which defines the following mapping from X to Y: _x \ufffd\u2192_ argmax _h_ ( _x, y_ ) _._ _y\u2208_ Y This naturally leads to the following definition of the _margin \u03c1_ _h_ ( _x, y_ ) _of the function_ _h at a labeled example_ ( _x, y_ ): _\u03c1_ _h_ ( _x, y_ ) = _h_ ( _x, y_ ) _\u2212_ max _y_ _[\u2032]_ =",
    "chunk_id": "foundations_machine_learning_209"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u0338_ _y_ _[h]_ [(] _[x, y]_ _[\u2032]_ [)] _[.]_ Thus, _h_ misclassifies ( _x, y_ ) iff _\u03c1_ _h_ ( _x, y_ ) _\u2264_ 0. For any _\u03c1 >_ 0, we can define the _empirical margin loss of a hypothesis h_ for multi-class classification as _\u0338_ \ufffd _R_ _S,\u03c1_ ( _h_ ) = [1] _m_ _\u0338_ _m_ \ufffd \u03a6 _\u03c1_ ( _\u03c1_ _h_ ( _x_ _i_ _, y_ _i_ )) _,_ (9.5) _i_ =1 _\u0338_ where \u03a6 _\u03c1_ is the margin loss function (definition 5.5). Thus, the empirical margin loss for multi-class classification is upper bounded by the fraction of the training points misclassified by _h_ or correctly classified but with confidence less than or **216** **Chapter 9** **Multi-Class Classification** equal to _\u03c1_ : \ufffd _R_ _S,\u03c1_ ( _h_ ) _\u2264_ [1] _m_ _m_ \ufffd 1 _\u03c1_ _h_ ( _x_ _i_ _,y_ _i_ ) _\u2264\u03c1_ _._ (9.6) _i_ =1 The following lemma will be used in the proof of the main result of this section. **Lemma 9.1** _Let F_ 1 _, . . ., F_ _l_ _be l hypothesis sets in_ R [X] _, l \u2265_ 1 _, and let G_ = _{_ max _{h_ 1 _, . . .,_ _h_ _l_ _}_ : _h_ _i_ _\u2208F_ _i_ _, i \u2208_ [ _l_ ] _}. Then, for any sample S of size m, the empirical Rademacher_ _complexity of G can be upper bounded as follows:_ \ufffd R _S_ ( _G_ ) _\u2264_ _l_ \ufffd \ufffd R _S_ ( _F_ _j_ ) _._ (9.7) _j_ =1 Proof: Let _S_ = ( _x_ 1 _, . . ., x_ _m_ ) be a sample of size _m_ . We first prove the result in the case _l_ = 2. By definition of the max operator, for any _h_ 1 _\u2208F_ 1 and _h_ 2 _\u2208F_ 2, max _{h_ 1 _, h_ 2 _}_ = [1] 2 [[] _[h]_ [1] [ +] _[ h]_ [2] [ +] _[ |][h]_ [1] _[ \u2212]_ _[h]_ [2] _[|]_ []] _[.]_ Thus, we can write: \ufffd R _S_ ( _G_ ) = [1] _m_ [E] _**\u03c3**_ sup \ufffd _h_ 1 _\u2208F_ 1 _h_ 2 _\u2208F_ 2 _m_ \ufffd _\u03c3_ _i_ max _{h_ 1 ( _x_ _i_ ) _, h_ 2 ( _x_ _i_ ) _}_ \ufffd _i_ =1 1 = 2 _m_ [E] _**\u03c3**_ sup \ufffd _h_ 1 _\u2208F_ 1 _h_ 2 _\u2208F_ 2 _m_ \ufffd _\u03c3_ _i_ \ufffd _h_ 1 ( _x_ _i_ ) + _h_ 2 ( _x_ _i_ ) + _|_ ( _h_ 1 _\u2212_ _h_ 2 )( _x_ _i_ ) _|_ \ufffd [\ufffd] _i_ =1 _\u2264_ [1] \ufffd [1] R _S_ ( _F_ 1 ) + [1] 2 2 \ufffd [1] R _S_ ( _F_ 2 ) + [1] 2 2 2 _m_ [E] _**\u03c3**_ _m_ \ufffd _\u03c3_ _i_ _|_ ( _h_ 1 _\u2212_ _h_ 2 )( _x_ _i_ ) _|_ \ufffd _,_ (9.8) _i_ =1 sup \ufffd _h_ 1 _\u2208F_ 1 _h_ 2 _\u2208F_ 2 using the sub-additivity of sup. Since _x \ufffd\u2192|x|_ is 1-Lipschitz, by Talagrand\u2019s lemma (lemma 5.7), the last term can",
    "chunk_id": "foundations_machine_learning_210"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "be bounded as follows sup \ufffd _h_ 1 _\u2208F_ 1 _h_ 2 _\u2208F_ 2 1 2 _m_ [E] _**\u03c3**_ sup \ufffd _h_ 1 _\u2208F_ 1 _h_ 2 _\u2208F_ 2 _m_ 1 \ufffd _\u03c3_ _i_ _|_ ( _h_ 1 _\u2212_ _h_ 2 )( _x_ _i_ ) _|_ \ufffd _\u2264_ 2 _m_ [E] _**\u03c3**_ _i_ =1 _m_ \ufffd _\u03c3_ _i_ ( _h_ 1 _\u2212_ _h_ 2 )( _x_ _i_ )\ufffd _i_ =1 _\u2264_ [1] \ufffd [1] R _S_ ( _F_ 1 ) + [1] 2 2 2 _m_ [E] _**\u03c3**_ _m_ \ufffd _\u2212\u03c3_ _i_ _h_ 2 ( _x_ _i_ )\ufffd _i_ =1 sup \ufffd _h_ 2 _\u2208F_ 2 = [1] \ufffd [1] R _S_ ( _F_ 1 ) + [1] 2 2 \ufffd R _S_ ( _F_ 2 ) _,_ (9.9) 2 where we again use the sub-additivity of sup for the second inequality and the fact that _\u03c3_ _i_ and _\u2212\u03c3_ _i_ have the same distribution for any _i \u2208_ [ _m_ ] for the last equality. Combining (9.8) and (9.9) yields R [\ufffd] _S_ ( _G_ ) _\u2264_ R [\ufffd] _S_ ( _F_ 1 )+ R [\ufffd] _S_ ( _F_ 2 ). The general case can be derived from the case _l_ = 2 using max _{h_ 1 _, . . ., h_ _l_ _}_ = max _{h_ 1 _,_ max _{h_ 2 _, . . ., h_ _l_ _}}_ and an immediate recurrence. **9.2** **Generalization bounds** **217** For any family of hypotheses mapping X _\u00d7_ Y to R, we define \u03a0 1 (H) by \u03a0 1 (H) = _{x \ufffd\u2192_ _h_ ( _x, y_ ): _y \u2208_ Y _, h \u2208_ H _}._ The following theorem gives a general margin bound for multi-class classification. **Theorem 9.2 (Margin bound for multi-class classification)** _Let_ H _\u2286_ R [X] _[\u00d7]_ [Y] _be a hypo-_ _thesis set with_ Y = _{_ 1 _, . . ., k}. Fix \u03c1 >_ 0 _. Then, for any \u03b4 >_ 0 _, with probability at_ _least_ 1 _\u2212_ _\u03b4, the following multi-class classification generalization bound holds for all_ _h \u2208_ H _:_ _\u0338_ _\u0338_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [4] _[k]_ _\u03c1_ [R] _[m]_ [(\u03a0] [1] [(][H][)) +] _\u0338_ _\u0338_ \ufffd _\u0338_ _\u0338_ log [1] _\u03b4_ (9.10) 2 _m_ _[.]_ _\u0338_ _\u0338_ Proof: We will need the following definition for this proof: _\u03c1_ _\u03b8,h_ ( _x, y_ ) = min _y_ _[\u2032]_ [ (] _[h]_ [(] _[x, y]_ [)] _[ \u2212]_ _[h]_ [(] _[x, y]_ _[\u2032]_ [) +] _[ \u03b8]_ [1] _[y]_ _[\u2032]_ [=] _[y]_ [)] _[,]_ where _\u03b8 >_ 0 is an arbitrary constant. Observe that E[1 _\u03c1_ _h_ ( _x,y_ ) _\u2264_ 0 ] _\u2264_ E[1 _\u03c1_ _\u03b8,h_ ( _x,y_ ) _\u2264_ 0 ] since the inequality _\u03c1_ _\u03b8,h_ ( _x, y_ ) _\u2264_ _\u03c1_ _h_ ( _x, y_ ) holds for all ( _x, y_ ) _\u2208_ X _\u00d7_ Y: _\u03c1_ _\u03b8,h_ ( _x, y_ ) = min \ufffd _h_ ( _x, y_ ) _\u2212_ _h_ ( _x, y_ _[\u2032]_ ) + _\u03b8_ 1 _y_ _\u2032_ = _y_",
    "chunk_id": "foundations_machine_learning_211"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd _y_ _[\u2032]_ _\u2264_ _y_ min _[\u2032]_ = _\u0338_ _y_ \ufffd _h_ ( _x, y_ ) _\u2212_ _h_ ( _x, y_ _[\u2032]_ ) + _\u03b8_ 1 _y_ _\u2032_ = _y_ \ufffd = min _y_ _[\u2032]_ = _\u0338_ _y_ \ufffd _h_ ( _x, y_ ) _\u2212_ _h_ ( _x, y_ _[\u2032]_ )\ufffd = _\u03c1_ _h_ ( _x, y_ ) _,_ where the inequality follows from taking the minimum over a smaller set. Now, similar to the proof of theorem 5.8, let H [\ufffd] = _{_ ( _x, y_ ) _\ufffd\u2192_ _\u03c1_ _\u03b8,h_ ( _x, y_ ): _h \u2208_ H _}_ and _H_ [\ufffd] = _{_ \u03a6 _\u03c1_ _\u25e6_ [\ufffd] _h_ : [\ufffd] _h \u2208_ H [\ufffd] _}_ . By theorem 3.3, with probability at least 1 _\u2212_ _\u03b4_, for all _h \u2208_ H, _\u0338_ _\u0338_ \ufffd _\u0338_ _\u0338_ log [1] _\u03b4_ 2 _m_ _[.]_ _\u0338_ _\u0338_ E \ufffd\u03a6 _\u03c1_ ( _\u03c1_ _\u03b8,h_ ( _x, y_ ))\ufffd _\u2264_ [1] _m_ _\u0338_ _\u0338_ _m_ \ufffd \u03a6 _\u03c1_ ( _\u03c1_ _\u03b8,h_ ( _x_ _i_ _, y_ _i_ )) + 2R _m_ ( _H_ [\ufffd] ) + _i_ =1 _\u0338_ _\u0338_ Since 1 _u\u2264_ 0 _\u2264_ \u03a6 _\u03c1_ ( _u_ ) for all _u \u2208_ R, the generalization error _R_ ( _h_ ) is a lower bound on the left-hand side, _R_ ( _h_ ) = E[1 _\u03c1_ _h_ ( _x,y_ ) _\u2264_ 0 ] _\u2264_ E[1 _\u03c1_ _\u03b8,h_ ( _x,y_ ) _\u2264_ 0 ] _\u2264_ E \ufffd\u03a6 _\u03c1_ ( _\u03c1_ _\u03b8,h_ ( _x, y_ ))\ufffd, and we can write: _\u0338_ _\u0338_ ~~\ufffd~~ _\u0338_ _\u0338_ log [1] _\u03b4_ 2 _m_ _[.]_ _\u0338_ _\u0338_ _R_ ( _h_ ) _\u2264_ [1] _m_ _\u0338_ _\u0338_ _m_ \ufffd \u03a6 _\u03c1_ ( _\u03c1_ _\u03b8,h_ ( _x_ _i_ _, y_ _i_ )) + 2R _m_ ( _H_ [\ufffd] ) + _i_ =1 _\u0338_ _\u0338_ Fixing _\u03b8_ = 2 _\u03c1_, we observe that \u03a6 _\u03c1_ ( _\u03c1_ _\u03b8,h_ ( _x_ _i_ _, y_ _i_ )) = \u03a6 _\u03c1_ ( _\u03c1_ _h_ ( _x_ _i_ _, y_ _i_ )). Indeed, either _\u03c1_ _\u03b8,h_ ( _x_ _i_ _, y_ _i_ ) = _\u03c1_ _h_ ( _x_ _i_ _, y_ _i_ ) or _\u03c1_ _\u03b8,h_ ( _x_ _i_ _, y_ _i_ ) = 2 _\u03c1 \u2264_ _\u03c1_ _h_ ( _x_ _i_ _, y_ _i_ ), which implies the desired result. Furthermore, Talagrand\u2019s lemma (lemma 5.7) yields R _m_ ( _H_ [\ufffd] ) _\u2264_ _\u03c1_ 1 [R] _[m]_ [(][H][ \ufffd] [)] since \u03a6 _\u03c1_ is a _\u03c1_ [1] [-Lipschitz function. Therefore, for any] _[ \u03b4 >]_ [ 0, with probability at] **218** **Chapter 9** **Multi-Class Classification** least 1 _\u2212_ _\u03b4_, for all _h \u2208_ H: _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] _\u03c1_ [R] _[m]_ [(][H][ \ufffd] [) +] \ufffd log [1] _\u03b4_ 2 _m_ _[.]_ and to complete the proof it suffices to show that R _m_ (H [\ufffd] ) _\u2264_ 2 _k_ R _m_ (\u03a0 1 (H)). Here R _m_ (H [\ufffd] ) can be upper-bounded as follows: \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ _i_ ) +",
    "chunk_id": "foundations_machine_learning_212"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[1] _i_ =1 \ufffd R _m_ (H [\ufffd] ) = _m_ [1] _S,\u03c3_ [E] _m_ \ufffd _i_ =1 _\u03c3_ _i_ ( _h_ ( _x_ _i_ _, y_ _i_ ) _\u2212_ max _y_ [(] _[h]_ [(] _[x]_ _[i]_ _[, y]_ [)] _[ \u2212]_ [2] _[\u03c1]_ [1] _[y]_ [=] _[y]_ _[i]_ [))] \ufffd _m_ _S,\u03c3_ [E] _m_ \ufffd _i_ =1 _\u03c3_ _i_ max _y_ [(] _[h]_ [(] _[x]_ _[i]_ _[, y]_ [)] _[ \u2212]_ [2] _[\u03c1]_ [1] _[y]_ [=] _[y]_ _[i]_ [)] \ufffd _._ _\u2264_ [1] _m_ _S,\u03c3_ [E] sup \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H _m_ \ufffd sup \ufffd _h\u2208_ H Now we bound the first term above. Observe that _m_ [E] _\u03c3_ _m_ \ufffd \ufffd _i_ =1 _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ _i_ )\ufffd = _m_ [1] _m_ \ufffd _i_ =1 \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ )1 _y_ _i_ = _y_ _y\u2208_ Y 1 _m_ [E] _\u03c3_ sup \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H \ufffd \ufffd \ufffd _\u2264_ [1] _m_ \ufffd E _\u03c3_ _y\u2208_ Y _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ )1 _y_ _i_ = _y_ _i_ =1 _m_ \ufffd _i_ 2 [+ 1] 2 2 sup \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H _m_ \ufffd _\u03f5_ _i_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ ) _i_ =1 \ufffd = \ufffd _y\u2208_ Y 1 _m_ [E] _\u03c3_ _,_ \ufffd\ufffd where _\u03f5_ _i_ = 2 _\u00b7_ 1 _y_ _i_ = _y_ _\u2212_ 1. Since _\u03f5_ _i_ _\u2208{\u2212_ 1 _,_ +1 _}_, we have that _\u03c3_ _i_ and _\u03c3_ _i_ _\u03f5_ _i_ admit the same distribution and, for any _y \u2208_ Y, each of the terms of the right-hand side can be bounded as follows: _i_ 2 [+ 1] 2 2 _m_ \ufffd _\u03f5_ _i_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ )\ufffd _i_ =1 1 _m_ [E] _\u03c3_ sup \ufffd _h\u2208_ H \ufffd [\ufffd] 2 _m_ [E] _\u03c3_ _m_ \ufffd \ufffd _i_ =1 _\u03c3_ _i_ _\u03f5_ _i_ _h_ ( _x_ _i_ _, y_ )\ufffd + 2 [1] sup \ufffd _h\u2208_ H _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ ) _i_ =1 \ufffd 1 _\u2264_ 2 _m_ [E] _\u03c3_ sup \ufffd _h\u2208_ H _\u2264_ R [\ufffd] _m_ (\u03a0 1 (H)) _._ Thus, we can write _m_ 1 [E] _[S,\u03c3]_ \ufffd sup _h\u2208_ H \ufffd _mi_ =1 _[\u03c3]_ _[i]_ _[h]_ [(] _[x]_ _[i]_ _[, y]_ _[i]_ [)] \ufffd _\u2264_ _k_ R _m_ (\u03a0 1 (H)). To bound the second term, we first apply lemma 9.1 which immediately yields that _m_ \ufffd _i_ =1 _\u03c3_ _i_ max _y_ [(] _[h]_ [(] _[x]_ _[i]_ _[, y]_ [)] _[ \u2212]_ [2] _[\u03c1]_ [1] _[y]_ [=] _[y]_ _[i]_ [)] \ufffd 1 _m_ _S,\u03c3_ [E] sup \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H _m_ \ufffd _\u03c3_ _i_ ( _h_ ( _x_ _i_ _, y_ ) _\u2212_ 2 _\u03c1_ 1 _y_ = _y_ _i_ ) _i_ =1 \ufffd _m_ \ufffd _\u2264_ \ufffd _y\u2208_ Y 1 _m_ _S,\u03c3_ [E] **9.2** **Generalization bounds** **219** and since Rademacher variables are mean zero, we observe",
    "chunk_id": "foundations_machine_learning_213"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "that sup \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H _m_ \ufffd _\u03c3_ _i_ 1 _y_ = _y_ _i_ _i_ =1 _m_ \ufffd _\u03c3_ _i_ ( _h_ ( _x_ _i_ _, y_ ) _\u2212_ 2 _\u03c1_ 1 _y_ = _y_ _i_ ) = E _i_ =1 \ufffd _S,\u03c3_ = E _S,\u03c3_ _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ ) _\u2212_ 2 _\u03c1_ \ufffd _i_ =1 \ufffd _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ _, y_ ) _\u2264_ R _m_ (\u03a0 1 (H)) _i_ =1 \ufffd \ufffd E _S,\u03c3_ sup \ufffd _h\u2208_ H which completes the proof. These bounds can be generalized to hold uniformly for all _\u03c1 >_ 0 at the cost of an additional term \ufffd(log log 2 (2 _/\u03c1_ )) _/m_, as in theorem 5.9 and exercise 5.2. As for other margin bounds presented in previous sections, they show the conflict between two terms: the larger the desired pairwise ranking margin _\u03c1_, the smaller the middle term, at the price of a larger empirical multi-class classification margin loss _R_ [\ufffd] _S,\u03c1_ . Note, however, that here there is additionally a dependency on the number of classes _k_ . This suggests either weaker guarantees when learning with a large number of classes or the need for even larger margins _\u03c1_ for which the empirical margin loss would be small. For some hypothesis sets, a simple upper bound can be derived for the Rademacher complexity of \u03a0 1 (H), thereby making theorem 9.2 more explicit. We will show this for kernel-based hypotheses. Let _K_ : X _\u00d7_ X _\u2192_ R be a PDS kernel and let **\u03a6** : X _\u2192_ H be a feature mapping associated to _K_ . In multi-class classification, a kernel-based hypothesis is based on _k_ weight vectors **w** 1 _, . . .,_ **w** _k_ _\u2208_ H. Each weight vector **w** _l_, _l \u2208_ [ _k_ ], defines a scoring function _x \ufffd\u2192_ **w** _l_ _\u00b7_ **\u03a6** ( _x_ ) and the class associated to point _x \u2208_ X is given by argmax **w** _y_ _\u00b7_ **\u03a6** ( _x_ ) _._ _y\u2208_ Y We denote by **W** the matrix formed by these weight vectors: **W** = ( **w** 1 _, . . .,_ **w** _k_ ) _[\u22a4]_ and for any _p \u2265_ 1 denote by _\u2225_ **W** _\u2225_ H _,p_ the _L_ H _,p_ group norm of **W** defined by _k_ 1 _/p_ _\u2225_ **W** _\u2225_ H _,p_ = \ufffd \ufffd _\u2225_ **w** _l_ _\u2225_ _[p]_ H \ufffd _._ _l_ =1 For any _p \u2265_ 1, the family of kernel-based hypotheses we will consider is [13] H _K,p_ = _{_ ( _x, y_ ) _\u2208_ X _\u00d7 {_ 1 _, . . ., k} \ufffd\u2192_ **w** _y_ _\u00b7_ **\u03a6** ( _x_ ): **W** = ( **w** 1 _, . . .,_ **w** _k_ ) _[\u22a4]_ _, \u2225_ **W** _\u2225_ H _,p_ _\u2264_ \u039b _}._ **Proposition 9.3 (Rademacher complexity of multi-class kernel-based hypotheses)** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS kernel and let_ **\u03a6** : X _\u2192_ H _be a feature mapping",
    "chunk_id": "foundations_machine_learning_214"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "associated to_ 13 The hypothesis set H can also be defined via H = _{h \u2208_ R X _\u00d7_ Y : _h_ ( _\u00b7, y_ ) _\u2208_ H _\u2227\u2225h\u2225_ _K,p_ _\u2264_ \u039b _}_, where _\u2225h\u2225_ _K,p_ = \ufffd\ufffd _ky_ =1 _[\u2225][h]_ [(] _[\u00b7][, y]_ [)] _[\u2225]_ _[p]_ H \ufffd 1 _/p_, without referring to a feature mapping for _K_ . **220** **Chapter 9** **Multi-Class Classification** _K. Assume that there exists r >_ 0 _such that K_ ( _x, x_ ) _\u2264_ _r_ [2] _for all x \u2208_ X _. Then, for_ _any m \u2265_ 1 _,_ R _m_ (\u03a0 1 (H _K,p_ )) _can be bounded as follows:_ R _m_ (\u03a0 1 (H _K,p_ )) _\u2264_ \ufffd _r_ [2] \u039b [2] _m_ _[.]_ Proof: Let _S_ = ( _x_ 1 _, . . ., x_ _m_ ) denote a sample of size _m_ . Observe that for all _k_ 1 _/p_ _l \u2208_ [ _k_ ], the inequality _\u2225_ **w** _l_ _\u2225_ H _\u2264_ \ufffd\ufffd _l_ =1 _[\u2225]_ **[w]** _[l]_ _[\u2225]_ H _[p]_ \ufffd = _\u2225_ **W** _\u2225_ H _,p_ holds. Thus, the condition _\u2225_ **W** _\u2225_ H _,p_ _\u2264_ \u039b implies that _\u2225_ **w** _l_ _\u2225_ H _\u2264_ \u039b for all _l \u2208_ [ _k_ ]. In view of that, the Rademacher complexity of the hypothesis set \u03a0 1 (H _K,p_ ) can be expressed and bounded as follows: R _m_ (\u03a0 1 (H _K,p_ )) = [1] _m_ _S,_ [E] _**\u03c3**_ **w** _y_ _,_ \ufffd _m_ \ufffd _\u03c3_ _i_ **\u03a6** ( _x_ _i_ )\ufffd [\ufffd] _i_ =1 _\u2264_ [1] _m_ _S,_ [E] _**\u03c3**_ sup \ufffd _y\u2208_ Y _\u2225_ **W** _\u2225\u2264_ \u039b sup \ufffd _y\u2208_ Y _\u2225_ **W** _\u2225\u2264_ \u039b _m_ _\u2225_ **w** _y_ _\u2225_ H \ufffd\ufffd\ufffd \ufffd _\u03c3_ _i_ \u03a6( _x_ _i_ )\ufffd\ufffd\ufffd H _i_ =1 (Cauchy-Schwarz ineq. ) \ufffd _m_ \ufffd _i_ =1 _\u03c3_ _i_ \u03a6( _x_ _i_ )\ufffd\ufffd\ufffd H \ufffd _\u2264_ [\u039b] _m_ _S,_ [E] _**\u03c3**_ \ufffd\ufffd\ufffd\ufffd H (Jensen\u2019s inequality) \ufffd [\ufffd] [1] _[/]_ [2] \ufffd \ufffd _[m]_ _K_ ( _x_ _i_ _, x_ _i_ )\ufffd [\ufffd] [1] _[/]_ [2] _i_ =1 _\u2264_ [\u039b] _m_ = [\u039b] _m_ = [\u039b] _m_ E \ufffd _S,_ _**\u03c3**_ E \ufffd _S,_ _**\u03c3**_ E \ufffd _S,_ _**\u03c3**_ \ufffd\ufffd\ufffd\ufffd _m_ \ufffd 2 \ufffd _\u03c3_ _i_ \u03a6( _x_ _i_ )\ufffd\ufffd\ufffd H _i_ =1 \ufffd \ufffd _[m]_ _\u2225_ \u03a6( _x_ _i_ ) _\u2225_ H [2] \ufffd [\ufffd] [1] _[/]_ [2] ( _i \u0338_ = _j \u21d2_ E _**\u03c3**_ [[] _[\u03c3]_ _[i]_ _[\u03c3]_ _[j]_ [] = 0)] _i_ =1 _\u221a_ _\u2264_ [\u039b] _mr_ [2] = _m_ \ufffd _r_ [2] \u039b [2] _m_ _[,]_ which concludes the proof. Combining theorem 9.2 and proposition 9.3 yields directly the following result. **Corollary 9.4 (Margin bound for multi-class classification with kernel-based hypotheses)** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS kernel and let_ **\u03a6** : X _\u2192_ H _be a feature mapping associ-_ _ated to K. Assume that there exists r >_ 0 _such that K_ ( _x, x_ ) _\u2264_ _r_ [2] _for all x \u2208_ X _. Fix_ _\u03c1 >_ 0 _. Then, for any",
    "chunk_id": "foundations_machine_learning_215"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, the following multi-class_ _classification generalization bound holds for all h \u2208_ H _K,p_ _:_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 4 _k_ ~~\ufffd~~ _r_ [2] \u039b [2] _/\u03c1_ [2] + _m_ _r_ [2] \u039b [2] _/\u03c1_ [2] \ufffd log [1] _\u03b4_ (9.11) 2 _m_ _[.]_ **9.3** **Uncombined multi-class algorithms** **221** In the next two sections, we describe multi-class classification algorithms that belong to two distinct families: _uncombined algorithms_, which are defined by a single optimization problem, and _aggregated algorithms_, which are obtained by training multiple binary classifications and by combining their outputs. **9.3** **Uncombined multi-class algorithms** In this section, we describe three algorithms designed specifically for multi-class classification. We start with a multi-class version of SVMs, then describe a boostingtype multi-class algorithm, and conclude with _decision trees_, which are often used as base learners in boosting. **9.3.1** **Multi-class SVMs** We describe an algorithm that can be derived directly from the theoretical guarantees presented in the previous section. Proceeding as in section 5.4 for classification, the guarantee of corollary 9.4 can be expressed as follows: for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, for all _h \u2208_ H _K,_ 2 = _{_ ( _x, y_ ) _\u2192_ **w** _y_ _\u00b7_ **\u03a6** ( _x_ ): **W** = ( **w** 1 _, . . .,_ **w** _k_ ) _[\u22a4]_ _,_ [\ufffd] _[k]_ _l_ =1 _[\u2225]_ **[w]** _[l]_ _[\u2225]_ [2] _[ \u2264]_ [\u039b] [2] _[}]_ [,] _\u0338_ \ufffd _\u0338_ \ufffd _\u0338_ log [1] _\u03b4_ (9.12) 2 _m_ _[,]_ _\u0338_ _R_ ( _h_ ) _\u2264_ [1] _m_ _\u0338_ _m_ \ufffd _\u03be_ _i_ + 4 _k_ _i_ =1 _\u0338_ _r_ [2] \u039b [2] + _m_ _\u0338_ where _\u03be_ _i_ = max \ufffd1 _\u2212_ [ **w** _y_ _i_ _\u00b7_ **\u03a6** ( _x_ _i_ ) _\u2212_ max _y_ _\u2032_ = _\u0338_ _y_ _i_ **w** _y_ _\u2032_ _\u00b7_ **\u03a6** ( _x_ _i_ )] _,_ 0\ufffd for all _i \u2208_ [ _m_ ]. An algorithm based on this theoretical guarantee consists of minimizing the righthand side of (9.12), that is, minimizing an objective function with a term corresponding to the sum of the slack variables _\u03be_ _i_, and another one minimizing _\u2225_ **W** _\u2225_ H _,_ 2 or equivalently [\ufffd] _[k]_ _l_ =1 _[\u2225]_ **[w]** _[l]_ _[\u2225]_ [2] [. This is precisely the optimization problem defining] the _multi-class SVM_ algorithm: _\u0338_ 1 min **W** _,_ _**\u03be**_ 2 _\u0338_ _k_ \ufffd _\u2225_ **w** _l_ _\u2225_ [2] + _C_ _l_ =1 _\u0338_ _m_ \ufffd _\u03be_ _i_ _i_ =1 _\u0338_ subject to: _\u2200i \u2208_ [ _m_ ] _, \u2200l \u2208_ Y _\u2212{y_ _i_ _},_ **w** _y_ _i_ _\u00b7_ **\u03a6** ( _x_ _i_ ) _\u2265_ **w** _l_ _\u00b7_ **\u03a6** ( _x_ _i_ ) + 1 _\u2212_ _\u03be_ _i_ _,_ _\u03be_ _i_ _\u2265_ 0 _._ The decision function learned is of the form _x \ufffd\u2192_ argmax _l\u2208_ Y **w** _l_ _\u00b7_ **\u03a6** ( _x_ ) _._ As with the primal problem of SVMs, this is a convex optimization problem: the objective function is convex, since it",
    "chunk_id": "foundations_machine_learning_216"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "is a sum of convex functions, and the constraints are affine and thus qualified. The objective and constraint functions are differentiable, and the KKT conditions hold at the optimum. Defining the Lagrangian and applying **222** **Chapter 9** **Multi-Class Classification** these conditions leads to the equivalent dual optimization problem, which can be expressed in terms of the kernel function _K_ alone: _\u0338_ _\u0338_ 2 _\u0338_ _\u0338_ max _**\u03b1**_ _\u2208_ R _[m][\u00d7][k]_ _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ \ufffd _**\u03b1**_ _i_ _\u00b7_ **e** _y_ _i_ _\u2212_ [1] 2 _i_ =1 _\u0338_ _\u0338_ _m_ \ufffd( _**\u03b1**_ _i_ _\u00b7_ _**\u03b1**_ _j_ ) _K_ ( _x_ _i_ _, x_ _j_ ) _i_ =1 _\u0338_ _\u0338_ subject to: _\u2200i \u2208_ [ _m_ ] _,_ (0 _\u2264_ _\u03b1_ _iy_ _i_ _\u2264_ _C_ ) _\u2227_ ( _\u2200j \u0338_ = _y_ _i_ _, \u03b1_ _ij_ _\u2264_ 0) _\u2227_ ( _**\u03b1**_ _i_ _\u00b7_ **1** = 0) _._ Here, _**\u03b1**_ _\u2208_ R _[m][\u00d7][k]_ is a matrix, _**\u03b1**_ _i_ denotes the _i_ th row of _**\u03b1**_, and **e** _l_ the _l_ th unit vector in R _[k]_, _l \u2208_ [ _k_ ]. Both the primal and dual problems are simple QPs generalizing those of the standard SVM algorithm. However, the size of the solution and the number of constraints for both problems is in \u2126( _mk_ ), which, for a large number of classes _k_, can make it difficult to solve. However, there exist specific optimization solutions designed for this problem based on a decomposition of the problem into _m_ disjoint sets of constraints. **9.3.2** **Multi-class boosting algorithms** We describe a boosting algorithm for multi-class classification called _AdaBoost.MH_, which in fact coincides with a special instance of AdaBoost. An alternative multiclass classification algorithm based on similar boosting ideas, AdaBoost.MR, is described and analyzed in exercise 9.4. AdaBoost.MH applies to the multi-label setting where Y = _{\u2212_ 1 _,_ +1 _}_ _[k]_ . As in the binary case, it returns a convex combination of base classifiers selected from a hypothesis set H = _{h_ 1 _, . . ., h_ _N_ _}_ . Let _F_ be the following objective function defined for all samples _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_ and \u00af _**\u03b1**_ = (\u00af _\u03b1_ 1 _, . . .,_ \u00af _\u03b1_ _N_ ) _\u2208_ R _[N]_, _N \u2265_ 1, by _\u0338_ _\u0338_ _k_ \ufffd _e_ _[\u2212][y]_ _[i]_ [[] _[l]_ []][ \ufffd] _j_ _[N]_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ _[,l]_ [)] _,_ (9.13) _l_ =1 _\u0338_ _\u0338_ _k_ \ufffd _\u0338_ _\u0338_ _k_ \ufffd _e_ _[\u2212][y]_ _[i]_ [[] _[l]_ []] _[f]_ _[N]_ [(] _[x]_ _[i]_ _[,l]_ [)] = _l_ =1 _\u0338_ _\u0338_ _m_ \ufffd _i_ =1 _\u0338_ _\u0338_ _F_ (\u00af _**\u03b1**_ ) = _\u0338_ _\u0338_ _m_ \ufffd _i_ =1 _\u0338_ _\u0338_ where _f_ _N_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [ and where] _[ y]_ _[i]_ [[] _[l]_ [] denotes the] _[ l]_ [th coordinate of] _[ y]_ _[i]_ [ for any] _i \u2208_ [ _m_ ] and",
    "chunk_id": "foundations_machine_learning_217"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_l \u2208_ [ _k_ ]. _F_ is a convex and differentiable upper bound on the multi-class multi-label loss: _\u0338_ _\u0338_ _k_ _\u0338_ \ufffd _e_ _[\u2212][y]_ _[i]_ [[] _[l]_ []] _[f]_ _[N]_ [(] _[x]_ _[i]_ _[,l]_ [)] _,_ (9.14) _l_ =1 _\u0338_ _k_ _\u0338_ \ufffd _\u0338_ _m_ \ufffd _\u0338_ _i_ =1 _\u0338_ _k_ \ufffd 1 _y_ _i_ [ _l_ ]= _\u0338_ _f_ _N_ ( _x_ _i_ _,l_ ) _\u2264_ _l_ =1 _\u0338_ _m_ _\u0338_ \ufffd _i_ =1 _\u0338_ _\u0338_ since for any _x \u2208_ X with label _y_ = _f_ ( _x_ ) and any _l \u2208_ [ _k_ ], the inequality 1 _y_ [ _l_ ]= _\u0338_ _f_ _N_ ( _x,l_ ) _\u2264_ _e_ _[\u2212][y]_ [[] _[l]_ []] _[f]_ _[N]_ [(] _[x,l]_ [)] holds. Using the same arguments as in section 7.2.2, we see that AdaBoost.MH coincides exactly with the application of coordinate descent to the objective function _F_ . Figure 9.1 gives the pseudocode of the algorithm in the case where the base classifiers are functions mapping from X _\u00d7_ Y to _{\u2212_ 1 _,_ +1 _}_ . The algorithm takes as input a labeled sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_ and maintains a distribution D _t_ over _{_ 1 _, . . ., m} \u00d7_ Y. The remaining details of the **9.3** **Uncombined multi-class algorithms** **223** AdaBoost.MH( _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ ))) 1 **for** _i \u2190_ 1 **to** _m_ **do** 2 **for** _l \u2190_ 1 **to** _k_ **do** 3 D 1 ( _i, l_ ) _\u2190_ _mk_ 1 4 **for** _j \u2190_ 1 **to** _N_ **do** 5 _h_ _j_ _\u2190_ base classifier in H with small error _\u03f5_ _j_ = P ( _i,l_ ) _\u223c_ D _j_ [ _h_ _j_ ( _x_ _i_ _, l_ ) _\u0338_ = _y_ _i_ [ _l_ ]] [1] 2 [log] [ 1] _[\u2212]_ _\u03f5_ _[\u03f5]_ _[j]_ 6 _\u03b1_ \u00af _j_ _\u2190_ [1] 2 _\u03f5_ _j_ 1 7 _Z_ _t_ _\u2190_ 2[ _\u03f5_ _j_ (1 _\u2212_ _\u03f5_ _j_ )] 2 _\u25b7_ normalization factor 8 **for** _i \u2190_ 1 **to** _m_ **do** 9 **for** _l \u2190_ 1 **to** _k_ **do** 10 D _j_ +1 ( _i, l_ ) _\u2190_ [D] _[j]_ [(] _[i][,][l]_ [)][ ex][p(] _[\u2212]_ _Z_ _[\u03b1]_ [\u00af] _j_ _[j]_ _[y]_ _[i]_ [[] _[l]_ []] _[h]_ _[j]_ [(] _[x]_ _[i]_ _[,][l]_ [))] 11 _f_ _N_ _\u2190_ [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ 12 **return** _h_ = sgn( _f_ _N_ ) **Figure 9.1** AdaBoost.MH algorithm, for H _\u2286_ ( _{\u2212_ 1 _,_ +1 _}_ _[k]_ ) [X] _[\u00d7]_ [Y] . algorithm are similar to AdaBoost. In fact, AdaBoost.MH exactly coincides with AdaBoost applied to the training sample derived from _S_ by splitting each labeled point ( _x_ _i_ _, y_ _i_ ) into _k_ labeled examples (( _x_ _i_ _, l_ ) _, y_ _i_ [ _l_ ]), with each example ( _x_ _i_ _, l_ ) in",
    "chunk_id": "foundations_machine_learning_218"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "X _\u00d7_ Y and its label in _{\u2212_ 1 _,_ +1 _}_ : ( _x_ _i_ _, y_ _i_ ) _\u2192_ (( _x_ _i_ _,_ 1) _, y_ _i_ [1]) _, . . .,_ (( _x_ _i_ _, k_ ) _, y_ _i_ [ _k_ ]) _, i \u2208_ [ _m_ ] _._ Let _S_ _[\u2032]_ denote the resulting sample, then _S_ _[\u2032]_ = (( _x_ 1 _,_ 1) _, y_ 1 [1]) _, . . .,_ ( _x_ _m_ _, k_ ) _, y_ _m_ [ _k_ ])). _S_ _[\u2032]_ contains _mk_ examples and the expression of the objective function _F_ in (9.13) coincides exactly with that of the objective function of AdaBoost for the sample _S_ _[\u2032]_ . In view of this connection, the theoretical analysis along with the other observations we presented for AdaBoost in chapter 7 also apply here. Hence, we will focus on aspects related to the computational efficiency and to the weak learning condition that are specific to the multi-class scenario. The complexity of the algorithm is that of AdaBoost applied to a sample of size _mk_ . For X _\u2286_ R _[d]_, using boosting stumps as base classifiers, the complexity of the algorithm is therefore in _O_ (( _mk_ ) log( _mk_ ) + _mkdN_ ). Thus, for a large number of classes _k_, the algorithm may become impractical using a single processor. The **Image:** [No caption returned] **Figure 9.2** Here, each leaf is marked with the region it defines. The class labeling for a leaf is obtained via majority vote based on the training points falling in the region it defines. Right: Partition of the two-dimensional space induced by that decision tree. weak learning condition for the application of AdaBoost in this scenario requires that at each round there exists a base classifier _h_ _j_ : X _\u00d7_ Y _\u2192{\u2212_ 1 _,_ +1 _}_ such that P ( _i,l_ ) _\u223c_ D _j_ [ _h_ _j_ ( _x_ _i_ _, l_ ) _\u0338_ = _y_ _i_ [ _l_ ]] _<_ 1 _/_ 2. This may be hard to achieve if some classes difficult to distinguish between. It is also more difficult in this context to come up with \u201crules of thumb\u201d _h_ _j_ defined over X _\u00d7_ Y. **9.3.3** **Decision trees** We present and discuss the general learning method of _decision trees_ that can be used in multi-class classification, but also in other learning problems such as regression (chapter 11) and clustering. Although the empirical performance of decision trees often is not state-of-the-art, decision trees can be used as weak learners with boosting to define effective learning algorithms. Decision trees are also typically fast to train and evaluate and relatively easy to interpret. **Definition 9.5 (Binary decision tree)** _A_ binary decision tree _is a tree representation of_ _a partition of the feature space. Figure 9.2 shows a simple example in the case of a_ _two-dimensional space based on two features X_ 1 _and X_ 2 _, as well as the partition it_ _represents. Each interior node of a decision tree corresponds to a question",
    "chunk_id": "foundations_machine_learning_219"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "related_ _to the features. It can be a_ numerical question _of the form X_ _i_ _\u2264_ _a for a feature_ _variable X_ _i_ _, i \u2208_ [ _N_ ] _, and some threshold a \u2208_ R _, as in the example of figure 9.2,_ _or a_ categorical question _such as X_ _i_ _\u2208{blue, white, red}, when feature X_ _i_ _takes a_ _categorical value such as a color. Each leaf is labeled with a label l \u2208_ Y _._ Decision trees can be defined using more complex node questions, resulting in partitions based on more complex decision surfaces. For example, _binary space_ **9.3** **Uncombined multi-class algorithms** **225** GreedyDecisionTrees( _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ ))) 1 tree _\u2190{_ n 0 _} \u25b7_ root node. 2 **for** _t \u2190_ 1 **to** _T_ **do** 3 (n _t_ _,_ q _t_ ) _\u2190_ argmax (n _,_ q) _F_ [\ufffd] (n _,_ q) 4 Split(tree _,_ n _t_ _,_ q _t_ ) 5 **return** tree **Figure 9.3** Greedy algorithm for building a decision tree from a labeled sample _S_ . The procedure Split(tree _,_ n _t_ _,_ q _t_ ) splits node n _t_ by making it an internal node with question q _t_ and leaf children n _\u2212_ (n _,_ q) and n + (n _,_ q), each labeled with the dominating class of the region it defines, with ties broken arbitrarily. _partition (BSP) trees_ partition the space with convex polyhedral regions, based on questions of the form [\ufffd] _[n]_ _i_ =1 _[\u03b1]_ _[i]_ _[X]_ _[i]_ _[ \u2264]_ _[a]_ [, and] _[ sphere trees]_ [ partition with pieces] of spheres based on questions of the form _\u2225X \u2212_ _a_ 0 _\u2225\u2264_ _a_, where _X_ is a feature vector, _a_ 0 a fixed vector, and _a_ is a fixed positive real number. More complex tree questions lead to richer partitions and thus hypothesis sets, which can cause overfitting in the absence of a sufficiently large training sample. They also increase the computational complexity of prediction and training. Decision trees can also be generalized to branching factors greater than two, but binary trees are most commonly used due their more limited computational cost. **Prediction/partitioning** : To predict the label of any point _x \u2208_ X we start at the root node of the decision tree and go down the tree until a leaf is found, by moving to the right child of a node when the response to the node question is positive, and to the left child otherwise. When we reach a leaf, we associate _x_ with the label of this leaf. Thus, each leaf defines a _region_ of X formed by the set of points corresponding exactly to the same node responses and thus the same traversal of the tree. By definition, no two regions intersect and all points belong to exactly one region. Thus, leaf regions define a partition of X, as shown in the example of figure 9.2. In multi-class classification, the label of a leaf is determined using the training",
    "chunk_id": "foundations_machine_learning_220"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "sample: the class with the majority representation among the training points falling in a leaf region defines the label of that leaf, with ties broken arbitrarily. **Learning** : We will discuss two different methods for learning a decision tree using a labeled sample. The first method is a greedy technique. This is motivated by the fact that the general problem of finding a decision tree with the smallest **226** **Chapter 9** **Multi-Class Classification** error is NP-hard. The method consists of starting with a tree reduced to a single (root) node, which is a leaf whose label is the class that has majority over the entire sample. Next, at each round, a node n _t_ is split based on some question q _t_ . The pair (n _t_ _,_ q _t_ ) is chosen so that the _node impurity_ is maximally decreased according to some measure of impurity _F_ . We denote by _F_ (n) the impurity of n. The decrease in node impurity after a split of node n based on question q is defined as follows. Let n + (n _,_ q) denote the right child of n after the split, n _\u2212_ (n _,_ q) the left child, and _\u03b7_ (n _,_ q) the fraction of the points in the region defined by n that are moved to n _\u2212_ (n _,_ q). The total impurity of the leaves n _\u2212_ (n _,_ q) and n + (n _,_ q) is therefore _\u03b7_ (n _,_ q) _F_ (n _\u2212_ (n _,_ q))+(1 _\u2212\u03b7_ (n _,_ q)) _F_ (n + (n _,_ q)). Thus, the decrease in impurity _F_ [\ufffd] (n _,_ q) by that split is given by \ufffd _F_ (n _,_ q) = _F_ (n) _\u2212_ [ _\u03b7_ (n _,_ q) _F_ (n _\u2212_ (n _,_ q)) + (1 _\u2212_ _\u03b7_ (n _,_ q)) _F_ (n + (n _,_ q))] _._ Figure 9.3 shows the pseudocode of this greedy construction based on _F_ [\ufffd] . In practice, the algorithm is stopped once all nodes have reached a sufficient level of purity, when the number of points per leaf has become too small for further splitting or based on some other similar heuristic. For any node n and class _l \u2208_ [ _k_ ], let _p_ _l_ (n) denote the fraction of points at n that belong to class _l_ . Then, the three most commonly used measures of node impurity _F_ are defined as follows: 1 _\u2212_ max _l\u2208_ [ _k_ ] _p_ _l_ (n) _misclassification_ ; _F_ (n) = \uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3 _\u2212_ [\ufffd] _[k]_ _l_ =1 _[p]_ _[l]_ [(][n][) log] 2 _[p]_ _[l]_ [(][n][)] _entropy_ ; _k_ \ufffd _l_ =1 _[p]_ _[l]_ [(][n][)(1] _[ \u2212]_ _[p]_ _[l]_ [(][n][))] _Gini index_ _._ Figure 9.4 illustrates these definitions in the special cases of two classes ( _k_ = 2). The entropy and Gini index impurity functions are upper bounds on the misclassification impurity function. All three functions are concave, which ensures that _F_ (n) _\u2212_ [ _\u03b7_ (n _,_ q) _F_ (n _\u2212_ (n _,_ q)) + (1 _\u2212_ _\u03b7_ (n",
    "chunk_id": "foundations_machine_learning_221"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_,_ q)) _F_ (n + (n _,_ q))] _\u2265_ 0 _._ However, the misclassification function is piecewise linear, so _F_ [\ufffd] (n _,_ q) is zero if the fraction of positive points remains less than (or more than) half after a split. In some cases, the impurity cannot be decreased by any split using that criterion. In contrast, the entropy and Gini functions are strictly concave, which guarantees a strict decrease in impurity. Furthermore, they are differentiable which is a useful feature for numerical optimization. Thus, the Gini index and the entropy criteria are typically preferred in practice. The greedy method just described faces some issues. One issue relates to the greedy nature of the algorithm: a seemingly bad split may dominate subsequent useful splits, which could lead to trees with less impurity overall. This can be **9.3** **Uncombined multi-class algorithms** **227** **Figure 9.4** **Image:** [No caption returned] Three node impurity definitions plotted as a function of the fraction of positive examples in the binary case: misclassification, entropy (scaled by 0 _._ 5 to set the maximum to the same value for all three functions), and the Gini index. addressed to a certain extent by using a look-ahead of some depth _d_ to determine the splitting decisions, but such look-aheads can be computationally very costly. Another issue relates to the size of the resulting tree. To achieve some desired level of impurity, trees of relatively large sizes may be needed. However, larger trees define overly complex hypotheses with high VC-dimensions (see exercise 9.5) and thus could overfit. An alternative method for learning decision trees using a labeled training sample is based on the so-called _grow-then-prune strategy_ . First a very large tree is grown until it fully fits the training sample or until no more than a very small number of points are left at each leaf. Then, the resulting tree, denoted as tree, is pruned back to minimize an objective function defined (based on generalization bounds) as the sum of an empirical error and a complexity term. The complexity can be expressed in terms of the size of tree [\ufffd], the set of leaves of tree. The resulting objective is _G_ _\u03bb_ (tree) = \ufffd _|_ n _|F_ (n) + _\u03bb|_ tree [\ufffd] _|,_ (9.15) n _\u2208_ tree [\ufffd] where _\u03bb \u2265_ 0 is a regularization parameter determining the trade-off between misclassification, or more generally impurity, versus tree complexity. For any tree tree _[\u2032]_, we denote by _R_ [\ufffd] (tree _[\u2032]_ ) the total empirical error [\ufffd] n _\u2208_ tree [\ufffd] _[\u2032]_ _[ |]_ [n] _[|][F]_ [(][n][). We seek a sub-tree] tree _\u03bb_ of tree that minimizes _G_ _\u03bb_ and that has the smallest size. tree _\u03bb_ can be shown to be unique. To determine tree _\u03bb_, the following pruning method is used, which defines a finite sequence of nested sub-trees tree [(0)] _, . . .,_ tree [(] _[n]_ [)] . We start with the full tree tree [(0)] = tree and for any _i \u2208{_ 0 _, . . ., n \u2212_ 1 _}_, define tree [(] _[i]_ [+1)]",
    "chunk_id": "foundations_machine_learning_222"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "from tree [(] _[i]_ [)] by **228** **Chapter 9** **Multi-Class Classification** collapsing an internal node n _[\u2032]_ of tree [(] _[i]_ [)], that is by replacing the sub-tree rooted at n _[\u2032]_ with a leaf, or equivalently by combining the regions of all the leaves dominated by\ufffd n _[\u2032]_ . n _[\u2032]_ is chosen so that collapsing it causes the smallest per node increase in _R_ (tree [(] _[i]_ [)] ), that is the smallest _r_ (tree [(] _[i]_ [)] _,_ n _[\u2032]_ ) defined by _r_ (tree [(] _[i]_ [)] _,_ n _[\u2032]_ ) = _[|]_ [n] _[\u2032]_ _[|][F]_ [(][n] _[\u2032]_ [)] _[ \u2212]_ _\u2032_ _[R]_ [\ufffd][(][tree] _[\u2032]_ [)] _,_ _|_ tree [\ufffd] _| \u2212_ 1 where n _[\u2032]_ is an internal node of tree [(] _[i]_ [)] . If several nodes n _[\u2032]_ in tree [(] _[i]_ [)] cause the same smallest increase per node _r_ (tree [(] _[i]_ [)] _,_ n _[\u2032]_ ), then all of them are pruned to define tree [(] _[i]_ [+1)] from tree [(] _[i]_ [)] . This procedure continues until the tree tree [(] _[n]_ [)] obtained has a single node. The sub-tree tree _\u03bb_ can be shown to be among the elements of the sequence tree [(0)] _, . . .,_ tree [(] _[n]_ [)] . The parameter _\u03bb_ is determined via _n_ -fold cross validation. Decision trees seem relatively easy to interpret, and this is often underlined as one of their most useful features. However, such interpretations should be carried out with care since decision trees are _unstable_ : small changes in the training data may lead to very different splits and thus entirely different trees, as a result of their hierarchical nature. Decision trees can also be used in a natural manner to deal with the problem of _missing features_, which often appears in learning applications; in practice, some features values may be missing because the proper measurements were not taken or because of some noise source causing their systematic absence. In such cases, only those variables available at a node can be used in prediction. Finally, decision trees can be used and learned from data in a similar way in the _regression_ setting (see chapter 11). [14] **9.4** **Aggregated multi-class algorithms** In this section, we discuss a different approach to multi-class classification that reduces the problem to that of multiple binary classification tasks. A binary classification algorithm is then trained for each of these tasks independently, and the multi-class predictor is defined as a combination of the hypotheses returned by each of these algorithms. We first discuss two standard techniques for the reduction of multi-class classification to binary classification, and then show that they are both special instances of a more general framework. 14 The only changes to the description for classification are the following. For prediction, the label of a leaf is defined as the mean squared average of the labels of the points falling in that region. For learning, the impurity function is the mean squared error. **9.4** **Aggregated multi-class algorithms** **229** **9.4.1** **One-versus-all** Let _S_ = ((",
    "chunk_id": "foundations_machine_learning_223"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_x_ 1 _, y_ 1 ) _, . . ., x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_ be a labeled training sample. A straightforward reduction of the multi-class classification to binary classification is based on the so-called _one-versus-all (OVA) or one-versus-the-rest technique_ . This technique consists of learning _k_ binary classifiers _h_ _l_ : X _\u2192{\u2212_ 1 _,_ +1 _}_, _l \u2208_ Y, each seeking to discriminate one class _l \u2208_ Y from all the others. For any _l \u2208_ Y, _h_ _l_ is obtained by training a binary classification algorithm on the full sample _S_ after relabeling points in class _l_ with 1 and all others with _\u2212_ 1. For _l \u2208_ Y, assume that _h_ _l_ is derived from the sign of a scoring function _f_ _l_ : X _\u2192_ R, that is _h_ _l_ = sgn( _f_ _l_ ), as in the case of many of the binary classification algorithms discussed in the previous chapters. Then, the multi-class hypothesis _h_ : X _\u2192_ Y defined by the OVA technique is given by: _\u2200x \u2208_ X _,_ _h_ ( _x_ ) = argmax _f_ _l_ ( _x_ ) _._ (9.16) _l\u2208_ Y This formula may seem similar to those defining a multi-class classification hypothesis in the case of uncombined algorithms. Note, however, that for uncombined algorithms the functions _f_ _l_ are learned together, while here they are learned independently. Formula (9.16) is well-founded when the scores given by functions _f_ _l_ can be interpreted as confidence scores, that is when _f_ _l_ ( _x_ ) is learned as an estimate of the probability of _x_ conditioned on class _l_ . However, in general, the scores given by functions _f_ _l_, _l \u2208_ Y, are not comparable and the OVA technique based on (9.16) admits _no principled justification_ . This is sometimes referred to as a _cali-_ _bration problem_ . Clearly, this problem cannot be corrected by simply normalizing the scores of each function to make their magnitudes uniform, or by applying other similar heuristics. When it is justifiable, the OVA technique is simple and its computational cost is _k_ times that of training a binary classification algorithm, which is similar to the computation costs for many uncombined algorithms. **9.4.2** **One-versus-one** An alternative technique, known as the _one-versus-one (OVO) technique_, consists of using the training data to learn (independently), for each pair of distinct classes ( _l, l_ _[\u2032]_ ) _\u2208_ Y [2], _l \u0338_ = _l_ _[\u2032]_, a binary classifier _h_ _ll_ _\u2032_ : X _\u2192{\u2212_ 1 _,_ 1 _}_ discriminating between classes _l_ and _l_ _[\u2032]_ . For any ( _l, l_ _[\u2032]_ ) _\u2208_ Y [2], _h_ _ll_ _\u2032_ is obtained by training a binary classification algorithm on the sub-sample containing exactly the points labeled with _l_ or _l_ _[\u2032]_, with the value +1 returned for class _l_ _[\u2032]_ and _\u2212_ 1 for class _l_ . This requires training \ufffd _k_ 2 \ufffd = _k_ ( _k_ _\u2212_ 1) _/_ 2 classifiers, which are combined to define a multi-class classification hypothesis _h_ via majority",
    "chunk_id": "foundations_machine_learning_224"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "vote: _\u2200x \u2208_ X _,_ _h_ ( _x_ ) = argmax _l_ _[\u2032]_ _\u2208_ Y \ufffd\ufffd _{l_ : _h_ _ll_ _\u2032_ ( _x_ ) = 1 _}_ \ufffd\ufffd _._ (9.17) **230** **Chapter 9** **Multi-Class Classification** **Table 9.1** Comparison of the time complexity the OVA and OVO techniques for both training and testing. The table assumes a full training sample of size _m_ with each class represented by _m/k_ points. The time for training a binary classification algorithm on a sample of size _n_ is assumed to be in _O_ ( _n_ _[\u03b1]_ ). Thus, the training time for the OVO technique is in _O_ ( _k_ [2] ( _m/k_ ) _[\u03b1]_ ) = _O_ ( _k_ [2] _[\u2212][\u03b1]_ _m_ _[\u03b1]_ ). _c_ _t_ denotes the cost of testing a single classifier. Training Testing OVA _O_ ( _km_ _[\u03b1]_ ) _O_ ( _kc_ _t_ ) OVO _O_ ( _k_ [2] _[\u2212][\u03b1]_ _m_ _[\u03b1]_ ) _O_ ( _k_ [2] _c_ _t_ ) Thus, for a fixed point _x \u2208_ X, if we describe the prediction values _h_ _ll_ _\u2032_ ( _x_ ) as the results of the matches in a tournament between two players _l_ and _l_ _[\u2032]_, with _h_ _ll_ _\u2032_ ( _x_ ) = 1 indicating _l_ _[\u2032]_ winning over _l_, then the class predicted by _h_ can be interpreted as the one with the largest number of wins in that tournament. Let _x \u2208_ X be a point belonging to class _l_ _[\u2032]_ . By definition of the OVO technique, if _h_ _ll_ _[\u2032]_ ( _x_ ) = 1 for all _l \u0338_ = _l_ _[\u2032]_, then the class associated to _x_ by OVO is the correct class _l_ _[\u2032]_ since \ufffd\ufffd _{l_ : _h_ _ll_ _\u2032_ ( _x_ ) = 1 _}_ \ufffd\ufffd = _k \u2212_ 1 and no other class can reach ( _k \u2212_ 1) wins. By contraposition, if the OVO hypothesis misclassifies _x_, then at least one of the ( _k\u2212_ 1) binary classifiers _h_ _ll_ _\u2032_, _l \u0338_ = _l_ _[\u2032]_, incorrectly classifies _x_ . Assume that the generalization error of all binary classifiers _h_ _ll_ _\u2032_ used by OVO is at most _r_, then, in view of this discussion, the generalization error of the hypothesis returned by OVO is at most ( _k \u2212_ 1) _r_ . The OVO technique is not subject to the calibration problem pointed out in the case of the OVA technique. However, when the size of the sub-sample containing members of the classes _l_ and _l_ _[\u2032]_ is relatively small, _h_ _ll_ _\u2032_ may be learned without sufficient data or with increased risk of overfitting. Another concern often raised for the use of this technique is the computational cost of training _k_ ( _k \u2212_ 1) _/_ 2 binary classifiers versus that of the OVA technique. Taking a closer look at the computational requirements of these two methods reveals, however, that the disparity may not be so great and that in fact under some assumptions the time complexity of training for OVO could be less than that of OVA. Table",
    "chunk_id": "foundations_machine_learning_225"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "9.1 compares the computational complexity of these methods both for training and testing assuming that the complexity of training a binary classifier on a sample of size _m_ is in _O_ ( _m_ _[\u03b1]_ ) and that each class is equally represented in the training set, that is by _m/k_ points. Under these assumptions, if _\u03b1 \u2208_ [2 _,_ 3) as in the case of some algorithms solving a QP problem, such as SVMs, then the time complexity of training for the OVO technique is in fact more favorable than that of OVA. For _\u03b1_ = 1, the two are comparable and it is only for sub-linear algorithms that the OVA technique would benefit from a better complexity. In all cases, at test time, OVO requires _k_ ( _k \u2212_ 1) _/_ 2 classifier evaluations, which is ( _k \u2212_ 1) **9.4** **Aggregated multi-class algorithms** **231** times more than OVA. However, for some algorithms the evaluation time for each classifier could be much smaller for OVO. For example, in the case of SVMs, the average number of support vectors may be significantly smaller for OVO, since each classifier is trained on a significantly smaller sample. If the number of support vectors is _k_ times smaller and if sparse feature representations are used, then the time complexities of both techniques for testing are comparable. **9.4.3** **Error-correcting output codes** A more general method for the reduction of multi-class to binary classification is based on the idea of _error-correcting output codes (ECOC)_ . This technique consists of assigning to each class _l \u2208_ Y a _code word_ of length _c \u2265_ 1, which in the simplest case is a binary vector **M** _l_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[c]_ . **M** _l_ serves as a signature for class _l_, and together these vectors define a matrix **M** _\u2208{\u2212_ 1 _,_ +1 _}_ _[k][\u00d7][c]_ whose _l_ th row is **M** _l_, as illustrated by figure 9.5. Next, for each column _j \u2208_ [ _c_ ], a binary classifier _h_ _j_ : X _\u2192{\u2212_ 1 _,_ +1 _}_ is learned using the full training sample _S_, after all points that belong to a class represented by +1 in column _j_ are labeled with +1, while all other points are labeled with _\u2212_ 1. For any _x \u2208_ X, let **h** ( _x_ ) denote the vector **h** ( _x_ ) = ( _h_ 1 ( _x_ ) _, . . ., h_ _c_ ( _x_ )) _[\u22a4]_ . Then, the multi-class hypothesis _h_ : X _\u2192_ Y is defined by _\u2200x \u2208_ X _,_ _h_ ( _x_ ) = argmin _d_ _H_ \ufffd **M** _l_ _,_ **h** ( _x_ )\ufffd _._ (9.18) _l\u2208_ Y Thus, the class predicted is the one whose signatures is the closest to **h** ( _x_ ) in Hamming distance. Figure 9.5 illustrates this definition: no row of matrix **M** matches the vector of predictions **h** ( _x_ ) in that case, but the third row shares the largest number of components with **h** ( _x_ ). The success of the ECOC technique",
    "chunk_id": "foundations_machine_learning_226"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "depends on the minimal Hamming distance between the class code words. Let _d_ denote that distance, then up to _r_ 0 = \ufffd _d\u2212_ 2 1 \ufffd binary classification errors can be corrected by this technique: by definition of _d_, even if _r < r_ 0 binary classifiers _h_ _l_ misclassify _x \u2208_ X, **h** ( _x_ ) is closest to the code word of the correct class of _x_ . For a fixed _c_, the design of error-correction matrix **M** is subject to a trade-off, since larger _d_ values may imply substantially more difficult binary classification tasks. In practice, each column may correspond to a class feature determined based on domain knowledge. The ECOC technique just described can be extended in two ways. First, instead of using only the label predicted by each classifier _h_ _j_ the magnitude of the scores defining _h_ _j_ is used. Thus, if _h_ _j_ = sgn( _f_ _j_ ) for some function _f_ _j_ whose values can be interpreted as confidence scores, then the multi-class hypothesis _h_ : X _\u2192_ Y is **232** **Chapter 9** **Multi-Class Classification** codes |f1(x)|f2(x)|f3(x)|f4(x)|f5(x)|f6(x)| |---|---|---|---|---|---| |-1|+1|+1|-1|+1|+1| new example _x_ |Col1|1|2|3|4|5|6| |---|---|---|---|---|---|---| |1|-1|-1|-1|+1|-1|-1| |2|+1|-1|-1|-1|-1|-1| |3|-1|+1|+1|-1|+1|-1| |4|+1|+1|-1|-1|-1|-1| |5|+1|+1|-1|-1|+1|-1| |6|-1|-1|+1|+1|-1|+1| |7|-1|-1|+1|-1|-1|-1| |8|-1|+1|-1|+1|-1|-1| **Figure** ~~**9**~~ **.** ~~**5**~~ Illustration of error-correcting output codes for multi-class classification. Left: binary code matrix **M**, with each row representing the code word of length _c_ = 6 of a class _l \u2208_ [8]. Right: vector of predictions **h** ( _x_ ) for a test point _x_ . The ECOC classifier assigns label 3 to _x_, since the binary code for the third class yields the minimal Hamming distance with **h** ( _x_ ) (distance of 1). defined by _c_ \ufffd _\u2200x \u2208_ X _,_ _h_ ( _x_ ) = argmin _l\u2208_ Y \ufffd _L_ ( _m_ _lj_ _f_ _j_ ( _x_ )) _,_ (9.19) _j_ =1 where ( _m_ _lj_ ) are the entries of **M** and where _L_ : R _\u2192_ R + is a loss function. When _L_ is defined by _L_ ( _x_ ) = [1] _[\u2212]_ [s][g] 2 [n][(] _[x]_ [)] for all _x \u2208_ X and _h_ _l_ = _f_ _l_, we can write: _c_ \ufffd _L_ ( _m_ _lj_ _f_ _j_ ( _x_ )) = _j_ =1 _c_ \ufffd _j_ =1 1 _\u2212_ sgn( _m_ _lj_ _h_ _j_ ( _x_ )) = _d_ _h_ ( **M** _l_ _,_ **h** ( _x_ )) _,_ 2 and (9.19) coincides with (9.18). Furthermore, ternary codes can be used with matrix entries in _{\u2212_ 1 _,_ 0 _,_ +1 _}_ so that examples in classes labeled with 0 are disregarded when training a binary classifier for each column. With these extensions, both OVA and OVO become special instances of the ECOC technique. The matrix **M** for OVA is a square matrix, that is _c_ = _k_, with all terms equal to _\u2212_ 1 except from the diagonal ones which are all equal to +1. The matrix **M** for OVO has _c_ = _k_ ( _k \u2212_ 1) _/_ 2 columns. Each column corresponds to a pair of",
    "chunk_id": "foundations_machine_learning_227"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "distinct classes ( _l, l_ _[\u2032]_ ), _l \u0338_ = _l_ _[\u2032]_, with all entries equal to 0 except from the one with row _l_, which is _\u2212_ 1, and the one with row _l_ _[\u2032]_, which is +1. Since the values of the scoring functions are assumed to be confidence scores, _m_ _lj_ _f_ _j_ ( _x_ ) can be interpreted as the margin of classifier _j_ on point _x_ and (9.19) is thus based on some loss _L_ defined with respect to the binary classifier\u2019s margin. A further extension of ECOC consists of extending discrete codes to continuous ones by letting the matrix entries take arbitrary real values and by using the training sample to _learn_ matrix **M** . Starting with a discrete version of **M**, _c_ binary classifiers **9.5** **Structured prediction algorithms** **233** with scoring functions _f_ _l_, _l \u2208_ [ _c_ ], are first learned as described previously. We will denote by **F** ( _x_ ) the vector ( _f_ 1 ( _x_ ) _, . . ., f_ _c_ ( _x_ )) _[\u22a4]_ for any _x \u2208_ X. Next, the entries of **M** are relaxed to take real values and learned from the training sample with the objective of making the row of **M** corresponding to the class of any point _x \u2208_ X more similar to **F** ( _x_ ) than other rows. The similarity can be measured using any PDS kernel _K_ . An example of an algorithm for learning **M** using a PDS kernel _K_ and the idea just discussed is in fact multi-class SVMs, which, in this context, can be formulated as follows: min _F_ [+] _[ C]_ **M** _,_ _**\u03be**_ _[\u2225]_ **[M]** _[\u2225]_ [2] _m_ \ufffd _\u03be_ _i_ _i_ =1 subject to: _\u2200_ ( _i, l_ ) _\u2208_ [ _m_ ] _\u00d7_ Y _,_ _K_ ( **f** ( _x_ _i_ ) _,_ **M** _y_ _i_ ) _\u2265_ _K_ ( **f** ( _x_ _i_ ) _,_ **M** _l_ ) + 1 _\u2212_ _\u03be_ _i_ _._ Similar algorithms can be defined using other matrix norms. The resulting multiclass classification decision function has the following form: _h_ : _x \ufffd\u2192_ argmax _K_ ( **f** ( _x_ ) _,_ **M** _l_ ) _._ _l\u2208{_ 1 _,...,k}_ **9.5** **Structured prediction algorithms** In this section, we briefly discuss an important class of problems related to multiclass classification that frequently arises in computer vision, computational biology, and natural language processing. These include all sequence labeling problems and complex problems such as parsing, machine translation, and speech recognition. In these applications, the output labels have a rich internal structure. For example, in _part-of-speech tagging_ the problem consists of assigning a part-of-speech tag such as _N_ (noun), _V_ (verb), or _A_ (adjective), to every word of a sentence. Thus, the label of the sentence _\u03c9_ 1 _. . . \u03c9_ _n_ made of the words _\u03c9_ _i_ is a sequence of part-of-speech tags _t_ 1 _. . . t_ _n_ . This can be viewed as a multi-class classification problem where each sequence of tags is a possible label. However, several",
    "chunk_id": "foundations_machine_learning_228"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "critical aspects common to such _structured output_ problems make them distinct from the standard multi-class classification. First, the label set is exponentially large as a function of the size of the output. For example, if \u03a3 denotes the alphabet of part-of-speech tags, for a sentence of length _n_ there are _|_ \u03a3 _|_ _[n]_ possible tag sequences. Second, there are dependencies between the substructures of a label that are important to take into account for an accurate prediction. For example, in part-of-speech tagging, some tag sequences may be ungrammatical or unlikely. Finally, the loss function used is typically not a zero-one loss, but one that depends on the substructures. Let _L_ : Y _\u00d7_ Y _\u2192_ R denote **234** **Chapter 9** **Multi-Class Classification** a loss function such that _L_ ( _y_ _[\u2032]_ _, y_ ) measures the penalty of predicting the label _y_ _[\u2032]_ _\u2208_ Y instead of the correct label _y \u2208_ Y. [15] In part-of-speech tagging, _L_ ( _y_ _[\u2032]_ _, y_ ) could be for example the Hamming distance between _y_ _[\u2032]_ and _y_ . The relevant features in structured output problems often depend on both the input and the output. Thus, we will denote by **\u03a6** ( _x, y_ ) _\u2208_ R _[N]_ the feature vector associated to a pair ( _x, y_ ) _\u2208_ X _\u00d7_ Y. To model the label structures and their dependency, the label set Y is typically assumed to be endowed with a _graphical model_ structure, that is, a graph giving a probabilistic model of the conditional dependence between the substructures. It is also assumed that both the feature vector **\u03a6** ( _x, y_ ) associated to an input _x \u2208_ X and output _y \u2208_ Y and the loss _L_ ( _y_ _[\u2032]_ _, y_ ) factorize according to the cliques of that graphical model. [16] A detailed treatment of this topic would require a further background in graphical models, and is thus beyond the scope of this section. The hypothesis set used by most structured prediction algorithms is then defined as the set of functions _h_ : X _\u2192_ Y such that _\u2200x \u2208_ X _,_ _h_ ( _x_ ) = argmax **w** _\u00b7_ **\u03a6** ( _x, y_ ) _,_ (9.20) _y\u2208_ Y for some vector **w** _\u2208_ R _[N]_ . Let _S_ = (( _x_ 1 _, y_ 1 ) _, . . ., x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_ be an i.i.d. labeled sample. Since the hypothesis set is linear, we can seek to define an algorithm similar to multi-class SVMs. The optimization problem for multi-class SVMs can be rewritten equivalently as follows: _\u0338_ 1 min **w** 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [+] _[C]_ _\u0338_ _m_ \ufffd max _y_ = _\u0338_ _y_ _i_ [max] \ufffd0 _,_ 1 _\u2212_ **w** _\u00b7_ [ **\u03a6** ( _x_ _i_ _, y_ _i_ ) _\u2212_ **\u03a6** ( _x_ _i_ _, y_ )]\ufffd _,_ (9.21) _i_ =1 _\u0338_ However, here we need to take into account the loss function _L_, that is _L_ ( _y, y_ _i_ ) for each",
    "chunk_id": "foundations_machine_learning_229"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_i \u2208_ [ _m_ ] and _y \u2208_ Y, and there are multiple ways to proceed. One possible way is to let the margin violation be penalized additively with _L_ ( _y, y_ _i_ ). Thus, in that case _L_ ( _y, y_ _i_ ) is added to the margin violation. Another natural method consists of penalizing the margin violation by multiplying it with _L_ ( _y, y_ _i_ ). A margin violation with a larger loss is then penalized more than one with a smaller loss. 15 More generally, in some applications, the loss function could also depend on the input. Thus, _L_ is then a function mapping _L_ : X _\u00d7_ Y _\u00d7_ Y _\u2192_ R, with _L_ ( _x, y_ _[\u2032]_ _, y_ ) measuring the penalty of predicting the label _y_ _[\u2032]_ instead of _y_ given the input _x_ . 16 In an undirected graph, a _clique_ is a set of fully connected vertices. **9.6** **Chapter notes** **235** The additive penalization leads to the following algorithm known as _Maximum_ _Margin Markov Networks (M_ [3] _N)_ : _\u0338_ _\u0338_ 1 min **w** 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [+] _[C]_ _\u0338_ _\u0338_ _m_ \ufffd max _y_ = _\u0338_ _y_ _i_ [max] \ufffd0 _, L_ ( _y_ _i_ _, y_ ) _\u2212_ **w** _\u00b7_ [ **\u03a6** ( _x_ _i_ _, y_ _i_ ) _\u2212_ **\u03a6** ( _x_ _i_ _, y_ )]\ufffd _._ (9.22) _i_ =1 _\u0338_ _\u0338_ An advantage of this algorithm is that, as in the case of SVMs, it admits a natural use of PDS kernels. As already indicated, the label set Y is assumed to be endowed with a graph structure with a Markov property, typically a chain or a tree, and the loss function is assumed to be decomposable in the same way. Under these assumptions, by exploiting the graphical model structure of the labels, a polynomial-time algorithm can be given to determine its solution. A multiplicative combination of the loss with the margin leads to the following algorithm known as _SVMStruct_ : _\u0338_ _\u0338_ 1 min **w** 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [+] _[C]_ _\u0338_ _\u0338_ _m_ \ufffd max _y_ = _\u0338_ _y_ _i_ _[L]_ [(] _[y]_ _[i]_ _[, y]_ [) max] \ufffd0 _,_ 1 _\u2212_ **w** _\u00b7_ [ **\u03a6** ( _x_ _i_ _, y_ _i_ ) _\u2212_ **\u03a6** ( _x_ _i_ _, y_ )]\ufffd _._ (9.23) _i_ =1 _\u0338_ _\u0338_ This problem can be equivalently written as a QP with an infinite number of constraints. In practice, it is solved iteratively by augmenting at each round the finite set of constraints of the previous round with the most violating constraint. This method can be applied in fact under very general assumptions and for arbitrary loss definitions. As in the case of M [3] N, SVMStruct naturally admits the use of PDS kernels and thus an extension to non-linear models for the solution. Another standard algorithm for structured prediction problems is _Conditional_ _Random Fields (CRFs)_ . We will not describe this algorithm in detail, but point out its similarity with the algorithms just described, in particular",
    "chunk_id": "foundations_machine_learning_230"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "M [3] N. The optimization problem for CRFs can be written as _\u0338_ _\u0338_ \ufffd exp \ufffd _L_ ( _y_ _i_ _, y_ ) _\u2212_ **w** _\u00b7_ [ **\u03a6** ( _x_ _i_ _, y_ _i_ ) _\u2212_ **\u03a6** ( _x_ _i_ _, y_ )]\ufffd _._ (9.24) _y\u2208_ Y _\u0338_ _\u0338_ 1 min **w** 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [+] _[C]_ _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ \ufffd log \ufffd _i_ =1 _\u2208_ _\u0338_ _\u0338_ Assume for simplicity that Y is finite and has cardinality _k_ and let _f_ denote the function ( _x_ 1 _, . . ., x_ _k_ ) _\ufffd\u2192_ log( [\ufffd] _[k]_ _j_ =1 _[e]_ _[x]_ _[j]_ [).] _[ f]_ [ is a convex function known as the] _[ soft-]_ _max_, since it provides a smooth approximation of ( _x_ 1 _, . . ., x_ _k_ ) _\ufffd\u2192_ max( _x_ 1 _, . . ., x_ _k_ ). Then, problem (9.24) is similar to (9.22) modulo the replacement of the max operator with the soft-max function just described. **9.6** **Chapter notes** The margin-based generalization bound for multi-class classification presented in theorem 9.2 is due to Kuznetsov, Mohri, and Syed [2014]. It admits only a lin **236** **Chapter 9** **Multi-Class Classification** ear dependency on the number of classes. This improves over a similar result by Koltchinskii and Panchenko [2002], which admits a quadratic dependency on the number of classes. Proposition 9.3 bounding the Rademacher complexity of multiclass kernel-based hypotheses and corollary 9.4 are new. An algorithm generalizing SVMs to the multi-class classification setting was first introduced by Weston and Watkins [1999]. The optimization problem for that algorithm was based on _k_ ( _k \u2212_ 1) _/_ 2 slack variables for a problem with _k_ classes and thus could be inefficient for a relatively large number of classes. A simplification of that algorithm by replacing the sum of the slack variables [\ufffd] _j_ = _\u0338_ _i_ _[\u03be]_ _[ij]_ [ related to point] _x_ _i_ by its maximum _\u03be_ _i_ = max _j_ = _\u0338_ _i_ _\u03be_ _ij_ considerably reduces the number of variables and leads to the multi-class SVM algorithm presented in this chapter [Crammer and Singer, 2001, 2002]. The AdaBoost.MH algorithm is presented and discussed by Schapire and Singer [1999, 2000]. As we showed in this chapter, the algorithm is a special instance of AdaBoost. Another boosting-type algorithm for multi-class classification, AdaBoost.MR, is presented by Schapire and Singer [1999, 2000]. That algorithm is also a special instance of the RankBoost algorithm presented in chapter 10. See exercise 10.5 for a detailed analysis of this algorithm, including generalization bounds. The most commonly used tools for learning decision trees are CART (classification and regression tree) [Breiman et al., 1984] and C4.5 [Quinlan, 1986, 1993]. The greedy technique we described for learning decision trees benefits in fact from an interesting analysis: remarkably, it has been shown by Kearns and Mansour [1999], Mansour and McAllester [1999] that, under a weak learner hypothesis assumption, such decision tree algorithms produce a strong hypothesis. The grow-then-prune method is from CART. It has been analyzed by a",
    "chunk_id": "foundations_machine_learning_231"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "variety of different studies, in particular by Kearns and Mansour [1998] and Mansour and McAllester [2000], who give generalization bounds for the resulting decision trees with respect to the error and size of the best sub-tree of the original tree pruned. Hardness of ERM for decision trees of a fixed size was shown by Grigni et al. [2000]. The idea of the ECOC framework for multi-class classification is due to Dietterich and Bakiri [1995]. Allwein et al. [2000] further extended and analyzed this method to margin-based losses, for which they presented a bound on the empirical error and a generalization bound in the more specific case of boosting. While the OVA technique is in general subject to a calibration issue and does not have any justification, it is very commonly used in practice. Rifkin [2002] reports the results of extensive experiments with several multi-class classification algorithms that are rather favorable to the OVA technique, with performances often very close or better than for those of several uncombined algorithms, unlike what has been claimed by some authors (see also Rifkin and Klautau [2004]). **9.7** **Exercises** **237** The CRFs algorithm was introduced by Lafferty, McCallum, and Pereira [2001]. M [3] N is due to Taskar, Guestrin, and Koller [2003] and StructSVM was presented by Tsochantaridis, Joachims, Hofmann, and Altun [2005]. An alternative technique for tackling structured prediction as a regression problem was presented and analyzed by Cortes, Mohri, and Weston [2007c]. **9.7** **Exercises** 9.1 Generalization bounds for multi-label case. Use similar techniques to those used in the proof of theorem 9.2 to derive a margin-based learning bound in the multi-label case. 9.2 Multi-class classification with kernel-based hypotheses constrained by an _L_ _p_ norm. Use corollary 9.4 to define alternative multi-class classification algorithms with kernel-based hypotheses constrained by an _L_ _p_ norm with _p \u0338_ = 2. For which value of _p \u2265_ 1 is the bound of proposition 9.3 tightest? Derive the dual optimization of the multi-class classification algorithm defined with _p_ = _\u221e_ . 9.3 Alternative multi-class boosting algorithm. Consider the objective function _G_ defined for any sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_ and _**\u03b1**_ = ( _\u03b1_ 1 _, . . ., \u03b1_ _n_ ) _\u2208_ R _[n]_, _n \u2265_ 1, by _m_ \ufffd \ufffd _e_ _[\u2212]_ _k_ [1] _i_ =1 _k_ [1] \ufffd _kl_ =1 _[y]_ _[i]_ [[] _[l]_ []][ \ufffd] _[n]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ _[,l]_ [)] _._ (9.25) _G_ ( _**\u03b1**_ ) = _m_ \ufffd \ufffd _e_ _[\u2212]_ _k_ [1] _i_ =1 _k_ [1] \ufffd _kl_ =1 _[y]_ _[i]_ [[] _[l]_ []] _[f]_ _[n]_ [(] _[x]_ _[i]_ _[,l]_ [)] = Use the convexity of the exponential function to compare _G_ with the objective function _F_ defining AdaBoost.MH. Show that _G_ is a convex function upper bounding the multi-label multi-class error. Discuss the properties of _G_ and derive an algorithm defined by the application of coordinate descent to _G_ . Give theoretical guarantees for the",
    "chunk_id": "foundations_machine_learning_232"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "performance of the algorithm and analyze its running-time complexity when using boosting stumps. 9.4 Multi-class algorithm based on RankBoost. This problem requires familiarity with the material presented both in this chapter and in chapter 10. An alternative boosting-type multi-class classification algorithm is one based on a ranking criterion. We will define and examine that algorithm in the mono-label setting. Let H be a family of base hypotheses mapping X _\u00d7_ Y to _{\u2212_ 1 _,_ +1 _}_ . Let _F_ be the **238** **Chapter 9** **Multi-Class Classification** following objective function defined for all samples _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_ and \u00af _**\u03b1**_ = (\u00af _\u03b1_ 1 _, . . .,_ \u00af _\u03b1_ _N_ ) _\u2208_ R _[N]_, _N \u2265_ 1, by _\u0338_ _\u0338_ \ufffd _e_ _[\u2212]_ [(] _[f]_ _[N]_ [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] _[\u2212][f]_ _[N]_ [(] _[x]_ _[i]_ _[,l]_ [))] = _l_ = _\u0338_ _y_ _i_ _\u0338_ \ufffd _e_ _[\u2212]_ [\ufffd] _j_ _[N]_ =1 _[\u03b1]_ [\u00af] _[j]_ [(] _[h]_ _[j]_ [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] _[\u2212][h]_ _[j]_ [(] _[x]_ _[i]_ _[,l]_ [))] _._ _\u0338_ _l_ = _\u0338_ _y_ _i_ _m_ \ufffd _\u0338_ _i_ =1 _\u0338_ _F_ (\u00af _**\u03b1**_ ) = _\u0338_ _\u0338_ _m_ \ufffd _i_ =1 _\u0338_ _\u0338_ _\u0338_ _\u0338_ (9.26) where _f_ _N_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [.] (a) Show that _F_ is convex and differentiable. 1 _m_ 1 (b) Show that _m_ \ufffd _i_ =1 [1] _[\u03c1]_ _fN_ [(] _[x]_ _i_ _[,y]_ _i_ [)] _[ \u2264]_ _k\u2212_ 1 _[F]_ [(\u00af] _**[\u03b1]**_ [), where] _[ f]_ _[N]_ [ =][ \ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [.] (c) Give the pseudocode of the algorithm obtained by applying coordinate descent to _F_ . The resulting algorithm is known as AdaBoost.MR. Show that AdaBoost.MR exactly coincides with the RankBoost algorithm applied to the problem of ranking pairs ( _x, y_ ) _\u2208_ X _\u00d7_ Y. Describe exactly the ranking target for these pairs. (d) Use question (9.4b) and the learning bounds of this chapter to derive marginbased generalization bounds for this algorithm. (e) Use the connection of the algorithm with RankBoost and the learning bounds of chapter 10 to derive alternative generalization bounds for this algorithm. Compare these bounds with those of the previous question. 9.5 Decision trees. Show that VC-dimension of a binary decision tree with _n_ nodes in dimension _N_ is in _O_ ( _n_ log _N_ ). 9.6 Give an example where the generalization error of each of the _k_ ( _k \u2212_ 1) _/_ 2 binary classifiers _h_ _ll_ _\u2032_, _l \u0338_ = _l_ _[\u2032]_, used in the definition of the OVO technique is _r_ and that of the OVO hypothesis ( _k \u2212_ 1) _r_ . # 10 Ranking The learning problem of ranking arises in many modern applications, including the design of search engines, information extraction platforms, and movie recommendation systems. In these applications, the ordering of the documents or movies returned is a critical aspect",
    "chunk_id": "foundations_machine_learning_233"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "of the system. The main motivation for ranking over classification in the binary case is the limitation of resources: for very large data sets, it may be impractical or even impossible to display or process all items labeled as relevant by a classifier. A standard user of a search engine is not willing to consult all the documents returned in response to a query, but only the top ten or so. Similarly, a member of the fraud detection department of a credit card company cannot investigate thousands of transactions classified as potentially fraudulent, but only a few dozens of the most suspicious ones. In this chapter, we study in depth the learning problem of ranking. We distinguish two general settings for this problem: the score-based and the preference-based settings. For the score-based setting, which is the most widely explored one, we present margin-based generalization bounds using the notion of Rademacher complexity. We then describe an SVM-based ranking algorithm that can be derived from these bounds and describe and analyze RankBoost, a boosting algorithm for ranking. We further study specifically the bipartite setting of the ranking problem where, as in binary classification, each point belongs to one of two classes. We discuss an efficient implementation of RankBoost in that setting and point out its connections with AdaBoost. We also introduce the notions of ROC curves and area under the ROC curve (AUC) which are directly relevant to bipartite ranking. For the preference-based setting, we present a series of results, in particular regret-based guarantees for both a deterministic and a randomized algorithm, as well as a lower bound in the deterministic case. **240** **Chapter 10** **Ranking** **10.1** **The problem of ranking** We first introduce the most commonly studied scenario of the ranking problem in machine learning. We will refer to this scenario as the _score-based setting_ of the ranking problem. In section 10.6, we present and analyze an alternative setting, the _preference-based setting_ . The general supervised learning problem of ranking consists of using labeled information to define an accurate ranking prediction function for all points. In the scenario examined here, the labeled information is supplied only for pairs of points and the quality of a predictor is similarly measured in terms of its average pairwise misranking. The predictor is a real-valued function, a _scoring function_ : the scores assigned to input points by this function determine their ranking. Let X denote the input space. We denote by D an unknown distribution over X _\u00d7_ X according to which pairs of points are drawn and by _f_ : X _\u00d7_ X _\u2192{\u2212_ 1 _,_ 0 _,_ +1 _}_ a target labeling function or _preference function_ . The three values assigned by _f_ are interpreted as follows: _f_ ( _x, x_ _[\u2032]_ ) = +1 if _x_ _[\u2032]_ is preferred to _x_ or ranked higher than _x_, _f_ ( _x, x_ _[\u2032]_ ) = _\u2212_ 1 if _x_ is preferred to _x_ _[\u2032]_, and _f_ ( _x, x_ _[\u2032]_ ) = 0 if both _x_ and _x_ _[\u2032]_ have the same preference or ranking, or",
    "chunk_id": "foundations_machine_learning_234"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "if there is no information about their respective ranking. This formulation corresponds to a deterministic scenario which we adopt for simplification. As discussed in section 2.4.1, it can be straightforwardly extended to a stochastic scenario where we have a distribution over X _\u00d7_ X _\u00d7 {\u2212_ 1 _,_ 0 _,_ +1 _}_ . Note that in general no particular assumption is made about the transitivity of the order induced by _f_ : we may have _f_ ( _x, x_ _[\u2032]_ ) = 1 and _f_ ( _x_ _[\u2032]_ _, x_ _[\u2032\u2032]_ ) = 1 but _f_ ( _x, x_ _[\u2032\u2032]_ ) = _\u2212_ 1 for three points _x_, _x_ _[\u2032]_, and _x_ _[\u2032\u2032]_ . While this may contradict an intuitive notion of preference, such preference orders are in fact commonly encountered in practice, in particular when they are based on human judgments. This is sometimes because the preference between two items are decided based on different features: for example, an individual may prefer movie _x_ _[\u2032]_ to _x_ because _x_ _[\u2032]_ is an action movie and _x_ a musical, and prefer _x_ _[\u2032\u2032]_ to _x_ _[\u2032]_ because _x_ _[\u2032\u2032]_ is an action movie with more special effects than _x_ _[\u2032]_ . Nevertheless, they may prefer _x_ to _x_ _[\u2032\u2032]_ because the cost of the movie ticket for _x_ _[\u2032\u2032]_ is signicantly higher. Thus, in this example, two features, the genre and the price, are invoked, each affecting the decision for different pairs. In fact, in general, no assumption is made about the preference function, not even the antisymmetry of the order induced; thus, we may have _f_ ( _x, x_ _[\u2032]_ ) = 1 and _f_ ( _x_ _[\u2032]_ _, x_ ) = 1 and yet _x \u0338_ = _x_ _[\u2032]_ . The learner receives a labeled sample _S_ = \ufffd( _x_ 1 _, x_ _[\u2032]_ 1 _[, y]_ [1] [)] _[, . . .,]_ [ (] _[x]_ _[m]_ _[, x]_ _[\u2032]_ _m_ _[, y]_ _[m]_ [)] \ufffd _\u2208_ X _\u00d7_ X _\u00d7 {\u2212_ 1 _,_ 0 _,_ +1 _}_ with ( _x_ 1 _, x_ _[\u2032]_ 1 [)] _[, . . .,]_ [ (] _[x]_ _[m]_ _[, x]_ _[\u2032]_ _m_ [) drawn i.i.d. according to][ D][ and] _[ y]_ _[i]_ [=] _f_ ( _x_ _i_ _, x_ _[\u2032]_ _i_ [) for all] _[ i][ \u2208]_ [[] _[m]_ []. Given a hypothesis set][ H][ of functions mapping][ X][ to][ R][,] the ranking problem consists of selecting a hypothesis _h \u2208_ H with small expected **10.2** **Generalization bound** **241** pairwise misranking or generalization error _R_ ( _h_ ) with respect to the target _f_ : _\u0338_ _R_ ( _h_ ) = P ( _x,x_ _[\u2032]_ ) _\u223c_ D _\u0338_ \ufffd\ufffd _f_ ( _x, x_ _[\u2032]_ ) _\u0338_ = 0\ufffd _\u2227_ \ufffd _f_ ( _x, x_ _[\u2032]_ )( _h_ ( _x_ _[\u2032]_ ) _\u2212_ _h_ ( _x_ )) _\u2264_ 0\ufffd [\ufffd] _._ (10.1) _\u0338_ The empirical pairwise misranking or empirical error of _h_ is denoted by _R_ [\ufffd] _S_ ( _h_ ) and defined by _\u0338_ \ufffd _R_ _S_ ( _h_ ) = [1]",
    "chunk_id": "foundations_machine_learning_235"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u0338_ _m_ _m_ \ufffd 1 ( _y_ _i_ _\u0338_ =0) _\u2227_ ( _y_ _i_ ( _h_ ( _x_ _[\u2032]_ _i_ [)] _[\u2212][h]_ [(] _[x]_ _[i]_ [))] _[\u2264]_ [0)] _[ .]_ (10.2) _i_ =1 _\u0338_ Note that while the target preference function _f_ is in general not transitive, the linear ordering induced by a scoring function _h \u2208_ H is by definition transitive. This is a drawback of the score-based setting for the ranking problem since, regardless of the complexity of the hypothesis set H, if the preference function is not transitive, no hypothesis _h \u2208_ H can faultlessly predict the target pairwise ranking. **10.2** **Generalization bound** In this section, we present margin-based generalization bounds for ranking. To simplify the presentation, we will assume for the results of this section that the pairwise labels are in _{\u2212_ 1 _,_ +1 _}_ . Thus, if a pair ( _x, x_ _[\u2032]_ ) is drawn according to D, then either _x_ is preferred to _x_ _[\u2032]_ or the opposite. The learning bounds for the general case have a very similar form but require more details. As in the case of classification, for any _\u03c1 >_ 0, we can define the empirical margin loss of a hypothesis _h_ for pairwise ranking as _\u0338_ \ufffd _R_ _S,\u03c1_ ( _h_ ) = [1] _m_ _\u0338_ _m_ \ufffd \u03a6 _\u03c1_ ( _y_ _i_ ( _h_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ [(] _[x]_ _[i]_ [)))] _[,]_ (10.3) _i_ =1 _\u0338_ where \u03a6 _\u03c1_ is the margin loss function (definition 5.5). Thus, the empirical margin loss for ranking is upper bounded by the fraction of the pairs ( _x_ _i_ _, x_ _[\u2032]_ _i_ [) that] _[ h]_ [ is] misranking or correctly ranking but with confidence less than _\u03c1_ : _\u0338_ \ufffd _R_ _S,\u03c1_ ( _h_ ) _\u2264_ [1] _m_ _\u0338_ _m_ \ufffd 1 _y_ _i_ ( _h_ ( _x_ _[\u2032]_ _i_ [)] _[\u2212][h]_ [(] _[x]_ _[i]_ [))] _[\u2264][\u03c1]_ _[.]_ (10.4) _i_ =1 _\u0338_ We denote by D 1 the marginal distribution of the first element of the pairs in X _\u00d7_ X derived from D, and by D 2 the marginal distribution with respect to the second element of the pairs. Similarly, _S_ 1 is the sample derived from _S_ by keeping only the first element of each pair: _S_ 1 = \ufffd( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )\ufffd and _S_ 2 the one obtained by keeping only the second element: _S_ 2 = \ufffd( _x_ _[\u2032]_ 1 _[, y]_ [1] [)] _[, . . .,]_ [ (] _[x]_ _[\u2032]_ _m_ _[, y]_ _[m]_ [)] \ufffd. We also denote by R [D] _m_ [1] [(][H][) the Rademacher complexity of][ H][ with respect to the marginal distribution] D 1, that is R [D] _m_ [1] [(][H][) =][ E][[][R][\ufffd] _[S]_ 1 [(][H][)], and similarly][ R] [D] _m_ [2] [(][H][) =][ E][[][R][\ufffd] _[S]_ 2 [(][H][)]. Clearly,] **242** **Chapter 10** **Ranking** if the distribution D is symmetric, the marginal distributions D 1 and D 2 coincide and R [D]",
    "chunk_id": "foundations_machine_learning_236"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_m_ [1] [(][H][) =][ R] [D] _m_ [2] [(][H][).] **Theorem 10.1 (Margin bound for ranking)** _Let_ H _be a set of real-valued functions. Fix_ _\u03c1 >_ 0 _; then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the choice of a sample_ _S of size m, each of the following holds for all h \u2208_ H _:_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] \ufffdR [D] _m_ [1] [(][H][) +][ R] [D] _m_ [2] [(][H][)] \ufffd + _\u03c1_ \ufffd \ufffd log [1] _\u03b4_ (10.5) 2 _m_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] \ufffd\ufffdR _S_ 1 (H) + \ufffdR _S_ 2 (H)\ufffd + 3 _\u03c1_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] log [2] _\u03b4_ (10.6) 2 _m_ _[.]_ Proof: The proof is similar to that of theorem 5.8. Let H [\ufffd] be the family of hypotheses mapping (X _\u00d7_ X) _\u00d7 {\u2212_ 1 _,_ +1 _}_ to R defined by H [\ufffd] = _{z_ = (( _x, x_ _[\u2032]_ ) _, y_ ) _\ufffd\u2192_ _y_ [ _h_ ( _x_ _[\u2032]_ ) _\u2212_ _h_ ( _x_ )]: _h \u2208_ H _}_ . Consider the family of functions H [\ufffd] = _{_ \u03a6 _\u03c1_ _\u25e6_ _f_ : _f \u2208_ H [\ufffd] _}_ derived from H [\ufffd] which are taking values in [0 _,_ 1]. By theorem 3.3, for any _\u03b4 >_ 0 with probability at least 1 _\u2212_ _\u03b4_, for all _h \u2208_ H, E \ufffd\u03a6 _\u03c1_ ( _y_ [ _h_ ( _x_ _[\u2032]_ ) _\u2212_ _h_ ( _x_ )])\ufffd _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 2R _m_ \ufffd\u03a6 _\u03c1_ _\u25e6_ H [\ufffd] \ufffd + \ufffd log [1] _\u03b4_ 2 _m_ _[.]_ log [1] Since 1 _u\u2264_ 0 _\u2264_ \u03a6 _\u03c1_ ( _u_ ) for all _u \u2208_ R, the generalization error _R_ ( _h_ ) is a lower bound on left-hand side, _R_ ( _h_ ) = E[1 _y_ [ _h_ ( _x_ _\u2032_ ) _\u2212h_ ( _x_ )] _\u2264_ 0 ] _\u2264_ E \ufffd\u03a6 _\u03c1_ ( _y_ [ _h_ ( _x_ _[\u2032]_ ) _\u2212_ _h_ ( _x_ )])\ufffd, and we can write: _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 2R _m_ \ufffd\u03a6 _\u03c1_ _\u25e6_ H [\ufffd] \ufffd + \ufffd log [1] _\u03b4_ 2 _m_ _[.]_ Since \u03a6 _\u03c1_ is a (1 _/\u03c1_ )-Lipschitz function, by Talagrand\u2019s lemma (lemma 5.7) we have that R _m_ \ufffd\u03a6 _\u03c1_ _\u25e6_ H [\ufffd] \ufffd _\u2264_ _\u03c1_ [1] [R] _[m]_ [(][H][ \ufffd] [). Here,][ R] _[m]_ [(][H][ \ufffd] [) can be upper bounded as follows:] R _m_ (H [\ufffd] ) = _m_ [1] _S,\u03c3_ [E] _m_ \ufffd _\u03c3_ _i_ _y_ _i_ ( _h_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ [(] _[x]_ _[i]_ [))] \ufffd _i_ =1 _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _[\u2032]_ _i_ [) + sup] _i_ =1 _h\u2208_ H = [1] _m_ _S,\u03c3_ [E] _m_ \ufffd _\u03c3_ _i_ ( _h_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ [(] _[x]_",
    "chunk_id": "foundations_machine_learning_237"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[i]_ [))] \ufffd ( _y_ _i_ _\u03c3_ _i_ and _\u03c3_ _i_ : same distrib.) _i_ =1 _m_ \ufffd _\u03c3_ _i_ _h_ ( _x_ _i_ )\ufffd (by sub-additivity of sup) _i_ =1 _\u2264_ [1] _m_ _S,\u03c3_ [E] sup \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H sup \ufffd _h\u2208_ H = E _S_ R _S_ 2 (H) + R _S_ 1 (H) (definition of _S_ 1 and _S_ 2 ) \ufffd \ufffd = R [D] _m_ [2] [(][H][) +][ R] [D] _m_ [1] [(][H][)] _[,]_ which proves (10.5). The second inequality, (10.6), can be derived in the same way by using the second inequality of theorem 3.3, (3.4), instead of (3.3). **10.3** **Ranking with SVMs** **243** These bounds can be generalized to hold uniformly for all _\u03c1 >_ 0 at the cost of an additional term \ufffd(log log 2 (2 _/\u03c1_ )) _/m_, as in theorem 5.9 and exercise 5.2. As for other margin bounds presented in previous sections, they show the conflict between two terms: the larger the desired pairwise ranking margin _\u03c1_, the smaller the middle term. However, the first term, the empirical pairwise ranking margin loss _R_ [\ufffd] _S,\u03c1_, increases as a function of _\u03c1_ . Known upper bounds for the Rademacher complexity of a hypothesis set H, including bounds in terms of VC-dimension, can be used directly to make theorem 10.1 more explicit. In particular, using theorem 10.1, we obtain immediately the following margin bound for pairwise ranking using kernel-based hypotheses. **Corollary 10.2 (Margin bounds for ranking with kernel-based hypotheses)** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS kernel with r_ = sup _x\u2208_ X _K_ ( _x, x_ ) _. Let_ \u03a6: X _\u2192_ H _be a feature mapping_ _associated to K and let_ H = _{x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ): _\u2225_ **w** _\u2225_ H _\u2264_ \u039b _} for some_ \u039b _\u2265_ 0 _. Fix \u03c1 >_ 0 _._ _Then, for any \u03b4 >_ 0 _, the following pairwise margin bound holds with probability at_ _least_ 1 _\u2212_ _\u03b4 for any h \u2208_ H _:_ \ufffd log [1] _\u03b4_ (10.7) 2 _m_ _[.]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + 4 ~~\ufffd~~ _r_ [2] \u039b [2] _/\u03c1_ [2] + _m_ _r_ [2] \u039b [2] _/\u03c1_ [2] As with theorem 5.8, the bound of this corollary can be generalized to hold uniformly for all _\u03c1 >_ 0 at the cost of an additional term \ufffd(log log 2 (2 _/\u03c1_ )) _/m_ . This generalization bound for kernel-based hypotheses is remarkable, since it does not depend directly on the dimension of the feature space, but only on the pairwise ranking margin. It suggests that a small generalization error can be achieved when _\u03c1/r_ is large (small second term) while the empirical margin loss is relatively small (first term). The latter occurs when few points are either classified incorrectly or correctly but with margin less than _\u03c1_ . **10.3** **Ranking with SVMs** In this section, we discuss an algorithm that is derived directly from the theoretical guarantees just presented. The algorithm",
    "chunk_id": "foundations_machine_learning_238"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "turns out to be a special instance of the SVM algorithm. Proceeding as in section 5.4 for classification, the guarantee of corollary 10.2 can be expressed as follows: for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, for all _h \u2208_ H = _{x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ): _\u2225_ **w** _\u2225\u2264_ \u039b _}_, \ufffd _r_ [2] \u039b [2] + _m_ \ufffd log [1] _\u03b4_ (10.8) 2 _m_ _[,]_ _R_ ( _h_ ) _\u2264_ [1] _m_ _m_ \ufffd _\u03be_ _i_ + 4 _i_ =1 _m_ \ufffd where _\u03be_ _i_ = max \ufffd1 _\u2212_ _y_ _i_ \ufffd **w** _\u00b7_ \ufffd **\u03a6** ( _x_ _[\u2032]_ _i_ [)] _[\u2212]_ **[\u03a6]** [(] _[x]_ _[i]_ [)] \ufffd\ufffd _,_ 0\ufffd for all _i \u2208_ [ _m_ ], and where **\u03a6** : X _\u2192_ H is a feature mapping associated to a PDS kernel _K_ . An algorithm based on this **244** **Chapter 10** **Ranking** theoretical guarantee consists of minimizing the right-hand side of (10.8), that is minimizing an objective function with a term corresponding to the sum of the slack variables _\u03be_ _i_, and another one minimizing _\u2225_ **w** _\u2225_ or equivalently _\u2225_ **w** _\u2225_ [2] . Its optimization problem can thus be formulated as 1 min **w** _,_ _**\u03be**_ 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [ +] _[ C]_ _m_ \ufffd _\u03be_ _i_ (10.9) _i_ =1 subject to: _y_ _i_ \ufffd **w** _\u00b7_ \ufffd **\u03a6** ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ **[\u03a6]** [(] _[x]_ _[i]_ [)] \ufffd [\ufffd] _\u2265_ 1 _\u2212_ _\u03be_ _i_ _\u03be_ _i_ _\u2265_ 0 _,_ _\u2200i \u2208_ [ _m_ ] _._ This coincides exactly with the primal optimization problem of SVMs, with a feature mapping **\u03a8** : X _\u00d7_ X _\u2192_ H defined by **\u03a8** ( _x, x_ _[\u2032]_ ) = **\u03a6** ( _x_ _[\u2032]_ ) _\u2212_ **\u03a6** ( _x_ ) for all ( _x, x_ _[\u2032]_ ) _\u2208_ X _\u00d7_ X, and with a hypothesis set of functions of the form ( _x, x_ _[\u2032]_ ) _\ufffd\u2192_ **w** _\u00b7_ **\u03a8** ( _x, x_ _[\u2032]_ ). Thus, clearly, all the properties already presented for SVMs apply in this instance. In particular, the algorithm can benefit from the use of PDS kernels. Problem (10.9) admits an equivalent dual that can be expressed in terms of the kernel matrix **K** _[\u2032]_ defined by **K** _[\u2032]_ _ij_ [=] **[ \u03a8]** [(] _[x]_ _[i]_ _[, x]_ _[\u2032]_ _i_ [)] _[\u00b7]_ **[\u03a8]** [(] _[x]_ _[j]_ _[, x]_ _[\u2032]_ _j_ [) =] _[ K]_ [(] _[x]_ _[i]_ _[, x]_ _[j]_ [)+] _[K]_ [(] _[x]_ _i_ _[\u2032]_ _[, x]_ _[\u2032]_ _j_ [)] _[\u2212][K]_ [(] _[x]_ _i_ _[\u2032]_ _[, x]_ _[j]_ [)] _[\u2212][K]_ [(] _[x]_ _[i]_ _[, x]_ _[\u2032]_ _j_ [)] _[,]_ [ (10.10)] for all _i, j \u2208_ [ _m_ ]. This algorithm can provide an effective solution for pairwise ranking in practice. The algorithm can also be used and extended to the case where the labels are in _{\u2212_ 1 _,_ 0 _,_ +1 _}_ . The next section presents an alternative algorithm for ranking in the score-based setting. **10.4** **RankBoost** This section presents a",
    "chunk_id": "foundations_machine_learning_239"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "boosting algorithm for pairwise ranking, _RankBoost_, similar to the AdaBoost algorithm for binary classification. RankBoost is based on ideas analogous to those discussed for classification: it consists of combining different _base rankers_ to create a more accurate predictor. The base rankers are hypotheses returned by a _weak learning algorithm_ for ranking. As for classification, these base hypotheses must satisfy a minimal accuracy condition that will be described precisely later. Let H denote the hypothesis set from which the base rankers are selected. Algorithm 10.1 gives the pseudocode of the RankBoost algorithm when H is a set of functions mapping from X to _{_ 0 _,_ 1 _}_ . For any _s \u2208{\u2212_ 1 _,_ 0 _,_ +1 _}_, we define _\u03f5_ _[s]_ _t_ [by] _\u03f5_ _[s]_ _t_ [=] _m_ \ufffd D _t_ ( _i_ )1 _y_ _i_ ( _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))=] _[s]_ [ =] _i\u223c_ E D _t_ [[1] _[y]_ _[i]_ [(] _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))=] _[s]_ []] _[,]_ (10.11) _i_ =1 **10.4** **RankBoost** **245** RankBoost( _S_ = (( _x_ 1 _, x_ _[\u2032]_ 1 _[, y]_ [1] [)] _[ . . .,]_ [ (] _[x]_ _[m]_ _[, x]_ _[\u2032]_ _m_ _[, y]_ _[m]_ [)))] 1 **for** _i \u2190_ 1 **to** _m_ **do** 2 D 1 ( _i_ ) _\u2190_ _m_ [1] 3 **for** _t \u2190_ 1 **to** _T_ **do** 4 _h_ _t_ _\u2190_ base ranker in H with smallest _\u03f5_ _[\u2212]_ _t_ _[\u2212]_ _[\u03f5]_ [+] _t_ [=] _[ \u2212]_ E [+] _i\u223c_ D _t_ \ufffd _y_ _i_ \ufffd _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)] \ufffd [\ufffd] 5 _\u03b1_ _t_ _\u2190_ [1] 2 [log] _\u03f5_ _[\u03f5]_ _[\u2212]_ _tt_ [+] 1 6 _Z_ _t_ _\u2190_ _\u03f5_ [0] _t_ [+ 2[] _[\u03f5]_ [+] _t_ _[\u03f5]_ _[\u2212]_ _t_ []] 2 _\u25b7_ normalization factor 5 _\u03b1_ _t_ _\u2190_ [1] 7 **for** _i \u2190_ 1 **to** _m_ **do** D _t_ ( _i_ ) exp \ufffd _\u2212\u03b1_ _t_ _y_ _i_ \ufffd _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [)] \ufffd\ufffd 8 D _t_ +1 ( _i_ ) _\u2190_ _Z_ _t_ 9 _f \u2190_ [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ 10 **return** _f_ **Figure 10.1** RankBoost algorithm for H _\u2286{_ 0 _,_ 1 _}_ [X] . and simplify the notation _\u03f5_ [+1] _t_ into _\u03f5_ [+] _t_ [and similarly write] _[ \u03f5]_ _[\u2212]_ _t_ [instead of] _[ \u03f5]_ _[\u2212]_ _t_ [1] [. With] these definitions, clearly the following equality holds: _\u03f5_ [0] _t_ [+] _[ \u03f5]_ [+] _t_ [+] _[ \u03f5]_ _[\u2212]_ _t_ [= 1.] The algorithm takes as input a labeled sample _S_ = \ufffd( _x_ 1 _, x_ _[\u2032]_ 1 _[, y]_ [1] [)] _[, . . .,]_ [ (] _[x]_ _[m]_ _[, x]_ _[\u2032]_ _m_ _[, y]_ _[m]_ [)] \ufffd with elements in X _\u00d7_ X _\u00d7 {\u2212_ 1 _,_ 0 _,_ +1 _}_, and maintains a distribution over the subset of the indices _i \u2208{_ 1 _, . . .,",
    "chunk_id": "foundations_machine_learning_240"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "m}_ for which _y_ _i_ _\u0338_ = 0. To simplify the presentation, we will assume that _y_ _i_ _\u0338_ = 0 for all _i \u2208{_ 1 _, . . ., m}_ and consider distributions defined over _{_ 1 _, . . ., m}_ . This can be guaranteed by simply first removing from the sample the pairs labeled with zero. Initially (lines 1\u20132), the distribution is uniform (D 1 ). At each round of boosting, that is at each iteration _t \u2208_ [ _T_ ] of the loop 3\u20138, a new base ranker _h_ _t_ _\u2208_ H is selected with the smallest difference _\u03f5_ _[\u2212]_ _t_ _[\u2212]_ _[\u03f5]_ [+] _t_ [, that is one with the smallest pairwise] misranking error and largest correct pairwise ranking accuracy for the distribution D _t_ : _h_ _t_ _\u2208_ argmin _h\u2208_ H \ufffd _\u2212_ _i\u223c_ E D _t_ \ufffd _y_ _i_ \ufffd _h_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ [(] _[x]_ _[i]_ [)] \ufffd [\ufffd\ufffd] _._ Note that _\u03f5_ _[\u2212]_ _t_ _[\u2212]_ _[\u03f5]_ [+] _t_ [=] _[ \u03f5]_ _[\u2212]_ _t_ _[\u2212]_ [(1] _[ \u2212]_ _[\u03f5]_ _[\u2212]_ _t_ _[\u2212]_ _[\u03f5]_ [0] _t_ [) = 2] _[\u03f5]_ _t_ _[\u2212]_ [+] _[ \u03f5]_ [0] _t_ _[\u2212]_ [1. Thus, finding the smallest] difference _\u03f5_ _[\u2212]_ _t_ _[\u2212][\u03f5]_ [+] _t_ [is equivalent to seeking the smallest 2] _[\u03f5]_ _[\u2212]_ _t_ [+] _[\u03f5]_ [0] _t_ [, which itself coincides] with seeking the smallest _\u03f5_ _[\u2212]_ _t_ [when] _[ \u03f5]_ [0] _t_ [= 0.] _[ Z]_ _[t]_ [is simply a normalization factor to] ensure that the weights D _t_ +1 ( _i_ ) sum to one. RankBoost relies on the assumption _\u2212_ E \ufffd _i\u223c_ D _t_ \ufffd _y_ _i_ \ufffd _h_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ [(] _[x]_ _[i]_ [)] \ufffd [\ufffd\ufffd] _._ **246** **Chapter 10** **Ranking** that at each round _t \u2208_ [ _T_ ], for the hypothesis _h_ _t_ found, the inequality _\u03f5_ [+] _t_ _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ _[>]_ [ 0] holds; thus, the probability mass of the pairs correctly ranked by _h_ _t_ (ignoring pairs with label zero) is larger than that of misranked pairs. We denote by _\u03b3_ _t_ the _edge_ of the base ranker _h_ _t_ : _\u03b3_ _t_ = _[\u03f5]_ _t_ [+] _[\u2212]_ 2 _[\u03f5]_ _[\u2212]_ _t_ . The precise reason for the definition of the coefficient _\u03b1_ _t_ (line 5) will become clear later. For now, observe that if _\u03f5_ [+] _t_ _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ _[>]_ [ 0, then] _[ \u03f5]_ [+] _t_ _[/\u03f5]_ _[\u2212]_ _t_ _[>]_ [ 1 and] _[ \u03b1]_ _[t]_ _[>]_ [ 0. Thus,] the new distribution D _t_ +1 is defined from D _t_ by increasing the weight on _i_ if the pair ( _x_ _i_ _, x_ _[\u2032]_ _i_ [) is misranked (] _[y]_ _[i]_ [(] _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[ \u2212]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)] _[ <]_ [ 0), and, on the contrary, decreasing] it if ( _x_ _i_ _, x_ _[\u2032]_ _i_ [) is ranked correctly (] _[y]_ _[i]_ [(] _[h]_ _[t]_ [(] _[x]_ _[\u2032]_ _i_",
    "chunk_id": "foundations_machine_learning_241"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[)] _[ \u2212]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)] _[ >]_ [ 0). The relative weight is] unchanged for a pair with _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [) = 0. This distribution update has the] effect of focusing more on misranked points at the next round of boosting. After _T_ rounds of boosting, the hypothesis returned by RankBoost is _f_, which is a linear combination of the base classifiers _h_ _t_ . The weight _\u03b1_ _t_ assigned to _h_ _t_ in that sum is a logarithmic function of the ratio of _\u03f5_ [+] _t_ [and] _[ \u03f5]_ _[\u2212]_ _t_ [. Thus, more accurate base] rankers are assigned a larger weight in that sum. For any _t \u2208_ [ _T_ ], we will denote by _f_ _t_ the linear combination of the base rankers after _t_ rounds of boosting: _f_ _t_ = [\ufffd] _[t]_ _s_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [. In particular, we have] _[ f]_ _[T]_ [ =] _[ f]_ [. The] distribution D _t_ +1 can be expressed in terms of _f_ _t_ and the normalization factors _Z_ _s_, _s \u2208_ [ _t_ ], as follows: _i_ [))] _[\u2212][f]_ _[t]_ [(] _[x]_ _[i]_ [)] _\u2200i \u2208_ [ _m_ ] _,_ D _t_ +1 ( _i_ ) = _[e]_ _[\u2212][y]_ _[i]_ [(] _[f]_ _[t]_ [(] _[x]_ _[\u2032]_ _._ (10.12) _m_ [\ufffd] _[t]_ _s_ =1 _[Z]_ _[s]_ We will make use of this identity several times in the proofs of the following sections. It can be shown straightforwardly by repeatedly expanding the definition of the distribution over the point _x_ _i_ : _i_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))] D _t_ +1 ( _i_ ) = [D] _[t]_ [(] _[i]_ [)] _[e]_ _[\u2212][\u03b1]_ _[t]_ _[y]_ _[i]_ [(] _[h]_ _[t]_ [(] _[x]_ _[\u2032]_ _Z_ _t_ _i_ [)] _[\u2212][h]_ _[t][\u2212]_ [1] [(] _[x]_ _[i]_ [))] _e_ _[\u2212][\u03b1]_ _[t]_ _[y]_ _[i]_ [(] _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))] = [D] _[t][\u2212]_ [1] [(] _[i]_ [)] _[e]_ _[\u2212][\u03b1]_ _[t][\u2212]_ [1] _[y]_ _[i]_ [(] _[h]_ _[t][\u2212]_ [1] [(] _[x]_ _[\u2032]_ _Z_ _t\u2212_ 1 _Z_ _t_ \ufffd _ts_ =1 _[\u03b1]_ _[s]_ [(] _[h]_ _[s]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[s]_ [(] _[x]_ _[i]_ [))] = _[e]_ _[\u2212][y]_ _[i]_ _m_ [\ufffd] _[t]_ _s_ =1 _[Z]_ _[s]_ _._ **10.4.1** **Bound on the empirical error** We first show that the empirical error of RankBoost decreases exponentially fast as a function of the number of rounds of boosting when the edge _\u03b3_ _t_ of each base ranker _h_ _t_ is lower bounded by some positive value _\u03b3 >_ 0. **10.4** **RankBoost** **247** **Theorem 10.3** _The empirical error of the hypothesis h_ : X _\u2192{_ 0 _,_ 1 _} returned by Rank-_ _Boost verifies:_ 2 [\ufffd] _._ (10.13) \ufffd \ufffd _R_ _S_ ( _h_ ) _\u2264_ exp _\u2212_ 2 \ufffd _T_ \ufffd _t_ =1 + \ufffd _\u03f5_ _t_ _[\u2212]_ 2 _[\u03f5]_ _[\u2212]_ _t_ _Furthermore, if there exists \u03b3 such that for all t \u2208_ [ _T_ ] _,_ 0 _< \u03b3 \u2264_ _[\u03f5]_ _t_ [+]",
    "chunk_id": "foundations_machine_learning_242"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u2212]_ 2 _[\u03f5]_ _[\u2212]_ _t_ _, then_ \ufffd _R_ _S_ ( _h_ ) _\u2264_ exp( _\u2212_ 2 _\u03b3_ [2] _T_ ) _._ (10.14) Proof: Using the general inequality 1 _u\u2264_ 0 _\u2264_ exp( _\u2212u_ ) valid for all _u \u2208_ R and identity 10.12, we can write: _m_ \ufffd _R_ _S_ ( _h_ ) = [1] _m_ _m_ \ufffd \ufffd 1 _y_ _i_ ( _f_ ( _x_ _[\u2032]_ _i_ [)] _[\u2212][f]_ [(] _[x]_ _[i]_ [))] _[\u2264]_ [0] _[ \u2264]_ _m_ [1] _i_ =1 _m_ \ufffd _e_ _[\u2212][y]_ _[i]_ [(] _[f]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][f]_ [(] _[x]_ _[i]_ [))] _i_ =1 _T_ \ufffd _Z_ _t_ _t_ =1 _T_ \ufffd _Z_ _t_ _._ _t_ =1 _\u2264_ [1] _m_ _m_ _m_ \ufffd _i_ =1 \ufffd D _T_ +1 ( _i_ ) = \ufffd By the definition of normalization factor, for all _t \u2208_ [ _T_ ], we have _Z_ _t_ = [\ufffd] _[m]_ _i_ =1 [D] _[t]_ [(] _[i]_ [)] _e_ _[\u2212][\u03b1]_ _[t]_ _[y]_ _[i]_ [(] _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))] . By grouping together the indices _i_ for which _y_ _i_ ( _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))] takes the values in +1, _\u2212_ 1, or 0, _Z_ _t_ can be rewritten as _Z_ _t_ = _\u03f5_ [+] _t_ _[e]_ _[\u2212][\u03b1]_ _[t]_ [ +] _[ \u03f5]_ _[\u2212]_ _t_ _[e]_ _[\u03b1]_ _[t]_ [ +] _[ \u03f5]_ [0] _t_ [=] _[ \u03f5]_ [+] _t_ ~~\ufffd~~ Since _\u03f5_ [+] _t_ [= 1] _[ \u2212]_ _[\u03f5]_ _[\u2212]_ _t_ _[\u2212]_ _[\u03f5]_ [0] _t_ [, we have] \ufffd _\u03f5_ _[\u2212]_ _t_ + _\u03f5_ _[\u2212]_ _t_ _\u03f5_ [+] _t_ _\u03f5_ [+] _t_ _\u03f5_ _[\u2212]_ _t_ + _\u03f5_ [0] _t_ [= 2] \ufffd _\u03f5_ [+] _t_ _[\u03f5]_ _[\u2212]_ _t_ [+] _[ \u03f5]_ [0] _t_ _[.]_ 4 _\u03f5_ [+] _t_ _[\u03f5]_ _[\u2212]_ _t_ [= (] _[\u03f5]_ [+] _t_ [+] _[ \u03f5]_ _[\u2212]_ _t_ [)] [2] _[ \u2212]_ [(] _[\u03f5]_ [+] _t_ _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] [2] [ = (1] _[ \u2212]_ _[\u03f5]_ [0] _t_ [)] [2] _[ \u2212]_ [(] _[\u03f5]_ _t_ [+] _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] [2] _[.]_ Thus, assuming that _\u03f5_ [0] _t_ _[<]_ [ 1,] _[ Z]_ _[t]_ [can be upper bounded as follows:] _Z_ _t_ = ~~\ufffd~~ (1 _\u2212_ _\u03f5_ [0] _t_ [)] [2] _[ \u2212]_ [(] _[\u03f5]_ [+] _t_ _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] [2] [ +] _[ \u03f5]_ [0] _t_ = (1 _\u2212_ _\u03f5_ [0] _t_ [)] \ufffd 1 _\u2212_ [(] _[\u03f5]_ _t_ [+] _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] [2] _t_ (1 _\u2212_ _\u03f5_ [0] _t_ [)] [2] [ +] _[ \u03f5]_ [0] 1 _\u2212_ [(] _[\u03f5]_ _t_ [+] _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] [2] _\u2264_ ~~\ufffd~~ 1 _\u2212_ [(] _[\u03f5]_ _t_ [+] _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] [2] (1 _\u2212_ _\u03f5_ [0] _t_ [)] _\u2212_ [(] _[\u03f5]_ _t_ [+] _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] [2] _\u2264_ exp \ufffd 2(1 _\u2212_ _\u03f5_ [0] _t_ [)] _\u2212_ [(] _[\u03f5]_ _t_ [+] _[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] [2] _\u2264_ exp \ufffd \ufffd 2 = exp \ufffd _\u2212_ 2[( _\u03f5_ [+] _t_",
    "chunk_id": "foundations_machine_learning_243"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [)] _[/]_ [2]] [2] [\ufffd] _,_ \ufffd where we used for the first inequality the concavity of the square-root function and the fact that 0 _<_ 1 _\u2212_ _\u03f5_ [0] _t_ _[\u2264]_ [1, and the inequality 1] _[ \u2212]_ _[x][ \u2264]_ _[e]_ _[\u2212][x]_ [ valid for all] _[ x][ \u2208]_ [R] for the second inequality. This upper bound on _Z_ _t_ also trivially holds when _\u03f5_ [0] _t_ [= 1] since in that case _\u03f5_ [+] _t_ [=] _[ \u03f5]_ _[\u2212]_ _t_ [= 0. This concludes the proof.] **248** **Chapter 10** **Ranking** _[\u0338]_ _[\u0338]_ As can be seen from the proof of the theorem, the weak ranking assumption _\u03b3 \u2264_ _\u03f5_ [+] _t_ _[\u2212]_ 2 _[\u03f5]_ _[\u2212]_ _t_ with _\u03b3 >_ 0 can be replaced with the somewhat weaker requirement _\u03b3 \u2264_ 2 _\u03f5_ _[\u221a]_ [+] _t_ _[\u2212]_ 1 _\u2212_ _[\u03f5]_ _[\u2212]_ _t_ _\u03f5_ [0] _t_ [, with] _[ \u03f5]_ _t_ [0] _[\u0338]_ [= 1, which can be rewritten as] _[ \u03b3][ \u2264]_ [1] 2 _\u221a_ _\u03f5_ [+] _t_ _\u03f5_ [+] _t_ _[\u2212]_ [+] _[\u03f5]_ _[\u2212]_ _t_ _[\u03f5]_ _[\u2212]_ _t_, with _\u03f5_ [+] _t_ [+] _[ \u03f5]_ _[\u2212]_ _t_ _[\u0338]_ [= 0,] _[\u2212]_ 1 _\u2212_ _[\u03f5]_ _t_ _\u03f5_ [0] _t_ [, with] _[ \u03f5]_ _t_ [0] _[\u0338]_ [= 1, which can be rewritten as] _[ \u03b3][ \u2264]_ [1] 2 _[\u0338]_ _[\u0338]_ _[\u2212][\u03f5]_ _t_, with _\u03f5_ [+] _t_ [+] _[ \u03f5]_ _[\u2212]_ _t_ _[\u0338]_ [= 0,] _\u03f5_ [+] _t_ [+] _[\u03f5]_ _[\u2212]_ _t_ _[\u0338]_ _[\u0338]_ where the quantity _\u03f5_ [+] _t_ _[\u2212][\u03f5]_ _[\u2212]_ _t_ _\u221a_ _\u03f5_ [+] _t_ [+] _[\u03f5]_ _[\u0338]_ [1] 2 _\u221a_ _\u03f5_ [+] _t_ _\u03f5_ [+] _t_ _[\u2212]_ [+] _[\u03f5]_ _[\u2212]_ _t_ _[\u03f5]_ _[\u0338]_ _[\u0338]_ _[\u0338]_ _[\u2212]_ _t_ can be interpreted as a (normalized) relative difference _\u03f5_ [+] _t_ [+] _[\u03f5]_ _[\u2212]_ _t_ _[\u0338]_ _[\u0338]_ between _\u03f5_ [+] _t_ [and] _[ \u03f5]_ _[\u2212]_ _t_ [.] The proof of the theorem also shows that the coefficient _\u03b1_ _t_ is selected to minimize _Z_ _t_ . Thus, overall, these coefficients are chosen to minimize the upper bound on the empirical error [\ufffd] _[T]_ _t_ =1 _[Z]_ _[t]_ [, as for AdaBoost. The RankBoost algorithm can be] generalized in several ways: _\u2022_ Instead of a hypothesis with minimal difference _\u03f5_ _[\u2212]_ _t_ _[\u2212]_ _[\u03f5]_ [+] _t_ [,] _[ h]_ _[t]_ [can be more generally] a base ranker returned by a weak ranking algorithm trained on D _t_ with _\u03f5_ [+] _t_ _[> \u03f5]_ _[\u2212]_ _t_ [;] _\u2022_ The range of the base rankers could be [0 _,_ +1], or more generally R. The coefficients _\u03b1_ _t_ can then be different and may not even admit a closed form. However, in general, they are chosen to minimize the upper bound [\ufffd] _[T]_ _t_ =1 _[Z]_ _[t]_ [ on the empirical] error. **10.4.2** **Relationship with coordinate descent** RankBoost coincides with the application of the coordinate descent technique to a convex and differentiable objective function _F_ defined for all samples _S_ = \ufffd( _x_ 1 _, x_ _[\u2032]_ 1 _[, y]_ [1] [)] _[, . . .,]_ [ (] _[x]_ _[m]_ _[,",
    "chunk_id": "foundations_machine_learning_244"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "x]_ _[\u2032]_ _m_ _[, y]_ _[m]_ [)] \ufffd _\u2208_ X _\u00d7_ X _\u00d7 {\u2212_ 1 _,_ 0 _,_ +1 _}_ and \u00af _**\u03b1**_ = (\u00af _\u03b1_ 1 _, . . .,_ \u00af _\u03b1_ _n_ ) _\u2208_ R _[N]_, _N \u2265_ 1 by _[\u0338]_ _[\u0338]_ _m_ \ufffd _e_ _[\u2212][y]_ _[i]_ \ufffd _Nj_ =1 _[\u03b1]_ [\u00af] _[j]_ [[] _[h]_ _[j]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[j]_ [(] _[x]_ _[i]_ [)]] _,_ (10.15) _i_ =1 _[\u0338]_ _[\u0338]_ _F_ (\u00af _**\u03b1**_ ) = _[\u0338]_ _[\u0338]_ _m_ \ufffd _e_ _[\u2212][y]_ _[i]_ [[] _[f]_ _[N]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][f]_ _[N]_ [(] _[x]_ _[i]_ [)]] = _i_ =1 _[\u0338]_ _[\u0338]_ where _f_ _N_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[j]_ _[h]_ _[j]_ [. This loss function is a convex upper bound on the zero-] one pairwise loss function \u00af _**\u03b1**_ _\ufffd\u2192_ [\ufffd] _[m]_ _i_ =1 [1] _[y]_ _i_ [[] _[f]_ _N_ [(] _[x]_ _[\u2032]_ _i_ [)] _[\u2212][f]_ _[N]_ [(] _[x]_ _[i]_ [)]] _[\u2264]_ [0] [, which is not convex.] Let **e** _k_ denote the unit vector corresponding to the _k_ th coordinate in R _[N]_ and let \u00af _**\u03b1**_ _t\u2212_ 1 = \u00af _**\u03b1**_ _t_ + _\u03b7_ **e** _k_ denote the parameter vector after iteration _t_ (with \u00af _**\u03b1**_ 0 = **0** ). Also, for any _t \u2208_ [ _T_ ], we define the function _f_ [\u00af] _t_ = [\ufffd] _[N]_ _j_ =1 _[\u03b1]_ [\u00af] _[t,j]_ _[h]_ _[j]_ [ and distribution \u00af][D] _[t]_ [ over] the indices _{_ 1 _, . . ., m}_ as follows: D\u00af _t_ +1 ( _i_ ) = _[e]_ _[\u2212][y]_ _[i]_ [( \u00af] _f_ _t_ ( _x_ _[\u2032]_ _i_ [))] _[\u2212]_ _f_ [\u00af] _t_ ( _x_ _i_ )) _,_ (10.16) _m_ [\ufffd] _[t]_ _s_ =1 _[Z]_ [\u00af] _[s]_ where _Z_ [\u00af] _t_ is the analogue of _Z_ _t_ computed as a function of D [\u00af] _t_ instead of D _t_ . Similarly, let \u00af _\u03f5_ [+] _t_ _[,]_ [ \u00af] _[\u03f5]_ _[\u2212]_ _t_ [, and \u00af] _[\u03f5]_ _[t]_ [denote the analogues of] _[ \u03f5]_ [+] _t_ _[, \u03f5]_ _[\u2212]_ _t_ [, and] _[ \u03f5]_ _[t]_ [defined with respect to \u00af][D] _[t]_ instead of D _t_ . Following very similar steps as in section 7.2.2, we will use an inductive argument argument to show that coordinate descent on _F_ and the RankBoost algorithm are **10.4** **RankBoost** **249** in fact equivalent. Clearly, D [\u00af] _t_ +1 = D _t_ +1 if we have _f_ [\u00af] _t_ = _f_ _t_ for all _t_ . We trivially have _f_ [\u00af] 0 = _f_ 0, so we will make the inductive assumption that _f_ [\u00af] _t\u2212_ 1 = _f_ _t\u2212_ 1 and show that this implies _f_ [\u00af] _t_ = _f_ _t_ . At each iteration _t \u2265_ 1, the direction **e** _k_ selected by coordinate descent is the one minimizing the directional derivative, which is defined as: \u00af \u00af _F_ ( _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_ ) _\u2212_ _F_ ( _**\u03b1**_ _t\u2212_ 1 ) _F_ _[\u2032]_ (\u00af _**\u03b1**_ _t\u2212_ 1 _,_ **e** _k_ ) = lim _._ _\u03b7\u2192_ 0 _\u03b7_ Since _F_",
    "chunk_id": "foundations_machine_learning_245"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(\u00af _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_ ) = [\ufffd] _[m]_ _i_ =1 _[e]_ _[\u2212][y]_ _[i]_ \ufffd _tj\u2212_ =11 _[\u03b1]_ [\u00af] _[t][\u2212]_ [1] _[,j]_ [(] _[h]_ _[j]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[j]_ [(] _[x]_ _[i]_ [))] _[\u2212][\u03b7y]_ _[i]_ [(] _[h]_ _[k]_ [(] _[x]_ _[\u2032]_ _i_ [)] _[\u2212][h]_ _[k]_ [(] _[x]_ _[i]_ [))], the directional derivative along **e** _k_ can be expressed as follows: _F_ _[\u2032]_ (\u00af _**\u03b1**_ _t\u2212_ 1 _,_ **e** _k_ ) _N_ \u00af \ufffd _\u03b1_ _t\u2212_ 1 _,j_ ( _h_ _j_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ _[j]_ [(] _[x]_ _[i]_ [))] \ufffd _j_ =1 = _\u2212_ = _\u2212_ _m_ \ufffd _y_ _i_ ( _h_ _k_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ _[k]_ [(] _[x]_ _[i]_ [)) exp] \ufffd _\u2212_ _y_ _i_ _i_ =1 _m_ \ufffd _m_ \ufffd _y_ _i_ ( _h_ _k_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ _[k]_ [(] _[x]_ _[i]_ [)) \u00af][D] _[t]_ [(] _[i]_ [)] \ufffd _m_ _i_ =1 _t\u2212_ 1 \ufffd _Z_ \u00af _s_ \ufffd _s_ =1 _m_ \u00af = _\u2212_ \ufffd D _t_ ( _i_ )1 _y_ _i_ ( _h_ _t_ ( _x_ _\u2032i_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))=+1] _[ \u2212]_ \ufffd _i_ =1 _m_ = _\u2212_ \ufffd \ufffd _m_ \u00af \ufffd D _t_ ( _i_ )1 _y_ _i_ ( _h_ _t_ ( _x_ _\u2032i_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))=] _[\u2212]_ [1] _i_ =1 _m_ \ufffd _m_ \ufffd\ufffd _t\u2212_ 1 \ufffd _Z_ \u00af _s_ \ufffd _s_ =1 = _\u2212_ [\u00af _\u03f5_ [+] _t_ _[\u2212]_ _[\u03f5]_ [\u00af] _[\u2212]_ _t_ []] _m_ \ufffd _t\u2212_ 1 \u00af \ufffd _Z_ _s_ \ufffd _._ _s_ =1 The first equality holds by differentiation and evaluation at _\u03b7_ = 0 and the second one follows from (10.12). In view of the final equality, since _m_ [\ufffd] _[t]_ _s_ _[\u2212]_ =1 [1] _[Z]_ [\u00af] _[s]_ [ is fixed,] the direction **e** _k_ selected by coordinate descent is the one minimizing \u00af _\u03f5_ _t_ . By the inductive hypothesis D [\u00af] _t_ = D _t_ and \u00af _\u03f5_ _t_ = _\u03f5_ _t_, thus, the chosen base ranker corresponds exactly to the base ranker _h_ _t_ selected by RankBoost. The step size _\u03b7_ is identified by setting the derivative to zero in order to minimize the function in the chosen direction **e** _k_ . Thus, using identity 10.12 and the definition **250** **Chapter 10** **Ranking** of \u00af _\u03f5_ _t_, we can write: \u00af _dF_ ( _**\u03b1**_ _t\u2212_ 1 + _\u03b7_ **e** _k_ ) = 0 _d\u03b7_ _\u21d4\u2212_ _\u21d4\u2212_ _\u21d4\u2212_ _m_ \ufffd _y_ _i_ ( _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [))] _[e]_ _[\u2212][y]_ _[i]_ \ufffd _Nj_ =1 _[\u03b1]_ [\u00af] _[t][\u2212]_ [1] _[,j]_ [(] _[h]_ _[j]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[j]_ [(] _[x]_ _[i]_ [))] _e_ _[\u2212][\u03b7y]_ _[i]_ [(] _[h]_ _[k]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[k]_ [(] _[x]_ _[i]_ [))] = 0 _i_ =1 _m_ \ufffd _y_ _i_ ( _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)) \u00af][D] _[t]_ [(] _[i]_ [)] _[e]_",
    "chunk_id": "foundations_machine_learning_246"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u2212][\u03b7y]_ _[i]_ [(] _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[i]_ [))] = 0 _i_ =1 _m_ \ufffd _y_ _i_ ( _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ _[t]_ [(] _[x]_ _[i]_ [)) \u00af][D] _[t]_ [(] _[i]_ [)] \ufffd _m_ _i_ =1 _t\u2212_ 1 \ufffd _Z_ \u00af _s_ \ufffd _e_ _[\u2212][\u03b7y]_ _[i]_ [(] _[h]_ _[k]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[k]_ [(] _[x]_ _[i]_ [))] = 0 _s_ =1 _\u21d4\u2212_ [\u00af _\u03f5_ [+] _t_ _[e]_ _[\u2212][\u03b7]_ _[ \u2212]_ _[\u03f5]_ [\u00af] _[\u2212]_ _t_ _[e]_ _[\u03b7]_ [] = 0] _t_ _\u21d4_ _\u03b7_ = [1] _[\u03f5]_ \u00af [\u00af] [+] _._ 2 [log] _\u03f5_ _[\u2212]_ _t_ By the inductive hypothesis, we have \u00af _\u03f5_ [+] _t_ [=] _[ \u03f5]_ [+] _t_ [and \u00af] _[\u03f5]_ _[\u2212]_ _t_ [=] _[ \u03f5]_ _[\u2212]_ _t_ [and this proves that] the step size chosen by coordinate descent matches the base ranker weight _\u03b1_ _t_ of RankBoost. Thus, by combining the previous results we have _f_ [\u00af] _t_ = _f_ _t_ and the proof by induction is complete. This shows that coordinate descent applied to _F_ precisely coincides with the RankBoost algorithm. As in the classification case, other convex loss functions upper bounding the zeroone pairwise misranking loss can be used. In particular, the following objective function based on the logistic loss can be used: \u00af _**\u03b1**_ _\ufffd\u2192_ [\ufffd] _[m]_ _i_ =1 [log(1 +] _[ e]_ _[\u2212][y]_ _[i]_ [[] _[f]_ _[N]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][f]_ _[N]_ [(] _[x]_ _[i]_ [)]] ) to derive an alternative boosting-type algorithm. **10.4.3** **Margin bound for ensemble methods in ranking** To simplify the presentation, we will assume for the results of this section, as in section 10.2, that the pairwise labels are in _{\u2212_ 1 _,_ +1 _}_ . By lemma 7.4, the empirical Rademacher complexity of the convex hull conv(H) equals that of H. Thus, theorem 10.1 immediately implies the following guarantee for ensembles of hypotheses in ranking. **Corollary 10.4** _Let_ H _be a set of real-valued functions. Fix \u03c1 >_ 0 _; then, for any_ _\u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the choice of a sample S of size m, each_ _of the following ranking guarantees holds for all h \u2208_ conv(H) _:_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] \ufffdR [D] _m_ [1] [(][H][) +][ R] [D] _m_ [2] [(][H][)] \ufffd + _\u03c1_ \ufffd \ufffd log [1] _\u03b4_ (10.17) 2 _m_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] \ufffd\ufffdR _S_ 1 (H) + \ufffdR _S_ 2 (H)\ufffd + 3 _\u03c1_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _h_ ) + [2] log [2] _\u03b4_ (10.18) 2 _m_ _[.]_ **10.5** **Bipartite ranking** **251** For RankBoost, these bounds apply to _f/\u2225_ _**\u03b1**_ _\u2225_ 1, where _f_ is the hypothesis returned by the algorithm. Since _f_ and _f/\u2225_ _**\u03b1**_ _\u2225_ 1 induce the same ordering of the points, for any _\u03b4 >_ 0, the following holds with probability at least 1 _\u2212_ _\u03b4_ : _R_ ( _f_",
    "chunk_id": "foundations_machine_learning_247"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ") _\u2264_ _R_ [\ufffd] _S,\u03c1_ ( _f/\u2225_ _**\u03b1**_ _\u2225_ 1 ) + [2] \ufffdR [D] _m_ [1] [(][H][) +][ R] [D] _m_ [2] [(][H][)] \ufffd + _\u03c1_ \ufffd log [1] _\u03b4_ (10.19) 2 _m_ Remarkably, the number of rounds of boosting _T_ does not appear in this bound. The bound depends only on the margin _\u03c1_, the sample size _m_, and the Rademacher complexity of the family of base classifiers H. Thus, the bound guarantees an effective generalization if the pairwise margin loss _R_ [\ufffd] _S,\u03c1_ ( _f/\u2225_ _**\u03b1**_ _\u2225_ 1 ) is small for a relatively large _\u03c1_ . A bound similar to that of theorem 7.7 for AdaBoost can be derived for the empirical pairwise ranking margin loss of RankBoost (see exercise 10.3) and similar comments on that result apply here. These results provide a margin-based analysis in support of ensemble methods in ranking and RankBoost in particular. As in the case of AdaBoost, however, RankBoost in general does not achieve a maximum margin. But, in practice, it has been observed to obtain excellent pairwise ranking performances. **10.5** **Bipartite ranking** This section examines an important ranking scenario within the score-based setting, the _bipartite ranking problem_ . In this scenario, the set of points X is partitioned into two classes: X + the class of positive points, and X _\u2212_ that of negative ones. The problem consists of ranking positive points higher than negative ones. For example, for a fixed search engine query, the task consists of ranking relevant (positive) documents higher than irrelevant (negative) ones. The bipartite problem could be treated in the way already discussed in the previous sections with exactly the same theory and algorithms. However, the setup typically adopted for this problem is different: instead of assuming that the learner receives a sample of random pairs, here pairs of positive and negative elements, it is assumed that it receives a sample of positive points from some distribution and a sample of negative points from another. This leads to the set of all pairs made of a positive point of the first sample and a negative point of the second. More formally, the learner receives a sample _S_ + = ( _x_ _[\u2032]_ 1 _[, . . ., x]_ _[\u2032]_ _m_ [) drawn i.i.d. ac-] cording to some distribution D + over X +, and a sample _S_ _\u2212_ = ( _x_ 1 _, . . ., x_ _n_ ) drawn i.i.d. according to some distribution D _\u2212_ over X _\u2212_ . [17] Given a hypothesis set H of 17 This two-distribution formulation also avoids a potential dependency issue that can arise for some modeling of the problem: if pairs are drawn according to some distribution D over X _\u2212_ _\u00d7_ X + **252** **Chapter 10** **Ranking** functions mapping X to R, the learning problem consists of selecting a hypothesis _h \u2208_ H with small expected bipartite misranking or generalization error _R_ ( _h_ ): _R_ ( _h_ ) = P [ _h_ ( _x_ _[\u2032]_ ) _< h_ ( _x_ )] _._ (10.20) _x\u223c_ D _\u2212_ _x_",
    "chunk_id": "foundations_machine_learning_248"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u2032]_ _\u223c_ D + The empirical pairwise misranking or empirical error of _h_ is denoted by _R_ [\ufffd] _S_ + _,S_ _\u2212_ ( _h_ ) and defined by _n_ \ufffd 1 _h_ ( _x_ _[\u2032]_ _i_ [)] _[<h]_ [(] _[x]_ _[j]_ [)] _[ .]_ (10.21) _j_ =1 \ufffd 1 _R_ _S_ + _,S_ _\u2212_ ( _h_ ) = _mn_ _m_ \ufffd _i_ =1 Note that while the bipartite ranking problem bears some similarity with binary classification, in particular, the presence of two classes, they are distinct problems, since their objectives and measures of success clearly differ. By the definition of the formulation of the bipartite ranking just presented, the learning algorithm must typically deal with _mn_ pairs. For example, the application of SVMs to ranking in this scenario leads to an optimization with _mn_ slack variables or constraints. With just a thousand positive and a thousand negative points, one million pairs would need to be considered. This can lead to a prohibitive computational cost for some learning algorithms. The next section shows that RankBoost admits an efficient implementation in the bipartite scenario. **10.5.1** **Boosting in bipartite ranking** This section shows the efficiency of RankBoost in the bipartite scenario and discusses the connection between AdaBoost and RankBoost in this context. The key property of RankBoost leading to an efficient algorithm in the bipartite setting is the fact that its objective function is based on the exponential function. As a result, it can be decomposed into the product of two functions, one depending on only the positive and the other on only the negative points. Similarly, the distribution D _t_ maintained by the algorithm can be factored as the product of two distributions D [+] _t_ and D _[\u2212]_ _t_ [.] This is clear for the uniform distribution D 1 at the first round as for any _i \u2208_ [ _m_ ] and _j \u2208_ [ _n_ ], D 1 ( _i, j_ ) = 1 _/_ ( _mn_ ) = D [+] 1 [(] _[i]_ [)][D] _[\u2212]_ 1 [(] _[j]_ [) with] D [+] 1 [(] _[i]_ [) = 1] _[/m]_ [ and][ D] _[\u2212]_ 1 [(] _[j]_ [) = 1] _[/n]_ [. This property is recursively preserved since, in] view of the following, the decomposition of D _t_ implies that of D _t_ +1 for any _t \u2208_ [ _T_ ]. and the learner makes use of this information to augment its training sample, then the resulting sample is in general not i.i.d. This is because if ( _x_ 1 _, x_ _[\u2032]_ 1 [) and (] _[x]_ [2] _[, x]_ _[\u2032]_ 2 [) are in the sample, then so are] the pairs ( _x_ 1 _, x_ _[\u2032]_ 2 [) and (] _[x]_ [2] _[, x]_ _[\u2032]_ 1 [) and thus the pairs are not independent. However, without sample] augmentation, the points are i.i.d., and this issue does not arise. **10.5** **Bipartite ranking** **253** For any _i \u2208_ [ _m_ ] and _j \u2208_ [ _n_ ], by definition of the update, we can write: _[j]_ [)] _[e]_ _[\u2212][\u03b1]_ _[t]_ [[]",
    "chunk_id": "foundations_machine_learning_249"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[j]_ [)]] D _t_ +1 ( _i, j_ ) = [D] _[t]_ [(] _[i][,]_ _[t]_ [[] _[h]_ _[t]_ [(] _[x]_ _[\u2032]_ _i_ [)] _[\u2212][h]_ _[t]_ [(] _[x]_ _[j]_ [)]] = [D] _t_ [+] [(] _[i]_ [)] _[e]_ _[\u2212][\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] _Z_ _t_ _Z_ + _Z_ _t,_ + D _[\u2212]_ _t_ [(] _[j]_ [)] _[e]_ _[\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _[j]_ [)] _,_ _Z_ _t,\u2212_ since the normalization factor _Z_ _t_ can also be decomposed as _Z_ _t_ = _Z_ _t_ _[\u2212]_ _[Z]_ _t_ [+] [, with] _Z_ _t_ [+] = [\ufffd] _[m]_ _i_ =1 [D] _t_ [+] [(] _[i]_ [)] _[e]_ _[\u2212][\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] and _Z_ _t_ _[\u2212]_ = [\ufffd] _[n]_ _j_ =1 [D] _t_ _[\u2212]_ [(] _[j]_ [)] _[e]_ _[\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _[j]_ [)] [.] Furthermore, the pairwise misranking of a hypothesis _h \u2208_ H based on the distribution D _t_ used to determine _h_ _t_ can also be computed as the difference of two quantities, one depending only on positive points, the other only on negative ones: E _i_ [)] _[ \u2212]_ _[h]_ [(] _[x]_ _[j]_ [)] =] E [ E [ _h_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[h]_ [(] _[x]_ _[j]_ [)]] =] E [ _h_ ( _x_ _[\u2032]_ _i_ [)]] _[ \u2212]_ E [ _h_ ( _x_ _j_ )] _._ ( _i,j_ ) _\u223c_ D _t_ [[] _[h]_ [(] _[x]_ _[\u2032]_ _i\u223c_ D [+] _t_ _j\u223c_ D _[\u2212]_ _t_ _i\u223c_ D [+] _t_ _j\u223c_ D _[\u2212]_ _t_ Thus, the time and space complexity of RankBoost depends only on the total number of points _m_ + _n_ and not the number of pairs _mn_ . More specifically, ignoring the call to the weak ranker or the cost of determining _h_ _t_, the time and space complexity of each round is linear, that is, in _O_ ( _m_ + _n_ ). Furthermore, the cost of determining _h_ _t_ is a function of _O_ ( _m_ + _n_ ) and not _O_ ( _mn_ ). Figure 10.2 gives the pseudocode of the algorithm adapted to the bipartite scenario. In the bipartite scenario, a connection can be made between the classification algorithm AdaBoost and the ranking algorithm RankBoost. In particular, the objective function of RankBoost can be expressed as follows for any _**\u03b1**_ = ( _\u03b1_ 1 _, . . ., \u03b1_ _T_ ) _\u2208_ R _[T]_, _T \u2265_ 1: _n_ \ufffd exp( _\u2212_ [ _f_ ( _x_ _[\u2032]_ _i_ [)] _[ \u2212]_ _[f]_ [(] _[x]_ _[j]_ [)])] _i_ =1 _F_ RankBoost ( _**\u03b1**_ ) = _m_ \ufffd _j_ =1 _n_ \ufffd _[m]_ _e_ _[\u2212]_ [\ufffd] _t_ _[T]_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] [\ufffd\ufffd] \ufffd _i_ =1 =1 = \ufffd \ufffd _[m]_ \ufffd _e_ [+][ \ufffd] _t_ _[T]_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _[j]_ [)] [\ufffd] _j_ =1 = _F_ + ( _**\u03b1**_ ) _F_ _\u2212_ ( _**\u03b1**_ ) _,_ where _F_ + denotes the function",
    "chunk_id": "foundations_machine_learning_250"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "defined by the sum over the positive points and _F_ _\u2212_ the function defined over the negative points. The objective function of AdaBoost can be defined in terms of these same two functions as follows: _F_ AdaBoost ( _**\u03b1**_ ) = = _m_ \ufffd exp( _\u2212y_ _i_ _[\u2032]_ _[f]_ [(] _[x]_ _[\u2032]_ _i_ [)) +] _i_ =1 _m_ _e_ _[\u2212]_ [\ufffd] _t_ _[T]_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _i_ _[\u2032]_ [)] + \ufffd _i_ =1 _n_ \ufffd exp( _\u2212y_ _j_ _f_ ( _x_ _j_ )) _j_ =1 _n_ \ufffd _n_ \ufffd _e_ [+][ \ufffd] _t_ _[T]_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ [(] _[x]_ _[j]_ [)] _j_ =1 _n_ \ufffd = _F_ + ( _**\u03b1**_ ) + _F_ _\u2212_ ( _**\u03b1**_ ) _._ **254** **Chapter 10** **Ranking** BipartiteRankBoost( _S_ = ( _x_ _[\u2032]_ 1 _[, . . ., x]_ _[\u2032]_ _m_ _[, x]_ [1] _[, . . ., x]_ _[n]_ [))] 1 **for** _j \u2190_ 1 **to** _m_ **do** 2 D [+] 1 [(] _[j]_ [)] _[ \u2190]_ _m_ [1] 3 **for** _i \u2190_ 1 **to** _n_ **do** 4 D _[\u2212]_ 1 [(] _[i]_ [)] _[ \u2190]_ _n_ [1] 5 **for** _t \u2190_ 1 **to** _T_ **do** 6 _h_ _t_ _\u2190_ base ranker in H with smallest _\u03f5_ _[\u2212]_ _t_ _[\u2212]_ _[\u03f5]_ [+] _t_ [=] E [ _h_ ( _x_ _j_ )] _\u2212_ E [ _h_ ( _x_ _[\u2032]_ _i_ [)]] _[\u03f5]_ [+] _j\u223c_ D _[\u2212]_ _t_ _i\u223c_ D [+] _t_ 7 _\u03b1_ _t_ _\u2190_ [1] 2 [log] _\u03f5_ _[\u03f5]_ _[\u2212]_ _tt_ [+] 8 _Z_ _t_ [+] _[\u2190]_ [1] _[ \u2212]_ _[\u03f5]_ [+] _t_ [+] \ufffd 7 _\u03b1_ _t_ _\u2190_ [1] 8 _Z_ _t_ [+] _[\u2190]_ [1] _[ \u2212]_ _[\u03f5]_ [+] _t_ [+] \ufffd _\u03f5_ [+] _t_ _[\u03f5]_ _[\u2212]_ _t_ 9 **for** _i \u2190_ 1 **to** _m_ **do** D [+] _t_ [(] _[i]_ [) exp] \ufffd _\u2212\u03b1_ _t_ _h_ _t_ ( _x_ _[\u2032]_ _i_ [)] \ufffd 10 D [+] _t_ +1 [(] _[i]_ [)] _[ \u2190]_ [+] _t_ +1 _Z_ _t_ [+] 11 _Z_ _t_ _[\u2212]_ _[\u2190]_ [1] _[ \u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [+] ~~\ufffd~~ _\u03f5_ [+] _t_ _[\u03f5]_ _[\u2212]_ _t_ 11 _Z_ _t_ _[\u2212]_ _[\u2190]_ [1] _[ \u2212]_ _[\u03f5]_ _[\u2212]_ _t_ [+] ~~\ufffd~~ _\u03f5_ [+] _t_ _[\u03f5]_ _[\u2212]_ _t_ 12 **for** _j \u2190_ 1 **to** _n_ **do** D _[\u2212]_ _t_ [(] _[j]_ [) exp] \ufffd + _\u03b1_ _t_ _h_ _t_ ( _x_ _j_ ) \ufffd 13 D _[\u2212]_ _t_ +1 [(] _[j]_ [)] _[ \u2190]_ _Z_ _t_ 14 _f \u2190_ [\ufffd] _[T]_ _t_ =1 _[\u03b1]_ _[t]_ _[h]_ _[t]_ 15 **return** _f_ **Figure 10.2** Pseudocode of RankBoost in a bipartite setting, with H _\u2286{_ 0 _,_ 1 _}_ [X], _\u03f5_ [+] _t_ = E _i\u223c_ D + _t_ [[] _[h]_ [(] _[x]_ _i_ _[\u2032]_ [)] and] _\u03f5_ _[\u2212]_ _t_ [=][ E] _j\u223c_ D _[\u2212]_ _t_ [[] _[h]_ [(] _[x]_ _[j]_ [)].] Note that the gradient of the objective function of RankBoost can be expressed in terms of AdaBoost as follows: _\u2207_ _**\u03b1**_ _F_ RankBoost ( _**\u03b1**_ ) = _F_ _\u2212_ ( _**\u03b1**_ ) _\u2207_ _**\u03b1**_ _F_ + ( _**\u03b1**_ ) +",
    "chunk_id": "foundations_machine_learning_251"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_F_ + ( _**\u03b1**_ ) _\u2207_ _**\u03b1**_ _F_ _\u2212_ ( _**\u03b1**_ ) (10.22) = _F_ _\u2212_ ( _**\u03b1**_ )( _\u2207_ _**\u03b1**_ _F_ + ( _**\u03b1**_ ) + _\u2207_ _**\u03b1**_ _F_ _\u2212_ ( _**\u03b1**_ )) + ( _F_ + ( _**\u03b1**_ ) _\u2212_ _F_ _\u2212_ ( _**\u03b1**_ )) _\u2207_ _**\u03b1**_ _F_ _\u2212_ ( _**\u03b1**_ ) = _F_ _\u2212_ ( _**\u03b1**_ ) _\u2207_ _**\u03b1**_ _F_ AdaBoost ( _**\u03b1**_ ) + ( _F_ + ( _**\u03b1**_ ) _\u2212_ _F_ _\u2212_ ( _**\u03b1**_ )) _\u2207_ _**\u03b1**_ _F_ _\u2212_ ( _**\u03b1**_ ) _._ If _**\u03b1**_ is a minimizer of _F_ AdaBoost, then _\u2207_ _**\u03b1**_ _F_ AdaBoost ( _**\u03b1**_ ) = 0 and it can be shown that the equality _F_ + ( _**\u03b1**_ ) _\u2212_ _F_ _\u2212_ ( _**\u03b1**_ ) = 0 also holds for _**\u03b1**_, provided that the family of base hypotheses H used for AdaBoost includes the constant hypothesis _h_ 0 : _x \ufffd\u2192_ 1, which often is the case in practice. Then, by (10.22), this implies that _\u2207_ _**\u03b1**_ _F_ RankBoost ( _**\u03b1**_ ) = 0 and therefore that _**\u03b1**_ is also a minimizer of the convex **10.5** **Bipartite ranking** **255** **Image:** [No caption returned] **Figure 10.3** The AUC (area under the ROC curve) is a measure of the performance of a bipartite ranking. function _F_ RankBoost . In general, _F_ AdaBoost does not admit a minimizer. Nevertheless, it can be shown that if lim _k\u2192\u221e_ _F_ AdaBoost ( _**\u03b1**_ _k_ ) = inf _**\u03b1**_ _F_ AdaBoost ( _**\u03b1**_ ) for some sequence ( _**\u03b1**_ _k_ ) _k\u2208_ N, then, under the same assumption on the use of a constant base hypothesis and for a non-linearly separable dataset, the following holds: lim _k\u2192\u221e_ _F_ RankBoost ( _**\u03b1**_ _k_ ) = inf _**\u03b1**_ _F_ RankBoost ( _**\u03b1**_ ). The connections between AdaBoost and RankBoost just mentioned suggest that AdaBoost could achieve a good ranking performance as well. This is often observed empirically, a fact that brings strong support to the use of AdaBoost both as a classifier and a ranking algorithm. Nevertheless, RankBoost may converge faster and achieve a good ranking faster than AdaBoost. **10.5.2** **Area under the ROC curve** The performance of a bipartite ranking algorithm is typically reported in terms of the _area under the receiver operating characteristic (ROC) curve_, or the _area under_ _the curve_ ( _AUC_ ) for short. Let _U_ be a test sample used to evaluate the performance of _h_ (or a training sample) with _m_ positive points _z_ 1 _[\u2032]_ _[, . . ., z]_ _m_ _[\u2032]_ [and] _[ n]_ [ negative points] _[ z]_ [1] _[, . . ., z]_ _[n]_ [. For] any _h \u2208_ H, let _R_ [\ufffd] ( _h, U_ ) denote the average pairwise misranking of _h_ over _U_ . Then, the AUC of _h_ for the sample _U_ is precisely 1 _\u2212_ _R_ [\ufffd] ( _h, U_ ), that is, its average pairwise ranking accuracy on _U_ : _n_ \ufffd 1 _h_ ( _z_ _i_ _[\u2032]_ [)] _[\u2265][h]_ [(] _[z]_ _[j]_ [)] [ =] P _j_ =1 _z\u223c_ D [\ufffd] _[\u2212]_ _U_",
    "chunk_id": "foundations_machine_learning_252"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_z_ _[\u2032]_ _\u223c_ D [\ufffd] [+] _U_ 1 AUC( _h, U_ ) = _mn_ _m_ \ufffd _i_ =1 [ _h_ ( _z_ _[\u2032]_ ) _\u2265_ _h_ ( _z_ )] _._ Here, D [\ufffd] [+] _U_ [denotes the empirical distribution corresponding to the positive points in] _U_ and D [\ufffd] [+] _U_ [the empirical distribution corresponding to the negative ones. AUC(] _[h, U]_ [)] **256** **Chapter 10** **Ranking** **Image:** [No caption returned] **Figure 10.4** _U_, and by definition it is in [0 _,_ 1]. Higher AUC values correspond to a better ranking performance. In particular, an AUC of one indicates that the points of _U_ are ranked perfectly using _h_ . AUC( _h, U_ ) can be computed in linear time from a sorted array containing the _m_ + _n_ elements _h_ ( _z_ _i_ _[\u2032]_ [) and] _[ h]_ [(] _[z]_ _[j]_ [), for] _[ i][ \u2208]_ [[] _[m]_ [] and] _[ j][ \u2208]_ [[] _[n]_ [].] Assuming that the array is sorted in increasing order (with a positive point placed higher than a negative one if they both have the same scores) the total number of correctly ranked pairs _r_ can be computed as follows. Starting with _r_ = 0, the array is inspected in increasing order of the indices while maintaining at any time the number of negative points seen _n_ and incrementing the current value of _r_ with _n_ whenever a positive point is found. After full inspection of the array, the AUC is given by _r/_ ( _mn_ ). Thus, assuming that a comparison-based sorting algorithm is used, the complexity of the computation of the AUC is in _O_ (( _m_ + _n_ ) log( _m_ + _n_ )). As indicated by its name, the AUC coincides with the area under the ROC curve (figure 10.3). An ROC curve plots the _true positive rate_, that is, the percentage of positive points correctly predicted as positive as a function of the _false positive_ _rate_, that is, the percentage of negative points incorrectly predicted as positive. Figure 10.4 illustrates the definition and construction of an ROC curve. Points are generated along the curve by varying a threshold value _\u03b8_ as in the right panel of figure 10.4, from higher values to lower ones. The threshold is used to determine the label of any point _x_ (positive or negative) based on sgn( _h_ ( _x_ ) _\u2212_ _\u03b8_ ). At one extreme, all points are predicted as negative; thus, the false positive rate is zero, but the true positive rate is zero as well. This gives the first point (0 _,_ 0) of the plot. At the other extreme, all points are predicted as positive; thus, both the true and the false positive rates are equal to one, which gives the point (1 _,_ 1). In the ideal case, as already discussed, the AUC value is one, and, with the exception of (0 _,_ 0), the curve coincides with a horizontal line reaching (1 _,_ 1). **10.6** **Preference-based setting** **257** **10.6** **Preference-based setting** This section examines a different setting for the",
    "chunk_id": "foundations_machine_learning_253"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "problem of learning to rank: the _preference-based setting_ . In this setting, the objective is to rank as accurately as possible any test subset _X \u2286_ X, typically a finite set that we refer to as a _finite query_ _subset_ . This is close to the query-based scenario of search engines or information extraction systems and the terminology stems from the fact that _X_ could be a set of items needed to rank in response to a particular query. The advantage of this setting over the score-based setting is that here the learning algorithm is not required to return a linear ordering of all points of X, which may be impossible to achieve faultlessly in accordance with a general possibly non-transitive pairwise preference labeling. Supplying a correct linear ordering for a query subset is more likely to be achievable exactly or at least with a better approximation. The preference-based setting consists of two stages. In the first stage, a sample of labeled pairs _S_, exactly as in the score-based setting, is used to learn a _preference_ _function h_ : X _\u00d7_ X _\ufffd\u2192_ [0 _,_ 1], that is, a function that assigns a higher value to a pair ( _u, v_ ) when _u_ is preferred to _v_ or is to be ranked higher than _v_, and smaller values in the opposite case. This preference function can be obtained as the output of a standard classification algorithm trained on _S_ . A crucial difference with the score-based setting is that, in general, the preference function _h_ is not required to induce a linear ordering. The relation it induces may be non-transitive; thus, we may have, for example, _h_ ( _u, v_ ) = _h_ ( _v, w_ ) = _h_ ( _w, u_ ) = 1 for three distinct points _u_, _v_, and _w_ . In the second stage, given a query subset _X \u2286_ X, the preference function _h_ is used to determine a ranking of _X_ . How can _h_ be used to generate an accurate ranking? This will be the main focus of this section. The computational complexity of the algorithm determining the ranking is also crucial. Here, we will measure its running time complexity in terms of the number of calls to _h_ . When the preference function is obtained as the output of a binary classification algorithm, the preference-based setting can be viewed as a reduction of ranking to classification: the second stage specifies how a ranking is obtained from a classifier\u2019s output. **10.6.1** **Second-stage ranking problem** The ranking problem of the second stage is modeled as follows. We assume that a preference function _h_ is given. From the point of view of this stage, the way the function _h_ has been determined is immaterial, it can be viewed as a black box. As already discussed, _h_ is not assumed to be transitive. But, we will assume that it is _pairwise consistent_, that is _h_ ( _u, v_ ) + _h_ ( _v, u_ ) = 1, for all _u, v \u2208_ X. **258** **Chapter 10** **Ranking**",
    "chunk_id": "foundations_machine_learning_254"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Let D be an unknown distribution according to which pairs ( _X, \u03c3_ _[\u2217]_ ) are drawn where _X \u2286_ X is a query subset and _\u03c3_ _[\u2217]_ a target ranking or permutation of _X_, that is, a bijective function from _X_ to _{_ 1 _, . . ., |X|}_ . Thus, we consider a stochastic scenario, and _\u03c3_ _[\u2217]_ is a random variable. The objective of a second-stage algorithm _A_ consists of using the preference function _h_ to return an accurate ranking _A_ ( _X_ ) for any query subset _X_ . The algorithm may be deterministic, in which case _A_ ( _X_ ) is uniquely determined from _X_ or it may be randomized, in which case we denote by _s_ the randomization seed it may depend on. The following loss function _L_ can be used to measure the disagreement between a ranking _\u03c3_ and a desired one _\u03c3_ _[\u2217]_ over a set _X_ of _n \u2265_ 1 elements: _\u0338_ _\u0338_ 2 _L_ ( _\u03c3, \u03c3_ _[\u2217]_ ) = _n_ ( _n \u2212_ 1) _\u0338_ _\u0338_ \ufffd 1 _\u03c3_ ( _u_ ) _<\u03c3_ ( _v_ ) 1 _\u03c3_ _\u2217_ ( _v_ ) _<\u03c3_ _\u2217_ ( _u_ ) _,_ (10.23) _u_ = _\u0338_ _v_ _\u0338_ _\u0338_ where the sum runs over all pairs ( _u, v_ ) with _u_ and _v_ distinct elements of _X_ . All the results presented in the following hold for a broader set of loss functions described later. Abusing the notation, we also define the loss of the preference function _h_ with respect to a ranking _\u03c3_ _[\u2217]_ of a set _X_ of _n \u2265_ 1 elements by _\u0338_ _\u0338_ 2 _L_ ( _h, \u03c3_ _[\u2217]_ ) = _n_ ( _n \u2212_ 1) _\u0338_ _\u0338_ \ufffd _h_ ( _u, v_ )1 _\u03c3_ _\u2217_ ( _v_ ) _<\u03c3_ _\u2217_ ( _u_ ) _._ (10.24) _u_ = _\u0338_ _v_ _\u0338_ _\u0338_ The expected loss for a deterministic algorithm _A_ is thus E ( _X,\u03c3_ _\u2217_ ) _\u223c_ D [ _L_ ( _A_ ( _X_ ) _, \u03c3_ _[\u2217]_ )]. The _regret_ of algorithm _A_ is then defined as the difference between its loss and that of the best fixed global ranking. This can be written as follows: Reg( _A_ ) = ( _X,\u03c3_ E _[\u2217]_ ) _\u223c_ D [[] _[L]_ [(] _[A]_ [(] _[X]_ [)] _[, \u03c3]_ _[\u2217]_ [)]] _[ \u2212]_ [min] _\u03c3_ _[\u2032]_ ( _X,\u03c3_ E _[\u2217]_ ) _\u223c_ D [[] _[L]_ [(] _[\u03c3]_ _|_ _[\u2032]_ _X_ _[, \u03c3]_ _[\u2217]_ [)]] _[,]_ (10.25) where _\u03c3_ _[\u2032]_ _|X_ [denotes the ranking induced on] _[ X]_ [ by a global ranking] _[ \u03c3]_ _[\u2032]_ [ of][ X][. Similarly,] we define the regret of the preference function as follows Reg( _h_ ) = ( _X,\u03c3_ E _[\u2217]_ ) _\u223c_ D [[] _[L]_ [(] _[h]_ _[|][X]_ _[, \u03c3]_ _[\u2217]_ [)]] _[ \u2212]_ [min] _h_ _[\u2032]_ ( _X,\u03c3_ E _[\u2217]_ ) _\u223c_ D [[] _[L]_ [(] _[h]_ _|_ _[\u2032]_ _X_ _[, \u03c3]_ _[\u2217]_ [)]] _[,]_ (10.26) where _h_ _|X_ denotes the restriction of _h_ to _X \u00d7 X_, and similarly with _h_ _[\u2032]_",
    "chunk_id": "foundations_machine_learning_255"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ". The regret results presented in this section hold assuming the following _pairwise independence_ _on irrelevant alternatives_ property: E E (10.27) _\u03c3_ _[\u2217]_ _|X_ 1 [[1] _[\u03c3]_ _[\u2217]_ [(] _[v]_ [)] _[<\u03c3]_ _[\u2217]_ [(] _[u]_ [)] [] =] _\u03c3_ _[\u2217]_ _|X_ 2 [[1] _[\u03c3]_ _[\u2217]_ [(] _[v]_ [)] _[<\u03c3]_ _[\u2217]_ [(] _[u]_ [)] []] _[,]_ **10.6** **Preference-based setting** **259** for any _u, v \u2208_ X and any two sets _X_ 1 and _X_ 2 containing _u_ and _v_, and where _\u03c3_ _[\u2217]_ _|X_ denotes the random variable _\u03c3_ _[\u2217]_ conditioned on _X_ . [18] Similar regret definitions can be given for a randomized algorithm additionally taking the expectation over _s_ . Clearly, the quality of the ranking output by the second-stage algorithm intimately depends on that of the preference function _h_ . In the next sections, we discuss both a deterministic and a randomized second-stage algorithm for which the regret can be upper bounded in terms of the regret of the preference function. **10.6.2** **Deterministic algorithm** A natural deterministic algorithm for the second-stage is based on the _sort-by-degree_ _algorithm_ . This consists of ranking each element of _X_ based on the number of other elements it is preferred to according to the preference function _h_ . Let _A_ sort-by-degree denote this algorithm. In the bipartite setting, the following bounds can be proven for the expected loss of this algorithm and its regret: E (10.30) _X,\u03c3_ _[\u2217]_ [[] _[L]_ [(] _[A]_ [sort-by-degree] [(] _[X]_ [)] _[, \u03c3]_ _[\u2217]_ [)]] _[ \u2264]_ [2][ E] _X,\u03c3_ _[\u2217]_ [[] _[L]_ [(] _[h, \u03c3]_ _[\u2217]_ [)]] Reg( _A_ sort-by-degree ( _X_ )) _\u2264_ 2 Reg( _h_ ) _._ (10.31) These results show that the sort-by-degree algorithm can achieve an accurate ranking when the loss or the regret of the preference function _h_ is small. They also bound the _ranking_ loss or regret of the algorithm in terms of the _classification_ loss or regret of _h_, which can be viewed as a guarantee for the reduction of ranking to classification using the sort-by-degree algorithm. Nevertheless, in some cases, the guarantee given by these results is weak or uninformative owing to the presence of the factor of two. Consider the case of a binary classifier _h_ with an error rate of just 25 percent, which is quite reasonable in many applications. Assume that the Bayes error is close to zero for the classification problem and, similarly, that for the ranking problem the regret and loss approximately coincide. Then, using the bound in (10.30) guarantees a worst-case pairwise misranking error of at most 50 percent for the ranking algorithm, which is the pairwise misranking error of random ranking. 18 More generally, they hold without that assumption using the following weaker notions of regret: Reg _[\u2032]_ ( _A_ ) = E ( _X,\u03c3_ _[\u2217]_ ) _\u223c_ D [[] _[L]_ [(] _[A]_ [(] _[X]_ [)] _[, \u03c3]_ _[\u2217]_ [)]] _[ \u2212]_ _X_ [E] min E (10.28) \ufffd _\u03c3_ _[\u2032]_ _\u03c3_ _[\u2217]_ _|X_ [[] _[L]_ [(] _[\u03c3]_ _[\u2032]_ _[, \u03c3]_ _[\u2217]_ [)]] \ufffd Reg _[\u2032]_ ( _h_ ) = E ( _X,\u03c3_ _[\u2217]_",
    "chunk_id": "foundations_machine_learning_256"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ") _\u223c_ D [[] _[L]_ [(] _[h]_ _[|][X]_ _[, \u03c3]_ _[\u2217]_ [)]] _[ \u2212]_ _X_ [E] min E _,_ (10.29) \ufffd _h_ _[\u2032]_ _\u03c3_ _[\u2217]_ _|X_ [[] _[L]_ [(] _[h]_ _[\u2032]_ _[, \u03c3]_ _[\u2217]_ [)]] \ufffd where _\u03c3_ _[\u2032]_ denotes a ranking of _X_ and _h_ _[\u2032]_ a preference function defined over _X \u00d7 X_ . **Image:** [No caption returned] **Figure 10.5** the factor of two appearing in the regret guarantee of the sort-by-degree algorithm. **Theorem 10.5 (Lower bound for deterministic algorithms)** _For any deterministic algori-_ _thm A, there is a bipartite distribution for which_ Reg( _A_ ) _\u2265_ 2 Reg( _h_ ) _._ (10.32) Proof: Consider the simple case where X = _X_ = _{u, v, w}_ and where the preference function induces a cycle as illustrated by figure 10.5a. An arrow from _u_ to _v_ indicates that _v_ is preferred to _u_ according to _h_ . The proof is based on an adversarial choice of the target _\u03c3_ _[\u2217]_ . Without loss of generality, either _A_ returns the ranking _u, v, w_ (figure 10.5b) or _w, v, u_ (figure 10.5c). In the first case, let _\u03c3_ _[\u2217]_ be defined by the labeling indicated in the figure. In that case, we have _L_ ( _h, \u03c3_ _[\u2217]_ ) = 1 _/_ 3, since _u_ is preferred to _w_ according to _h_ while _w_ is labeled positively and _u_ negatively. The loss of the algorithm is _L_ ( _A, \u03c3_ _[\u2217]_ ) = 2 _/_ 3, since both _u_ and _v_ are ranked higher than the positively labeled _w_ by the algorithm. Similarly, _\u03c3_ _[\u2217]_ can be defined as in figure 10.5c in the second case, and we find again that _L_ ( _h, \u03c3_ _[\u2217]_ ) = 1 _/_ 3 and _L_ ( _A, \u03c3_ _[\u2217]_ ) = 2 _/_ 3. This concludes the proof. The theorem suggests that randomization is necessary in order to achieve a better guarantee. In the next section, we present a randomized algorithm that benefits both from better guarantees and a better time complexity. **10.6.3** **Randomized algorithm** The general idea of the algorithm described in this section is to use a straightforward extension of the randomized QuickSort algorithm in the second stage. Unlike in **Image:** [No caption returned] **Figure 10.6** preference function, which in general is not transitive. Nevertheless, it can be shown here, too, that the expected time complexity of the algorithm is in _O_ ( _n_ log _n_ ) when applied to an array of size _n_ . The algorithm works as follows, as illustrated by figure 10.6. At each recursive step, a _pivot_ element _u_ is selected uniformly at random from _X_ . For each _v \u0338_ = _u_, _v_ is placed on the left of _u_ with probability _h_ ( _v, u_ ) and to its right with the remaining probability _h_ ( _u, v_ ). The algorithm proceeds recursively with the array to the left of _u_ and the one to its right and returns the concatenation of the permutation returned by the left recursion, _u_, and the permutation returned by",
    "chunk_id": "foundations_machine_learning_257"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the right recursion. Let _A_ QuickSort denote this algorithm. In the bipartite setting, the following guarantees can be proven: E E (10.33) _X,\u03c3_ _[\u2217]_ _,s_ [[] _[L]_ [(] _[A]_ [QuickSort] [(] _[X, s]_ [)] _[, \u03c3]_ _[\u2217]_ [)] =] _X,\u03c3_ _[\u2217]_ [[] _[L]_ [(] _[h, \u03c3]_ _[\u2217]_ [)]] Reg( _A_ QuickSort ) _\u2264_ Reg( _h_ ) _._ (10.34) Thus, here, the factor of two of the bounds in the deterministic case has vanished, which is substantially more favorable. Furthermore, the guarantee for the loss is an equality. Moreover, the expected time complexity of the algorithm is only in _O_ ( _n_ log _n_ ), and, if only the top _k_ items are needed to be ranked, as in many applications, the time complexity is reduced to _O_ ( _n_ + _k_ log _k_ ). For the QuickSort algorithm, the following guarantee can also be proven in the case of general ranking setting (not necessarily bipartite setting): E (10.35) _X,\u03c3_ _[\u2217]_ _,s_ [[] _[L]_ [(] _[A]_ [QuickSort] [(] _[X, s]_ [)] _[, \u03c3]_ _[\u2217]_ [)]] _[ \u2264]_ [2][ E] _X,\u03c3_ _[\u2217]_ [[] _[L]_ [(] _[h, \u03c3]_ _[\u2217]_ [)]] _[.]_ **262** **Chapter 10** **Ranking** **10.6.4** **Extension to other loss functions** All of the results just presented hold for a broader class of loss functions _L_ _\u03c9_ defined in terms of a _weight function_ or _emphasis function \u03c9_ . _L_ _\u03c9_ is similar to (10.23), but measures the weighted disagreement between a ranking _\u03c3_ and a desired one _\u03c3_ _[\u2217]_ over a set _X_ of _n \u2265_ 1 elements as follows: _\u0338_ _\u0338_ 2 _L_ _\u03c9_ ( _\u03c3, \u03c3_ _[\u2217]_ ) = _n_ ( _n \u2212_ 1) _\u0338_ _\u0338_ \ufffd _\u03c9_ ( _\u03c3_ _[\u2217]_ ( _v_ ) _, \u03c3_ _[\u2217]_ ( _u_ )) 1 _\u03c3_ ( _u_ ) _<\u03c3_ ( _v_ ) 1 _\u03c3_ _\u2217_ ( _v_ ) _<\u03c3_ _\u2217_ ( _u_ ) _,_ (10.36) _u_ = _\u0338_ _v_ _\u0338_ _\u0338_ where the sum runs over all pairs ( _u, v_ ) with _u_ and _v_ distinct elements of _X_, and where _\u03c9_ is a symmetric function whose properties are described below. Thus, the loss counts the number of pairwise misrankings of _\u03c3_ with respect to _\u03c3_ _[\u2217]_, each weighted by _\u03c9_ . The function _\u03c9_ is assumed to satisfy the following three natural axioms: _\u2022_ symmetry: _\u03c9_ ( _i, j_ ) = _\u03c9_ ( _j, i_ ) for all _i, j_ ; _\u2022_ monotonicity: _\u03c9_ ( _i, j_ ) _\u2264_ _\u03c9_ ( _i, k_ ) if either _i < j < k_ or _i > j > k_ ; _\u2022_ triangle inequality: _\u03c9_ ( _i, j_ ) _\u2264_ _\u03c9_ ( _i, k_ ) + _\u03c9_ ( _k, j_ ). The motivation for this last property stems from the following: if correctly ordering items in positions ( _i, k_ ) and ( _k, j_ ) is not of great importance, then the same should hold for items in positions ( _i, j_ ). Using different functions _\u03c9_, the family of functions _L_ _\u03c9_ can cover several familiar and important losses. Here are some examples. Setting",
    "chunk_id": "foundations_machine_learning_258"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u03c9_ ( _i, j_ ) = 1 for all _i \u0338_ = _j_ yields the unweighted pairwise misranking measure. For a fixed integer _k \u2265_ 1, the function _\u03c9_ defined by _\u03c9_ ( _i, j_ ) = 1 (( _i\u2264k_ ) _\u2228_ ( _j\u2264k_ )) _\u2227_ ( _i_ = _\u0338_ _j_ ) for all ( _i, j_ ) can be used to emphasize ranking at the top _k_ elements. Misranking of pairs with at least one element ranked among the top _k_ is penalized by this function. This can be of interest in applications such as information extraction or search engines where the ranking of the top documents matters more. For this emphasis function, all elements ranked below _k_ are in a tie. Any tie relation can be encoded using _\u03c9_ . Finally, in a bipartite ranking scenario with _m_ [+] positive and _m_ _[\u2212]_ negative points and _m_ [+] + _m_ _[\u2212]_ = _n_, choosing _\u03c9_ ( _i, j_ ) = 2 _nm_ ( _n_ _[\u2212]_ _\u2212m_ 1 [+] ) [ yields the standard loss function] coinciding with 1 _\u2212_ AUC. **10.7** **Other ranking criteria** The objective function for the ranking problems discussed in this chapter were all based on pairwise misranking. Other ranking criteria have been introduced in information retrieval and used to derive alternative ranking algorithms. Here, we briefly present several of these criteria. **10.8** **Chapter notes** **263** _\u2022_ Precision, precision@ _n_, average precision, recall. All of these criteria assume that points are partitioned into two classes (positives and negatives), as in the bipartite ranking setting. _Precision_ is the fraction of positively predicted points that are in fact positive. Whereas precision takes into account all positive predictions, _precision@n_ only considers the top _n_ predictions. For example, precision@5 considers only the top 5 positively predicted points. _Average precision_ involves computing precision@ _n_ for each value of _n_, and averaging across these values. Each precision@ _n_ computation can be interpreted as computing precision for a fixed value of _recall_, or the fraction of positive points that are predicted to be positive (recall coincides with the notion of true positive rate). _\u2022_ DCG, NDCG. These criteria assume the existence of relevance scores associated with the points to be ranked, e.g., given a web search query, each website returned by a search engine has an associated relevance score. Moreover, these criteria measure the extent to which points with large relevance scores appear at or near the beginning of a ranking. Define ( _c_ _i_ ) _i\u2208_ N as a predefined sequence of non-increasing and non-negative discount factors, e.g., _c_ _i_ = log( _i_ ) _[\u2212]_ [1] . Then, given a ranking of _m_ points and defining _r_ _i_ as the relevance score of the _i_ th point in this ranking, the _discounted cumulative gain (DCG)_ is defined as _DCG_ = [\ufffd] _[m]_ _i_ =1 _[c]_ _[i]_ _[r]_ _[i]_ [. Note] that DCG is an increasing function of _m_ . In contrast, the _normalized discounted_ _cumulative gain (NDCG)_ normalizes the DCG across values of _m_ by dividing the DCG by the IDCG, or the",
    "chunk_id": "foundations_machine_learning_259"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "ideal DCG that would result from an optimal ordering of the points. **10.8** **Chapter notes** The problem of learning to rank is distinct from the purely algorithmic one of rank aggregation, which, as shown by Dwork, Kumar, Naor, and Sivakumar [2001], is NP-hard even for _k_ = 4 rankings. The Rademacher complexity and margin-based generalization bounds for pairwise ranking given in theorem 10.1 and corollary 6.13 are novel. Margin bounds based on covering numbers were also given by Rudin, Cortes, Mohri, and Schapire [2005]. Other learning bounds in the score-based setting of ranking, including VC-dimension and stability-based learning bounds, have been given by Agarwal and Niyogi [2005], Agarwal et al. [2005] and Cortes et al. [2007b]. The ranking algorithm based on SVMs presented in section 10.3 has been used and discussed by several researchers. One early and specific discussion of its use can be found in Joachims [2002]. The fact that the algorithm is simply a special instance of SVMs seems not to be clearly stated in the literature. The theoretical justification presented here for its use in ranking is novel. **264** **Chapter 10** **Ranking** RankBoost was introduced by Freund et al. [2003]. The version of the algorithm presented here is the coordinate descent RankBoost from Rudin et al. [2005]. RankBoost in general does not achieve a maximum margin and may not increase the margin at each iteration. A Smooth Margin ranking algorithm [Rudin et al., 2005] based on a modified version of the objective function of RankBoost can be shown to increase the smooth margin at every iteration, but the comparison of its empirical performance with that of RankBoost has not been reported. For the empirical ranking quality of AdaBoost and the connections between AdaBoost and RankBoost in the bipartite setting, see Cortes and Mohri [2003] and Rudin et al. [2005]. The Receiver Operating Characteristics (ROC) curves were originally developed in signal detection theory [Egan, 1975] in connection with radio signals during World War II. They also had applications to psychophysics [Green and Swets, 1966] and have been used since then in a variety of other applications, in particular for medical decision making. The area under an ROC curve (AUC) is equivalent to the Wilcoxon-Mann-Whitney statistic [Hanley and McNeil, 1982] and is closely related to the Gini index [Breiman et al., 1984] (see also chapter 9). For a statistical analysis of the AUC and confidence intervals depending on the error rate, see Cortes and Mohri [2003, 2005]. The deterministic algorithm in the preference-based setting discussed in this chapter was presented and analyzed by Balcan et al. [2008]. The randomized algorithm as well as much of the results presented in section 10.6 are due to Ailon and Mohri [2008]. A somewhat related problem of _ordinal regression_ has been studied by some authors [McCullagh, 1980, McCullagh and Nelder, 1983, Herbrich et al., 2000] which consists of predicting the correct label of each item out of a finite set, as in multiclass classification, with the additional assumption of an ordering among the labels. This problem is distinct, however, from the pairwise ranking problem discussed",
    "chunk_id": "foundations_machine_learning_260"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "in this chapter. The DCG ranking criterion was introduced by J\u00a8arvelin and Kek\u00a8al\u00a8ainen [2000], and has been used and discussed in a number of subsequent studies, in particular Cossock and Zhang [2008] who consider a subset ranking problem formulated in terms of DCG, for which they consider a regression-based solution. **10.9** **Exercises** 10.1 Uniform margin-bound for ranking. Use theorem 10.1 to derive a margin-based learning bound for ranking that holds uniformly for all _\u03c1 >_ 0 (see similar binary classification bounds of theorem 5.9 and exercise 5.2). **10.9** **Exercises** **265** 10.2 On-line ranking. Give an on-line version of the SVM-based ranking algorithm presented in section 10.3. 10.3 Empirical margin loss of RankBoost. Derive an upper bound on the empirical pairwise ranking margin loss of RankBoost similar to that of theorem 7.7 for AdaBoost. 10.4 Margin maximization and RankBoost. Give an example showing that RankBoost does not achieve the maximum margin, as in the case of AdaBoost. 10.5 RankPerceptron. Adapt the Perceptron algorithm to derive a pairwise ranking algorithm based on a linear scoring function. Assume that the training sample is linear separable for pairwise ranking. Give an upper bound on the number of updates made by the algorithm in terms of the ranking margin. 10.6 Margin-maximization ranking. Give a linear programming (LP) algorithm returning a linear hypothesis for pairwise ranking based on margin maximization. 10.7 Bipartite ranking. Suppose that we use a binary classifier for ranking in the bipartite setting. Prove that if the error of the binary classifier is _\u03f5_, then that of the ranking it induces is also at most _\u03f5_ . Show that the converse does not hold. 10.8 Multipartite ranking. Consider the ranking scenario in a _k_ -partite setting where X is partitioned into _k_ subsets X 1 _, . . .,_ X _k_ with _k \u2265_ 1. The bipartite case ( _k_ = 2) is already specifically examined in the chapter. Give a precise formulation of the problem in terms of _k_ distributions. Does RankBoost admit an efficient implementation in this case? Give the pseudocode of the algorithm. 10.9 Deviation bound for the AUC. Let _h_ be a fixed scoring function used to rank the points of X. Use Hoeffding\u2019s bound to show that with high probability the AUC of _h_ for a finite sample is close to its average. 10.10 _k_ -partite weight function. Show how the weight function _\u03c9_ can be defined so that _L_ _\u03c9_ encodes the natural loss function associated to a _k_ -partite ranking scenario. # 11 Regression This chapter discusses in depth the learning problem of _regression_, which consists of using data to predict, as closely as possible, the correct real-valued labels of the points or items considered. Regression is a common task in machine learning with a variety of applications, which justifies the specific chapter we reserve to its analysis. The learning guarantees presented in the previous sections focused largely on classification problems. Here we present generalization bounds for regression, both for finite and infinite hypothesis sets. Several of these learning bounds are based on the familiar notion of Rademacher",
    "chunk_id": "foundations_machine_learning_261"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "complexity, which is useful for characterizing the complexity of hypothesis sets in regression as well. Others are based on a combinatorial notion of complexity tailored to regression that we will introduce, _pseudo-dimension_, which can be viewed as an extension of the VC-dimension to regression. We describe a general technique for reducing regression problems to classification and deriving generalization bounds based on the notion of pseudodimension. We present and analyze several regression algorithms, including _linear_ _regression_, _kernel ridge regression_, _support-vector regression_, _Lasso_, and several on-line versions of these algorithms. We discuss in detail the properties of these algorithms, including the corresponding learning guarantees. **11.1** **The problem of regression** We first introduce the learning problem of regression. Let X denote the input space and Y a measurable subset of R. Here, we will adopt the stochastic scenario and will denote by D a distribution over X _\u00d7_ Y. As discussed in section 2.4.1, the deterministic scenario is a straightforward special case where input points admit a unique label determined by a target function _f_ : X _\u2192_ Y. As in all supervised learning problems, the learner receives a labeled sample _S_ = \ufffd( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )\ufffd _\u2208_ (X _\u00d7_ Y) _[m]_ drawn i.i.d. according to D. Since the labels are real numbers, it is not reasonable to hope that the learner could predict **268** **Chapter 11** **Regression** precisely the correct label when it is unique, or precisely its average label. Instead, we can require that its predictions be close to the correct ones. This is the key difference between regression and classification: in regression, the measure of error is based on the magnitude of the difference between the real-valued label predicted and the true or correct one, and not based on the equality or inequality of these two values. We denote by _L_ : Y _\u00d7_ Y _\u2192_ R + the _loss function_ used to measure the magnitude of error. The most common loss function used in regression is the _squared loss L_ 2 defined by _L_ ( _y, y_ _[\u2032]_ ) = _|y_ _[\u2032]_ _\u2212_ _y|_ [2] for all _y, y_ _[\u2032]_ _\u2208_ Y, or, more generally, an _L_ _p_ loss defined by _L_ ( _y, y_ _[\u2032]_ ) = _|y_ _[\u2032]_ _\u2212_ _y|_ _[p]_, for some _p \u2265_ 1 and all _y, y_ _[\u2032]_ _\u2208_ Y. Given a hypothesis set H of functions mapping X to Y, the regression problem consists of using the labeled sample _S_ to find a hypothesis _h \u2208_ H with small expected loss or generalization error _R_ ( _h_ ) with respect to the target _f_ : _R_ ( _h_ ) = E ( _x,y_ ) _\u223c_ D \ufffd _L_ \ufffd _h_ ( _x_ ) _, y_ \ufffd\ufffd _._ (11.1) As in the previous chapters, the empirical loss or error of\ufffd _h \u2208_ H is denoted by _R_ _S_ ( _h_ ) and defined by \ufffd _R_ _S_ ( _h_ ) = [1] _m_ _m_ \ufffd _L_ \ufffd _h_ ( _x_ _i_ )",
    "chunk_id": "foundations_machine_learning_262"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_, y_ _i_ \ufffd _._ (11.2) _i_ =1 In the common case where _L_ is the squared loss, this represents the _mean squared_ _error_ of _h_ on the sample _S_ . When the loss function _L_ is bounded by some _M >_ 0, that is _L_ ( _y_ _[\u2032]_ _, y_ ) _\u2264_ _M_ for all _y, y_ _[\u2032]_ _\u2208_ Y or, more strictly, _L_ ( _h_ ( _x_ ) _, y_ ) _\u2264_ _M_ for all _h \u2208_ H and ( _x, y_ ) _\u2208_ X _\u00d7_ Y, the problem is referred to as a _bounded regression problem_ . Much of the theoretical results presented in the following sections are based on that assumption. The analysis of _unbounded regression problems_ is technically more elaborate and typically requires some other types of assumptions. **11.2** **Generalization bounds** This section presents learning guarantees for bounded regression problems. We start with the simple case of a finite hypothesis set. **11.2.1** **Finite hypothesis sets** In the case of a finite hypothesis, we can derive a generalization bound for regression by a straightforward application of Hoeffding\u2019s inequality and the union bound. **Theorem 11.1** _Let L be a bounded loss function. Assume that the hypothesis set_ H _is_ _finite. Then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, the following inequality_ **11.2** **Generalization bounds** **269** _holds for all h \u2208_ H _:_ \ufffd _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + _M_ log _|_ H _|_ + log [1] _\u03b4_ 2 _m_ _._ log _|_ H _|_ + log [1] Proof: By Hoeffding\u2019s inequality, since _L_ takes values in [0 _, M_ ], for any _h \u2208_ H, the following holds: P _R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _> \u03f5_ _\u2264_ _e_ _[\u2212]_ [2] _M_ _[m\u03f5]_ [2][2] _._ \ufffd \ufffd Thus, by the union bound, we can write \ufffd P \ufffd _R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _> \u03f5_ \ufffd _\u2264|_ H _|e_ _[\u2212]_ [2] _M_ _[m\u03f5]_ [2][2] _h\u2208_ H P \ufffd _\u2203h \u2208_ H : _R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _> \u03f5_ \ufffd _\u2264_ \ufffd _M_ [2] _._ Setting the right-hand side to be equal to _\u03b4_ yields the statement of the theorem. \u25a1 With the same assumptions and using the same proof, a two-sided bound can be derived: with probability at least 1 _\u2212_ _\u03b4_, for all _h \u2208_ H, \ufffd _|R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _| \u2264_ _M_ log _|_ H _|_ + log [2] _\u03b4_ 2 _m_ _._ log _|_ H _|_ + log [2] These learning bounds are similar to those derived for classification. In fact, they coincide with the classification bounds given in the inconsistent case when _M_ = 1. Thus, all the remarks made in that context apply identically here. In particular, a larger sample size _m_ guarantees better generalization; the bound increases as a function of log _|_ H _|_ and suggests selecting, for the same empirical error, a",
    "chunk_id": "foundations_machine_learning_263"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "smaller hypothesis set. This is an instance of Occam\u2019s razor principle for regression. In the next sections, we present other instances of this principle for the general case of infinite hypothesis sets using the notions of Rademacher complexity and pseudo dimension. **11.2.2** **Rademacher complexity bounds** Here, we show how the Rademacher complexity bounds of theorem 3.3 can be used to derive generalization bounds for regression in the case of the family of _L_ _p_ loss functions. We first show an upper bound for the Rademacher complexity of a relevant family of functions. **Proposition 11.2 (Rademacher complexity of** _\u00b5_ **-Lipschitz loss functions)** _Let L_ : Y _\u00d7_ Y _\u2192_ R _be a non-negative loss upper bounded by M >_ 0 _(L_ ( _y, y_ _[\u2032]_ ) _\u2264_ _M for all y, y_ _[\u2032]_ _\u2208_ Y _)_ _and such that for any fixed y_ _[\u2032]_ _\u2208_ Y _, y \ufffd\u2192_ _L_ ( _y, y_ _[\u2032]_ ) _is \u00b5-Lipschitz for some \u00b5 >_ 0 _._ _Then, for any sample S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _, the Rademacher complexity of_ _the family_ G = _{_ ( _x, y_ ) _\ufffd\u2192_ _L_ ( _h_ ( _x_ ) _, y_ ): _h \u2208_ H _} is upper bounded as follows:_ R\ufffd _S_ (G) _\u2264_ _\u00b5_ \ufffdR _S_ (H) _._ **270** **Chapter 11** **Regression** Proof: Since for any fixed _y_ _i_, _y \ufffd\u2192_ _L_ ( _y, y_ _i_ ) is _\u00b5_ -Lipschitz, by Talagrand\u2019s contraction lemma (lemma 5.7), we can write \ufffd _R_ _S_ (G) = [1] _m_ [E] _**\u03c3**_ _m_ \ufffd \ufffd \ufffd _\u03c3_ _i_ _L_ ( _h_ ( _x_ _i_ ) _, y_ _i_ ) _\u2264_ [1] _i_ =1 \ufffd _m_ [E] _**\u03c3**_ _m_ \ufffd _\u03c3_ _i_ _\u00b5 h_ ( _x_ _i_ ) = _\u00b5_ R [\ufffd] _S_ (H) _,_ \ufffd _i_ =1 \ufffd which completes the proof. **Theorem 11.3 (Rademacher complexity regression bounds)** _Let L_ : Y _\u00d7_ Y _\u2192_ R _be a non-_ _negative loss upper bounded by M >_ 0 _(L_ ( _y, y_ _[\u2032]_ ) _\u2264_ _M for all y, y_ _[\u2032]_ _\u2208_ Y _) and such_ _that for any fixed y_ _[\u2032]_ _\u2208_ Y _, y \ufffd\u2192_ _L_ ( _y, y_ _[\u2032]_ ) _is \u00b5-Lipschitz for some \u00b5 >_ 0 _._ \ufffd log [1] _\u03b4_ 2 _m_ _m_ \ufffd _L_ ( _x_ _i_ _, y_ _i_ ) + 2 _\u00b5_ R _m_ (H) + _M_ _i_ =1 E ( _x,y_ ) _\u223c_ D E ( _x,y_ ) _\u223c_ D \ufffd _L_ ( _x, y_ )\ufffd _\u2264_ [1] _m_ \ufffd log [2] _\u03b4_ 2 _m_ _[.]_ \ufffd _L_ ( _x, y_ )\ufffd _\u2264_ [1] _m_ _m_ \ufffd _L_ ( _x_ _i_ _, y_ _i_ ) + 2 _\u00b5_ R [\ufffd] _S_ (H) + 3 _M_ _i_ =1 Proof: Since for any fixed _y_ _i_, _y \ufffd\u2192_ _L_ ( _y, y_ _i_ ) is _\u00b5_ -Lipschitz, by Talagrand\u2019s contraction lemma (lemma 5.7), we can write \ufffd _R_ _S_ (G) = [1] _m_ [E] _**\u03c3**_ _m_ \ufffd \ufffd \ufffd _\u03c3_",
    "chunk_id": "foundations_machine_learning_264"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_i_ _L_ ( _h_ ( _x_ _i_ ) _, y_ _i_ ) _\u2264_ [1] _i_ =1 \ufffd _m_ [E] _**\u03c3**_ _m_ \ufffd _\u03c3_ _i_ _\u00b5 h_ ( _x_ _i_ ) = _\u00b5_ R [\ufffd] _S_ (H) _._ \ufffd _i_ =1 \ufffd Combining this inequality with the general Rademacher complexity learning bound of theorem 3.3 completes the proof. Let _p \u2265_ 1 and assume that _|h_ ( _x_ ) _\u2212_ _y| \u2264_ _M_ for all ( _x, y_ ) _\u2208_ X _\u00d7_ Y and _h \u2208_ H. Then, since for any _y_ _[\u2032]_ the function _y \ufffd\u2192|y\u2212y_ _[\u2032]_ _|_ _[p]_ is _pM_ _[p][\u2212]_ [1] -Lipschitz for ( _y\u2212y_ _[\u2032]_ ) _\u2208_ [ _\u2212M, M_ ], the theorem applies to any _L_ _p_ -loss. As an example, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_ over a sample _S_ of size _m_, each of the following inequalities holds for all _h \u2208_ H: _p_ [\ufffd] \ufffd\ufffd\ufffd _h_ ( _x_ ) _\u2212_ _y_ \ufffd\ufffd _\u2264_ _m_ [1] _p_ [\ufffd] \ufffd\ufffd\ufffd _h_ ( _x_ ) _\u2212_ _y_ \ufffd\ufffd _\u2264_ _m_ [1] _m_ \ufffd _i_ =1 \ufffd log [1] _\u03b4_ 2 _m_ _[.]_ E ( _x,y_ ) _\u223c_ D \ufffd\ufffd _h_ ( _x_ _i_ ) _\u2212_ _y_ _i_ \ufffd\ufffd _p_ + 2 _pM_ _p\u2212_ 1 R _m_ (H) + _M_ _p_ As in the case of classification, these generalization bounds suggest a trade-off between reducing the empirical error, which may require more complex hypothesis sets, and controlling the Rademacher complexity of H, which may increase the empirical error. An important benefit of the last learning bound of the theorem is that it is data-dependent. This can lead to more accurate learning guarantees. The upper bounds on R _m_ (H) or R _S_ (H) for kernel-based hypotheses (theorem 6.12) can be used directly here to derive generalization bounds in terms of the trace of the kernel matrix or the maximum diagonal entry. **11.2** **Generalization bounds** **271** _t_ 1 _t_ 2 **Image:** [No caption returned] _z_ 1 _z_ 2 **Figure 11.1** Illustration of the shattering of a set of two points _{z_ 1 _, z_ 2 _}_ with witnesses _t_ 1 and _t_ 2 . **11.2.3** **Pseudo-dimension bounds** As previously discussed in the case of classification, it is sometimes computationally hard to estimate the empirical Rademacher complexity of a hypothesis set. In chapter 3, we introduce other measures of the complexity of a hypothesis set such as the VC-dimension, which are purely combinatorial and typically easier to compute or upper bound. However, the notion of shattering or that of VC-dimension introduced for binary classification are not readily applicable to real-valued hypothesis classes. We first introduce a new notion of _shattering_ for families of real-valued functions. As in previous chapters, we will use the notation G for a family of functions, whenever we intend to later interpret it (at least in some cases) as the family of loss functions associated to some hypothesis set H: G = _{z_ = ( _x, y_ ) _\ufffd\u2192_ _L_ ( _h_ ( _x_ ) _, y_ ): _h \u2208_ H",
    "chunk_id": "foundations_machine_learning_265"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_}_ . **Definition 11.4 (Shattering)** _Let_ G _be a family of functions from a set_ Z _to_ R _. A set_ _{z_ 1 _, . . ., z_ _m_ _} \u2286_ X _is said to be_ shattered _by_ G _if there exist t_ 1 _, . . ., t_ _m_ _\u2208_ R _such that,_ \uf8f1\uf8f4\uf8f4\uf8ee sgn \ufffd _g_ ( _z_ 1 ) _\u2212_ _t_ 1 \ufffd \uf8f9 \uf8fc\uf8f4\uf8f4 _..._ : _g \u2208_ G = 2 _[m]_ _._ \uf8f2\uf8f4\uf8f4 \uf8fd\uf8f4\uf8f4 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\uf8f3\uf8ef\uf8ef\uf8f0sgn \ufffd _g_ ( _z_ _m_ ) _\u2212_ _t_ _m_ \ufffd\uf8fa\uf8fa\uf8fb \uf8fe\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd _When they exist, the threshold values t_ 1 _, . . ., t_ _m_ _are said to_ witness _the shattering._ Thus, _{z_ 1 _, . . ., z_ _m_ _}_ is shattered if for some witnesses _t_ 1 _, . . ., t_ _m_, the family of functions G is rich enough to contain a function going above a subset A of the set of points I = _{_ ( _z_ _i_ _, t_ _i_ ): _i \u2208_ [ _m_ ] _}_ and below the others (I _\u2212_ A), for any choice of the subset A. Figure 11.1 illustrates this shattering in a simple case. The notion of shattering naturally leads to the following definition. \uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3 sgn \ufffd _g_ ( _z_ _m_ ) _\u2212_ _t_ _m_ \ufffd \uf8ee \uf8ef\uf8ef\uf8f0 sgn \ufffd _g_ ( _z_ 1 ) _\u2212_ _t_ 1 \ufffd \uf8f9 \uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd _..._ = 2 _[m]_ _._ : _g \u2208_ G \uf8fa\uf8fa\uf8fb **272** **Chapter 11** **Regression** 1.5 1.0 0.5 0.0 |Col1|L(h(x), y)<br>t<br>1L(h(x),y)>t|Col3|Col4|Col5|Col6|Col7| |---|---|---|---|---|---|---| |||||||| |||||||| |||||||| |||||||| -2 -1 0 1 2 _z_ **Figure 11.2** A function _g_ : _z_ = ( _x, y_ ) _\ufffd\u2192_ _L_ ( _h_ ( _x_ ) _, y_ ) (in blue) defined as the loss of some fixed hypothesis _h \u2208_ H, and its thresholded version ( _x, y_ ) _\ufffd\u2192_ 1 _L_ ( _h_ ( _x_ ) _,y_ ) _>t_ (in red) with respect to the threshold _t_ (in yellow). **Definition 11.5 (Pseudo-dimension)** _Let_ G _be a family of functions mapping from_ X _to_ R _. Then, the_ pseudo-dimension of G _, denoted by_ Pdim(G) _, is the size of the largest_ _set shattered by_ G _._ By definition of the shattering just introduced, the notion of pseudo-dimension of a family of real-valued functions G coincides with that of the VC-dimension of the corresponding thresholded functions mapping X to _{_ 0 _,_ 1 _}_ : Pdim(G) = VCdim\ufffd\ufffd( _x, t_ ) _\ufffd\u2192_ 1 ( _g_ ( _x_ ) _\u2212t_ ) _>_ 0 : _g \u2208_ G\ufffd [\ufffd] _._ (11.3) Figure 11.2 illustrates this interpretation. In view of this interpretation, the following two results follow directly the properties of the VC-dimension. **Theorem 11.6** _The pseudo-dimension of hyperplanes in_ R _[N]_ _is given by_ Pdim( _{_ **x** _\ufffd\u2192_ **w** _\u00b7_ **x** + _b_ : **w** _\u2208_ R _[N]_ _, b \u2208_ R _}_ ) = _N_ + 1 _._ **Theorem 11.7** _The pseudo-dimension of a vector space of real-valued functions_ H _is_ _equal to the dimension of the vector space:_ Pdim(H) = dim(H) _._ The",
    "chunk_id": "foundations_machine_learning_266"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "following theorem gives a generalization bound for bounded regression in terms of the pseudo-dimension of a family of loss function G = _{z_ = ( _x, y_ ) _\ufffd\u2192_ _L_ ( _h_ ( _x_ ) _, y_ ): _h \u2208_ H _}_ associated to a hypothesis set H. The key technique to derive these bounds consists of reducing the problem to that of classification by making use of the following general identity for the expectation of a random variable _X_ : 0 E[ _X_ ] = _\u2212_ \ufffd P[ _X > t_ ] _dt,_ (11.4) 0 0 + _\u221e_ P[ _X < t_ ] _dt_ + _\u2212\u221e_ \ufffd 0 **11.2** **Generalization bounds** **273** which holds by definition of the Lebesgue integral. In particular, for any distribution D and any non-negative measurable function _f_, we can write _\u221e_ E P (11.5) _z\u223c_ D [[] _[f]_ [(] _[z]_ [)] =] \ufffd 0 _z\u223c_ D [[] _[f]_ [(] _[z]_ [)] _[ > t]_ []] _[dt .]_ **Theorem 11.8** _Let_ H _be a family of real-valued functions and_ G = _{_ ( _x, y_ ) _\ufffd\u2192_ _L_ ( _h_ ( _x_ ) _,_ _y_ ): _h \u2208_ H _} the family of loss functions associated to_ H _. Assume that_ Pdim(G) = _d_ _and that the loss function L is non-negative and bounded by M_ _. Then, for any_ _\u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the choice of am i.i.d. sample S of size m_ _drawn from_ D _[m]_ _, the following inequality holds for all h \u2208_ H _:_ \ufffd log [1] _\u03b4_ (11.6) 2 _m_ _[.]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + _M_ \ufffd 2 _d_ log _[em]_ _d_ + _M_ _m_ Proof: Let _S_ be a sample of size _m_ drawn i.i.d. according to D and let D [\ufffd] denote the empirical distribution defined by _S_ . For any _h \u2208_ H and _t \u2265_ 0, we denote by _c_ ( _h, t_ ) the classifier defined by _c_ ( _h, t_ ): ( _x, y_ ) _\ufffd\u2192_ 1 _L_ ( _h_ ( _x_ ) _,y_ ) _>t_ . The error of _c_ ( _h, t_ ) can be defined by _R_ ( _c_ ( _h, t_ )) = P P ( _x,y_ ) _\u223c_ D [[] _[c]_ [(] _[h, t]_ [)(] _[x, y]_ [) = 1] =] ( _x,y_ ) _\u223c_ D [[] _[L]_ [(] _[h]_ [(] _[x]_ [)] _[, y]_ [)] _[ > t]_ []] _[,]_ and, similarly, its empirical error is _R_ [\ufffd] _S_ ( _c_ ( _h, t_ )) = P ( _x,y_ ) _\u223c_ D\ufffd [[] _[L]_ [(] _[h]_ [(] _[x]_ [)] _[, y]_ [)] _[ > t]_ [].] Now, in view of the identity (11.5) and the fact that the loss function _L_ is bounded by _M_, we can write: _|R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _|_ = E E [ _L_ ( _h_ ( _x_ ) _, y_ )] \ufffd\ufffd\ufffd ( _x,y_ ) _\u223c_ D [[] _[L]_ [(] _[h]_",
    "chunk_id": "foundations_machine_learning_267"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[(] _[x]_ [)] _[, y]_ [)]] _[ \u2212]_ ( _x,y_ ) _\u223c_ D [\ufffd] \ufffd\ufffd\ufffd \ufffd _dt_ \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd P P [ _L_ ( _h_ ( _x_ ) _, y_ ) _> t_ ] ( _x,y_ ) _\u223c_ D [[] _[L]_ [(] _[h]_ [(] _[x]_ [)] _[, y]_ [)] _[ > t]_ []] _[ \u2212]_ ( _x,y_ ) _\u223c_ D [\ufffd] = \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd 0 _M_ _\u2264_ _M_ sup _t\u2208_ [0 _,M_ ] = _M_ sup _t\u2208_ [0 _,M_ ] P P [ _L_ ( _h_ ( _x_ ) _, y_ ) _> t_ ] \ufffd\ufffd\ufffd\ufffd\ufffd ( _x,y_ ) _\u223c_ D [[] _[L]_ [(] _[h]_ [(] _[x]_ [)] _[, y]_ [)] _[ > t]_ []] _[ \u2212]_ ( _x,y_ ) _\u223c_ D [\ufffd] \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd _R_ ( _c_ ( _h, t_ )) _\u2212_ _R_ _S_ ( _c_ ( _h, t_ )) _._ \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd This implies the following inequality: \uf8f9 _._ \uf8fa\uf8fb P sup _|R_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) _| > \u03f5_ _\u2264_ P \ufffd _h\u2208_ H \ufffd \uf8ee sup _h\u2208_ H \uf8ef\uf8f0 _t\u2208_ [0 _,M_ ] \uf8f9 \ufffd _\u03f5_ _R_ ( _c_ ( _h, t_ )) _\u2212_ _R_ _S_ ( _c_ ( _h, t_ )) _>_ \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd _M_ \ufffd _\u03f5_ _R_ ( _c_ ( _h, t_ )) _\u2212_ _R_ _S_ ( _c_ ( _h, t_ )) _>_ \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd _M_ The right-hand side can be bounded using a standard generalization bound for classification (corollary 3.19) in terms of the VC-dimension of the family of hy **274** **Chapter 11** **Regression** **Figure 11.3** **Image:** [No caption returned] For _N_ = 1, linear regression consists of finding the line of best fit, measured in terms of the squared loss. potheses _{c_ ( _h, t_ ): _h \u2208_ H _, t \u2208_ [0 _, M_ ] _}_, which, by definition of the pseudo-dimension, is precisely Pdim(G) = _d_ . The resulting bound coincides with (11.6). The notion of pseudo-dimension is suited to the analysis of regression as demonstrated by the previous theorem; however, it is not a scale-sensitive notion. There exists an alternative complexity measure, the _fat-shattering dimension_, that is scalesensitive and that can be viewed as a natural extension of the pseudo-dimension. Its definition is based on the notion of _\u03b3_ -shattering. **Definition 11.9 (** _\u03b3_ **-shattering)** _Let_ G _be a family of functions from_ Z _to_ R _and let \u03b3 >_ 0 _._ _A set {z_ 1 _, . . ., z_ _m_ _} \u2286_ X _is said to be \u03b3_ -shattered _by_ G _if there exist t_ 1 _, . . ., t_ _m_ _\u2208_ R _such that for all_ **y** _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _, there exists g \u2208_ G _such that:_ _\u2200i \u2208_ [ _m_ ] _,_ _y_ _i_ ( _g_ ( _z_ _i_ ) _\u2212_ _t_ _i_ ) _\u2265_ _\u03b3 ._ Thus, _{z_ 1 _, . . ., z_ _m_ _}_ is _\u03b3_ -shattered if for some witnesses _t_ 1 _, . . ., t_ _m_, the family of functions G is rich enough to contain a function going at",
    "chunk_id": "foundations_machine_learning_268"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "least _\u03b3_ above a subset A of the set of points I = _{_ ( _z_ _i_ _, t_ _i_ ): _i \u2208_ [ _m_ ] _}_ and at least _\u03b3_ below the others (I _\u2212_ A), for any choice of the subset A. **Definition 11.10 (** _\u03b3_ **-fat-dimension)** _The \u03b3_ -fat-dimension _of_ G _,_ fat _\u03b3_ (G) _, is the size of the_ _largest set that is \u03b3-shattered by_ G _._ Finer generalization bounds than those based on the pseudo-dimension can be derived in terms of the _\u03b3_ -fat-dimension. However, the resulting learning bounds, are not more informative than those based on the Rademacher complexity, which is also a scale-sensitive complexity measure. Thus, we will not detail an analysis based on the _\u03b3_ -fat-dimension. **11.3** **Regression algorithms** **275** **11.3** **Regression algorithms** The results of the previous sections show that, for the same empirical error, hypothesis sets with smaller complexity measured in terms of the Rademacher complexity or in terms of pseudo-dimension benefit from better generalization guarantees. One family of functions with relatively small complexity is that of linear hypotheses. In this section, we describe and analyze several algorithms based on that hypothesis set: _linear regression_, _kernel ridge regression_ (KRR), _support vector regression_ (SVR), and _Lasso_ . These algorithms, in particular the last three, are extensively used in practice and often lead to state-of-the-art performance results. **11.3.1** **Linear regression** We start with the simplest algorithm for regression known as _linear regression_ . Let **\u03a6** : X _\u2192_ R _[N]_ be a feature mapping from the input space X to R _[N]_ and consider the family of linear hypotheses H = _{x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ) + _b_ : **w** _\u2208_ R _[N]_ _, b \u2208_ R _} ._ (11.7) Linear regression consists of seeking a hypothesis in H with the smallest empirical mean squared error. Thus, for a sample _S_ = \ufffd( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )\ufffd _\u2208_ (X _\u00d7_ Y) _[m]_, the following is the corresponding optimization problem: 1 min **w** _,b_ _m_ _m_ \ufffd ( **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) + _b \u2212_ _y_ _i_ ) [2] _._ (11.8) _i_ =1 Figure 11.3 illustrates the algorithm in the simple case where _N_ = 1. The optimization problem admits the simpler formulation: min **W** _[F]_ [(] **[W]** [) =] _m_ [1] _[\u2225]_ **[X]** _[\u22a4]_ **[W]** _[ \u2212]_ **[Y]** _[\u2225]_ [2] _[,]_ (11.9) \ufffd . The objec\ufffd using the notation **X** = \ufffd \u03a6(1 _x_ 1 ) _......_ \u03a6( _x_ 1 _m_ ) \ufffd, **W** = _w_ 1 ... _w_ _N_ \ufffd _b_ _y_ 1 and **Y** = ... \ufffd _y_ _m_ tive function _F_ is convex, by composition of the convex function **u** _\ufffd\u2192\u2225_ **u** _\u2225_ [2] with the affine function **W** _\ufffd\u2192_ **X** _[\u22a4]_ **W** _\u2212_ **Y**, and it is differentiable. Thus, _F_ admits a global minimum at **W** if and only if _\u2207F_ ( **W** ) = 0, that is if and only if 2 (11.10) _m_ **[X]** [(] **[X]** _[\u22a4]_ **[W]**",
    "chunk_id": "foundations_machine_learning_269"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[ \u2212]_ **[Y]** [) = 0] _[ \u21d4]_ **[XX]** _[\u22a4]_ **[W]** [ =] **[ XY]** _[ .]_ When **XX** _[\u22a4]_ is invertible, this equation admits a unique solution. Otherwise, the equation admits a family of solutions that can be given in terms of the pseudo-inverse of matrix **XX** _[\u22a4]_ (see appendix A) by **W** = ( **XX** _[\u22a4]_ ) _[\u2020]_ **XY** + ( _I \u2212_ ( **XX** _[\u22a4]_ ) _[\u2020]_ ( **XX** _[\u22a4]_ )) **W** 0, where **W** 0 is an arbitrary matrix in R _[N]_ _[\u00d7][N]_ . Among these, **276** **Chapter 11** **Regression** the solution **W** = ( **XX** _[\u22a4]_ ) _[\u2020]_ **XY** is the one with the minimal norm and is often preferred for that reason. Thus, we will write the solutions as **W** = ( **XX** _[\u22a4]_ ) _[\u2212]_ [1] **XY** if **XX** _[\u22a4]_ is invertible _,_ (11.11) \ufffd( **XX** _[\u22a4]_ ) _[\u2020]_ **XY** otherwise _._ The matrix **XX** _[\u22a4]_ can be computed in _O_ ( _mN_ [2] ). The cost of its inversion or that of computing its pseudo-inverse is in _O_ ( _N_ [3] ). [19] Finally, the multiplication with **X** and **Y** takes _O_ ( _mN_ [2] ). Therefore, the overall complexity of computing the solution **W** is in _O_ ( _mN_ [2] + _N_ [3] ). Thus, when the dimension of the feature space _N_ is not too large, the solution can be computed efficiently. While linear regression is simple and admits a straightforward implementation, it does not benefit from a strong generalization guarantee, since it is limited to minimizing the empirical error without controlling the norm of the weight vector and without any other regularization. Its performance is also typically poor in most applications. The next sections describe algorithms with both better theoretical guarantees and improved performance in practice. **11.3.2** **Kernel ridge regression** We first present a learning guarantee for regression with bounded linear hypotheses in a feature space defined by a PDS kernel. This will provide a strong theoretical support for the _kernel ridge regression_ algorithm presented in this section. The learning bounds of this section are given for the squared loss. Thus, in particular, the generalization error of a hypothesis _h_ is defined by _R_ ( _h_ ) = E ( _x,y_ ) _\u223c_ D \ufffd( _h_ ( _x_ ) _\u2212_ _y_ ) [2] [\ufffd] . **Theorem 11.11** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS kernel,_ \u03a6: X _\u2192_ H _a feature mapping_ _associated to K, and_ H = _{x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ): _\u2225_ **w** _\u2225_ H _\u2264_ \u039b _}. Assume that there exists_ _r >_ 0 _such that K_ ( _x, x_ ) _\u2264_ _r_ [2] _and M >_ 0 _such that |h_ ( _x_ ) _\u2212_ _y| < M for all_ ( _x, y_ ) _\u2208_ X _\u00d7_ Y _. Then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, each of the_ _following inequalities holds for all h \u2208_ H _:_ \ufffd log [1] _\u03b4_ 2 _m_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_",
    "chunk_id": "foundations_machine_learning_270"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ") + 4 _M_ ~~\ufffd~~ _r_ [2] \u039b [2] + _M_ [2] _m_ ~~\ufffd~~ Tr[ **K** ] _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + [4] _[M]_ [\u039b] + 3 _M_ [2] _m_ \ufffd log [2] _\u03b4_ 2 _m_ _[.]_ 19 In the analysis of the computational complexity of the algorithms discussed in this chapter, the cubic-time complexity of matrix inversion can be replaced by a more favorable complexity _O_ ( _N_ [2+] _[\u03c9]_ ), with _\u03c9_ = _._ 376 using asymptotically faster matrix inversion methods such as that of Coppersmith and Winograd. **11.3** **Regression algorithms** **277** Proof: By the bound on the empirical Rademacher complexity of kernel-based hypotheses (theorem 6.12), the following holds for any sample _S_ of size _m_ : \ufffd ~~\ufffd~~ Tr[ **K** ] R _S_ (H) _\u2264_ [\u039b] _\u2264_ _m_ \ufffd _r_ [2] \u039b [2] _m_ _[,]_ which implies that R _m_ (H) _\u2264_ ~~\ufffd~~ _r_ [2] \u039b [2] which implies that R _m_ (H) _\u2264_ _r_ _m_ [. Combining these inequalities with the learn-] ing bounds of Theorem 11.3 yield immediately the inequalities claimed. The learning bounds of the theorem suggests minimizing a trade-off between the empirical squared loss (first term on the right-hand side), and the norm of the weight vector (upper bound \u039b on the norm appearing in the second term), or equivalently the norm squared. Kernel ridge regression is defined by the minimization of an objective function that has precisely this form and thus is directly motivated by the theoretical analysis just presented: min **w** _[F]_ [(] **[w]** [) =] _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [2] [ +] _m_ \ufffd ( **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) _\u2212_ _y_ _i_ ) [2] _._ (11.12) _i_ =1 Here, _\u03bb_ is a positive parameter determining the trade-off between the regularization term _\u2225_ **w** _\u2225_ [2] and the empirical mean squared error. The objective function differs from that of linear regression only by the first term, which controls the norm of **w** . As in the case of linear regression, the problem can be rewritten in a more compact form as min **W** _[F]_ [(] **[W]** [) =] _[ \u03bb][\u2225]_ **[W]** _[\u2225]_ [2] [ +] _[ \u2225]_ **[X]** _[\u22a4]_ **[W]** _[ \u2212]_ **[Y]** _[\u2225]_ [2] _[,]_ (11.13) where **X** _\u2208_ R _[N]_ _[\u00d7][m]_ is the matrix formed by the feature vectors, **X** = [ [\u03a6(] _[x]_ 1 [)] _[ ...]_ [ \u03a6(] _[x]_ _m_ [)] ], **W** = **w**, and **Y** = ( _y_ 1 _, . . ., y_ _m_ ) _[\u22a4]_ . Here too, _F_ is convex, by the convexity of **w** _\ufffd\u2192\u2225_ **w** _\u2225_ [2] and that of the sum of two convex functions, and is differentiable. Thus _F_ admits a global minimum at **W** if and only if _\u2207F_ ( **W** ) = 0 _\u21d4_ ( **XX** _[\u22a4]_ + _\u03bb_ **I** ) **W** = **XY** _\u21d4_ **W** = ( **XX** _[\u22a4]_ + _\u03bb_ **I** ) _[\u2212]_ [1] **XY** _._ (11.14) Note that the matrix **XX** _[\u22a4]_ + _\u03bb_ **I** is always invertible, since its eigenvalues are the sum of",
    "chunk_id": "foundations_machine_learning_271"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the non-negative eigenvalues of the symmetric positive semidefinite matrix **XX** _[\u22a4]_ and _\u03bb >_ 0. Thus, kernel ridge regression admits a closed-form solution. An alternative formulation of the optimization problem for kernel ridge regression equivalent to (11.12) is min **w** _m_ \ufffd( **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) _\u2212_ _y_ _i_ ) [2] subject to: _\u2225_ **w** _\u2225_ [2] _\u2264_ \u039b [2] _._ _i_ =1 This makes the connection with the bounded linear hypothesis set of theorem 11.11 even more evident. Using slack variables _\u03be_ _i_, for all _i \u2208_ [ _m_ ], the problem can be **278** **Chapter 11** **Regression** equivalently written as min **w** _m_ \ufffd _\u03be_ _i_ [2] subject to: ( _\u2225_ **w** _\u2225_ [2] _\u2264_ \u039b [2] ) _\u2227_ \ufffd _\u2200i \u2208_ [ _m_ ] _, \u03be_ _i_ = _y_ _i_ _\u2212_ **w** _\u00b7_ **\u03a6** ( _x_ _i_ )\ufffd _._ _i_ =1 This is a convex optimization problem with differentiable objective function and constraints. To derive the equivalent dual problem, we introduce the Lagrangian _L_, which is defined for all _**\u03be**_ _,_ **w** _,_ _**\u03b1**_ _[\u2032]_, and _\u03bb \u2265_ 0 by _L_ ( _**\u03be**_ _,_ **w** _,_ _**\u03b1**_ _[\u2032]_ _, \u03bb_ ) = _m_ \ufffd _\u03be_ _i_ [2] [+] _i_ =1 _m_ \ufffd _\u03b1_ _i_ _[\u2032]_ [(] _[y]_ _[i]_ _[\u2212]_ _[\u03be]_ _[i]_ _[\u2212]_ **[w]** _[ \u00b7]_ **[ \u03a6]** [(] _[x]_ _[i]_ [)) +] _[ \u03bb]_ [(] _[\u2225]_ **[w]** _[\u2225]_ [2] _[ \u2212]_ [\u039b] [2] [)] _[ .]_ _i_ =1 _m_ \ufffd The KKT conditions lead to the following equalities: 2 _\u03bb_ _\u2207_ **w** _L_ = _\u2212_ _m_ \ufffd \ufffd _\u03b1_ _i_ _[\u2032]_ **[\u03a6]** [(] _[x]_ _[i]_ [) + 2] _[\u03bb]_ **[w]** [ = 0] = _\u21d2_ **w** = 2 [1] _\u03bb_ _i_ =1 _m_ \ufffd _\u03b1_ _i_ _[\u2032]_ **[\u03a6]** [(] _[x]_ _[i]_ [)] _i_ =1 _\u2207_ _\u03be_ _i_ _L_ = 2 _\u03be_ _i_ _\u2212_ _\u03b1_ _i_ _[\u2032]_ [= 0] = _\u21d2_ _\u03be_ _i_ = _\u03b1_ _i_ _[\u2032]_ _[/]_ [2] _\u2200i \u2208_ [ _m_ ] _, \u03b1_ _i_ _[\u2032]_ [(] _[y]_ _[i]_ _[\u2212]_ _[\u03be]_ _[i]_ _[\u2212]_ **[w]** _[ \u00b7]_ **[ \u03a6]** [(] _[x]_ _[i]_ [)) = 0] _\u03bb_ ( _\u2225_ **w** _\u2225_ [2] _\u2212_ \u039b [2] ) = 0 _._ Plugging in the expressions of **w** and _\u03be_ _i_ s in that of _L_ gives _m_ \ufffd _\u03b1_ _i_ _[\u2032]_ _[y]_ _[i]_ _[\u2212]_ _i_ =1 _i_ _\u2212_ [1] 2 2 _\u03bb_ 2 _\u03bb_ _\u03b1_ _[\u2032]_ [2] _i_ 4 [+] 2 _\u03b1_ _i_ _[\u2032]_ _m_ \ufffd _i_ =1 _L_ = _m_ \ufffd _i_ =1 _m_ \ufffd _\u03b1_ _i_ _[\u2032]_ _[\u03b1]_ _j_ _[\u2032]_ **[\u03a6]** [(] _[x]_ _[i]_ [)] _[\u22a4]_ **[\u03a6]** [(] _[x]_ _[j]_ [)] _i,j_ =1 1 + _\u03bb_ \ufffd 4 _\u03bb_ [2] _[ \u2225]_ _m_ \ufffd _\u03b1_ _i_ _[\u2032]_ **[\u03a6]** [(] _[x]_ _[i]_ [)] _[\u2225]_ [2] _[ \u2212]_ [\u039b] [2] [\ufffd] _i_ =1 _m_ \ufffd \ufffd _\u03b1_ _i_ _[\u2032]_ _[y]_ _[i]_ _[\u2212]_ 4 [1] _\u03bb_ _i_ =1 4 _\u03bb_ = _\u2212_ [1] 4 = _\u2212\u03bb_ _m_ \ufffd _\u03b1_ _[\u2032]_ [2] _i_ [+] _i_ =1 _m_ \ufffd _m_ \ufffd _\u03b1_ _i_ [2] [+ 2] _i_ =1 _m_ \ufffd _m_ \ufffd",
    "chunk_id": "foundations_machine_learning_272"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u03b1_ _i_ _[\u2032]_ _[\u03b1]_ _j_ _[\u2032]_ **[\u03a6]** [(] _[x]_ _[i]_ [)] _[\u22a4]_ **[\u03a6]** [(] _[x]_ _[j]_ [)] _[ \u2212]_ _[\u03bb]_ [\u039b] [2] _i,j_ =1 _m_ \ufffd _\u03b1_ _i_ _\u03b1_ _j_ **\u03a6** ( _x_ _i_ ) _[\u22a4]_ **\u03a6** ( _x_ _j_ ) _\u2212_ _\u03bb_ \u039b [2] _,_ _i,j_ =1 _m_ \ufffd _\u03b1_ _i_ _y_ _i_ _\u2212_ _i_ =1 _m_ \ufffd with _\u03b1_ _i_ _[\u2032]_ [= 2] _[\u03bb\u03b1]_ _[i]_ [. Thus, the equivalent dual optimization problem for KRR can be] written as follows: max (11.15) _**\u03b1**_ _\u2208_ R _[m]_ _[ \u2212][\u03bb]_ _**[\u03b1]**_ _[\u22a4]_ _**[\u03b1]**_ [ + 2] _**[\u03b1]**_ _[\u22a4]_ **[Y]** _[ \u2212]_ _**[\u03b1]**_ _[\u22a4]_ [(] **[X]** _[\u22a4]_ **[X]** [)] _**[\u03b1]**_ _[,]_ or, more compactly, as max (11.16) _**\u03b1**_ _\u2208_ R _[m]_ _[ G]_ [(] _**[\u03b1]**_ [) =] _[ \u2212]_ _**[\u03b1]**_ _[\u22a4]_ [(] **[K]** [ +] _[ \u03bb]_ **[I]** [)] _**[\u03b1]**_ [ + 2] _**[\u03b1]**_ _[\u22a4]_ **[Y]** _[,]_ where **K** = **X** _[\u22a4]_ **X** is the kernel matrix associated to the training sample. The objective function _G_ is concave and differentiable. The optimal solution is obtained **11.3** **Regression algorithms** **279** by differentiating the function and setting it to zero: _\u2207G_ ( _**\u03b1**_ ) = 0 _\u21d0\u21d2_ 2( **K** + _\u03bb_ **I** ) _**\u03b1**_ = 2 **Y** _\u21d0\u21d2_ _**\u03b1**_ = ( **K** + _\u03bb_ **I** ) _[\u2212]_ [1] **Y** _._ (11.17) Note that ( **K** + _\u03bb_ **I** ) is invertible, since its eigenvalues are the sum of the eigenvalues of the SPSD matrix **K** and _\u03bb >_ 0. Thus, as in the primal case, the dual optimization problem admits a closed-form solution. By the first KKT equation, **w** can be determined from _**\u03b1**_ by **w** = _m_ \ufffd _\u03b1_ _i_ **\u03a6** ( **x** _i_ ) = **X** _**\u03b1**_ = **X** ( **K** + _\u03bb_ **I** ) _[\u2212]_ [1] **Y** _._ (11.18) _i_ =1 The hypothesis _h_ solution can be given as follows in terms of _**\u03b1**_ : _\u2200x \u2208_ X _,_ _h_ ( _x_ ) = **w** _\u00b7_ **\u03a6** ( _x_ ) = _m_ \ufffd _\u03b1_ _i_ _K_ ( _x_ _i_ _, x_ ) _._ (11.19) _i_ =1 Note that the form of the solution, _h_ = [\ufffd] _[m]_ _i_ =1 _[\u03b1]_ _[i]_ _[K]_ [(] _[x]_ _[i]_ _[,][ \u00b7]_ [), could be immediately] predicted using the Representer theorem, since the objective function minimized by KRR falls within the general framework of theorem 6.11. This also could show that **w** could be written as **w** = **X** _\u03b1_ . This fact, combined with the following simple lemma, can be used to determine _**\u03b1**_ in a straightforward manner, without the intermediate derivation of the dual problem. **Lemma 11.12** _The following identity holds for any matrix_ **X** _:_ ( **XX** _[\u22a4]_ + _\u03bb_ **I** ) _[\u2212]_ [1] **X** = **X** ( **X** _[\u22a4]_ **X** + _\u03bb_ **I** ) _[\u2212]_ [1] _._ Proof: Observe that ( **XX** _[\u22a4]_ + _\u03bb_ **I** ) **X** = **X** ( **X** _[\u22a4]_ **X** + _\u03bb_ **I** ). Left-multiplying by ( **XX** _[\u22a4]_ + _\u03bb_ **I** ) _[\u2212]_ [1] this equality and right-multiplying it by ( **X** _[\u22a4]_ **X** + _\u03bb_ **I** ) _[\u2212]_",
    "chunk_id": "foundations_machine_learning_273"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[1] yields the statement of the lemma. Now, using this lemma, the primal solution of **w** can be rewritten as follows: **w** = ( **XX** _[\u22a4]_ + _\u03bb_ **I** ) _[\u2212]_ [1] **XY** = **X** ( **X** _[\u22a4]_ **X** + _\u03bb_ **I** ) _[\u2212]_ [1] **Y** = **X** ( **K** + _\u03bb_ **I** ) _[\u2212]_ [1] **Y** _._ Comparing with **w** = **X** _\u03b1_ gives immediately _**\u03b1**_ = ( **K** + _\u03bb_ **I** ) _[\u2212]_ [1] **Y** . Our presentation of the KRR algorithm was given for linear hypotheses with no offset, that is we implicitly assumed _b_ = 0. It is common to use this formulation and to extend it to the general case by augmenting the feature vector **\u03a6** ( _x_ ) with an extra component equal to one for all _x \u2208_ X and the weight vector **w** with an extra component _b \u2208_ R. For the augmented feature vector **\u03a6** _[\u2032]_ ( _x_ ) _\u2208_ R _[N]_ [+1] and weight vector **w** _[\u2032]_ _\u2208_ R _[N]_ [+1], we have **w** _[\u2032]_ _\u00b7_ **\u03a6** _[\u2032]_ ( _x_ ) = **w** _\u00b7_ **\u03a6** ( _x_ ) + _b_ . Nevertheless, this formulation does not coincide with the general KRR algorithm where a solution of the form _x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ) + _b_ is sought. This is because for the general KRR, the regularization term is _\u03bb\u2225_ **w** _\u2225_, while for the extension just described it is _\u03bb\u2225_ **w** _[\u2032]_ _\u2225_ . **280** **Chapter 11** **Regression** **Table 11.1** Comparison of the running-time complexity of KRR for computing the solution or the prediction value of a point in both the primal and the dual case. _\u03ba_ denotes the time complexity of computing a kernel value; for polynomial and Gaussian kernels, _\u03ba_ = _O_ ( _N_ ). Solution Prediction Primal _O_ ( _mN_ [2] + _N_ [3] ) _O_ ( _N_ ) Dual _O_ ( _\u03bam_ [2] + _m_ [3] ) _O_ ( _\u03bam_ ) In both the primal and dual cases, KRR admits a closed-form solution. Table 11.1 gives the time complexity of the algorithm for computing the solution and the one for determining the prediction value of a point in both cases. In the primal case, determining the solution **w** requires computing matrix **XX** _[\u22a4]_, which takes _O_ ( _mN_ [2] ), the inversion of ( **XX** _[\u22a4]_ + _\u03bb_ **I** ), which is in _O_ ( _N_ [3] ), and multiplication with **X**, which is in _O_ ( _mN_ [2] ). Prediction requires computing the inner product of **w** with a feature vector of the same dimension that can be achieved in _O_ ( _N_ ). The dual solution first requires computing the kernel matrix **K** . Let _\u03ba_ be the maximum cost of computing _K_ ( _x, x_ _[\u2032]_ ) for all pairs ( _x, x_ _[\u2032]_ ) _\u2208_ X _\u00d7_ X. Then, **K** can be computed in _O_ ( _\u03bam_ [2] ). The inversion of matrix **K** + _\u03bb_ **I** can be achieved in _O_ ( _m_ [3] ) and multiplication with **Y** takes",
    "chunk_id": "foundations_machine_learning_274"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_O_ ( _m_ [2] ). Prediction requires computing the vector ( _K_ ( _x_ 1 _, x_ ) _, . . ., K_ ( _x_ _m_ _, x_ )) _[\u22a4]_ for some _x \u2208_ X, which requires _O_ ( _\u03bam_ ), and the inner product with _**\u03b1**_, which is in _O_ ( _m_ ). Thus, in both cases, the main step for computing the solution is a matrix inversion, which takes _O_ ( _N_ [3] ) in the primal case, _O_ ( _m_ [3] ) in the dual case. When the dimension of the feature space is relatively small, solving the primal problem is advantageous, while for high-dimensional spaces and medium-sized training sets, solving the dual is preferable. Note that for relatively large matrices, the space complexity could also be an issue: the size of relatively large matrices could be prohibitive for memory storage and the use of external memory could significantly affect the running time of the algorithm. For sparse matrices, there exist several techniques for faster computations of the matrix inversion. This can be useful in the primal case where the features can be relatively sparse. On the other hand, the kernel matrix **K** is typically dense; thus, there is less hope for benefiting from such techniques in the dual case. In such cases, or, more generally, to deal with the time and space complexity issues arising when _m_ and _N_ are large, approximation methods using low-rank approximations via the Nystr\u00a8om method or the partial Cholesky decomposition can be used very effectively. The KRR algorithm admits several advantages: it benefits from favorable theoretical guarantees since it can be derived directly from the generalization bound we **11.3** **Regression algorithms** **281** _y_ **w** _\u00b7_ **\u03a6** ( _x_ )+ _b_ **\u03a6** ( _x_ ) **Figure 11.4** **Image:** [No caption returned] SVR attempts to fit a \u201ctube\u201d with width _\u03f5_ to the data. Training data within the \u201cepsilon tube\u201d (blue points) incur no loss. presented; it admits a closed-form solution, which can make the analysis of many of its properties convenient; and it can be used with PDS kernels, which extends its use to non-linear regression solutions and more general features spaces. KRR also admits favorable stability properties that we discuss in chapter 14. The algorithm can be generalized to learning a mapping from X to R _[p]_, _p >_ 1. This can be done by formulating the problem as _p_ independent regression problems, each consisting of predicting one of the _p_ target components. Remarkably, the computation of the solution for this generalized algorithm requires only a single matrix inversion, e.g., ( **K** + _\u03bb_ **I** ) _[\u2212]_ [1] in the dual case, regardless of the value of _p_ . One drawback of the KRR algorithm, in addition to the computational issues for determining the solution for relatively large matrices, is the fact that the solution it returns is typically not sparse. The next two sections present two sparse algorithms for linear regression. **11.3.3** **Support vector regression** In this section, we present the _support vector regression_ (SVR) algorithm, which is inspired by the SVM",
    "chunk_id": "foundations_machine_learning_275"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "algorithm presented for classification in chapter 5. The main idea of the algorithm consists of fitting a tube of width _\u03f5 >_ 0 to the data, as illustrated by figure 11.4. As in binary classification, this defines two sets of points: those falling inside the tube, which are _\u03f5_ -close to the function predicted and thus not penalized, and those falling outside, which are penalized based on their distance to the predicted function, in a way that is similar to the penalization used by SVMs in classification. Using a hypothesis set H of linear functions: H = _{x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ) + _b_ : **w** _\u2208_ R _[N]_ _, b \u2208_ R _}_, where **\u03a6** is the feature mapping corresponding some PDS kernel _K_, the optimization problem for SVR can be written as follows: 1 min **w** _,b_ 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [ +] _[ C]_ _m_ \ufffd _i_ =1 \ufffd\ufffd _y_ _i_ _\u2212_ ( **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) + _b_ )\ufffd\ufffd _\u03f5_ _[,]_ (11.20) **282** **Chapter 11** **Regression** where _| \u00b7 |_ _\u03f5_ denotes the _\u03f5-insensitive loss_ : _\u2200y, y_ _[\u2032]_ _\u2208_ Y _,_ _|y_ _[\u2032]_ _\u2212_ _y|_ _\u03f5_ = max(0 _, |y_ _[\u2032]_ _\u2212_ _y| \u2212_ _\u03f5_ ) _._ (11.21) The use of this loss function leads to sparse solutions with a relatively small number of support vectors. Using slack variables _\u03be_ _i_ _\u2265_ 0 and _\u03be_ _i_ _[\u2032]_ _[\u2265]_ [0,] _[ i][ \u2208]_ [[] _[m]_ [], the optimization] problem can be equivalently written as 1 min **w** _,b,_ _**\u03be**_ _,_ _**\u03be**_ _[\u2032]_ 2 _[\u2225]_ **[w]** _[\u2225]_ [2] [ +] _[ C]_ _m_ \ufffd( _\u03be_ _i_ + _\u03be_ _i_ _[\u2032]_ [)] (11.22) _i_ =1 subject to ( **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) + _b_ ) _\u2212_ _y_ _i_ _\u2264_ _\u03f5_ + _\u03be_ _i_ _y_ _i_ _\u2212_ ( **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) + _b_ ) _\u2264_ _\u03f5_ + _\u03be_ _i_ _[\u2032]_ _\u03be_ _i_ _\u2265_ 0 _, \u03be_ _i_ _[\u2032]_ _[\u2265]_ [0] _[,][ \u2200][i][ \u2208]_ [[] _[m]_ []] _[.]_ This is a convex quadratic program (QP) with affine constraints. Introducing the Lagrangian and applying the KKT conditions leads to the following equivalent dual problem in terms of the kernel matrix **K** : max (11.23) _**\u03b1**_ _,_ _**\u03b1**_ _[\u2032]_ _[ \u2212]_ _[\u03f5]_ [(] _**[\u03b1]**_ _[\u2032]_ [ +] _**[ \u03b1]**_ [)] _[\u22a4]_ **[1]** [ + (] _**[\u03b1]**_ _[\u2032]_ _[ \u2212]_ _**[\u03b1]**_ [)] _[\u22a4]_ **[y]** _[ \u2212]_ [1] 2 [(] _**[\u03b1]**_ _[\u2032]_ _[ \u2212]_ _**[\u03b1]**_ [)] _[\u22a4]_ **[K]** [(] _**[\u03b1]**_ _[\u2032]_ _[ \u2212]_ _**[\u03b1]**_ [)] subject to: ( **0** _\u2264_ _**\u03b1**_ _\u2264_ **C** ) _\u2227_ ( **0** _\u2264_ _**\u03b1**_ _[\u2032]_ _\u2264_ **C** ) _\u2227_ (( _**\u03b1**_ _[\u2032]_ _\u2212_ _**\u03b1**_ ) _[\u22a4]_ **1** = 0) _._ Any PDS kernel _K_ can be used with SVR, which extends the algorithm to nonlinear regression solutions. Problem (11.23) is a convex QP similar to the dual problem of SVMs and can be solved using similar optimization techniques. The solutions _**\u03b1**_ and _**\u03b1**_ _[\u2032]_ define the hypothesis _h_ returned by SVR as follows: _\u2200x \u2208_ X _,_",
    "chunk_id": "foundations_machine_learning_276"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_h_ ( _x_ ) = _m_ \ufffd( _\u03b1_ _i_ _[\u2032]_ _[\u2212]_ _[\u03b1]_ _[i]_ [)] _[K]_ [(] **[x]** _[i]_ _[,]_ **[ x]** [) +] _[ b,]_ (11.24) _i_ =1 where the offset _b_ can be obtained from a point _x_ _j_ with 0 _< \u03b1_ _j_ _< C_ by _b_ = _\u2212_ _m_ \ufffd( _\u03b1_ _i_ _[\u2032]_ _[\u2212]_ _[\u03b1]_ _[i]_ [)] _[K]_ [(] _[x]_ _[i]_ _[, x]_ _[j]_ [) +] _[ y]_ _[j]_ [+] _[ \u03f5,]_ (11.25) _i_ =1 or from a point _x_ _j_ with 0 _< \u03b1_ _j_ _[\u2032]_ _[< C]_ [ via] _b_ = _\u2212_ _m_ \ufffd( _\u03b1_ _i_ _[\u2032]_ _[\u2212]_ _[\u03b1]_ _[i]_ [)] _[K]_ [(] _[x]_ _[i]_ _[, x]_ _[j]_ [) +] _[ y]_ _[j]_ _[\u2212]_ _[\u03f5.]_ (11.26) _i_ =1 By the complementarity conditions, for all _i \u2208_ [ _m_ ], the following equalities hold: _\u03b1_ _i_ \ufffd( **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) + _b_ ) _\u2212_ _y_ _i_ _\u2212_ _\u03f5 \u2212_ _\u03be_ _i_ \ufffd = 0 _\u03b1_ _i_ _[\u2032]_ \ufffd( **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) + _b_ ) _\u2212_ _y_ _i_ + _\u03f5_ + _\u03be_ _i_ _[\u2032]_ \ufffd = 0 _._ **11.3** **Regression algorithms** **283** Thus, if _\u03b1_ _i_ _\u0338_ = 0 or _\u03b1_ _i_ _[\u2032]_ _[\u0338]_ [= 0, that is if] _[ x]_ _[i]_ [ is a support vector, then, either (] **[w]** _[ \u00b7]_ **[ \u03a6]** [(] _[x]_ _[i]_ [) +] _b_ ) _\u2212_ _y_ _i_ _\u2212_ _\u03f5_ = _\u03be_ _i_ holds or _y_ _i_ _\u2212_ ( **w** _\u00b7_ **\u03a6** ( _x_ _i_ )+ _b_ ) _\u2212_ _\u03f5_ = _\u03be_ _i_ _[\u2032]_ [. This shows that support vectors] points lying outside the _\u03f5_ -tube. Of course, at most one of _\u03b1_ _i_ or _\u03b1_ _i_ _[\u2032]_ [is non-zero for] any point _x_ _i_ : the hypothesis either overestimates or underestimates the true label by more than _\u03f5_ . For the points within the _\u03f5_ -tube, we have _\u03b1_ _j_ = _\u03b1_ _j_ _[\u2032]_ [= 0; thus,] these points do not contribute to the definition of the hypothesis returned by SVR. Thus, when the number of points inside the tube is relatively large, the hypothesis returned by SVR is relatively sparse. The choice of the parameter _\u03f5_ determines a trade-off between sparsity and accuracy: larger _\u03f5_ values provide sparser solutions, since more points can fall within the _\u03f5_ -tube, but may ignore too many key points for determining an accurate solution. The following generalization bounds hold for the _\u03f5_ -insensitive loss and kernelbased hypotheses and thus for the SVR algorithm. We denote by D the distribution according to which sample points are drawn and by D [\ufffd] the empirical distribution defined by a training sample of size _m_ . **Theorem 11.13** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS kernel, let_ \u03a6: X _\u2192_ H _be a feature_ _mapping associated to K and let_ H = _{x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ): _\u2225_ **w** _\u2225_ H _\u2264_ \u039b _}. Assume that_ _there exists r >_ 0 _such that K_ ( _x, x_ ) _\u2264_ _r_ [2] _and M",
    "chunk_id": "foundations_machine_learning_277"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ">_ 0 _such that |h_ ( _x_ ) _\u2212_ _y| \u2264_ _M for_ _all_ ( _x, y_ ) _\u2208_ X _\u00d7_ Y _. Fix \u03f5 >_ 0 _. Then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4,_ _each of the following inequalities holds for all h \u2208_ H _,_ _[\u0338]_ ~~\ufffd~~ _[\u0338]_ log [1] _\u03b4_ 2 _m_ _[\u0338]_ \ufffd _|h_ ( _x_ ) _\u2212_ _y|_ _\u03f5_ \ufffd + 2 _[\u0338]_ ~~\ufffd~~ _[\u0338]_ _r_ [2] \u039b [2] + _M_ _m_ _[\u0338]_ E ( _x,y_ ) _\u223c_ D E ( _x,y_ ) _\u223c_ D _[\u0338]_ \ufffd _|h_ ( _x_ ) _\u2212_ _y|_ _\u03f5_ \ufffd _\u2264_ E ( _x,y_ ) _\u223c_ D [\ufffd] \ufffd _|h_ ( _x_ ) _\u2212_ _y|_ _\u03f5_ \ufffd _\u2264_ E ( _x,y_ ) _\u223c_ D [\ufffd] _[\u0338]_ ~~\ufffd~~ Tr[ **K** ] \ufffd _|h_ ( _x_ ) _\u2212_ _y|_ _\u03f5_ \ufffd + [2\u039b] + 3 _M_ _m_ _[\u0338]_ \ufffd _[\u0338]_ log [2] _\u03b4_ 2 _m_ _[.]_ _[\u0338]_ Proof: Since for any _y_ _[\u2032]_ _\u2208_ Y, the function _y \ufffd\u2192|y \u2212_ _y_ _[\u2032]_ _|_ _\u03f5_ is 1-Lipschitz, the result follows Theorem 11.3 and the bound on the empirical Rademacher complexity of H. These results provide theoretical guarantees for the SVR algorithm. Notice, however, that the theorem does not provide guarantees for the expected loss of the hypotheses in terms of the squared loss. For 0 _< \u03f5 <_ 1 _/_ 4, the inequality _|x|_ [2] _\u2264_ _|x|_ _\u03f5_ holds for all _x_ in [ _\u2212\u03b7_ _\u03f5_ _[\u2032]_ _[,][ \u2212][\u03b7]_ _[\u03f5]_ []] _[ \u222a]_ [[] _[\u03b7]_ _[\u03f5]_ _[, \u03b7]_ _\u03f5_ _[\u2032]_ [] with] _[ \u03b7]_ _[\u03f5]_ [=] [ 1] _[\u2212]_ ~~_[\u221a]_~~ 2 [1] _[\u2212]_ [4] _[\u03f5]_ and _\u03b7_ _\u03f5_ _[\u2032]_ [=] [1][+] ~~_[\u221a]_~~ 2 [1] _[\u2212]_ [4] _[\u03f5]_ . For small values of _\u03f5_, _\u03b7_ _\u03f5_ _\u2248_ 0 and _\u03b7_ _\u03f5_ _[\u2032]_ _[\u2248]_ [1, thus, if] _[ M]_ [ = 2] _[r\u03bb][ \u2264]_ [1, then, the squared loss] can be upper bounded by the _\u03f5_ -insensitive loss for almost all values of ( _h_ ( _x_ ) _\u2212_ _y_ ) in [ _\u2212_ 1 _,_ 1] and the theorem can be used to derive a useful generalization bound for the squared loss. More generally, if the objective is to achieve a small squared loss, then, SVR can be modified by using the _quadratic \u03f5-insensitive loss_, that is the square of the _\u03f5_ insensitive loss, which also leads to a convex QP. We will refer by _quadratic SVR_ to **284** **Chapter 11** **Regression** this version of the algorithm. Introducing the Lagrangian and applying the KKT conditions leads to the following equivalent dual optimization problem for quadratic SVR in terms of the kernel matrix **K** : max _**\u03b1**_ _,_ _**\u03b1**_ _[\u2032]_ _[ \u2212]_ _[\u03f5]_ [(] _**[\u03b1]**_ _[\u2032]_ [ +] _**[ \u03b1]**_ [)] _[\u22a4]_ **[1]** [ + (] _**[\u03b1]**_ _[\u2032]_ _[ \u2212]_ _**[\u03b1]**_ [)] _[\u22a4]_ **[y]** _[ \u2212]_ 2 [1] [1] **K** + [1] 2 [(] _**[\u03b1]**_ _[\u2032]_ _[ \u2212]_ _**[\u03b1]**_ [)] _[\u22a4]_ [\ufffd] _C_ ( _**\u03b1**_ _[\u2032]_ _\u2212_ _**\u03b1**_ ) _C_ **[I]** \ufffd (11.27) subject to: ( _**\u03b1**_ _\u2265_ **0** )",
    "chunk_id": "foundations_machine_learning_278"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2227_ ( _**\u03b1**_ _[\u2032]_ _\u2265_ **0** ) _\u2227_ ( _**\u03b1**_ _[\u2032]_ _\u2212_ _**\u03b1**_ ) _[\u22a4]_ **1** = 0) _._ Any PDS kernel _K_ can be used with quadratic SVR, which extends the algorithm to non-linear regression solutions. Problem (11.27) is a convex QP similar to the dual problem of SVMs in the separable case and can be solved using similar optimization techniques. The solutions _**\u03b1**_ and _**\u03b1**_ _[\u2032]_ define the hypothesis _h_ returned by SVR as follows: _h_ ( _x_ ) = _m_ \ufffd( _\u03b1_ _i_ _[\u2032]_ _[\u2212]_ _[\u03b1]_ _[i]_ [)] _[K]_ [(] **[x]** _[i]_ _[,]_ **[ x]** [) +] _[ b,]_ (11.28) _i_ =1 where the offset _b_ can be obtained from a point _x_ _j_ with 0 _< \u03b1_ _j_ _< C_ or 0 _< \u03b1_ _j_ _[\u2032]_ _[< C]_ exactly as in the case of SVR with (non-quadratic) _\u03f5_ -insensitive loss. Note that for _\u03f5_ = 0, the quadratic SVR algorithm coincides with KRR as can be seen from the dual optimization problem (the additional constraint ( _**\u03b1**_ _[\u2032]_ _\u2212_ _**\u03b1**_ ) _[\u22a4]_ **1** = 0 appears here due to use of an offset _b_ ). The following generalization bound holds for quadratic SVR. It can be shown in a way that is similar to the proof of theorem 11.13 using the fact that the quadratic _\u03f5_ -insensitive function _x \ufffd\u2192|x|_ [2] _\u03f5_ [is 2] _[M]_ [-Lipschitz over the] interval [ _\u2212M,_ + _M_ ]. **Theorem 11.14** _Let K_ : X _\u00d7_ X _\u2192_ R _be a PDS kernel, let_ \u03a6: X _\u2192_ H _be a feature_ _mapping associated to K and let_ H = _{x \ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x_ ): _\u2225_ **w** _\u2225_ H _\u2264_ \u039b _}. Assume that_ _there exists r >_ 0 _such that K_ ( _x, x_ ) _\u2264_ _r_ [2] _and M >_ 0 _such that |h_ ( _x_ ) _\u2212_ _y| \u2264_ _M for_ _all_ ( _x, y_ ) _\u2208_ X _\u00d7_ Y _. Fix \u03f5 >_ 0 _. Then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4,_ _each of the following inequalities holds for all h \u2208_ H _,_ \ufffd log [1] _\u03b4_ 2 _m_ \ufffd _|h_ ( _x_ ) _\u2212_ _y|_ [2] _\u03f5_ \ufffd + 4 _M_ \ufffd _r_ [2] \u039b [2] + _M_ [2] _m_ _r_ [2] \u039b [2] E ( _x,y_ ) _\u223c_ D E ( _x,y_ ) _\u223c_ D \ufffd _|h_ ( _x_ ) _\u2212_ _y|_ [2] _\u03f5_ \ufffd _\u2264_ E ( _x,y_ ) _\u223c_ D [\ufffd] \ufffd _|h_ ( _x_ ) _\u2212_ _y|_ [2] _\u03f5_ \ufffd _\u2264_ E ( _x,y_ ) _\u223c_ D [\ufffd] \ufffdTr[ **K** ] \ufffd _|h_ ( _x_ ) _\u2212_ _y|_ [2] _\u03f5_ \ufffd + [4] _[M]_ [\u039b] + 3 _M_ [2] _m_ \ufffd log [2] _\u03b4_ 2 _m_ _[.]_ This theorem provides a strong justification for the quadratic SVR algorithm. Alternative convex loss functions can be used to define regression algorithms, in particular the _Huber loss_ (see figure 11.5), which penalizes smaller errors quadratically and larger ones only linearly. SVR admits several advantages: the algorithm is",
    "chunk_id": "foundations_machine_learning_279"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "based on solid theoretical guarantees, the solution returned is sparse, and it allows a natural use of PDS kernels, **11.3** **Regression algorithms** **285** **Image:** [No caption returned] **Figure 11.5** **Image:** [No caption returned] Alternative loss functions that can be used in conjunction with SVR. which extend the algorithm to non-linear regression solutions. SVR also admits favorable stability properties that we discuss in chapter 14. However, one drawback of the algorithm is that it requires the selection of two parameters, _C_ and _\u03f5_ . These can be selected via cross-validation, as in the case of SVMs, but this requires a relatively larger validation set. Some heuristics are often used to guide the search for their values: _C_ is searched near the maximum value of the labels in the absence of an offset ( _b_ = 0) and for a normalized kernel, and _\u03f5_ is chosen close to the average difference of the labels. As already discussed, the value of _\u03f5_ determines the number of support vectors and the sparsity of the solution. Another drawback of SVR is that, as in the case of SVMs or KRR, it may be computationally expensive when dealing with large training sets. One effective solution in such cases, as for KRR, consists of approximating the kernel matrix using low-rank approximations via the Nystr\u00a8om method or the partial Cholesky decomposition. In the next section, we discuss an alternative sparse algorithm for regression. **11.3.4** **Lasso** Unlike the KRR and SVR algorithms, the Lasso (least absolute shrinkage and selection operator) algorithm does not admit a natural use of PDS kernels. Thus, here, we assume that the input space X is a subset of R _[N]_ and consider a family of linear hypotheses H = _{x \ufffd\u2192_ **w** _\u00b7_ **x** + _b_ : **w** _\u2208_ R _[N]_ _, b \u2208_ R _}_ . Let _S_ = \ufffd( **x** 1 _, y_ 1 ) _, . . .,_ ( **x** _m_ _, y_ _m_ )\ufffd _\u2208_ (X _\u00d7_ Y) _[m]_ be a labeled training sample. Lasso is based on the minimization of the empirical squared error on _S_ with a regularization term depending on the norm of the weight vector, as in the case of the ridge regression, but using the _L_ 1 norm instead of the _L_ 2 norm and without squaring the **286** **Chapter 11** **Regression** **Image:** [No caption returned] **Image:** [No caption returned] ~~L1~~ ~~regularization~~ ~~L2~~ ~~regularization~~ **Figure 11.6** Comparison of the Lasso and ridge regression solutions. norm: min **w** _,b_ _[F]_ [(] **[w]** _[, b]_ [) =] _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [1] [ +] _m_ \ufffd ( **w** _\u00b7_ **x** _i_ + _b \u2212_ _y_ _i_ ) [2] _._ (11.29) _i_ =1 Here _\u03bb_ denotes a positive parameter as for ridge regression. This is a convex optimization problem, since _\u2225\u00b7\u2225_ 1 is convex as with all norms and since the empirical error term is convex, as already discussed for linear regression. The optimization for Lasso can be written equivalently as min **w** _,b_ _m_ \ufffd ( **w** _\u00b7_ **x** _i_ + _b \u2212_ _y_ _i_ ) [2] subject to:",
    "chunk_id": "foundations_machine_learning_280"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2225_ **w** _\u2225_ 1 _\u2264_ \u039b 1 _,_ (11.30) _i_ =1 where \u039b 1 is a positive parameter. The key property of Lasso as in the case of other algorithms using the _L_ 1 norm constraint is that it leads to a sparse solution **w**, that is one with few non-zero components. Figure 11.6 illustrates the difference between the _L_ 1 and _L_ 2 regularizations in dimension two. The objective function of (11.30) is a quadratic function, thus its contours are ellipsoids, as illustrated by the figure (in blue). The areas corresponding to _L_ 1 and _L_ 2 balls of a fixed radius \u039b 1 are also shown in the left and right panel (in red). The Lasso solution is the point of intersection of the contours with the _L_ 1 ball. As can be seen form the figure, this can typically occur at a corner of the _L_ 1 ball where some coordinates are zero. In contrast, the ridge regression solution is at the point of intersection of the contours and the _L_ 2 ball, where none of the coordinates is typically zero. **11.3** **Regression algorithms** **287** The following results show that Lasso also benefits from strong theoretical guarantees. We first give a general upper bound on the empirical Rademacher complexity of _L_ 1 norm-constrained linear hypotheses . **Theorem 11.15 (Rademacher complexity of linear hypotheses with bounded** _L_ 1 **norm)** _Let_ X _\u2286_ R _[N]_ _and let S_ = \ufffd( **x** 1 _, y_ 1 ) _, . . .,_ ( **x** _m_ _, y_ _m_ )\ufffd _\u2208_ (X _\u00d7_ Y) _[m]_ _be a sample of size m._ _Assume that for all i \u2208_ [ _m_ ] _, \u2225_ **x** _i_ _\u2225_ _\u221e_ _\u2264_ _r_ _\u221e_ _for some r_ _\u221e_ _>_ 0 _, and let_ H = _{_ **x** _\u2208_ X _\ufffd\u2192_ **w** _\u00b7_ **x** : _\u2225_ **w** _\u2225_ 1 _\u2264_ \u039b 1 _}. Then, the empirical Rademacher complexity of_ H _can be_ _bounded as follows:_ ~~\ufffd~~ \ufffd R _S_ (H) _\u2264_ 2 _r_ _\u221e_ [2] \u039b [2] 1 [lo][g(][2] _[N]_ [)] _._ (11.31) _m_ 2 _r_ _\u221e_ [2] \u039b [2] 1 [lo][g(][2] _[N]_ [)] Proof: For any _i \u2208_ [ _m_ ] we denote by _x_ _ij_ the _j_ th component of **x** _i_ . \ufffd \ufffd R _S_ (H) = [1] _m_ [E] _**\u03c3**_ \ufffd R _S_ (H) = [1] \ufffd sup _\u2225_ **w** _\u2225_ 1 _\u2264_ \u039b 1 _m_ \ufffd _\u03c3_ _i_ **w** _\u00b7_ **x** _i_ _i_ =1 \ufffd = [\u039b] [1] _m_ [E] _**\u03c3**_ = [\u039b] [1] \ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u03c3_ _i_ **x** _i_ \ufffd\ufffd\ufffd _\u221e_ _i_ =1 _m_ \ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd = [\u039b] [1] _m_ [E] _**\u03c3**_ = [\u039b] [1] max _j\u2208_ [ _N_ ] \ufffd\ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u03c3_ _i_ _x_ _ij_ _i_ =1 _m_ \ufffd (by definition of the dual norm) (by definition of _\u2225\u00b7 \u2225_ _\u221e_ ) (by definition of _\u2225\u00b7 \u2225_ _\u221e_ ) \ufffd = [\u039b] [1] _m_ [E] _**\u03c3**_ = [\u039b] [1] max max _j\u2208_ [ _N_ ] _s\u2208{\u2212_ 1 _,_ +1 _}_ _[s]_ _m_ \ufffd _\u03c3_ _i_ _x_ _ij_ _i_ =1",
    "chunk_id": "foundations_machine_learning_281"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_,_ \ufffd = [\u039b] [1] _m_ [E] _**\u03c3**_ = [\u039b] [1] \ufffd \ufffd \ufffd sup **z** _\u2208A_ _m_ \ufffd _\u03c3_ _i_ _z_ _i_ _i_ =1 where _A_ denotes the set of _N_ vectors _{s_ ( _x_ 1 _j_ _, . . ., x_ _mj_ ) _[\u22a4]_ : _j \u2208_ [ _N_ ] _, s \u2208{\u2212_ 1 _,_ +1 _}}_ . For any **z** _\u2208_ _A_, we have _\u2225_ **z** _\u2225_ 2 _\u2264_ \ufffd _mr_ _\u221e_ [2] = _r_ _\u221e_ _\u221am_ . Thus, by Massart\u2019s lemma (theorem 3.7), since _A_ contains at most 2 _N_ elements, the following inequality holds: ~~\ufffd~~ = _r_ _\u221e_ \u039b 1 _m_ R\ufffd _S_ (H) _\u2264_ \u039b 1 _r_ _\u221e_ _\u221am_ 2 log(2 _N_ ) \ufffd 2 log(2 _N_ ) _,_ _m_ which concludes the proof. Note that dependence of the bound on the dimension _N_ is only logarithmic, which suggests that using very high-dimensional feature spaces does not significantly affect generalization. Combining the Rademacher complexity bound just proven and the general result of Theorem 11.3 yields the following generalization bound for the hypothesis set used by Lasso, using the squared loss. **Theorem 11.16** _Let_ X _\u2286_ R _[N]_ _and_ H = _{_ **x** _\u2208_ X _\ufffd\u2192_ **w** _\u00b7_ **x** : _\u2225_ **w** _\u2225_ 1 _\u2264_ \u039b 1 _}._ _Assume_ _that there exists r_ _\u221e_ _>_ 0 _such for all_ **x** _\u2208_ X _, \u2225_ **x** _\u2225_ _\u221e_ _\u2264_ _r_ _\u221e_ _and M >_ 0 _such that_ **288** **Chapter 11** **Regression** _|h_ ( _x_ ) _\u2212_ _y| \u2264_ _M for all_ ( _x, y_ ) _\u2208_ X _\u00d7_ Y _. Then, for any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4, each of the following inequalities holds for all h \u2208_ H _:_ \ufffd log [1] _\u03b4_ (11.32) 2 _m_ _[.]_ _R_ ( _h_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ ) + 2 _r_ _\u221e_ \u039b 1 _M_ \ufffd 2 log(2 _N_ ) + _M_ [2] _m_ As in the case of ridge regression, we observe that the objective function minimized by Lasso has the same form as the right-hand side of this generalization bound. There exist a variety of different methods for solving the optimization problem of Lasso, including an efficient algorithm (LARS) for computing the entire _regulariza-_ _tion path_ of solutions, that is, the Lasso solutions for all values of the regularization parameter _\u03bb_, and other on-line solutions that apply more generally to optimization problems with an _L_ 1 norm constraint. Here, we show that the Lasso problems (11.29) or (11.30) are equivalent to a quadratic program (QP), and therefore that any QP solver can be used to compute the solution. Observe that any weight vector **w** can be written as **w** = **w** [+] _\u2212_ **w** _[\u2212]_, with **w** [+] _\u2265_ 0, **w** _[\u2212]_ _\u2265_ 0, and _w_ _j_ [+] [= 0 or] _[ w]_ _j_ _[\u2212]_ [= 0 for any] _[ j][ \u2208]_ [[] _[N]_ [], which implies] _\u2225_ **w** _\u2225_ 1 = [\ufffd] _[N]_ _j_ =1 _[w]_ _j_ [+] [+] _[ w]_ _j_ _[\u2212]_ [. This can be done",
    "chunk_id": "foundations_machine_learning_282"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "by defining the] _[ j]_ [th component of] **[ w]** [+] [ as] _w_ _j_ if _w_ _j_ _\u2265_ 0, 0 otherwise, and similarly the _j_ th component of **w** _[\u2212]_ as _\u2212w_ _j_ if _w_ _j_ _\u2264_ 0, 0 otherwise, for any _j \u2208_ [ _N_ ]. With the replacement **w** = **w** [+] _\u2212_ **w** _[\u2212]_, with **w** [+] _\u2265_ 0, **w** _[\u2212]_ _\u2265_ 0, and _\u2225_ **w** _\u2225_ 1 = [\ufffd] _[N]_ _j_ =1 _[w]_ _j_ [+] [+] _[ w]_ _j_ _[\u2212]_ [, the Lasso problem (11.29) becomes] **w** [+] _\u2265_ min 0 _,_ **w** _[\u2212]_ _\u2265_ 0 _,b_ _[\u03bb]_ _N_ \ufffd( _w_ _j_ [+] [+] _[ w]_ _j_ _[\u2212]_ [) +] _j_ =1 _m_ \ufffd _i_ =1 2 \ufffd( **w** [+] _\u2212_ **w** _[\u2212]_ ) _\u00b7_ **x** _i_ + _b \u2212_ _y_ _i_ \ufffd _._ (11.33) Conversely, a solution **w** = **w** [+] _\u2212_ **w** _[\u2212]_ of (11.33) verifies the condition _w_ _j_ [+] [= 0 or] _w_ _j_ _[\u2212]_ [= 0 for any] _[ j][ \u2208]_ [[] _[N]_ [], thus] _[ w]_ _[j]_ [ =] _[ w]_ _j_ [+] [when] _[ w]_ _[j]_ _[ \u2265]_ [0 and] _[ w]_ _[j]_ [ =] _[ \u2212][w]_ _j_ _[\u2212]_ [when] _[ w]_ _[j]_ _[ \u2264]_ [0.] This is because if _\u03b4_ _j_ = min( _w_ _j_ [+] _[, w]_ _j_ _[\u2212]_ [)] _[ >]_ [ 0 for some] _[ j][ \u2208]_ [[] _[N]_ [], replacing] _[ w]_ _j_ [+] with ( _w_ _j_ [+] _[\u2212]_ _[\u03b4]_ _[j]_ [) and] _[ w]_ _j_ _[\u2212]_ [with (] _[w]_ _j_ _[\u2212]_ _[\u2212]_ _[\u03b4]_ _[j]_ [) would not affect] _[ w]_ _j_ [+] _[\u2212]_ _[w]_ _j_ _[\u2212]_ [= (] _[w]_ _j_ [+] _[\u2212]_ _[\u03b4]_ [)] _[\u2212]_ [(] _[w]_ _j_ _[\u2212]_ _[\u2212]_ _[\u03b4]_ [),] but would reduce the term ( _w_ _j_ [+] [+] _[ w]_ _j_ _[\u2212]_ [) in the objective function by 2] _[\u03b4]_ _[j]_ _[ >]_ [ 0 and] provide a better solution. In view of this analysis, problems (11.29) and (11.33) admit the same optimal solution and are equivalent. Problem (11.33) is a QP since the objective function is quadratic in **w** [+], **w** _[\u2212]_, and _b_, and since the constraints are affine. With this formulation, the problem can be straightforwardly shown to admit a natural online algorithmic solution (exercise 11.10). [20] Thus, Lasso has several advantages: it benefits from strong theoretical guarantees and returns a sparse solution, which is advantageous when there are accurate solutions based on few features. The sparsity of the solution is also computationally 20 The technique we described to avoid absolute values in the objective function can be used similarly in other optimization problems. **11.3** **Regression algorithms** **289** attractive; sparse feature representations of the weight vector can be used to make the inner product with a new vector more efficient. The algorithm\u2019s sparsity can also be used for feature selection. The main drawback of the algorithm is that it does not admit a natural use of PDS kernels and thus an extension to non-linear regression, unlike KRR and SVR. One solution is then to use empirical",
    "chunk_id": "foundations_machine_learning_283"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "kernel maps, as discussed in chapter 6. Also, Lasso\u2019s solution does not admit a closed-form solution. This is not a critical property from the optimization point of view but one that can make some mathematical analyses very convenient. **11.3.5** **Group norm regression algorithms** Other types of regularization aside from the _L_ 1 or _L_ 2 norm can be used to define regression algorithms. For instance, in some situations, the feature space may be naturally partitioned into subsets, and it may be desirable to find a sparse solution that selects or omits entire subsets of features. A natural norm in this setting is the group or mixed norm _L_ 2 _,_ 1, which is a combination of the _L_ 1 and _L_ 2 norms. Imagine that we partition **w** _\u2208_ R _[N]_ as **w** 1 _, . . .,_ **w** _k_, where **w** _j_ _\u2208_ R _[N]_ _[j]_ for 1 _\u2264_ _j \u2264_ _k_ and [\ufffd] _[N]_ _[j]_ [ =] _[ N]_ [, and define] **[ W]** [ = (] **[w]** 1 _[\u22a4]_ _[, . . .,]_ **[ w]** _k_ _[\u22a4]_ [)] _[\u22a4]_ [. Then the] _[ L]_ [2] _[,]_ [1] [ norm of] **[ W]** [ is] and [\ufffd] _j_ _[N]_ _[j]_ [ =] _[ N]_ [, and define] **[ W]** [ = (] **[w]** 1 _[\u22a4]_ _[, . . .,]_ **[ w]** _k_ _[\u22a4]_ [)] _[\u22a4]_ [. Then the] _[ L]_ [2] _[,]_ [1] [ norm of] **[ W]** [ is] defined as _\u2225_ **W** _\u2225_ 2 _,_ 1 = _k_ \ufffd \ufffd _\u2225_ **w** _j_ _\u2225_ _._ _j_ =1 Combining the _L_ 2 _,_ 1 norm with the empirical mean squared error leads to the _Group_ _Lasso_ formulation. More generally, an _L_ _q,p_ group norm regularization can be used for _q, p \u2265_ 1 (see appendix A for the definition of group norms). **11.3.6** **On-line regression algorithms** The regression algorithms presented in the previous sections admit natural online versions. Here, we briefly present two examples of these algorithms. These algorithms are particularly useful for applications to very large data sets for which a batch solution can be computationally too costly to derive and more generally in all of the on-line learning settings discussed in chapter 8. Our first example is known as the _Widrow-Hoff algorithm_ and coincides with the application of stochastic gradient descent techniques to the linear regression objective function. Figure 11.7 gives the pseudocode of the algorithm. A similar algorithm can be derived by applying the stochastic gradient technique to ridge regression. At each round, the weight vector is augmented with a quantity that depends on the prediction error ( **w** _t_ _\u00b7_ **x** _t_ _\u2212_ _y_ _t_ ). Our second example is an online version of the SVR algorithm, which is obtained by application of stochastic gradient descent to the dual objective function of SVR. Figure 11.8 gives the pseudocode of the algorithm for an arbitrary PDS kernel _K_ **290** **Chapter 11** **Regression** WidrowHoff( **w** 0 ) 1 **w** 1 _\u2190_ **w** 0 _\u25b7_ typically **w** 0 = **0** 2 **for** _t \u2190_ 1 **to** _T_ **do**",
    "chunk_id": "foundations_machine_learning_284"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "3 Receive( **x** _t_ ) 4 _y_ \ufffd _t_ _\u2190_ **w** _t_ _\u00b7_ **x** _t_ 5 Receive( _y_ _t_ ) 6 **w** _t_ +1 _\u2190_ **w** _t_ + 2 _\u03b7_ ( **w** _t_ _\u00b7_ **x** _t_ _\u2212_ _y_ _t_ ) **x** _t_ _\u25b7_ learning rate _\u03b7 >_ 0. 7 **return w** _T_ +1 **Figure 11.7** The Widrow-Hoff algorithm. in the absence of any offset ( _b_ = 0). Another on-line regression algorithm is given by exercise 11.10 for Lasso. **11.4** **Chapter notes** The generalization bounds presented in this chapter are for bounded regression problems. When _{x \ufffd\u2192_ _L_ ( _h_ ( _x_ ) _, y_ ): _h \u2208_ H _}_, the family of losses of the hypotheses, is not bounded, a single function can take arbitrarily large values with arbitrarily small probabilities. This is the main issue for deriving uniform convergence bounds for unbounded losses. This problem can be avoided either by assuming the existence of an _envelope_, that is a single non-negative function with a finite expectation lying above the absolute value of the loss of every function in the hypothesis set [Dudley, 1984, Pollard, 1984, Dudley, 1987, Pollard, 1989, Haussler, 1992], or by assuming that some moment of the loss functions is bounded [Vapnik, 1998, 2006]. Cortes, Greenberg, and Mohri [2013] (see also [Cortes et al., 2010a]) give two-sided generalization bounds for unbounded losses with finite second moments. The onesided version of their bounds coincides with that of Vapnik [1998, 2006] modulo a constant factor, but the proofs given by Vapnik in both books seem to be incomplete and incorrect. The notion of pseudo-dimension is due to Pollard [1984]. Its equivalent definition in terms of VC-dimension is discussed by Vapnik [2000]. The notion of fat-shattering was introduced by Kearns and Schapire [1990]. The linear regression algorithm is a classical algorithm in statistics that dates back at least to the nine **11.4** **Chapter notes** **291** OnLineDualSVR() 1 _**\u03b1**_ _\u2190_ **0** 2 _**\u03b1**_ _[\u2032]_ _\u2190_ **0** 3 **for** _t \u2190_ 1 **to** _T_ **do** 4 Receive( _x_ _t_ ) 5 _y_ \ufffd _t_ _\u2190_ [\ufffd] _[t]_ _s_ =1 [(] _[\u03b1]_ _s_ _[\u2032]_ _[\u2212]_ _[\u03b1]_ _[s]_ [)] _[K]_ [(] _[x]_ _[s]_ _[, x]_ _[t]_ [)] 6 Receive( _y_ _t_ ) 7 _\u03b1_ _t_ _[\u2032]_ +1 _[\u2190]_ _[\u03b1]_ _t_ _[\u2032]_ [+ min(max(] _[\u03b7]_ [(] _[y]_ _[t]_ _[\u2212]_ _[y]_ [\ufffd] _[t]_ _[\u2212]_ _[\u03f5]_ [)] _[,][ \u2212][\u03b1]_ _t_ _[\u2032]_ [)] _[, C][ \u2212]_ _[\u03b1]_ _t_ _[\u2032]_ [)] \ufffd 8 _\u03b1_ _t_ +1 _\u2190_ _\u03b1_ _t_ + min(max( _\u03b7_ ( _y_ _t_ _\u2212_ _y_ _t_ _\u2212_ _\u03f5_ ) _, \u2212\u03b1_ _t_ ) _, C \u2212_ _\u03b1_ _t_ ) 9 **return** [\ufffd] _[T]_ _t_ =1 [(] _[\u03b1]_ _t_ _[\u2032]_ _[\u2212]_ _[\u03b1]_ _[t]_ [)] _[K]_ [(] _[x]_ _[t]_ _[,][ \u00b7]_ [)] **Figure 11.8** An on-line version of dual SVR. teenth century. The ridge regression algorithm is due to Hoerl and Kennard [1970]. Its kernelized version (KRR) was introduced and discussed by Saunders, Gammerman, and Vovk [1998]. An extension of KRR to outputs in R _[p]_ with _p >_ 1 with possible constraints on the regression is presented and analyzed",
    "chunk_id": "foundations_machine_learning_285"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "by Cortes, Mohri, and Weston [2007c]. The support vector regression (SVR) algorithm is discussed in Vapnik [2000]. Lasso was introduced by Tibshirani [1996]. The LARS algorithm for solving its optimization problem was later presented by Efron et al. [2004]. The Widrow-Hoff on-line algorithm is due to Widrow and Hoff [1988]. The dual on-line SVR algorithm was first introduced and analyzed by Vijayakumar and Wu [1999]. The kernel stability analysis of exercise 10.3 is from Cortes et al. [2010b]. For large-scale problems where a straightforward batch optimization of a primal or dual objective function is intractable, general iterative stochastic gradient descent methods similar to those presented in section 11.3.6, or quasi-Newton methods such as the limited-memory BFGS (Broyden-Fletcher-Goldfard-Shanno) algorithm [Nocedal, 1980] can be practical alternatives in practice. In addition to the linear regression algorithms presented in this chapter and their kernel-based non-linear extensions, there exist many other algorithms for regression, including decision trees for regression (see chapter 9), boosting trees for regression, and artificial neural networks. **292** **Chapter 11** **Regression** **11.5** **Exercises** 11.1 Pseudo-dimension and monotonic functions. Assume that _\u03c6_ is a strictly monotonic function and let _\u03c6 \u25e6_ H be the family of functions defined by _\u03c6 \u25e6_ H = _{\u03c6_ ( _h_ ( _\u00b7_ )) : _h \u2208_ H _}_, where H is some set of real-valued functions. Show that Pdim( _\u03c6 \u25e6_ H) = Pdim(H). 11.2 Pseudo-dimension of linear functions. Let H be the set of all linear functions in dimension _d_, i.e. _h_ ( **x** ) = **w** _[\u22a4]_ **x** for some **w** _\u2208_ R _[d]_ . Show that Pdim(H) = _d_ . 11.3 Linear regression. (a) What condition is required on the data **X** in order to guarantee that **XX** _[\u22a4]_ is invertible? (b) Assume the problem is under-determined. Then, we can choose a solution **w** such that the equality **X** _[\u22a4]_ **w** = **X** _[\u22a4]_ ( **XX** _[\u22a4]_ ) _[\u2020]_ **Xy** (which can be shown to equal **X** _[\u2020]_ **Xy** ) holds. One particular choice that satisfies this equality is **w** _[\u2217]_ = ( **XX** _[\u22a4]_ ) _[\u2020]_ **Xy** . However, this is not the unique solution. As a function of **w** _[\u2217]_, characterize all choices of **w** that satisfy **X** _[\u22a4]_ **w** = **X** _[\u2020]_ **Xy** ( _Hint_ : use the fact that **XX** _[\u2020]_ **X** = **X** ). 11.4 Perturbed kernels. Suppose two different kernel matrices, **K** and **K** _[\u2032]_, are used to train two kernel ridge regression hypothesis with the same regularization parameter _\u03bb_ . In this problem, we will show that the difference in the optimal dual variables, _**\u03b1**_ and _**\u03b1**_ _[\u2032]_ respectively, is bounded by a quantity that depends on _\u2225_ **K** _[\u2032]_ _\u2212_ **K** _\u2225_ 2 . (a) Show _**\u03b1**_ _[\u2032]_ _\u2212_ _**\u03b1**_ = \ufffd( **K** _[\u2032]_ + _\u03bb_ **I** ) _[\u2212]_ [1] ( **K** _[\u2032]_ _\u2212_ **K** )( **K** + _\u03bb_ **I** ) _[\u2212]_ [1] [\ufffd] **y** . ( _Hint_ : Show that for any invertible matrix **M**, **M** _[\u2032]_ [1] _\u2212_ **M** [1] = _\u2212_ **M** _[\u2032\u2212]_ [1] ( **M** _[\u2032]_ _\u2212_ **M** ) **M** _[\u2212]_ [1] .) (b)",
    "chunk_id": "foundations_machine_learning_286"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Assuming _\u2200y \u2208_ Y _, |y| \u2264_ _M_, show that _\u2225_ _**\u03b1**_ _[\u2032]_ _\u2212_ _**\u03b1**_ _\u2225\u2264_ _\u221amM_ _\u2225_ **K** _\u2032_ _\u2212_ **K** _\u2225_ 2 _._ _\u03bb_ [2] 11.5 Huber loss. Derive the primal and dual optimization problem used to solve the SVR problem with the Huber loss: _L_ _c_ ( _\u03be_ _i_ ) = where _\u03be_ _i_ = **w** _\u00b7_ \u03a6( **x** _i_ ) + _b \u2212_ _y_ _i_ . 12 _[\u03be]_ _i_ [2] _[,]_ if _|\u03be_ _i_ _| \u2264_ _c_ \ufffd _c\u03be_ _i_ _\u2212_ [1] 2 _[c]_ [2] _[,]_ otherwise [1] 2 _[c]_ [2] _[,]_ otherwise _,_ _c\u03be_ _i_ _\u2212_ [1] **11.5** **Exercises** **293** OnLineLasso( **w** 0 [+] _[,]_ **[ w]** 0 _[\u2212]_ [)] 1 **w** 1 [+] _[\u2190]_ **[w]** 0 [+] _\u25b7_ **w** 0 [+] _[\u2265]_ [0] 2 **w** 1 _[\u2212]_ _[\u2190]_ **[w]** 0 _[\u2212]_ _\u25b7_ **w** 0 _[\u2212]_ _[\u2265]_ [0] 3 **for** _t \u2190_ 1 **to** _T_ **do** 4 Receive( **x** _t_ _, y_ _t_ ) 5 **for** _j \u2190_ 1 **to** _N_ **do** 6 _w_ _t_ [+] +1 _j_ _[\u2190]_ [max] \ufffd0 _, w_ _tj_ [+] _[\u2212]_ _[\u03b7]_ \ufffd _\u03bb \u2212_ \ufffd _y_ _t_ _\u2212_ ( **w** _t_ [+] _[\u2212]_ **[w]** _t_ _[\u2212]_ [)] _[ \u00b7]_ **[ x]** _[t]_ \ufffd **x** _tj_ \ufffd\ufffd 7 _w_ _t_ _[\u2212]_ +1 _j_ _[\u2190]_ [max] \ufffd0 _, w_ _tj_ _[\u2212]_ _[\u2212]_ _[\u03b7]_ \ufffd _\u03bb_ + \ufffd _y_ _t_ _\u2212_ ( **w** _t_ [+] _[\u2212]_ **[w]** _t_ _[\u2212]_ [)] _[ \u00b7]_ **[ x]** _[t]_ \ufffd **x** _tj_ \ufffd\ufffd 8 **return w** _T_ [+] +1 _[\u2212]_ **[w]** _T_ _[\u2212]_ +1 **Figure 11.9** On-line algorithm for Lasso. 11.6 SVR and squared loss. Assuming that 2 _r_ \u039b _\u2264_ 1, use theorem 11.13 to derive a generalization bound for the squared loss. 11.7 SVR dual formulations. Give a detailed and carefully justified derivation of the dual formulations of the SVR algorithm both for the _\u03f5_ -insensitive loss and the quadratic _\u03f5_ -insensitive loss. 11.8 Optimal kernel matrix. Suppose in addition to optimizing the dual variables _\u03b1 \u2208_ R _[m]_, as in (11.16), we also wish to optimize over the entries of the PDS kernel matrix **K** _\u2208_ R _[m][\u00d7][m]_ . min s _._ t _. \u2225_ **K** _\u2225_ 2 _\u2264_ 1 **K** _\u2ab0_ 0 [max] _**\u03b1**_ _[\u2212][\u03bb]_ _**[\u03b1]**_ _[\u22a4]_ _**[\u03b1]**_ _[ \u2212]_ _**[\u03b1]**_ _[\u22a4]_ **[K]** _**[\u03b1]**_ [ + 2] _**[\u03b1]**_ _[\u22a4]_ **[y]** _[,]_ (a) What is the closed-form solution for the optimal **K** for the joint optimization? (b) Optimizing over the choice of kernel matrix will provide a better value of the objective function. Explain, however, why the resulting kernel matrix is not useful in practice. 11.9 Leave-one-out error. In general, the computation of the leave-one-out error can be very costly since, for a sample of size _m_, it requires training the algorithm _m_ times. The objective of this problem is to show that, remarkably, in the case of **294** **Chapter 11** **Regression** kernel ridge regression, the leave-one-out error can be computed efficiently by training the algorithm only once. Let _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_",
    "chunk_id": "foundations_machine_learning_287"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_, y_ _m_ )) denote a training sample of size _m_ and for any _i \u2208_ [ _m_ ], let _S_ _i_ denote the sample of size _m \u2212_ 1 obtained from _S_ by removing ( _x_ _i_ _, y_ _i_ ): _S_ _i_ = _S \u2212{_ ( _x_ _i_ _, y_ _i_ ) _}_ . For any sample _T_, let _h_ _T_ denote a hypothesis obtained by training _T_ . By definition (see definition 5.2), for the squared loss, the leave-one-out error with respect to _S_ is defined by \ufffd _R_ LOO (KRR) = [1] _m_ _m_ \ufffd( _h_ _S_ _i_ ( _x_ _i_ ) _\u2212_ _y_ _i_ ) [2] _._ _i_ =1 (a) Let _S_ _i_ _[\u2032]_ [= ((] _[x]_ [1] _[, y]_ [1] [)] _[, . . .,]_ [ (] _[x]_ _[i]_ _[, h]_ _[S]_ _i_ [(] _[y]_ _[i]_ [))] _[, . . .,]_ [ (] _[x]_ _[m]_ _[, y]_ _[m]_ [)). Show that] _[ h]_ _[S]_ _i_ [=] _[ h]_ _[S]_ _i_ _[\u2032]_ [.] (b) Define **y** _i_ = **y** _\u2212_ _y_ _i_ _**e**_ _i_ + _h_ _S_ _i_ ( _x_ _i_ ) _**e**_ _i_, that is the vector of labels with the _i_ th component replaced with _h_ _S_ _i_ ( _x_ _i_ ). Prove that for KRR _h_ _S_ _i_ ( _x_ _i_ ) = **y** _i_ _[\u22a4]_ [(] **[K]** [ +] _\u03bb_ **I** ) _[\u2212]_ [1] **K** _**e**_ _i_ . (c) Prove that the leave-one-out error admits the following simple expression in terms of _h_ _S_ : 2 _._ (11.34) \ufffd \ufffd _R_ LOO (KRR) = [1] _m_ _m_ \ufffd _i_ =1 _h_ _S_ ( _x_ _i_ ) _\u2212_ _y_ _i_ \ufffd _**e**_ _[\u22a4]_ _i_ [(] **[K]** [ +] _[ \u03bb]_ **[I]** [)] _[\u2212]_ [1] **[K]** _**[e]**_ _[i]_ (d) Suppose that the diagonal entries of matrix **M** = ( **K** + _\u03bb_ **I** ) _[\u2212]_ [1] **K** are all equal to _\u03b3_ . How do the empirical error _R_ [\ufffd] _S_ of the algorithm and the leave-one-out error _R_ [\ufffd] LOO relate? Is there any value of _\u03b3_ for which the two errors coincide? 11.10 On-line Lasso. Use the formulation (11.33) of the optimization problem of Lasso and stochastic gradient descent (see section 8.3.1) to show that the problem can be solved using the on-line algorithm of figure 11.9. 11.11 On-line quadratic SVR. Derive an on-line algorithm for the quadratic SVR algorithm (provide the full pseudocode). # 12 Maximum Entropy Models In this chapter, we introduce and discuss maximum entropy models, also known as _Maxent models_, a widely used family of algorithms for density estimation that can exploit rich feature sets. We first introduce the standard density estimation problem and briefly describe the Maximum Likelihood and Maximum a Posteriori solutions. Next, we describe a richer density estimation problem where the learner additionally has access to features. This is the problem addressed by Maxent models. We introduce the key principle behind Maxent models and formulate their primal optimization problem. Next, we prove a duality theorem showing that Maxent models coincide with Gibbs distribution solutions of a",
    "chunk_id": "foundations_machine_learning_288"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "regularized Maximum Likelihood problem. We present generalization guarantees for these models and also give an algorithm for solving their dual optimization problem using a coordinate descent technique. We further extend these models to the case where an arbitrary Bregman divergence is used with other norms, and prove a general duality theorem leading to an equivalent optimization problem with alternative regularizations. We also give a specific theoretical analysis of Maxent models with _L_ 2 -regularization, which are commonly used in applications. **12.1** **Density estimation problem** Let _S_ = ( _x_ 1 _, . . ., x_ _m_ ) be a sample of size _m_ drawn i.i.d. from an unknown distribution D. Then, the density estimation problem consists of using that sample to select out of a family of possible distributions _P_ a distribution p that is close to D. The choice of the family _P_ is critical. A relatively small family may not contain D or even any distribution close to D. On the other hand, a very rich family defined by a large set of parameters may make the task of selecting p very difficult if only a sample of a relatively modest size _m_ is available. **296** **Chapter 12** **Maximum Entropy Models** **12.1.1** **Maximum Likelihood (ML) solution** One common solution adopted for selecting a distribution p is based on the _maxi-_ _mum likelihood principle_ . This consists of choosing a distribution out of the family _P_ that assigns the largest probability to the sample _S_ observed. Thus, using the fact that the sample is drawn i.i.d., the solution p ML selected by maximum likelihood is defined by _m_ \ufffd log p( _x_ _i_ ) _._ (12.1) _i_ =1 p ML = argmax p _\u2208P_ _m_ \ufffd p( _x_ _i_ ) = argmax _i_ =1 p _\u2208P_ _m_ \ufffd The maximum likelihood principle can be equivalently formulated in terms of the relative entropy. Let D [\ufffd] denote the empirical distribution corresponding to the sample _S_ . Then, p ML coincides with the distribution p with respect to which the empirical distribution D [\ufffd] admits the smallest relative entropy: p ML = argmin D(D [\ufffd] _\u2225_ p) _._ (12.2) p _\u2208P_ This can be seen straightforwardly from the following: D(D [\ufffd] _\u2225_ p) = \ufffd D\ufffd ( _x_ ) log \ufffdD( _x_ ) _\u2212_ \ufffd _x_ _x_ D\ufffd ( _x_ ) log p( _x_ ) _x_ = _\u2212_ H(D [\ufffd] ) _\u2212_ \ufffd _x_ _m_ \ufffd _i_ =1 [1] _[x]_ [=] _[x]_ _i_ log p( _x_ ) _m_ = _\u2212_ H(D [\ufffd] ) _\u2212_ = _\u2212_ H(D [\ufffd] ) _\u2212_ _m_ \ufffd _i_ =1 _m_ \ufffd _i_ =1 log p( _x_ _i_ ) _,_ _m_ \ufffd _x_ 1 _x_ = _x_ _i_ log p( _x_ ) _m_ since the first term of the last expression, the negative entropy of the empirical distribution, does not vary with p. As an example of application of the maximum likelihood principle, suppose we wish to estimate the bias _p_ 0 of a coin from an i.i.d. sample _S_ = ( _x_ 1 _, . . ., x_",
    "chunk_id": "foundations_machine_learning_289"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_m_ ) where _x_ _i_ _\u2208{_ h _,_ t _}_ with h denoting heads and t tails. _p_ 0 _\u2208_ [0 _,_ 1] is the probability of h according to the unknown distribution D. Let _P_ be the family of all distributions p = ( _p,_ 1 _\u2212_ _p_ ) where _p \u2208_ [0 _,_ 1] is an arbitrary possible bias value. Let _n_ h denote the number of occurrences of h in _S_ . Then, choosing p = ( _p_ \ufffd _S_ _,_ 1 _\u2212_ _p_ \ufffd _S_ ) = D [\ufffd] where _p_ \ufffd _S_ = _[n]_ _m_ [h] [leads to][ D][(][D][\ufffd] _[ \u2225]_ [p][) = 0, which, by (12.2), shows that][ p] [ML] [ =][ \ufffd][D][. Thus, the] maximum likelihood estimate _p_ ML of the bias is the empirical value _p_ ML = _[n]_ [h] (12.3) _m_ _[.]_ **12.2** **Density estimation problem augmented with features** **297** **12.1.2** **Maximum a Posteriori (MAP) solution** An alternative solution based on the so-called _Maximum a Posteriori_ solution con sists of selecting a distribution p _\u2208P_ that is the most likely, given the observed sample _S_ and a prior P[p] over the distributions p _\u2208P_ . By the Bayes rule, the problem can be formulated as follows: p MAP = argmax P[p _|S_ ] = argmax p _\u2208P_ p _\u2208P_ P[ _S|_ p] P[p] = argmax P[ _S|_ p] P[p] _._ (12.4) P[ _S_ ] p _\u2208P_ Notice that, for a uniform prior, P[p] is a constant and the Maximum a Posteriori solution then coincides with the Maximum Likelihood solution. The following is a standard example illustrating the MAP solution and its difference with the ML solution. **Example 12.1 (Application of the MAP solution)** Suppose we need to determine if a patient has a rare disease, given a laboratory test of that patient. We consider a set of two simple distributions: d (disease with probability one) and d [\u00af] (no disease with probability one), thus _P_ = _{_ d _,_ d [\u00af] _}_ . The laboratory test is either pos (positive) or neg (negative), thus _S \u2208{_ pos _,_ neg _}_ . Suppose that the disease is rare, say P[d] = _._ 005 and that the laboratory is relatively accurate: P[pos _|_ d] = _._ 98, and P[neg _|_ d [\u00af] ] = _._ 95. Then, if the test is positive, what should be the diagnosis? We can compute the right-hand side of (12.4) for both outcomes, given the positive test result, to determine the MAP estimate: P[pos _|_ d] P[d] = _._ 98 _\u00d7 ._ 005 = _._ 0049 P[pos _|_ d [\u00af] ] P[d [\u00af] ] = (1 _\u2212_ _._ 95) _\u00d7_ (1 _\u2212_ _._ 005) = _._ 04975 _> ._ 0049 _._ Thus, in this case, the MAP prediction is no disease: according to the MAP solution, with the values indicated, a patient with a positive test result is nonetheless more likely not to have the disease! We will not analyze the properties of the Maximum Likelihood and Maximum a Posteriori solutions here, which depend on the size of the sample",
    "chunk_id": "foundations_machine_learning_290"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "and the choice of the family _P_ . Instead, we will consider a richer density estimation problem where the learner has access to features, which is the learning problem addressed by Maximum Entropy (Maxent) models. **12.2** **Density estimation problem augmented with features** As with the standard density estimation problem, we consider a scenario where the learner receives a sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ) _\u2286_ X of size _m_ drawn i.i.d. according to some distribution D. But, here, additionally, we assume that the learner has access to a feature mapping **\u03a6** from X to R _[N]_ with _\u2225_ **\u03a6** _\u2225_ _\u221e_ _\u2264_ _r_ . In the most general case, **298** **Chapter 12** **Maximum Entropy Models** we may have _N_ = + _\u221e_ . We will denote by H a family of real-valued functions containing the component feature functions \u03a6 _j_ with _j \u2208_ [ _N_ ]. Different feature functions can be considered in practice. H may be the family of threshold functions **x** _\ufffd\u2192_ 1 _x_ _i_ _\u2264\u03b8_, **x** _\u2208_ R _[n]_, _\u03b8 \u2208_ R, defined over _n_ variables as for boosting stumps, or it may be a family of functions defined by more complex decision trees or regression trees. Other features often used in practice are monomials of degree _k_ based on the input variables. To simplify the presentation, in what follows, we will assume that the input set X is finite. **12.3** **Maxent principle** Maxent models are derived from a principle based on the key property that, with high probability, the empirical average of any feature is close to its true average. By the Rademacher complexity bound, for any _\u03b4 >_ 0, the following inequality holds with probability at least 1 _\u2212_ _\u03b4_ over the choice of a sample _S_ of size _m_ : E [ **\u03a6** ( _x_ )] log [2] _\u03b4_ (12.5) \ufffd\ufffd\ufffd _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ _x\u223c_ [E] D [\ufffd] \ufffd\ufffd\ufffd _\u221e_ _[\u2264]_ [2][R] _[m]_ [(][H][) +] _[ r]_ \ufffd 2 _m_ _[,]_ where we denote by D [\ufffd] the empirical distribution defined by the sample _S_ . This is the theoretical guarantee that guides the definition of the Maxent principle. Let p 0 be a distribution over X with p 0 ( _x_ ) _>_ 0 for all _x \u2208_ X, which is often chosen to be the uniform distribution. Then, the _Maxent principle_ consists of seeking a distribution p that is as agnostic as possible, that is as close as possible to the uniform distribution or, more generally, to a prior p 0, while verifying an inequality similar to (12.5): \ufffd\ufffd\ufffd _x_ E _\u223c_ p [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ _x\u223c_ [E] D [\ufffd] [ **\u03a6** ( _x_ )]\ufffd\ufffd\ufffd _\u221e_ _[\u2264]_ _[\u03bb,]_ (12.6) where _\u03bb \u2265_ 0 is a parameter. Here, closeness is measured using the relative entropy. Choosing _\u03bb_ = 0 corresponds to standard _Maxent_ or _unregularized Maxent_ and to requiring the expectation of the features with respect to p to precisely match the empirical averages. As we will see",
    "chunk_id": "foundations_machine_learning_291"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "later, its relaxation, that is the inequality case ( _\u03bb \u0338_ = 0), translates into a regularization. Notice that, unlike Maximum likelihood, the Maxent principle does not require specifying a family of probability distributions _P_ to choose from. \ufffd _\u03b4_ (12.5) 2 _m_ _[,]_ log [2] **12.4** **Maxent models** **299** **12.4** **Maxent models** Let \u2206denote the simplex of all distributions over X, then, the Maxent principle can be formulated as the following optimization problem: min (12.7) p _\u2208_ \u2206 [D][(][p] _[ \u2225]_ [p] [0] [)] subject to: \ufffd\ufffd\ufffd _x_ E _\u223c_ p [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ _x\u223c_ [E] D [\ufffd] [ **\u03a6** ( _x_ )]\ufffd\ufffd\ufffd _\u221e_ _[\u2264]_ _[\u03bb.]_ This defines a convex optimization problem since the relative entropy D is convex with respect to its arguments (appendix E), since the constraints are affine, and since \u2206is a convex set. The solution is in fact unique since the relative entropy is strictly convex. The empirical distribution is clearly a feasible point, thus problem (12.7) is feasible. For a uniform prior p 0, problem (12.7) can be equivalently formulated as an entropy maximization, which explains the name given to these models. Let H(p) = _\u2212_ [\ufffd] _x\u2208_ X [p][(] _[x]_ [) log][ p][(] _[x]_ [) denote the entropy of][ p][. Then, the objective function of (12.7)] can be rewritten as follows: _x_ \ufffd _\u2208_ X p( _x_ ) log p [p] 0 [(] ( _[x]_ _x_ [)] ) D(p _\u2225_ p 0 ) = \ufffd p 0 ( _x_ ) = _\u2212_ \ufffd \ufffd p( _x_ ) log p( _x_ ) _x\u2208_ X \ufffd p( _x_ ) log p 0 ( _x_ ) + \ufffd _x\u2208_ X _x\u2208_ X = log _|_ X _| \u2212_ H(p) _._ Thus, since log _|_ X _|_ is a constant, minimizing the relative entropy D(p _\u2225_ p 0 ) is then equivalent to maximizing H(p). Maxent models are the solutions of the optimization problem just described. As already discussed, they admit two important benefits: they are based on a fundamental theoretical guarantee of closeness of empirical and true feature averages, and they do not require specifying a particular family of distributions _P_ . In the next sections, we will further analyze the properties of Maxent models. **12.5** **Dual problem** Here, we derive an equivalent dual problem for (12.7) which, as we will show, can be formulated as a regularized maximum likelihood problem over the family of _Gibbs_ _distributions_ . For any convex set _K_, let _I_ _K_ denote the function defined by _I_ _K_ ( _x_ ) = 0 if _x \u2208_ _K_, _I_ _K_ ( _x_ ) = + _\u221e_ otherwise. Then, the Maxent optimization problem (12.7) can be equivalently expressed as the unconstrained optimization problem min p _F_ (p) with, **300** **Chapter 12** **Maximum Entropy Models** for all p _\u2208_ R [X], _F_ (p) = D [\ufffd] (p _\u2225_ p 0 ) + _I_ C (E (12.8) p [[] **[\u03a6]** [])] _[,]_ with D [\ufffd] (p _\u2225_ p 0 ) = D(p _\u2225_ p 0 ) if p is in the simplex \u2206,",
    "chunk_id": "foundations_machine_learning_292"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "D [\ufffd] (p _\u2225_ p 0 ) = + _\u221e_ otherwise, and with C _\u2286_ R _[N]_ the convex set defined by C = _{_ **u** : _\u2225_ **u** _\u2212_ E ( _x,y_ ) _\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x, y]_ [)]] _[ \u2225]_ _[\u221e]_ _[\u2264]_ _[\u03bb][}]_ [.] The general form of a Gibbs distribution p **w** with prior p 0, parameter **w**, and feature vector **\u03a6** is p **w** [ _x_ ] = [p] [0] [[] _[x]_ []] _[e]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ [)] (12.9) _Z_ ( **w** ) where _Z_ ( **w** ) = [\ufffd] _x\u2208_ X [p] [0] [[] _[x]_ []] _[e]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ [)] [ is a normalization factor also known as the] _[ par-]_ _tition function_ . Let _G_ be the function defined for all **w** _\u2208_ R _[N]_ by p 0 [ _x_ _i_ ] _G_ ( **w** ) = [1] _m_ _m_ \ufffd p **w** [ _x_ _i_ ] \ufffd _i_ =1 log \ufffd p 0 [ _x_ _i_ ] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 _._ (12.10) \ufffd Then, the following theorem shows the equivalence of the primal problem (12.7) or (12.8) and a dual problem based on _G_ . **Theorem 12.2 (Maxent duality)** _Problems_ (12.7) _or_ (12.8) _are equivalent to the opti-_ _mization problem_ sup **w** _\u2208_ R _N_ _G_ ( **w** ) _:_ sup _F_ (p) _._ (12.11) **w** _\u2208_ R _[N]_ _[ G]_ [(] **[w]** [) = min] p _Furthermore, let_ p _[\u2217]_ = argmin p _F_ (p) _and d_ _[\u2217]_ = sup **w** _\u2208_ R _N_ _G_ ( **w** ) _, then, for any \u03f5 >_ 0 _and any_ **w** _such that |G_ ( **w** ) _\u2212d_ _[\u2217]_ _| < \u03f5, the following inequality holds:_ D(p _[\u2217]_ _\u2225_ p **w** ) _\u2264_ _\u03f5._ Proof: The first part of the proof follows by application of the Fenchel duality theorem (theorem B.39) to the optimization problem (12.8) with the functions _f_, _g_, and _A_ defined for all p _\u2208_ R [X] and **u** _\u2208_ R _[N]_ by _f_ (p) = D [\ufffd] (p _\u2225_ p 0 ), _g_ ( **u** ) = _I_ C ( **u** ) and _A_ p = [\ufffd] _x\u2208_ X [p][(] _[x]_ [)] **[\u03a6]** [(] _[x]_ [).] _[ A]_ [ is a bounded linear map since for any][ p][, we have] _[ \u2225][A]_ [p] _[\u2225\u2264]_ _\u2225_ p _\u2225_ 1 sup _x_ _\u2225_ **\u03a6** ( _x_ ) _\u2225_ _\u221e_ _\u2264_ _r\u2225_ p _\u2225_ 1 . Also, notice that for all **w** _\u2208_ R _[N]_, _A_ _[\u2217]_ **w** = **w** _\u00b7_ **\u03a6** . Consider **u** 0 _\u2208_ R _[N]_ defined by **u** 0 = E _x\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x]_ [)] =] _[ A]_ [D][\ufffd] [. Since][ \ufffd][D][ is in \u2206= dom(] _[f]_ [),] **u** 0 is in _A_ (dom( _f_ )). Furthermore, since _\u03bb >_ 0, **u** 0 is in int(C). _g_ = _I_ C equals zero over int(C) and is therefore continuous over int(C), thus _g_ is continuous at **u** 0 and we have **u** 0 _\u2208_ _A_ (dom( _f_ )) _\u2229_ cont( _g_",
    "chunk_id": "foundations_machine_learning_293"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "). Thus, the assumptions of Theorem B.39 hold. By Lemma B.37, the conjugate of _f_ is the function _f_ _[\u2217]_ : R [X] _\u2192_ R defined by _f_ _[\u2217]_ (q) = log \ufffd\ufffd _x\u2208_ X [p] [0] [[] _[x]_ []] _[e]_ [q][[] _[x]_ []] [\ufffd] for all q _\u2208_ R [X] . The conjugate function of _g_ = _I_ C is **12.5** **Dual problem** **301** the function _g_ _[\u2217]_ defined for all **w** _\u2208_ R _[N]_ by _g_ _[\u2217]_ ( **w** ) = sup \ufffd **w** _\u00b7_ **u** _\u2212_ _I_ C ( **u** )\ufffd = sup( **w** _\u00b7_ **u** ) **u** **u** _\u2208_ C = sup ( **w** _\u00b7_ **u** ) _\u2225_ **u** _\u2212_ E D\ufffd [[] **[\u03a6]** []] _[\u2225]_ _[\u221e]_ _[\u2264][\u03bb]_ = **w** _\u00b7_ E D\ufffd [ **\u03a6** ] + _\u2225_ **u** sup _\u2225_ _\u221e_ _\u2264\u03bb_ ( **w** _\u00b7_ **u** ) = E D\ufffd [ **w** _\u00b7_ **\u03a6** ] + _\u03bb\u2225_ **w** _\u2225_ 1 _,_ where the last equality holds by definition of the dual norm. In view of these identities, we can write _\u2212f_ _[\u2217]_ ( _A_ _[\u2217]_ **w** ) _\u2212_ _g_ _[\u2217]_ ( _\u2212_ **w** ) = _\u2212_ log \ufffd [\ufffd] p 0 [ _x_ ] _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ [)] [\ufffd] + E \ufffd [ **w** _\u00b7_ **\u03a6** ] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 D _x\u2208_ X = _\u2212_ log _Z_ ( **w** ) + [1] _m_ _m_ \ufffd **w** _\u00b7_ **\u03a6** ( _x_ _i_ ) _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 _i_ =1 _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 _Z_ ( **w** ) = [1] _m_ = [1] _m_ _m_ \ufffd _i_ =1 log _[e]_ **[w]** _Z_ _[\u00b7]_ ( **[\u03a6]** **w** [(] _[x]_ ) _[i]_ [)] _m_ \ufffd p **w** [ _x_ _i_ ] \ufffd _i_ =1 log \ufffd p 0 [ _x_ _i_ ] _m_ \ufffd p 0 [ _x_ _i_ ] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 = _G_ ( **w** ) _,_ \ufffd which proves that sup **w** _\u2208_ R _N_ _G_ ( **w** ) = min p _F_ (p). Now, for any **w** _\u2208_ R _[N]_, we can write _G_ ( **w** ) _\u2212_ D(p _[\u2217]_ _\u2225_ p 0 ) + D(p _[\u2217]_ _\u2225_ p **w** ) + E \ufffd _x\u223c_ p _[\u2217]_ \ufffd log [p] _[\u2217]_ [[] _[x]_ []] \ufffd p 0 [ _x_ ] log [p] _[\u2217]_ [[] _[x]_ []] \ufffd p **w** [ _x_ ] = E _x\u223c_ D [\ufffd] \ufffdlog [p] p **[w]** 0 [ [[] _x_ _[x]_ ] []] \ufffd _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 _\u2212_ _x\u223c_ E p _[\u2217]_ = _\u2212\u03bb\u2225_ **w** _\u2225_ 1 + E log [p] **[w]** [[] _[x]_ []] _x\u223c_ D [\ufffd] \ufffd p 0 [ _x_ ] _\u2212_ E \ufffd _x\u223c_ p _[\u2217]_ log [p] **[w]** [(] _[x]_ [)] \ufffd p 0 ( _x_ ) \ufffd = _\u2212\u03bb\u2225_ **w** _\u2225_ 1 + E _x\u223c_ D [\ufffd] [ **w** _\u00b7_ **\u03a6** ( _x_ ) _\u2212_ log _Z_ ( **w** )] _\u2212_ _x\u223c_ E p _[\u2217]_ [[] **[w]** _[ \u00b7]_ **[ \u03a6]** [(] _[x]_ [)] _[ \u2212]_ [log] _[ Z]_ [(] **[w]** [)]] = _\u2212\u03bb\u2225_ **w**",
    "chunk_id": "foundations_machine_learning_294"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2225_ 1 + **w** _\u00b7_ \ufffd _x\u223c_ E D [\ufffd] [ **\u03a6** ( _x_ )] _\u2212_ _x\u223c_ E p _[\u2217]_ [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd _._ The solution of the primal optimization, p _[\u2217]_, verifies the constraint _I_ C (E p _[\u2217]_ [ **\u03a6** ]) = 0, that is _\u2225_ E _x\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ [E] _[x][\u223c]_ [p] _[\u2217]_ [[] **[\u03a6]** [(] _[x]_ [)]] _[\u2225]_ _[\u221e]_ _[\u2264]_ _[\u03bb]_ [. By H\u00a8older\u2019s inequality, this implies] the following inequality: _\u2212\u03bb\u2225_ **w** _\u2225_ 1 + **w** _\u00b7_ \ufffd _x\u223c_ E D [\ufffd] [ **\u03a6** ( _x_ )] _\u2212_ _x\u223c_ E p _[\u2217]_ [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd _\u2264\u2212\u03bb\u2225_ **w** _\u2225_ 1 + _\u03bb\u2225_ **w** _\u2225_ 1 = 0 _._ **302** **Chapter 12** **Maximum Entropy Models** Thus, we can write, for any **w** _\u2208_ R _[N]_, D(p _[\u2217]_ _\u2225_ p **w** ) _\u2264_ D(p _[\u2217]_ _\u2225_ p 0 ) _\u2212_ _G_ ( **w** ) _._ Now, assume that **w** verifies _|G_ ( **w** ) _\u2212_ sup **w** _\u2208_ R _N_ _G_ ( **w** ) _| \u2264_ _\u03f5_ for some _\u03f5 >_ 0. Then, D(p _[\u2217]_ _\u2225_ p 0 ) _\u2212_ _G_ ( **w** ) = (sup **w** _G_ ( **w** )) _\u2212_ _G_ ( **w** ) _\u2264_ _\u03f5_ implies D(p _[\u2217]_ _\u2225_ p **w** ) _\u2264_ _\u03f5_ . This concludes the proof of the theorem. In view of the theorem, if **w** is an _\u03f5_ -solution of the dual optimization problem, then D(p _[\u2217]_ _\u2225_ p **w** ) _\u2264_ _\u03f5_, which, by Pinsker\u2019s inequality (Proposition E.7) implies that p **w** is _\u221a_ 2 _\u03f5_ -close in _L_ 1 -norm to the optimal solution of the primal: _\u2225_ p _[\u2217]_ _\u2212_ p **w** _\u2225_ 1 _\u2264_ _\u221a_ 2 _\u03f5_ . Thus, the solution of our Maxent problem can be determined by solving the dual problem, which can be written equivalently as follows: inf **w** _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [1] _[ \u2212]_ _m_ [1] _m_ \ufffd log p **w** [ _x_ _i_ ] _._ (12.12) _i_ =1 Notice that the solution may not be achieved for any finite **w** for _\u03bb_ = 0, which is why the infimum is needed. This result may seem surprising since it shows that Maxent coincides with Maximum Likelihood ( _\u03bb_ = 0) or regularized Maximum Likelihood ( _\u03bb >_ 0) over a specific family _P_ of distributions, that of Gibbs distributions, while, as pointed out earlier, the Maxent principle does not explicitly specify any family _P_ . What can then explain that the solution of Maxent belongs to the specific family of Gibbs distributions? The reason is the specific choice of the relative entropy as the measure of closeness of p to the prior distribution p 0 . Other measures of closeness between distributions lead to different forms for the solution. Thus, in some sense, the choice of the measure of closeness is the (dual) counterpart of that of the family of distributions _P_ in maximum likelihood. Gibbs distributions form a very rich family. In particular, when X is a subset of",
    "chunk_id": "foundations_machine_learning_295"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "a vector space and the features \u03a6 _j_ ( **x** ) associated to **x** = ( _x_ 1 _, . . ., x_ _n_ ) _\u2208_ X are monomials of degree at most 2 based on the input variables _x_ _j_, that is _x_ _j_ _x_ _k_, _x_ _j_, or the constant _a \u2208_ R, then **w** _\u00b7_ **\u03a6** ( _x_ ) is a quadratic form as a function of the _x_ _j_ s. Thus, Gibbs distributions include the family of distributions defined by the normalized exponential of a quadratic form, which includes as a special case Gaussian distributions but also bi-modal distributions and normalized exponentials of non-positive definite quadratic forms. More complex multi-modal distributions can be further defined using higher-order monomials or more complex functions of the input variables. Figure 12.1 shows two examples of Gibbs distributions illustrating the richness of this family. **12.6** **Generalization bound** **303** **Image:** [No caption returned] **Image:** [No caption returned] (a) (b) **Figure 12.1** Examples of Gibbs distributions in R [2] . (a) Unimodal Gaussian distribution p[( _x_ 1 _, x_ 2 )] = _e_ _[\u2212]_ [(] _[x]_ 1 [2][+] _[x]_ [2] 2 [)] _e_ _[\u2212]_ [(] _[x]_ 1 [4][+] _[x]_ [4] 2 [)+] _[x]_ [2] 1 _[\u2212][x]_ [2] 2 _Z_ ; (b) Bimodal distribution p[( _x_ 1 _, x_ 2 )] = _Z_ . In each case, _Z_ is a normalization factor. **12.6** **Generalization bound** Let _L_ D ( **w** ) denote the log-loss of the distribution p **w** with respect to a distribution D, _L_ D ( **w** ) = E _x\u223c_ D [ _\u2212_ log p **w** [ _x_ ]], and similarly _L_ _S_ ( **w** ) its log-loss with respect to the empirical distribution defined by a sample _S_ . **Theorem 12.3** _Fix \u03b4 >_ 0 _. Let_ \ufffd **w** _be a solution of the optimization_ (12.12) _for \u03bb_ = 2R _m_ (H) + _r_ ~~\ufffd~~ log [2] 2R _m_ (H) + _r_ 2 _m_ _\u03b4_ _[. Then, with probability at least]_ [ 1] _[ \u2212]_ _[\u03b4][ over the draw of an i.i.d.]_ _sample S of size m from_ D _, the following inequality holds:_ log [2] _\u03b4_ 2 _m_ _._ \ufffd _L_ D ( **w** \ufffd ) _\u2264_ inf **w** _[L]_ [D] [(] **[w]** [) + 2] _[\u2225]_ **[w]** _[\u2225]_ [1] \ufffd 2R _m_ (H) + _r_ ~~\ufffd~~ Proof: Using the definition of _L_ D ( **w** ) and _L_ _S_ ( **w** ), H\u00a8older\u2019s inequality, and inequality (12.5), with probability at least 1 _\u2212_ _\u03b4_, the following holds: _L_ D ( **w** \ufffd ) _\u2212L_ _S_ ( **w** \ufffd ) = \ufffd **w** _\u00b7_ [ D E \ufffd [ **\u03a6** ] _\u2212_ D E [[] **[\u03a6]** []]] _[ \u2264\u2225]_ **[w]** [\ufffd] _[\u2225]_ [1] _[ \u2225]_ D [E] \ufffd [ **\u03a6** ] _\u2212_ D E [[] **[\u03a6]** []] _[\u2225]_ _[\u221e]_ _[\u2264]_ _[\u03bb][\u2225]_ **[w]** [\ufffd] _[\u2225]_ [1] _[.]_ Thus, since \ufffd **w** is a minimizer, we can write, for any **w**, \ufffd \ufffd \ufffd \ufffd _L_ D ( **w** ) _\u2212L_ D ( **w** ) = _L_ D ( **w** ) _\u2212L_",
    "chunk_id": "foundations_machine_learning_296"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_S_ ( **w** ) + _L_ _S_ ( **w** ) _\u2212L_ D ( **w** ) \ufffd \ufffd _\u2264_ _\u03bb\u2225_ **w** _\u2225_ 1 + _L_ _S_ ( **w** ) _\u2212L_ D ( **w** ) _\u2264_ _\u03bb\u2225_ **w** _\u2225_ 1 + _L_ _S_ ( **w** ) _\u2212L_ D ( **w** ) _\u2264_ 2 _\u03bb\u2225_ **w** _\u2225_ 1 _,_ **304** **Chapter 12** **Maximum Entropy Models** where we used for the last inequality the left inequality counterpart of inequality (12.5). This concludes the proof. Assume that **w** _[\u2217]_ achieves the infimum of the loss, that is _L_ D ( **w** _[\u2217]_ ) = inf **w** _L_ D ( **w** ) and that R _m_ (H) = (1 _/_ _[\u221a]_ _m_ ). Then, the theorem shows that, with high probability, the following inequality holds: \ufffd _\u2225_ **w** _\u2217_ _\u2225_ 1 _L_ D ( **w** ) _\u2264_ inf **w** _[L]_ [D] [(] **[w]** [) +] _[ O]_ \ufffd _\u221am_ **12.7** **Coordinate descent algorithm** _._ \ufffd The dual objective function in the optimization (12.12) is convex since the Lagrange _m_ dual is always concave (appendix B). Ignoring the constant term _\u2212_ _m_ [1] \ufffd _i_ =1 [log][ p] [0] [[] _[x]_ _[i]_ [],] the optimization problem (12.12) can be rewritten as inf **w** _J_ ( **w** ) with _J_ ( **w** ) = _\u03bb\u2225_ **w** _\u2225_ 1 _\u2212_ **w** _\u00b7_ E D\ufffd [ **\u03a6** ] + log p 0 [ _x_ ] _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ [)] \ufffd\ufffd _x\u2208_ X \ufffd _._ Note in particular that the function **w** _\ufffd\u2192_ log \ufffd\ufffd _x\u2208_ X [p] [0] [[] _[x]_ []] _[e]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ [)] [\ufffd] is convex as the conjugate function _f_ _[\u2217]_ of the function _f_ defined in the proof of Theorem 12.2. Different optimization techniques can be used to solve this convex optimization problem, including standard stochastic gradient descent and several special-purpose techniques. In this section, we will describe a solution based on coordinate descent which is particularly advantageous in presence of a very large number of features. Function _J_ is not differentiable but since it is convex, it admits a subdifferential at any point. The Maxent algorithm we describe consists of applying coordinate descent to the objective function (12.12). **Direction** Let **w** _t\u2212_ 1 denote the weight vector defined after ( _t \u2212_ 1) iterations. At each iteration _t \u2208_ [ _T_ ], the direction **e** _j_, _j \u2208_ [ _N_ ] considered by coordinate descent is _\u03b4J_ ( **w** _t\u2212_ 1 _,_ **e** _j_ ). If _w_ _t\u2212_ 1 _,j_ _\u0338_ = 0, then _J_ admits a directional derivative along **e** _j_ given by _J_ _[\u2032]_ ( **w** _t\u2212_ 1 _,_ **e** _j_ ) = _\u03bb_ sgn( _w_ _t\u2212_ 1 _,j_ ) + _\u03f5_ _t\u2212_ 1 _,j_ _._ where _\u03f5_ _t\u2212_ 1 _,j_ = E p **w** _t\u2212_ 1 [\u03a6 _j_ ] _\u2212_ E D \ufffd [[\u03a6] _[j]_ []. If] _[ w]_ _[t][\u2212]_ [1] _[,j]_ [ = 0,] _[ J]_ [ admits right and left directional] derivatives along **e** _j_ : _J_ + _[\u2032]_ [(] **[w]** _[t][\u2212]_ [1] _[,]_ **[ e]**",
    "chunk_id": "foundations_machine_learning_297"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[j]_ [) =] _[ \u03bb]_ [ +] _[ \u03f5]_ _[t][\u2212]_ [1] _[,j]_ _J_ _\u2212_ _[\u2032]_ [(] **[w]** _[t][\u2212]_ [1] _[,]_ **[ e]** _[j]_ [) =] _[ \u2212][\u03bb]_ [ +] _[ \u03f5]_ _[t][\u2212]_ [1] _[,j]_ _[.]_ **12.7** **Coordinate descent algorithm** **305** CDMaxent( _S_ = ( _x_ 1 _, . . ., x_ _m_ )) 1 **for** _t \u2190_ 1 **to** _T_ **do** 2 **for** _j \u2190_ 1 **to** _N_ **do** 3 **if** ( _w_ _t\u2212_ 1 _,j_ _\u0338_ = 0) **then** 4 _d_ _j_ _\u2190_ _\u03bb_ sgn( _w_ _t\u2212_ 1 _,j_ ) + _\u03f5_ _t\u2212_ 1 _,j_ 5 **elseif** _|\u03f5_ _t\u2212_ 1 _,j_ _| \u2264_ _\u03bb_ **then** 6 _d_ _j_ _\u2190_ 0 7 **else** _d_ _j_ _\u2190\u2212\u03bb_ sgn( _\u03f5_ _t\u2212_ 1 _,j_ ) + _\u03f5_ _t\u2212_ 1 _,j_ 8 _j \u2190_ argmax _|d_ _j_ _|_ _j\u2208_ [ _N_ ] 9 **if** ( _|w_ _t\u2212_ 1 _,j_ _r_ [2] _\u2212_ _\u03f5_ _t\u2212_ 1 _,j_ _| \u2264_ _\u03bb_ ) **then** 10 _\u03b7 \u2190\u2212w_ _t\u2212_ 1 _,j_ 11 **elseif** ( _w_ _t\u2212_ 1 _,j_ _r_ [2] _\u2212_ _\u03f5_ _t\u2212_ 1 _,j_ _> \u03bb_ ) **then** 12 _\u03b7 \u2190_ _r_ 1 [2] [ [] _[\u2212][\u03bb][ \u2212]_ _[\u03f5]_ _[t][\u2212]_ [1] _[,j]_ []] 13 **else** _\u03b7 \u2190_ _r_ 1 [2] [ [] _[\u03bb][ \u2212]_ _[\u03f5]_ _[t][\u2212]_ [1] _[,j]_ []] 14 **w** _t_ _\u2190_ **w** _t\u2212_ 1 + _\u03b7_ **e** _j_ p 0 [ _x_ ] _e_ **[w]** _[t][\u00b7]_ **[\u03a6]** [(] _[x]_ [)] 15 p **w** _t_ _\u2190_ \ufffd _x\u2208_ X [p] [0] [[] _[x]_ []] _[e]_ **[w]** _[t][\u00b7]_ **[\u03a6]** [(] _[x]_ [)] 16 **return** p **w** _t_ **Figure 12.2** Pseudocode of the Coordinate Descent Maxent algorithm. For all _j \u2208_ [ _N_ ], _\u03f5_ _t\u2212_ 1 _,j_ = E p **w** _t\u2212_ 1 [\u03a6 _j_ ] _\u2212_ E D \ufffd [[\u03a6] _[j]_ [].] Thus, in summary, we can define, for all _j \u2208_ [ _N_ ], _\u03bb_ sgn( _w_ _t\u2212_ 1 _,j_ ) + _\u03f5_ _t\u2212_ 1 _,j_ if ( _w_ _t\u2212_ 1 _,j_ _\u0338_ = 0) _\u03b4J_ ( **w** _t\u2212_ 1 _,_ **e** _j_ ) = \uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3 0 else if \ufffd\ufffd _\u03f5_ _t\u2212_ 1 _,j_ \ufffd\ufffd _\u2264_ _\u03bb_ _\u2212\u03bb_ sgn( _\u03f5_ _t\u2212_ 1 _,j_ ) + _\u03f5_ _t\u2212_ 1 _,j_ otherwise _._ The coordinate descent algorithm selects the direction **e** _j_ with the largest absolute value of _\u03b4J_ ( **w** _t\u2212_ 1 _,_ **e** _j_ ). **Step size** Given the direction **e** _j_, the optimal step value _\u03b7_ is given by argmin _\u03b7_ _J_ ( **w** _t\u2212_ 1 + _\u03b7_ **e** _j_ ). _\u03b7_ can be found via a line search or other numerical methods. A closed-form expression for the step can also be derived by minimizing an upper **306** **Chapter 12** **Maximum Entropy Models** bound on _J_ ( **w** _t\u2212_ 1 + _\u03b7_ **e** _j_ ). Notice that we can write _J_ ( **w** _t\u2212_ 1 + _\u03b7_ **e** _j_ ) _\u2212J_ ( **w** _t\u2212_ 1 ) = _\u03bb_ ( _|w_ _j_ + _\u03b7|\u2212|w_ _j_ _|_ ) _\u2212\u03b7_ E E [ _e_ _[\u03b7]_ [\u03a6] _[j]_ ] _._ (12.13) _S_ [[\u03a6] _[j]_ []+log] \ufffd p **w** _t\u2212_ 1",
    "chunk_id": "foundations_machine_learning_298"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd In view of \u03a6 _j_ _\u2208_ [ _\u2212r,_ + _r_ ], by Hoeffding\u2019s lemma, the following inequality holds: log E [ _e_ _[\u03b7]_ [\u03a6] _[j]_ ] _\u2264_ _\u03b7_ E [\u03a6 _j_ ] + _[\u03b7]_ [2] _[r]_ [2] _._ p **w** _t\u2212_ 1 p **w** _t\u2212_ 1 2 Combining this inequality with (12.13) and disregarding constant terms, minimizing the resulting upper bound on _J_ ( **w** _t\u2212_ 1 + _\u03b7_ **e** _j_ ) _\u2212_ _J_ ( **w** _t\u2212_ 1 ) becomes equivalent to minimizing _\u03d5_ ( _\u03b7_ ) defined for all _\u03b7 \u2208_ R by _\u03d5_ ( _\u03b7_ ) = _\u03bb|w_ _j_ + _\u03b7|_ + _\u03b7\u03f5_ _t\u2212_ 1 _,j_ + _[\u03b7]_ [2] _[r]_ [2] _._ 2 Let _\u03b7_ _[\u2217]_ denote the minimizer of _\u03d5_ ( _\u03b7_ ). If _w_ _t\u2212_ 1 _,j_ + _\u03b7_ _[\u2217]_ = 0, then the subdifferential of _|w_ _t\u2212_ 1 _,j_ + _\u03b7|_ at _\u03b7_ _[\u2217]_ is the set _{\u03bd_ : _\u03bd \u2208_ [ _\u2212_ 1 _,_ +1] _}_ . Thus, in that case, the subdifferential _\u2202\u03d5_ ( _\u03b7_ _[\u2217]_ ) contains 0 iff there exists _\u03bd \u2208_ [ _\u2212_ 1 _,_ +1] such that _\u03bb\u03bd_ + _\u03f5_ _t\u2212_ 1 _,j_ + _\u03b7_ _[\u2217]_ _r_ [2] = 0 _\u21d4_ _w_ _t\u2212_ 1 _,j_ _r_ [2] _\u2212_ _\u03f5_ _t\u2212_ 1 _,j_ = _\u03bb\u03bd._ The condition is therefore equivalent to _|w_ _t\u2212_ 1 _,j_ _r_ [2] _\u2212_ _\u03f5_ _t\u2212_ 1 _,j_ _| \u2264_ _\u03bb_ . If _w_ _t\u2212_ 1 _,j_ + _\u03b7_ _[\u2217]_ _>_ 0, then _\u03d5_ is differentiable at _\u03b7_ _[\u2217]_ and _\u03d5_ _[\u2032]_ ( _\u03b7_ _[\u2217]_ ) = 0, that is _\u03bb_ + _\u03f5_ _t\u2212_ 1 _,j_ + _\u03b7_ _[\u2217]_ _r_ [2] = 0 _\u21d4_ _\u03b7_ _[\u2217]_ = [1] _r_ [2] [ [] _[\u2212][\u03bb][ \u2212]_ _[\u03f5]_ _[t][\u2212]_ [1] _[,j]_ []] _[.]_ In view of that expression, the condition _w_ _t\u2212_ 1 _,j_ + _\u03b7_ _[\u2217]_ _>_ 0 is equivalent to _w_ _t\u2212_ 1 _,j_ _r_ [2] _\u2212_ _\u03f5_ _t\u2212_ 1 _,j_ _> \u03bb_ . Similarly, if _w_ _t\u2212_ 1 _,j_ + _\u03b7_ _[\u2217]_ _<_ 0, _\u03d5_ is differentiable at _\u03b7_ _[\u2217]_ and _\u03d5_ _[\u2032]_ ( _\u03b7_ _[\u2217]_ ) = 0, which gives _\u03b7_ _[\u2217]_ = [1] _r_ [2] [ [] _[\u03bb][ \u2212]_ _[\u03f5]_ _[t][\u2212]_ [1] _[,j]_ []] _[.]_ Figure 12.2 shows the pseudocode of the Coordinate Descent Maxent algorithm using the closed-form solution for the step size just presented. Note that we do not need to update distribution p **w** _t_ at every iteration of the algorithm (line 15) and we only need to be able to compute E p **w** _t_ [\u03a6 _j_ ] which defines _\u03f5_ _t,j_ . Various approximation strategies can be used to do this efficiently, including for instance rejection sampling techniques. **12.8** **Extensions** As already pointed out, the Gibbs distribution form of the Maxent models is tightly related to the choice of the divergence (relative entropy) used to measure closeness in the Maxent principle. For distributions, the relative entropy coincides with the **12.8** **Extensions** **307** unnormalized relative entropy, which is a Bregman divergence. Maxent models can be generalized by using an arbitrary Bregman",
    "chunk_id": "foundations_machine_learning_299"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "divergence B \u03a8 instead (appendix E), where \u03a8 is a convex function. Moreover, other norms _\u2225\u00b7 \u2225_ can be used to bound the difference of the empirical and true average feature vectors. This leads to the following general primal optimization problem for Maxent models: min (12.14) p _\u2208_ \u2206 [B] [\u03a8] [(][p] _[ \u2225]_ [p] [0] [)] subject to: \ufffd\ufffd\ufffd _x_ E _\u223c_ p [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ _x\u223c_ [E] D [\ufffd] [ **\u03a6** ( _x_ )]\ufffd\ufffd\ufffd _\u2264_ _\u03bb,_ which, as with (12.7), is a convex optimization problem since B \u03a8 is convex with respect to its first argument and in fact strictly convex if \u03a8 is strictly convex. The following general duality theorem gives the form of the dual problem equivalent to (12.14) in terms of the conjugate function \u03a8 _[\u2217]_ of \u03a8. Here, _\u2225\u00b7 \u2225_ is an arbitrary norm over R _[N]_ and _\u2225\u00b7 \u2225_ _\u2217_ its conjugate. We will assume here that sup _x_ _\u2225_ **\u03a6** ( _x_ ) _\u2225\u2264_ _r_ . **Theorem 12.4** _Let_ \u03a8 _be a convex function defined over_ R [X] _. Then, problem_ (12.14) _admits the following equivalent dual:_ min p _\u2208_ \u2206 [B] [\u03a8] [(][p] _[ \u2225]_ [p] [0] [)] _subject to:_ \ufffd\ufffd\ufffd _x_ E _\u223c_ p [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ _x\u223c_ [E] D [\ufffd] [ **\u03a6** ( _x_ )]\ufffd\ufffd\ufffd _\u2264_ _\u03bb_ = sup **w** _\u00b7_ **\u03a6** + _\u2207_ \u03a8(p 0 )\ufffd + **w** _\u00b7_ E [ **\u03a6** ( _x_ )] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ _\u2217_ _\u2212_ _C_ (p 0 ) _,_ **w** _\u2208_ R _[N]_ _[ \u2212]_ [\u03a8] _[\u2217]_ [\ufffd] _x\u223c_ D [\ufffd] _where C_ (p 0 ) = \u03a8(p 0 ) _\u2212\u27e8\u2207_ \u03a8(p 0 ) _,_ p 0 _\u27e9._ Proof: The proof is similar to that of Theorem 12.2 and follows by application of the Fenchel duality theorem (Theorem B.39) to the following optimization problem: min _f_ (p) + _g_ ( _A_ p) _,_ (12.15) p with the functions _f_, _g_, and _A_ defined for all p _\u2208_ R [X] and **u** _\u2208_ R _[N]_ by _f_ ( _p_ ) = B \u03a8 (p _\u2225_ p 0 ) + _I_ \u2206 ( _p_ ), _g_ ( **u** ) = _I_ C ( **u** ), and _A_ p = [\ufffd] _x\u2208_ X [p][(] _[x]_ [)] **[\u03a6]** [(] _[x]_ [). Given these defini-] tions, problem (12.15) is equivalent to (12.14). _A_ is a bounded linear map since for any p, we have _\u2225A_ p _\u2225\u2264\u2225_ p _\u2225_ 1 sup _x_ _\u2225_ **\u03a6** ( _x_ ) _\u2225\u2264_ _r\u2225_ p _\u2225_ 1 . Also, notice that for all **w** _\u2208_ R _[N]_, _A_ _[\u2217]_ **w** = **w** _\u00b7_ **\u03a6** . Consider **u** 0 _\u2208_ R _[N]_ defined by **u** 0 = E _x\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x]_ [)] =] _[ A]_ [D][\ufffd] [. Since][ \ufffd][D][ is in \u2206= dom(] _[f]_ [),] **u** 0 is in _A_ (dom( _f_ )). Furthermore, since _\u03bb >_ 0, **u** 0 is in int(C). _g_ = _I_ C equals zero over int(C) and is therefore continuous over int(C), thus _g_ is continuous at **u**",
    "chunk_id": "foundations_machine_learning_300"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "0 and we have **u** 0 _\u2208_ _A_ (dom( _f_ )) _\u2229_ cont( _g_ ). Thus, the assumptions of Theorem B.39 hold. **308** **Chapter 12** **Maximum Entropy Models** The conjugate function of _f_ is defined for all q _\u2208_ R [X] by _f_ _[\u2217]_ (q) = sup p _[\u27e8]_ [p] _[,]_ [ q] _[\u27e9\u2212]_ [B] [\u03a8] [(][p] _[ \u2225]_ [p] [0] [)] _[ \u2212]_ _[I]_ [\u2206] [(][p][)] = sup _\u27e8_ p _,_ q _\u27e9\u2212_ B \u03a8 (p _\u2225_ p 0 ) p _\u2208_ \u2206 = sup _\u27e8_ p _,_ q _\u27e9\u2212_ \u03a8(p) + \u03a8(p 0 ) + _\u27e8\u2207_ \u03a8(p 0 ) _,_ p _\u2212_ p 0 _\u27e9_ p _\u2208_ \u2206 = sup _\u27e8_ p _,_ q + _\u2207_ \u03a8(p 0 ) _\u27e9\u2212_ \u03a8(p) + \u03a8(p 0 ) _\u2212\u27e8\u2207_ \u03a8(p 0 ) _,_ p 0 _\u27e9_ p _\u2208_ \u2206 = \u03a8 _[\u2217]_ (q + _\u2207_ \u03a8(p 0 )) + \u03a8(p 0 ) _\u2212\u27e8\u2207_ \u03a8(p 0 ) _,_ p 0 _\u27e9._ The conjugate function of _g_ = _I_ C is defined for all **w** _\u2208_ R _[N]_ by _g_ _[\u2217]_ ( **w** ) = sup **u** _[\u27e8]_ **[w]** _[,]_ **[ u]** _[\u27e9\u2212]_ _[I]_ [C] [(] **[u]** [)] = sup _\u27e8_ **w** _,_ **u** _\u27e9_ **u** _\u2208_ C = sup _\u27e8_ **w** _,_ **u** _\u27e9_ _\u2225_ **u** _\u2212_ E D\ufffd [[] **[\u03a6]** []] _[\u2225\u2264][\u03bb]_ = _\u27e8_ **w** _,_ E D\ufffd [ **\u03a6** ] _\u27e9_ + sup _\u2225_ **u** _\u2225\u2264\u03bb_ _\u27e8_ **w** _,_ **u** _\u27e9_ = _\u27e8_ **w** _,_ E D\ufffd [ **\u03a6** ] _\u27e9_ + _\u03bb\u2225_ **w** _\u2225_ _\u2217_ _,_ where the last equality holds by definition of the dual norm. In view of these identities, by Theorem B.39, we have min _f_ (p) + _g_ ( _A_ p) = sup p **w** _\u2208_ R _[N]_ _[ \u2212][f]_ _[ \u2217]_ [(] _[A]_ _[\u2217]_ **[w]** [)] _[ \u2212]_ _[g]_ _[\u2217]_ [(] **[w]** [)] = sup **w** _\u2208_ R _[N]_ _[ \u2212]_ [\u03a8] _[\u2217]_ [(] **[w]** _[ \u00b7]_ **[ \u03a6]** [ +] _[ \u2207]_ [\u03a8(][p] [0] [)) +] **[ w]** _[ \u00b7]_ [ E] D\ufffd [ **\u03a6** ] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ _\u2217_ _\u2212_ \u03a8(p 0 ) + _\u27e8\u2207_ \u03a8(p 0 ) _,_ p 0 _\u27e9,_ which completes the proof. Note, the previous proof and its use of Fenchel duality holds even when considering norms that are not inner product norms and, more generally, Banach spaces are considered (as mentioned in section B.4). Much of the analysis and theoretical guarantees presented in previous sections in the special case of the unnormalized relative entropy straightforwardly extend to a broad family of Bregman divergences. **12.9** _**L**_ **2** **-regularization** In this section, we study a common variant of the Maxent algorithm where a regularization based on the norm-2 squared of the weight vector **w** is used. Observe that this is not covered by the general framework discussed in the previous section **12.9** _L_ 2 **-regularization** **309** where the regularization was based on some norm of **w** . The corresponding (dual) optimization problem is the following: min 2 _[\u2212]_ [1] **w** _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [2] _m_ _m_",
    "chunk_id": "foundations_machine_learning_301"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd log p **w** [ _x_ _i_ ] _._ (12.16) _i_ =1 Let _L_ D ( **w** ) denote the log-loss of the distribution p **w** with respect to a distribution D, _L_ D ( **w** ) = E _x\u223c_ D [ _\u2212_ log p **w** [ _x_ ]], and similarly _L_ _S_ ( **w** ) its log-loss with respect to the empirical distribution defined by a sample _S_ . Then, the algorithm admits the following guarantee. **Theorem 12.5** _Let_ \ufffd **w** _be a solution of the optimization problem_ (12.16) _. Then, for_ _any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the draw of an i.i.d. sample S of size_ _m from_ D _, the following inequality holds:_ \ufffd _L_ D ( **w** ) _\u2264_ inf **w** _[L]_ [D] [(] **[w]** [) +] _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ 2 [2] [+] _\u03bbm_ _[r]_ [2] 1 + \ufffd ~~\ufffd~~ log [1] _\u03b4_ 2 _._ \ufffd Proof: Let _D_ [\ufffd] denote the empirical distribution defined by the sample _S_ . Then, the optimization problem (12.16) can be formulated as follows: min 2 _[\u2212]_ [E] **w** _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [2] _x\u223c_ D [\ufffd] \ufffd log p **w** [ _x_ ]\ufffd = _\u03bb\u2225_ **w** _\u2225_ 2 [2] _[\u2212]_ **[w]** _[ \u00b7]_ [ E] [ **\u03a6** ( _x_ )] + log _Z_ ( **w** ) _,_ _x\u223c_ D [\ufffd] where _Z_ ( **w** ) = \ufffd\ufffd _x_ [exp(] **[w]** _[ \u00b7]_ **[ \u03a6]** [(] _[x]_ [))] \ufffd. Similarly, let **w** D denote the solution of the minimization problem with the distribution D: **w** min _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ 2 [2] _[\u2212]_ _x\u223c_ [E] D \ufffd log p **w** [ _x_ ]\ufffd = _\u03bb\u2225_ **w** _\u2225_ 2 [2] _[\u2212]_ **[w]** _[ \u00b7]_ [ E] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)] + log] _[ Z]_ [(] **[w]** [)] _[.]_ We first give an upper-bound on _L_ D ( **w** \ufffd ) valid for all **w** _\u2208_ R _[N]_, starting with a decom \ufffd \ufffd \ufffd position of _L_ D ( **w** ) as a sum of terms, next using the expression of _L_ D ( **w** ) _\u2212L_ _S_ ( **w** ) in terms of average feature values, then the optimality of \ufffd **w**, next the expression of _L_ _S_ ( **w** D ) _\u2212L_ D ( **w** D ) in terms of average feature values, and finally the CauchySchwarz inequality and the optimality of **w** D : \ufffd _L_ D ( **w** ) \ufffd \ufffd \ufffd \ufffd = _L_ D ( **w** ) _\u2212L_ _S_ ( **w** ) + _L_ _S_ ( **w** ) _\u2212L_ D ( **w** D ) + _L_ D ( **w** D ) + _\u03bb\u2225_ **w** _\u2225_ 2 [2] _[\u2212]_ _[\u03bb][\u2225]_ **[w]** [\ufffd] _[\u2225]_ [2] 2 \ufffd \ufffd = \ufffd **w** _\u00b7_ E [ **\u03a6** ( _x_ )] _\u2212_ E + _L_ _S_ ( **w** ) _\u2212L_ D ( **w** D ) + _L_ D ( **w** D ) + _\u03bb\u2225_ **w** _\u2225_ 2 [2] _[\u2212]_ _[\u03bb][\u2225]_ **[w]** [\ufffd] _[\u2225]_ [2] 2",
    "chunk_id": "foundations_machine_learning_302"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd _x\u223c_ D [\ufffd] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd \ufffd _\u2264_ **w** _\u00b7_ E [ **\u03a6** ( _x_ )] _\u2212_ E + _L_ _S_ ( **w** D ) _\u2212L_ D ( **w** D ) + _L_ D ( **w** D ) + _\u03bb\u2225_ **w** D _\u2225_ 2 [2] _[\u2212]_ _[\u03bb][\u2225]_ **[w]** [\ufffd] _[\u2225]_ [2] 2 \ufffd _x\u223c_ D [\ufffd] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd \ufffd _\u2264_ [ **w** _\u2212_ **w** D ] _\u00b7_ E [ **\u03a6** ( _x_ )] _\u2212_ E + _L_ D ( **w** D ) + _\u03bb\u2225_ **w** D _\u2225_ 2 [2] _[\u2212]_ _[\u03bb][\u2225]_ **[w]** [\ufffd] _[\u2225]_ [2] 2 \ufffd _x\u223c_ D [\ufffd] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd \ufffd _\u2264\u2225_ **w** _\u2212_ **w** D _\u2225_ 2 E [ **\u03a6** ( _x_ )] _\u2212_ E 2 _[.]_ \ufffd\ufffd\ufffd _x\u223c_ D [\ufffd] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd\ufffd\ufffd 2 [+] _[ L]_ [D] [(] **[w]** [) +] _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [2] **310** **Chapter 12** **Maximum Entropy Models** \ufffd Next, we bound _\u2225_ **w** _\u2212_ **w** D _\u2225_ 2 using the fact that \ufffd **w** and **w** D are solutions of the minimization of convex and differentiable objectives functions, whose gradients must be zero at the minimizing values: 2 _\u03bb_ **w** \ufffd _\u2212_ E [ **\u03a6** ( _x_ )] + _\u2207_ log _Z_ ( **w** \ufffd ) = 0 _x\u223c_ D [\ufffd] 2 _\u03bb_ **w** D _\u2212_ E _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)] +] _[ \u2207]_ [log] _[ Z]_ [(] **[w]** [D] [) = 0] _[,]_ which implies 2 _\u03bb_ ( **w** \ufffd _\u2212_ **w** D ) = E [ **\u03a6** ( _x_ )] _\u2212_ E _x\u223c_ D [\ufffd] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)] +] _[ \u2207]_ [log] _[ Z]_ [(] **[w]** [D] [)] _[ \u2212\u2207]_ [log] _[ Z]_ [(] **[w]** [\ufffd] [)] _[.]_ \ufffd Multiplying both sides by ( **w** _\u2212_ **w** D ) gives 2 _\u03bb\u2225_ **w** \ufffd _\u2212_ **w** D _\u2225_ 2 [2] \ufffd \ufffd \ufffd = E [ **\u03a6** ( _x_ )] _\u2212_ E _\u00b7_ [ **w** _\u2212_ **w** D ] _\u2212_ [ _\u2207_ log _Z_ ( **w** ) _\u2212\u2207_ log _Z_ ( **w** D )] _\u00b7_ [ **w** _\u2212_ **w** D ] \ufffd _x\u223c_ D [\ufffd] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd \ufffd _\u2264_ E [ **\u03a6** ( _x_ )] _\u2212_ E _\u00b7_ [ **w** _\u2212_ **w** D ] _,_ \ufffd _x\u223c_ D [\ufffd] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd in view of the convexity of **w** _\ufffd\u2192_ log _Z_ ( **w** ). Using the Cauchy-Schwarz inequality and simplifying, we obtain \ufffd _\u2225_ **w** _\u2212_ **w** D _\u2225_ 2 _\u2264_ \ufffd\ufffd\ufffd E _x\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ [E] _[x][\u223c]_ [D] [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd\ufffd\ufffd 2 2 _\u03bb_ _._ \ufffd Plugging this back in the upper bound previously derived for _L_ D ( **w** ) yields \ufffd _L_ D ( **w** ) _\u2264_ 2 \ufffd\ufffd\ufffd E _x\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ [E] _[x][\u223c]_ [D] [[] **[\u03a6]** [(] _[x]_",
    "chunk_id": "foundations_machine_learning_303"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[)]] \ufffd\ufffd\ufffd 2 + _L_ D ( **w** ) + _\u03bb\u2225_ **w** _\u2225_ 2 [2] _[.]_ 2 _\u03bb_ We now use McDiarmid\u2019s inequality to bound E _x\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ [E] _[x][\u223c]_ [D] [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd 2 [. Let] \u03a8( _S_ ) denote this quantity for a sample _S_ . Let _S_ _[\u2032]_ be a sample differing from _S_ by one point, say _x_ _m_ for _S_, _x_ _[\u2032]_ _m_ [for] _[ S]_ _[\u2032]_ [. Then, by the triangle inequality,] _|_ \u03a8( _S_ _[\u2032]_ ) _\u2212_ \u03a8( _S_ ) _|_ = E E [ **\u03a6** ( _x_ )] _\u2212_ E \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd _x\u223c_ D [\ufffd] _[\u2032]_ [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ _x\u223c_ [E] D [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd\ufffd\ufffd 2 _[\u2212]_ \ufffd\ufffd\ufffd _x\u223c_ D [\ufffd] _x\u223c_ D [[] **[\u03a6]** [(] _[x]_ [)]] \ufffd\ufffd\ufffd 2 _\u2264_ E [ **\u03a6** ( _x_ )] \ufffd\ufffd\ufffd _x\u223c_ D [\ufffd] _[\u2032]_ [[] **[\u03a6]** [(] _[x]_ [)]] _[ \u2212]_ _x\u223c_ [E] D [\ufffd] \ufffd\ufffd\ufffd 2 \ufffd\ufffd\ufffd\ufffd **\u03a6** ( _x_ _\u2032m_ [)] _[ \u2212]_ **[\u03a6]** [(] _[x]_ _[m]_ [)] _\u2264_ \ufffd\ufffd\ufffd _m_ \ufffd\ufffd\ufffd 2 _[\u2264]_ [2] _m_ _[r]_ _[.]_ Thus, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, the following inequality holds \u03a8( _S_ ) _\u2264_ E _S\u223c_ D _[m]_ [[\u03a8(] _[S]_ [)] + 2] _[r]_ ~~\ufffd~~ log [1] _\u03b4_ 2 _m_ _[.]_ **12.9** _L_ 2 **-regularization** **311** For any _i \u2208_ [ _m_ ], let **Z** _i_ denote the random variable E _x\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x]_ _[i]_ [)]] _[ \u2212]_ [E] _[x][\u223c]_ [D] [[] **[\u03a6]** [(] _[x]_ [)].] Then, by Jensen\u2019s inequality, E _S\u223c_ D _[m]_ [\u03a8( _S_ )] can be upper-bounded as follows: _\u0338_ _\u2264_ \ufffd _\u0338_ ~~\ufffd~~ \ufffd 1 \ufffdE \ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u0338_ 2 _\u0338_ _._ \ufffd _\u0338_ _m_ \ufffd _\u0338_ 2 \ufffd **Z** _i_ \ufffd\ufffd\ufffd 2 _i_ =1 _\u0338_ 1 E _S\u223c_ D _[m]_ [[\u03a8(] _[S]_ [)] =][ E] \ufffd\ufffd\ufffd\ufffd _m_ _\u0338_ _m_ \ufffd **Z** _i_ \ufffd\ufffd\ufffd 2 _i_ =1 _\u0338_ _m_ \ufffd _\u0338_ Since the random variables **Z** _i_ s are i.i.d. and centered (E[ **Z** _i_ ] = 0), we have _\u0338_ _m_ \ufffd \ufffd _\u0338_ \ufffd E \ufffd _\u2225_ **Z** _i_ _\u2225_ [2] [\ufffd] + \ufffd _i_ =1 _i_ = _\u0338_ \ufffd E[ **Z** _i_ ] _\u00b7_ E[ **Z** _j_ ] _i_ = _\u0338_ _j_ \ufffd 1 E \ufffd\ufffd\ufffd\ufffd _m_ _\u0338_ _m_ 2 [\ufffd] 1 \ufffd **Z** _i_ \ufffd\ufffd\ufffd = _m_ [2] _i_ =1 _\u0338_ _\u0338_ = [E] \ufffd _\u2225_ **Z** 1 _\u2225_ [2] [\ufffd] _m_ = [E] \ufffd _\u2225_ **Z** 1 _\u2225_ [2] + _\u2225_ **Z** 2 _\u2225_ [2] [\ufffd] 2 _m_ = [E] \ufffd _\u2225_ **Z** 1 2 _\u2212m_ **Z** 2 _\u2225_ [2] [\ufffd] _._ where, for the last equality, we used the fact that E[ **Z** 1 _\u00b7_ **Z** 2 ] = E[ **Z** 1 ] _\u00b7_ E[ **Z** 2 ] = 0. This shows that E[\u03a8( _S_ )] _\u2264_ ~~_\u221a_~~ 22 _rm_ [and that, with probability at least 1] _[ \u2212]_ _[\u03b4]_ [, the following] holds _\u0338_ \ufffd _\u0338_ 2",
    "chunk_id": "foundations_machine_learning_304"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_r_ \u03a8( _S_ ) _\u2264_ _\u221a_ 2 _m_ _\u0338_ 2 _r_ \u03a8( _S_ ) _\u2264_ _\u221a_ 2 _\u0338_ 1 + \ufffd _\u0338_ log [1] _\u03b4_ _\u0338_ log [1] _\u0338_ _,_ \ufffd _\u0338_ and therefore also _\u0338_ 2 + _L_ D ( **w** ) + _\u03bb\u2225_ **w** _\u2225_ 2 [2] \ufffd _\u0338_ [1] 2 _r_ [2] 2 _\u03bb_ _m_ _\u0338_ \ufffd _L_ D ( **w** ) _\u2264_ [1] _\u0338_ _m_ _\u0338_ log [1] _\u0338_ 1 + \ufffd _\u0338_ \ufffd _\u0338_ log [1] _\u03b4_ _\u0338_ _\u2264_ _[r]_ [2] _\u03bbm_ _\u0338_ 1 + \ufffd _\u0338_ ~~\ufffd~~ _\u0338_ log [1] _\u0338_ _\u03b4_ _\u0338_ 2 + _L_ D ( **w** ) + _\u03bb\u2225_ **w** _\u2225_ 2 [2] _[,]_ \ufffd _\u0338_ which ends the proof. Assume that **w** _[\u2217]_ achieves the infimum of the loss, that is _L_ D ( **w** _[\u2217]_ ) = inf **w** _L_ D ( **w** ) and that we are given an upper bound \u039b 2 on its norm: _\u2225_ **w** _[\u2217]_ _\u2225_ 2 _\u2264_ \u039b 2 . Then, we can use that upper bound and choose _\u03bb_ to minimize the two terms containing _\u03bb_ : _\u03bb_ \u039b [2] 2 [=] _\u03bbmr_ [2] [, that is] _[ \u03bb]_ [ =] \u039b 2 ~~_\u221a_~~ _r_ _m_ and the theorem would then guarantee the following inequality with probability 1 _\u2212_ _\u03b4_ for \ufffd **w** : _\u0338_ 2 [\ufffd] _._ \ufffd _\u0338_ _L_ D ( **w** \ufffd ) _\u2264_ inf **w** _[L]_ [D] [(] **[w]** [) +] _\u221a_ _[r]_ [\u039b] _m_ [2] _\u0338_ 1 + 1 + \ufffd \ufffd _\u0338_ \ufffd _\u0338_ log [1] _\u03b4_ _\u0338_ log [1] _\u0338_ _._ **312** **Chapter 12** **Maximum Entropy Models** **12.10** **Chapter notes** The Maxent principle was first explicitly advocated by Jaynes [1957] (see also Jaynes [1983]) who referred to Shannon\u2019s notion of entropy (appendix E) to support this principle. As seen in Section 12.5, standard Maxent models coincide with Gibbs distributions, as in the original Boltzmann models in statistical mechanics. In fact, Jaynes [1957] argued that statistical mechanics could be viewed as a form of statistical inference, as opposed to a physical theory, and that the thermodynamic notion of entropy could be replaced by the information-theoretical notion. The justification of the Maxent principle presented in this chapter is instead based upon learning theory arguments. Maximum entropy models, commonly referred to as Maxent models, are used in a variety of tasks in natural language processing [Berger et al., 1996, Rosenfeld, 1996, Pietra et al., 1997, Malouf, 2002, Manning and Klein, 2003, Ratnaparkhi, 2010] and in many other applications, including species habitat modeling [Phillips et al., 2004, 2006, Dud\u00b4\u0131k et al., 2007, Elith et al., 2011]. One key benefit of Maxent models is that they allow the use of diverse features that can be selected and augmented by the user. The richness of the features used in many tasks as well as small sample sizes have motivated the use of regularized Maxent models where the _L_ 1 -norm [Kazama and Tsujii, 2003] or the _L_ 2 -norm [Chen and Rosenfeld, 2000, Lebanon and Lafferty, 2001] of the parameter vector defining the",
    "chunk_id": "foundations_machine_learning_305"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Gibbs distribution is controlled. This can be shown to be equivalent to the introduction of a Laplacian or Gaussian prior over the parameter vectors in a Bayesian interpretation [Williams, 1994, Goodman, 2004], thereby making Maxent models coincide with Maximum a Posteriori solutions with specific choices of the prior. An extensive theoretical study of these regularizations and the introduction of other more general ones were given by Dud\u00b4\u0131k, Phillips, and Schapire [2007] and by Altun and Smola [2006] who studied the extensions to arbitrary Bregman divergences and norms (Section 12.8) using Fenchel duality (see also [Lafferty, Pietra, and Pietra, 1997]). Cortes, Kuznetsov, Mohri, and Syed [2015] give a more general family of density estimation models, _Structural Maxent models_, with feature functions selected from a union of possibly very complex sub-families for which they also give a duality theorem, strong learning guarantees, and algorithms. These models can also be viewed as Maxent with a more general type of regularization. The Maxent duality theorem is due to Pietra, Pietra, and Lafferty [1997] (see also [Dud\u00b4\u0131k et al., 2007] and [Altun and Smola, 2006]). Theorem 12.2 is a slight extension giving a guarantee for an _\u03f5_ -solution of the dual and is a special instance of a more general theorem given for Structural Maxent models [Cortes et al., 2015]. The generalization bounds of Sections 12.6 and 12.9 and their proofs are variants **12.11** **Exercises** **313** of results due to Dud\u00b4\u0131k et al. [2007]. The stability analysis used in the proof of Theorem 12.5 is equivalent to the one described in Chapter 14 using Bregman divergences. A variety of different techniques have been suggested to solve the Maxent optimization problem including standard gradient descent and stochastic gradient descent. Some specific algorithms were introduced for this problem, including _gen-_ _eralized iterative scaling_ (GIS) [Darroch and Ratcliff, 1972] and _improved iterative_ _scaling_ (IIS) [Pietra et al., 1997]. It was shown by Malouf [2002] that these algorithms perform poorly in several natural language processing tasks in comparison with conjugate gradient techniques and limited-memory BFGS methods (see also [Andrew and Gao, 2007]). The coordinate descent solution presented in this chapter is due to Cortes et al. [2015]. It is a simpler version of an algorithm of Dud\u00b4\u0131k et al. [2007] which uses a tighter upper bound on _J_ ( **w** _t\u2212_ 1 + _\u03b7_ **e** _j_ ) but which is subject to various technical conditions. Both algorithms benefit from a similar asymptotic convergence rate [Cortes et al., 2015] and are particularly adapted to cases where the number of features is very large and where updating all feature weights is impractical. A sequential greedy approximation due to Zhang [2003b] is also advocated by Altun and Smola [2006] as a general algorithm for general forms of the Maxent problem. **12.11** **Exercises** 12.1 Convexity. Prove directly that the function **w** _\ufffd\u2192_ log _Z_ ( **w** ) = log( [\ufffd] _x\u2208_ X _[e]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ [)] [)] is convex ( _Hint_ : compute its Hessian). 12.2 Lagrange duality. Derive the dual problem of the Maxent problem and justify it carefully in the",
    "chunk_id": "foundations_machine_learning_306"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "case of the stricter constraint of positivity for the distribution p: p( _x_ ) _>_ 0 for all _x \u2208_ X. 12.3 Dual of norm-2 squared regularized Maxent. Derive the dual formulation of the norm-2 squared regularized Maxent optimization shown in equation (12.16). 12.4 Extension to Bregman divergences. Derive theoretical guarantees for the extensions discussed in Section 12.8. What additional property is needed for the Bregman divergence so that your learning guarantees hold? 12.5 _L_ 2 -regularization. Let **w** be the solution of Maxent with a norm-2 squared regularization. **314** **Chapter 12** **Maximum Entropy Models** (a) Prove the following inequality: _\u2225_ **w** _\u2225_ 2 _\u2264_ [2] _\u03bb_ _[r]_ [(] _[Hint]_ [: you could compare the] values of the objective function at **w** and 0.). Generalize this result to other _\u2225\u00b7 \u2225_ _[p]_ _p_ [-regularizations with] _[ p >]_ [ 1.] (b) Use the previous question to derive an explicit learning guarantee for Maxent with norm-2 squared regularization ( _Hint_ : you could use the last inequality given in Section 12.9 and derive an explicit expression for \u039b 2 ). # 13 Conditional Maximum Entropy Models This chapter presents algorithms for estimating the conditional probability of a class given an example, rather than only predicting the class label for that example. This is motivated by several applications where confidence values are sought, in addition to the class prediction. The algorithms discussed, _conditional Maxent_ _models_, also known as _multinomial logistic regression_ algorithms, are among the most well-known and most widely used multi-class classification algorithms. In the special case of two classes, the algorithm is known as _logistic regression_ . As suggested by their name, these algorithms can be viewed as Maxent models for conditional probabilities. To introduce them, we will extend the ideas discussed in the previous chapter (Chapter 12), starting from an extension of the Maxent principle to the conditional case. Next, we will prove a duality theorem leading to an equivalent dual optimization problem for conditional Maxent. We will specifically discuss different aspects of multi-class classification using conditional Maxent and reserve a special section to the analysis of logistic regression. **13.1** **Learning problem** We consider a multi-class classification problem with _c_ classes, _c \u2265_ 1. Let Y = _{_ 1 _, . . ., c}_ denote the output space and D a distribution over X _\u00d7_ Y. The learner receives a labeled training sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) _\u2208_ (X _\u00d7_ Y) _[m]_ drawn i.i.d. according to D. As in Chapter 12, we assume that, additionally, the learner has access to a feature mapping **\u03a6** : X _\u00d7_ Y _\u2192_ R _[N]_ with R _[N]_ a normed vector space and with _\u2225_ **\u03a6** _\u2225_ _\u221e_ _\u2264_ _r_ . We will denote by H a family of real-valued functions containing the component feature functions \u03a6 _j_ with _j \u2208_ [ _N_ ]. Note that in the most general case, we may have _N_ = + _\u221e_ . The problem consists of using the training sample _S_ to",
    "chunk_id": "foundations_machine_learning_307"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "learn an accurate conditional probability p[ _\u00b7|x_ ], for any _x \u2208_ X. **316** **Chapter 13** **Conditional Maximum Entropy Models** **13.2** **Conditional Maxent principle** As for Maxent models, conditional Maxent or logistic regression models can be derived from a key concentration inequality. By the general Rademacher complexity bound (Theorem 3.3), for any _\u03b4 >_ 0, the following inequality holds with probability at least 1 _\u2212_ _\u03b4_ over the choice of a sample of size _m_ : E E [ **\u03a6** ( _x, y_ )] _\u2264_ 2R _m_ (H) + log [2] _\u03b4_ (13.1) \ufffd\ufffd\ufffd\ufffd ( _x,y_ ) _\u223c_ D [[] **[\u03a6]** [(] _[x, y]_ [)]] _[ \u2212]_ ( _x,y_ ) _\u223c_ D [\ufffd] \ufffd\ufffd\ufffd\ufffd _\u221e_ ~~\ufffd~~ 2 _m_ _[,]_ where we denote by D [\ufffd] the empirical distribution defined by the sample _S_ . We will also denote by D [\ufffd] [1] ( _x_ ) the empirical distribution of _x_ in the sample _S_ . For any _x \u2208_ X, let p 0 [ _\u00b7|x_ ] denote a conditional probability, often chosen to be the uniform distribution. Then, the _conditional Maxent principle_ consists of seeking conditional probabilities p[ _\u00b7|x_ ] that are as agnostic as possible, that is as close as possible to the uniform distribution, or, more generally, to priors p 0 [ _\u00b7|x_ ], while verifying an inequality similar to (13.1): E [ **\u03a6** ( _x, y_ )] _\u2212_ E [ **\u03a6** ( _x, y_ )] _\u2264_ _\u03bb,_ (13.2) \ufffd\ufffd\ufffd\ufffd\ufffd _yx\u223c\u223c_ pD[ [\ufffd] _\u00b7|_ [1] _x_ ] ( _x,y_ ) _\u223c_ D [\ufffd] \ufffd\ufffd\ufffd\ufffd\ufffd _\u221e_ where _\u03bb \u2265_ 0 is a parameter. Here, closeness is defined via the _conditional relative_ _entropy_ (appendix E) based on the empirical marginal distribution D [\ufffd] [1] of input points. Choosing _\u03bb_ = 0 corresponds to standard _conditional Maxent_ or _unregu-_ _larized conditional Maxent_ and to requiring the expectation of the features based on D [1] and the conditional probabilities p[ _\u00b7|x_ ] to precisely match the empirical averages. As we will see later, its relaxation, that is the inequality case ( _\u03bb \u0338_ = 0), translates into a regularization. Notice that the conditional Maxent principle does not require specifying a family of conditional probability distributions _P_ to choose from. ~~\ufffd~~ log [2] _\u03b4_ (13.1) 2 _m_ _[,]_ where we denote by D [\ufffd] the empirical distribution defined by the sample _S_ . We will also denote by D [\ufffd] [1] ( _x_ ) the empirical distribution of _x_ in the sample _S_ . For any _x \u2208_ X, let p 0 [ _\u00b7|x_ ] denote a conditional probability, often chosen to be the uniform distribution. Then, the _conditional Maxent principle_ consists of seeking conditional probabilities p[ _\u00b7|x_ ] that are as agnostic as possible, that is as close as possible to the uniform distribution, or, more generally, to priors p 0 [ _\u00b7|x_ ], while verifying an inequality similar to (13.1): E [ **\u03a6** ( _x, y_ )] _\u2212_ E [ **\u03a6** ( _x, y_ )] _\u2264_ _\u03bb,_ (13.2) \ufffd\ufffd\ufffd\ufffd\ufffd _yx\u223c\u223c_ pD[ [\ufffd] _\u00b7|_ [1] _x_ ] ( _x,y_ ) _\u223c_ D [\ufffd] \ufffd\ufffd\ufffd\ufffd\ufffd",
    "chunk_id": "foundations_machine_learning_308"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u221e_ **13.3** **Conditional Maxent models** Let \u2206denote the simplex of the probability distributions over Y, X 1 = supp(D [\ufffd] [1] ) the support of D [\ufffd] [1], and \u00afp _\u2208_ \u2206 [X] [1] the family of conditional probabilities, \u00afp = (p[ _\u00b7|x_ ]) _x\u2208_ X 1 . Then, the conditional Maxent principle can be formulated as the following optimiza **13.4** **Dual problem** **317** tion problem: D\ufffd [1] ( _x_ ) D\ufffdp[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd (13.3) min \u00afp _\u2208_ \u2206 [X][1] s.t. \ufffd _x\u2208_ X 1 E [ **\u03a6** ( _x, y_ )] _\u2212_ E [ **\u03a6** ( _x, y_ )] _\u2264_ _\u03bb._ \ufffd\ufffd\ufffd\ufffd\ufffd _yx\u223c\u223c_ pD[ [\ufffd] _\u00b7|_ [1] _x_ ] ( _x,y_ ) _\u223c_ D [\ufffd] \ufffd\ufffd\ufffd\ufffd\ufffd _\u221e_ This defines a convex optimization problem since the objective is a positive sum of relative entropies and since the relative entropy D is convex with respect to its arguments (appendix E), since the constraints are affine functions of \u00afp, and since \u2206 [X] [1] is a convex set. The solution is in fact unique, since the objective is strictly convex as a positive sum of relative entropies, each strictly convex. The empirical conditional probabilities D [\ufffd] [1] ( _\u00b7|x_ ), _x \u2208_ X 1, clearly form a feasible solution, thus problem (12.7) is feasible. For uniform priors p 0 [ _\u00b7|x_ ], problem (13.3) can be equivalently formulated as a conditional entropy maximization, which explains the name given to these models. Let H [\u00af] (\u00afp) = _\u2212_ E _x\u223c_ D\ufffd [1] \ufffd\ufffd _y\u2208_ Y [p][[] _[y][|][x]_ [] log][ p][[] _[y][|][x]_ []] \ufffd denote the conditional entropy of p with respect to the marginal D [\ufffd] [1] . Then, the objective function of (12.7) can be rewritten as follows: D\ufffdp[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd = E _x\u223c_ D [\ufffd] [1] = E _x\u223c_ D [\ufffd] [1] \ufffd\ufffd _\u2208_ Y \ufffd p[ _y|x_ ] log [p][[] _[y][|][x]_ []] p 0 [ _y|x_ ] _y\u2208_ Y p 0 [ _y|x_ ] \ufffd \ufffd p[ _y|x_ ] log(1 _/c_ ) + \ufffd _y\u2208_ Y _y\u2208_ Y _\u2212_ \ufffd \ufffd p[ _y|x_ ] log p[ _y|x_ ] _y\u2208_ Y \ufffd = log( _c_ ) _\u2212_ H [\u00af] (\u00afp) _._ Thus, since log( _c_ ) is a constant, minimizing the objective is then equivalent to maximizing H [\u00af] (\u00afp). Conditional Maxent models are the solutions of the optimization problem just described. As in the non-conditional case, they admit two important benefits: they are based on a fundamental theoretical guarantee of closeness of empirical and true feature averages, and they do not require specifying a particular family of distributions _P_ . In the next sections, we will further analyze the properties of conditional Maxent models. **13.4** **Dual problem** Here, we derive an equivalent dual problem for (13.3) which, as we will show, can be formulated as a regularized conditional maximum likelihood problem over the family of _Gibbs distributions_ . **318** **Chapter 13** **Conditional Maximum Entropy Models** The Maxent optimization problem (13.3) can be equivalently expressed as the unconstrained optimization problem min",
    "chunk_id": "foundations_machine_learning_309"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\u00afp _F_ (\u00afp) with, for all \u00afp = (p[ _\u00b7|x_ ] _\u2208_ (R [Y] ) [X] [1], E [ **\u03a6** ( _x, y_ )] _x\u223c_ D [\ufffd] [1] _y\u223c_ p[ _\u00b7|x_ ] \ufffd _F_ (\u00afp) = E _x\u223c_ D [\ufffd] [1] \ufffdD\ufffd\ufffdp[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd [\ufffd] + _I_ C \ufffd _,_ (13.4) with D [\ufffd] \ufffdp[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd = D\ufffdp[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd if p[ _\u00b7|x_ ] is in \u2206, D [\ufffd] \ufffdp[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd = + _\u221e_ otherwise, and with C = _{_ **u** _\u2208_ R _[N]_ : _\u2225_ **u** _\u2212_ E ( _x,y_ ) _\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x, y]_ [)]] _[ \u2225]_ _[\u221e]_ _[\u2264]_ _[\u03bb][}]_ [, which is a] convex set. Let _G_ be the function defined for all **w** _\u2208_ R _[N]_ by p 0 [ _y_ _i_ _|x_ _i_ ] _G_ ( **w** ) = [1] _m_ _m_ \ufffd p **w** [ _y_ _i_ _|x_ _i_ ] \ufffd _i_ =1 log \ufffd p 0 [ _y_ _i_ _|x_ _i_ ] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 _,_ (13.5) \ufffd with, for all _x \u2208_ X 1 and _y \u2208_ Y, p **w** [ _y|x_ ] = [p] [0] [[] _[y][|][x]_ []] _[e]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,y]_ [)] \ufffd p 0 [ _y|x_ ] _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,y]_ [)] _._ (13.6) _y\u2208_ Y _Z_ ( **w** _, x_ ) and _Z_ ( **w** _, x_ ) = \ufffd Then, the following theorem gives a result similar to the duality theorem presented in the non-conditional case (Theorem 12.2, Section 12.5). **Theorem 13.1** _Problem_ (13.3) _is equivalent to the dual optimization problem_ sup **w** _\u2208_ R _N_ _G_ ( **w** ) _:_ sup \u00af min (13.7) **w** _\u2208_ R _[N]_ _[ G]_ [(] **[w]** [) =] p _\u2208_ (R [Y] ) [X][1] _[F]_ [(\u00af][p][)] _[.]_ _Furthermore, let_ \u00afp _[\u2217]_ = argmin \u00afp _F_ (\u00afp) _. Then, for any \u03f5 >_ 0 _and any_ **w** _such that_ _|G_ ( **w** ) _\u2212_ sup **w** _\u2208_ R _N_ _G_ ( **w** ) _| < \u03f5, we have_ E _x\u223c_ D\ufffd [1] \ufffdD\ufffdp\u00af _\u2217_ [ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd\ufffd _\u2264_ _\u03f5._ The proof is similar to that of Theorem 12.2 and is given at the end of this chapter since it is somewhat longer (Section 13.9). In view of the theorem, if **w** is an _\u03f5_ -solution of the dual optimization problem, then E _x\u223c_ D\ufffd [1] \ufffdD\ufffdp _[\u2217]_ [ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd\ufffd _\u2264_ _\u03f5_, which, by Jensen\u2019s inequality and Pinsker\u2019s inequality (Proposition E.7) implies that _\u2264_ _\u221a_ \ufffd _\u2264_ E \ufffd ~~\ufffd~~ _x\u223c_ D [\ufffd] [1] _\u2217_ 2 \ufffd\ufffd\ufffdp [ _\u00b7|x_ ] _\u2212_ p **w** [ _\u00b7|x_ ]\ufffd\ufffd 1 E _x\u223c_ D [\ufffd] [1] _\u2217_ \ufffd\ufffd\ufffdp [ _\u00b7|x_ ] _\u2212_ p **w** [ _\u00b7|x_ ]\ufffd\ufffd 1 2 _\u03f5._ Thus, p **w** [ _\u00b7|x_ ] is then _\u221a_ 2 _\u03f5_ -close in D [\ufffd] [1] -averaged _L_ 1 -norm",
    "chunk_id": "foundations_machine_learning_310"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "to the optimal solution of the primal and the theorem suggests that the solution of the conditional Maxent problem can be determined by solving the dual problem, which can be written equivalently as follows for a uniform prior: inf **w** _[\u03bb][\u2225]_ **[w]** _[\u2225]_ [1] _[ \u2212]_ _m_ [1] _m_ \ufffd log \ufffdp **w** [ _y_ _i_ _|x_ _i_ ]\ufffd _._ (13.8) _i_ =1 **13.5** **Properties** **319** Similar remarks to those made for non-conditional Maxent models apply here. In particular, the solution may not be achieved for any finite **w** for _\u03bb_ = 0, which is why the infimum is needed. Also, this result may seem surprising since it shows that conditional Maxent coincides with conditional Maximum Likelihood ( _\u03bb_ = 0) or regularized conditional Maximum Likelihood ( _\u03bb >_ 0) using for the family _P_ of conditional probabilities to choose from that of Gibbs distributions, while the conditional Maxent principle does not explicitly specify any family of conditional probabilities _P_ . The reason is the specific choice of the conditional relative entropy as the measure of closeness of p[ _\u00b7|x_ ] to the prior conditional distributions p 0 [ _\u00b7|x_ ]. Other measures of closeness between distributions lead to different forms for the solution. Thus, in some sense, the choice of the measure of closeness is the (dual) counterpart of that of the family of conditional distributions in maximum likelihood. Also, as already mentioned in the standard Maxent case, Gibbs distributions form a very rich family. Notice that both the primal and the dual optimization problems for conditional Maxent involve only conditional probabilities p[ _\u00b7|x_ ] for _x_ in X 1, that is for _x_ in the training sample. Thus, they do not provide us with any information about other conditional probabilities. However, the dual shows that, for _x_ in X 1, the solution admits the same general form p **w** [ _\u00b7|x_ ], which only depends on the weight vector **w** . In view of that, we extend the definition of Maxent conditional probabilities to all _x \u2208_ X by using the same general form p **w** [ _\u00b7|x_ ] and the same vector **w** for all _x_ . Observe also that in the definition of the primal or dual problems we could have used some other distribution Q over X in lieu of D [\ufffd] [1] . It is straightforward to verify that the duality theorem would continue to hold in that case using the same proof. In fact, ideally, we would have chosen Q to be D [1] . However, that optimization problem would require knowledge of the feature vectors for all _x \u2208_ supp(D [1] ), which of course is not accessible to us given a finite sample. The weighted vector **w** found when using D [\ufffd] [1] can be viewed as an approximation of the one obtained if using D [1] . **13.5** **Properties** In this section, we discuss several aspects of conditional Maxent models, including the form of the dual optimization problems, the feature vectors used, and prediction with these models. **320** **Chapter 13** **Conditional Maximum",
    "chunk_id": "foundations_machine_learning_311"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Entropy Models** **13.5.1** **Optimization problem** _L_ 1 -regularized conditional Maxent models are therefore conditional probability models solutions of the primal problem (13.3) or, equivalently, models defined by p **w** [ _y|x_ ] = _[e]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,y]_ [)] \ufffd _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,y]_ [)] _,_ (13.9) _y\u2208_ Y _Z_ ( _x_ ) and _Z_ ( _x_ ) = \ufffd where **w** is solution of the dual problem min **w** _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [1] _[ \u2212]_ _m_ [1] _m_ \ufffd log p **w** [ _y_ _i_ _|x_ _i_ ] _,_ _i_ =1 with _\u03bb \u2265_ 0 is a parameter. Using the expression of the conditional probabilities, this optimization problem can be written more explicitly as _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ [)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] \ufffd\ufffd _y\u2208_ Y \ufffd min [1] **w** _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [1] [ +] _m_ or, equivalently, as _m_ \ufffd log _i_ =1 _._ (13.10) _m_ _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ [)] [\ufffd] _._ (13.11) _y\u2208_ Y min [1] **w** _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [1] _[ \u2212]_ **[w]** _[ \u00b7]_ _m_ _m_ \ufffd \ufffd **\u03a6** ( _x_ _i_ _, y_ _i_ ) + _m_ [1] _i_ =1 _m_ \ufffd \ufffd log _i_ =1 \ufffd\ufffd _\u2208_ By definition of the dual problem, this is an unconstrained convex optimization problem in **w** . This can be also seen from the fact that the log-sum function **w** _\ufffd\u2192_ log \ufffd\ufffd _y\u2208_ Y _[e]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,y]_ [)] [\ufffd] is convex for any _x \u2208_ X. There are many optimization solutions available for this problem, including several special-purpose algorithms, general first-order and second-order solutions, and special-purpose distributed solutions. One common method is simply to use stochastic gradient descent (SGD), which has been reported to be more efficient than most special-purpose methods in applications. When the dimension of the feature vectors **\u03a6** (or the cardinality of the family of feature functions H) is very large, these methods are typically inefficient. An alternative method then consists of applying coordinate descent to solve this problem. In that case, the resulting algorithm coincides with the version of _L_ 1 -regularized boosting where, instead of the exponential function, the logistic function is used. **13.5.2** **Feature vectors** Using feature vectors **\u03a6** ( _x, y_ ) depending on both the input _x_ and the output _y_ is often important in applications. For example, in machine translation, it is convenient to use features whose values may depend on the presence of some words in the input sentence and some others in the output sequence. A common choice of the feature vector is however one where the column vectors **\u03a6** ( _x, y_ ) and **w** admit _c_ **13.6** **Generalization bounds** **321** blocks of equal size and where only the block in **\u03a6** ( _x, y_ ) corresponding to the class _y_ is non-zero and equal to a feature vector **\u0393** ( _x_ ) independent of the class labels: \uf8f9 **w** = \uf8fa\uf8fa\uf8fa\uf8fb",
    "chunk_id": "foundations_machine_learning_312"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**w** 1 ... **w** _y\u2212_ 1 **w** _y_ **w** _y_ +1 ... **w** _c_ \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 0 ... 0 **\u0393** ( _x_ ) 0 ... 0 \uf8f9 _._ \uf8fa\uf8fa\uf8fa\uf8fb **\u03a6** ( _x, y_ ) = \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 In view of that, the inner product of **w** and **\u03a6** ( _x, y_ ) can be expressed in terms of the feature vector **\u0393** ( _x_ ), which only depends on _x_, but with a distinct parameter vector **w** _y_ : **w** _\u00b7_ **\u03a6** ( _x, y_ ) = **w** _y_ _\u00b7_ **\u0393** ( _x_ ) _._ The optimization problem for _L_ 1 -regularized conditional Maxent can then be written in terms of the vectors **w** _y_ as follows: _e_ **[w]** _[y]_ _[\u00b7]_ **[\u0393]** [(] _[x]_ _[i]_ [)] _[\u2212]_ **[w]** _[yi]_ _[\u00b7]_ **[\u0393]** [(] _[x]_ _[i]_ [)] _._ (13.12) \ufffd _y\u2208_ Y \ufffd _\u2225_ **w** _y_ _\u2225_ 1 + _m_ [1] _y\u2208_ Y **w** min _\u2208_ R _[N]_ _[ \u03bb]_ \ufffd _m_ _m_ \ufffd \ufffd log _i_ =1 \ufffd\ufffd _\u2208_ Notice that, if the vectors **w** _y_ were not correlated via the second term of the objective function (for example if, instead of the log of the sum, this term were replaced by the sum of the logs), then the problem would be reduced to _c_ separate optimization functions learning a distinct weight vector for each class, as in the one-vs-all setup of multi-class classification. **13.5.3** **Prediction** Finally, note that the class \ufffd _y_ ( _x_ ) predicted by a conditional Maxent model with parameter **w** is given by \ufffd _y_ ( _x_ ) = argmax p **w** [ _y|x_ ] = argmax **w** _\u00b7_ **\u03a6** ( _x, y_ ) _._ (13.13) _y\u2208Y_ _y\u2208Y_ Thus, conditional Maxent models define linear classifiers. Conditional Maxent models are also sometimes referred to as log _-linear models_ . **13.6** **Generalization bounds** In this section, we will present learning guarantees for conditional Maxent models in two different settings: one where the dimension of the feature vectors **\u03a6** (or the cardinality of the family of feature functions H) is infinite or extremely large and where a coordinate-descent or boosting-type algorithm is more suitable, and another one where the dimension of the feature vectors **\u03a6** is finite and not too large. **322** **Chapter 13** **Conditional Maximum Entropy Models** We start with the case where the dimension of the feature vectors **\u03a6** is very large. The following margin-based guarantee holds in that case. **Theorem 13.2** _For any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the draw of an_ _i.i.d. sample S of size m, the following holds for all \u03c1 >_ 0 _and f \u2208_ F = _{_ ( _x, y_ ) _\ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x, y_ ): _\u2225_ **w** _\u2225_ 1 _\u2264_ 1 _}:_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ log [2] _\u03b4_ 2 _m_ _[,]_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd\ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ ~~\ufffd~~ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _R_ ( _f_ ) _\u2264_ [1] _m_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_",
    "chunk_id": "foundations_machine_learning_313"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u0338_ _\u0338_ _\u0338_ \ufffd log _u_ 0 _i_ =1 _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u03c1_ [R] _[m]_ [(\u03a0] [1] [(][H][))+] _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ log log 2 4 _\u03c1r_ + _m_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ log log 2 4 _r_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _e_ _y\u2208_ Y _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _f_ ( _xi,y_ ) _\u2212f_ ( _xi,yi_ ) _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _f_ ( _xi,yi_ ) _\u03c1_ + [8] _[c]_ \ufffd _\u03c1_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _where u_ 0 = log(1 + 1 _/e_ ) _and_ \u03a0 1 (H) = _{x \ufffd\u2192_ _\u03c6_ ( _x, y_ ): _\u03c6 \u2208_ H _, y \u2208Y}._ Proof: For any _f_ : ( _x, y_ ) _\ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x, y_ ) and _i \u2208_ [ _m_ ], let _\u03c1_ _f_ ( _x_ _i_ _, y_ _i_ ) denote the margin of _f_ at ( _x_ _i_ _, y_ _i_ ): _\u03c1_ _f_ ( _x_ _i_ _, y_ _i_ ) = min _y_ = _\u0338_ _y_ _i_ _[f]_ [(] _[x]_ _[i]_ _[, y]_ _[i]_ [)] _[ \u2212]_ _[f]_ [(] _[x]_ _[i]_ _[, y]_ [) = min] _y_ = _\u0338_ _y_ _i_ **[w]** _[ \u00b7]_ [ (] **[\u03a6]** [(] _[x]_ _[i]_ _[, y]_ _[i]_ [)] _[ \u2212]_ **[\u03a6]** [(] _[x]_ _[i]_ _[, y]_ [))] _[.]_ Fix _\u03c1 >_ 0. Then, by Theorem 9.2, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, the following inequality holds for all _f \u2208_ H and _\u03c1 \u2208_ (0 _,_ 2 _r_ ]: _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ ~~\ufffd~~ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ log [2] _\u03b4_ 2 _m_ _[,]_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u03c1_ [R] _[m]_ [(\u03a0] [1] [(][F][)) +] _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _R_ ( _f_ ) _\u2264_ [1] _m_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd 1 _\u03c1_ _f_ ( _x_ _i_ _,y_ _i_ ) _\u2264\u03c1_ + [4] _[c]_ _i_ =1 _\u03c1_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ ~~\ufffd~~ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ log log 2 4 _\u03c1r_ + _m_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ where \u03a0 1 (F) = _{x \ufffd\u2192_ _f_ ( _x, y_ ): _y \u2208_ Y _, f \u2208_ H _}_ . The inequality trivially holds for all _\u03c1 >_ 0 since for _\u03c1 \u2265_ 2 _r_, by H\u00a8older\u2019s inequality, we have _|_ **w** _\u00b7_ **\u03a6** ( _x, y_ ) _| \u2264_ _\u2225_ **w** _\u2225_ 1 _\u2225_ **\u03a6** ( _x, y_ ) _\u2225_ _\u221e_ _\u2264_ _r_ for _\u2225_ **w** _\u2225_ 1 _\u2264_ 1, and thus min _y_ = _\u0338_ _y_ _i_ _f_ ( _x_ _i_ _, y_ _i_ ) _\u2212_ _f_ ( _x_ _i_ _, y_ ) _\u2264_ 2 _r \u2264_ _\u03c1_ for all _i \u2208_ [ _m_ ] and _y \u2208_ Y. Now, for any _\u03c1 >_ 0, the _\u03c1_ -margin loss can be upper bounded by the _\u03c1_ -logistic loss: _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _[u]_ _\u03c1_ _\u03c1_ _[\u2212]_ [1] _[\u2264]_ [0] _[ \u2264]_ [log] _[u]_ 0 [(1 +] _[ e]_ _[\u2212]_ _[u]_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u2200u \u2208_ R _,_",
    "chunk_id": "foundations_machine_learning_314"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "1 _u\u2264\u03c1_ = 1 _[u]_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u03c1_ ) _._ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ Thus, the _\u03c1_ -margin loss of _f_ at ( _x_ _i_ _, y_ _i_ ) can be upper bounded as follows: 1 _\u03c1_ _f_ ( _x_ _i_ _,y_ _i_ ) _\u2264\u03c1_ _\u2264_ log _u_ 0 (1 + _e_ _[\u2212]_ _[\u03c1]_ [(] _[f,][x]_ _\u03c1_ _[i,yi]_ [)] ) _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ = log _u_ 0 (1 + max _y_ = _\u0338_ _y_ _i_ _[e]_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _f_ ( _xi,y_ ) _\u2212f_ ( _xi,yi_ ) _\u03c1_ ) _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _f_ ( _xi,y_ ) _\u2212f_ ( _xi,yi_ ) _\u03c1_ _._ \ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _f_ ( _xi,yi_ ) _\u03c1_ _._ \ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _e_ \ufffd\ufffd _\u0338_ _y\u2208_ Y _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd\ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u2264_ log _u_ 0 _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ 1 + \ufffd _e_ \ufffd _y_ = _\u0338_ _y_ _i_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _f_ ( _xi,y_ ) _\u2212f_ ( _xi,yi_ ) _\u03c1_ = log _u_ 0 \ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ Thus, with probability at least 1 _\u2212_ _\u03b4_, the following inequality holds for all _f \u2208_ H and _\u03c1 >_ 0: _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ log [2] _\u03b4_ 2 _m_ _[.]_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd\ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _R_ ( _f_ ) _\u2264_ [1] _m_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ \ufffd log _u_ 0 _i_ =1 _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u03c1_ [R] _[m]_ [(\u03a0] [1] [(][F][))+] _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ log log 2 4 _\u03c1r_ + _m_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ log log 2 4 _r_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _e_ _y\u2208_ Y _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _f_ ( _xi,y_ ) _\u2212f_ ( _xi,yi_ ) _\u0338_ _\u0338_ _\u0338_ _\u0338_ _\u0338_ _f_ ( _xi,yi_ ) _\u03c1_ + [4] _[c]_ \ufffd _\u03c1_ **13.6** **Generalization bounds** **323** For any sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ) of size _m_, the empirical Rademacher complexity of \u03a0 1 (F) can be bounded as follows: sup \u03a6 _\u2208_ H _y\u2208_ Y \ufffd _N_ \ufffd \ufffd _w_ _j_ \u03a6 _j_ ( _x_ _i_ _, y_ ) _j_ =1 \ufffd R _S_ (\u03a0 1 (F)) = [1] _m_ [E] _**\u03c3**_ _m_ \ufffd _\u03c3_ _i_ _i_ =1 _m_ \ufffd \ufffd _\u03c3_ _i_ \u03a6 _j_ ( _x_ _i_ _, y_ ) _i_ =1 \ufffd = [1] _m_ [E] _**\u03c3**_ sup _\u2225_ **w** _\u2225_ 1 _\u2264_ 1 _y\u2208_ Y sup _\u2225_ **w** _\u2225_ 1 _\u2264_ 1 _y\u2208_ Y _N_ \ufffd _w_ _j_ _j_ =1 _N_ \ufffd = [1] _m_ [E] _**\u03c3**_ sup _j\u2208_ [ _N_ ] _y\u2208_ Y \ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u03c3_ _i_ \u03a6 _j_ ( _x_ _i_ _, y_ ) _i_ =1 \ufffd\ufffd\ufffd\ufffd _m_ \ufffd \ufffd _\u2264_ 2 R [\ufffd] _S_ (\u03a0 1 (H)) _,_ \ufffd _\u2264_ [1] _m_ [E] _**\u03c3**_ \ufffd \ufffd \ufffd \ufffd \ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u03c3_",
    "chunk_id": "foundations_machine_learning_315"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_i_ \u03a6( _x_ _i_ _, y_ ) _i_ =1 \ufffd\ufffd\ufffd\ufffd _m_ \ufffd which completes the proof. The learning guarantee of the theorem is remarkable since it does not depend on the dimension _N_ and since it only depends on the complexity of the family H of feature functions (or base hypotheses). Since for any _\u03c1 >_ 0, _f/\u03c1_ admits the same generalization error as _f_, the theorem implies that with probability at least 1 _\u2212_ _\u03b4_, the following inequality holds for all _f \u2208{_ ( _x, y_ ) _\ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x, y_ ): _\u2225_ **w** _\u2225_ 1 _\u2264_ _\u03c1_ [1] _[}]_ [ and] _\u03c1 >_ 0: \ufffd \ufffd\ufffd \ufffd _R_ ( _f_ ) _\u2264_ [1] _m_ _m_ \ufffd \ufffd log _u_ 0 _i_ =1 log [2] _\u03b4_ 2 _m_ _[.]_ log log 2 4 _r_ _\u03c1_ + _m_ _e_ _[f]_ [(] _[x]_ _[i]_ _[,y]_ [)] _[\u2212][f]_ [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] + [8] _[c]_ \ufffd _\u03c1_ _y\u2208_ Y _\u03c1_ [R] _[m]_ [(\u03a0] [1] [(][H][))+] This inequality can be used to derive an algorithm that selects **w** and _\u03c1 >_ 0 to minimize the right-hand side. The minimization with respect to _\u03c1_ does not lead to a convex optimization and depends on theoretical constant factors affecting the second and third term. Thus, instead, _\u03c1_ is left as a free parameter of the algorithm, typically determined via cross-validation. Now, since only the first term of the right-hand side depends on **w**, for any _\u03c1 >_ 0, the bound suggests selecting **w** as the solution of the following optimization problem: _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ [)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] _._ (13.14) \ufffd _y\u2208_ Y _m_ \ufffd \ufffd log _i_ =1 \ufffd\ufffd _\u2208_ min _\u2225_ **w** _\u2225_ 1 _\u2264_ _\u03c1_ [1] 1 _m_ **324** **Chapter 13** **Conditional Maximum Entropy Models** Introducing a Lagrange variable _\u03bb \u2265_ 0, the optimization problem can be written equivalently as _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ [)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] _._ (13.15) \ufffd _y\u2208_ Y min [1] **w** _[\u03bb][\u2225]_ **[w]** _[\u2225]_ [1] [ +] _m_ _m_ \ufffd \ufffd log _i_ =1 \ufffd\ufffd _\u2208_ Y Since for any choice of _\u03c1_ in the constraint of (13.14), there exists an equivalent dual variable _\u03bb_ in the formulation of (13.15) that achieves the same optimal **w**, _\u03bb_ can be freely selected via cross-validation. The resulting algorithm precisely coincides with conditional Maxent. When the dimension _N_ of the feature vectors **\u03a6** is finite, the following marginbased guarantee holds. **Theorem 13.3** _For any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the draw of an_ _i.i.d. sample S of size m, the following holds for all \u03c1 >_ 0 _and f \u2208_ F = _{_ ( _x, y_ ) _\ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x, y_ ): _\u2225_ **w** _\u2225_ 1 _\u2264_ 1 _}:_ ~~\ufffd~~ \ufffd\ufffd \ufffd log log 2 4 _r_ _R_ ( _f_ ) _\u2264_ [1] _m_ _m_ \ufffd \ufffd log _u_ 0 _i_ =1 log [2]",
    "chunk_id": "foundations_machine_learning_316"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u03b4_ 2 _m_ _[,]_ _e_ _y\u2208_ Y 2 log(2 _cN_ ) _\u03c1_ + _m_ _f_ ( _xi,y_ ) _\u2212\u03c1f_ ( _xi,yi_ ) + [4] _[cr]_ \ufffd \ufffd + _\u03c1_ _where u_ 0 = log(1 + 1 _/e_ ) _._ Proof: The proof coincides with that of Theorem 13.2, modulo the upper bound on R _m_ (\u03a0 1 (F)). For any sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ) of size _m_, the empirical Rademacher complexity of \u03a0 1 (F) can be bounded as follows: sup _j\u2208_ [ _N_ ] _y\u2208_ Y _,s\u2208{\u2212_ 1 _,_ +1 _}_ \ufffd R _S_ (\u03a0 1 (F)) = [1] _m_ [E] _**\u03c3**_ _m_ \ufffd \ufffd _\u03c3_ _i_ **w** _\u00b7_ **\u03a6** ( _x_ _i_ _, y_ ) _i_ =1 \ufffd \ufffd = [1] _m_ [E] _**\u03c3**_ sup _\u2225_ **w** _\u2225_ 1 _\u2264_ 1 _y\u2208_ Y sup _\u2225_ **w** _\u2225_ 1 _\u2264_ 1 _y\u2208_ Y **w** _\u00b7_ _m_ \ufffd \ufffd _\u03c3_ _i_ **\u03a6** ( _x_ _i_ _, y_ ) _i_ =1 \ufffd = [1] _m_ [E] _**\u03c3**_ sup _y\u2208_ Y \ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u03c3_ _i_ **\u03a6** ( _x_ _i_ _, y_ ) _i_ =1 \ufffd\ufffd\ufffd\ufffd _\u221e_ _m_ \ufffd \ufffd = [1] _m_ [E] _**\u03c3**_ \ufffd \ufffd \ufffd \ufffd _s_ _m_ \ufffd \ufffd _\u03c3_ _i_ \u03a6 _j_ ( _x_ _i_ _, y_ ) _i_ =1 _\u2264_ _r_ \ufffd 2 log(2 _cN_ ) _,_ where the third equality holds by definition of the dual norm, and the last inequality by the maximal inequality (Corollary D.11), since the supremum is taken over 2 _cN_ choices. **13.7** **Logistic regression** **325** This learning guarantee of the theorem is very favorable even for relatively highdimensional problems since its dependency on the dimension _N_ is only logarithmic. **13.7** **Logistic regression** The binary case of conditional Maxent models ( _c_ = 2) is known as _logistic regression_ and is one of the most well-known algorithms for binary classification. **13.7.1** **Optimization problem** In the binary case, the sum appearing in the optimization problem of conditional Maxent models can be simplified as follows: _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ [)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] = _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,]_ [+1)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] + _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,][\u2212]_ [1)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] \ufffd _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ [)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] = _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,]_ [+1)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] + _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,][\u2212]_ [1)] _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,y]_ _[i]_ [)] \ufffd _y\u2208_ Y _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a6]** _[x]_ _[\u2212]_ **[\u03a6]** _[x]_ _[\u2212]_ = 1 + _e_ _[\u2212][y]_ _[i]_ **[w]** _[\u00b7]_ [[] **[\u03a6]** [(] _[x]_ _[i]_ _[,]_ [+1)] _[\u2212]_ **[\u03a6]** [(] _[x]_ _[i]_ _[,][\u2212]_ [1)]] = 1 + _e_ _[\u2212][y]_ _[i]_ **[w]** _[\u00b7]_ **[\u03a8]** [(] _[x]_ _[i]_ [)] _,_ where for all _x",
    "chunk_id": "foundations_machine_learning_317"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\u2208_ X, **\u03a8** ( _x_ ) = **\u03a6** ( _x,_ +1) _\u2212_ **\u03a6** ( _x, \u2212_ 1). This leads to the following optimization problem, which defines _L_ 1 _-regularized logistic regression_ : min [1] **w** _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [1] [ +] _m_ _m_ \ufffd log \ufffd1 + _e_ _[\u2212][y]_ _[i]_ **[w]** _[\u00b7]_ **[\u03a8]** [(] _[x]_ _[i]_ [)] [\ufffd] _._ (13.16) _i_ =1 As discussed in the general case, this is a convex optimization problem which admits a variety of different solutions. A common solution is SGD, another one is coordinate descent. When coordinate descent is used, then the algorithm coincides with the alternative to AdaBoost where the logistic loss is used instead of the exponential loss ( _\u03c6_ ( _\u2212u_ ) = log 2 (1 + _e_ _[\u2212][u]_ ) _\u2265_ 1 _u\u2264_ 0 ). **13.7.2** **Logistic model** In the binary case, the conditional probability defined by the weight vector **w** can be expressed as follows: p **w** [ _y_ = +1 _| x_ ] = _[e]_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,]_ [+1)] _,_ (13.17) _Z_ ( _x_ ) with _Z_ ( _x_ ) = _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,]_ [+1)] + _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,][\u2212]_ [1)] . Thus, prediction is based on a linear decision rule defined by the sign of log-odds ratio: _[|][ x]_ []] log [p] **[w]** [[] _[y]_ [ = +1] \ufffd **\u03a6** ( _x,_ +1) _\u2212_ **\u03a6** ( _x, \u2212_ 1)\ufffd = **w** _\u00b7_ **\u03a8** ( _x_ ) _._ p **w** [ _y_ = _\u2212_ 1 _| x_ ] [=] **[ w]** _[ \u00b7]_ **326** **Chapter 13** **Conditional Maximum Entropy Models** **Figure 13.1** Plot of the logistic function _f_ logistic . **Image:** [No caption returned] This is why logistic regression is also known as a log _-linear model_ . Observe also that the conditional probability admits the following _logistic form_ : 1 1 p **w** [ _y_ = +1 _| x_ ] = \ufffd **w** _\u00b7_ **\u03a8** ( _x_ )\ufffd _,_ 1 + _e_ _[\u2212]_ **[w]** _[\u00b7]_ [[] **[\u03a6]** [(] _[x,]_ [+1)] _[\u2212]_ **[\u03a6]** [(] _[x,][\u2212]_ [1)]] [ =] 1 + _e_ _[\u2212]_ **[w]** _[\u00b7]_ **[\u03a8]** [(] _[x]_ [)] [ =] _[ f]_ [logistic] where _f_ logistic is the function defined over R by _f_ logistic : _x \ufffd\u2192_ 1+1 _e_ _[\u2212][x]_ [ . Figure 13.1] shows the plot of this function. The logistic function maps the images of the linear function _x \ufffd\u2192_ **\u03a8** ( _x_ ) to the interval [0 _,_ 1], which makes them interpretable as probabilities. _L_ 1 -regularized logistic regression benefits from the strong learning guarantees already presented for conditional maxent models, in the special case of two classes ( _c_ = 2). The learning guarantees for _L_ 2 -regularized logistic regression will be similarly special cases of those presented in the next section. **13.8** _**L**_ **2** **-regularization** A common variant of conditional Maxent models is one where the dimension _N_ is finite and where the regularization is based on the norm-2 squared of the weight vector **w** . The optimization problem is thus given by",
    "chunk_id": "foundations_machine_learning_318"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "min 2 _[\u2212]_ [1] **w** _\u2208_ R _[N]_ _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [2] _m_ where for all ( _x, y_ ) _\u2208_ X _\u00d7_ Y, _m_ \ufffd log p **w** [ _y_ _i_ _|x_ _i_ ] _,_ _i_ =1 _[y]_ [))] p **w** [ _y|x_ ] = [ex][p(] **[w]** _[ \u00b7]_ **[ \u03a6]** [(] _[x][,]_ \ufffd exp( **w** _\u00b7_ **\u03a6** ( _x, y_ )) _._ (13.18) _y\u2208_ Y _Z_ _[ \u00b7]_ ( _x_ ) _[,]_ and _Z_ ( _x_ ) = \ufffd As for the norm-1 regularization, there are many optimization solutions available for this problem, including special-purpose algorithms, general first-order and second **13.8** _L_ 2 **-regularization** **327** order solutions, and special-purpose distributed solutions. Here, the objective is additionally differentiable. A common optimization method is simply stochastic gradient descent (SGD). In contrast to norm-1-regularized conditional Maxent models, which lead to sparser weight vectors, norm-2 conditional Maxent models lead to non-sparse solutions, which may be preferable and lead to more accurate solutions in some applications such as natural language processing. The following margin-based guarantee holds for norm-2 regularized conditional Maxent, assuming that the norm-2 of the feature vector is bounded. **Theorem 13.4** _For any \u03b4 >_ 0 _, with probability at least_ 1 _\u2212_ _\u03b4 over the draw of an_ _i.i.d. sample S of size m, the following holds for all \u03c1 >_ 0 _and f \u2208_ F = _{_ ( _x, y_ ) _\ufffd\u2192_ **w** _\u00b7_ **\u03a6** ( _x, y_ ): _\u2225_ **w** _\u2225_ 2 _\u2264_ 1 _}:_ \ufffd log [2] _\u03b4_ 2 _m_ _[,]_ \ufffd\ufffd _R_ ( _f_ ) _\u2264_ [1] _m_ _m_ \ufffd \ufffd log _u_ 0 _i_ =1 _e_ _y\u2208_ Y _f_ ( _xi,y_ ) _\u2212f_ ( _xi,yi_ ) \ufffd log log 2 4 _\u03c1r_ 2 + _m_ _\u2212f_ ( _xi,yi_ ) _\u03c1_ + [4] _[r]_ [2] _[c]_ [2] \ufffd _\u03c1_ _[\u221a]_ _m_ _\u03c1_ _[\u221a]_ _m_ [+] _where u_ 0 = log(1 + 1 _/e_ ) _and r_ 2 = sup ( _x,y_ ) _\u2225_ **\u03a6** ( _x, y_ ) _\u2225_ 2 _._ Proof: The proof is similar to that of Theorem 13.3, modulo the observation that here _|_ **w** _\u00b7_ **\u03a6** ( _x, y_ ) _| \u2264\u2225_ **w** _\u2225_ 2 _\u2225_ **\u03a6** ( _x, y_ ) _\u2225_ 2 _\u2264_ _r_ 2 and modulo the upper bound on R _m_ (\u03a0 1 (F)). For any sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ) of size _m_, the empirical Rademacher complexity of \u03a0 1 (F) can be bounded as follows: \ufffd R _S_ (\u03a0 1 (F)) = [1] _m_ [E] _**\u03c3**_ _m_ \ufffd \ufffd _\u03c3_ _i_ **w** _\u00b7_ **\u03a6** ( _x_ _i_ _, y_ ) _i_ =1 \ufffd \ufffd = [1] _m_ [E] _**\u03c3**_ sup _\u2225_ **w** _\u2225_ 2 _\u2264_ 1 _y\u2208_ Y sup _\u2225_ **w** _\u2225_ 2 _\u2264_ 1 _y\u2208_ Y _m_ \ufffd \ufffd _\u03c3_ _i_ **\u03a6** ( _x_ _i_ _, y_ ) _i_ =1 **w** _\u00b7_ \ufffd = [1] _m_ [E] _**\u03c3**_ \ufffd \ufffd \ufffd sup _y\u2208_ Y \ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u03c3_ _i_ **\u03a6** ( _x_ _i_ _, y_ ) _i_ =1 \ufffd\ufffd\ufffd\ufffd",
    "chunk_id": "foundations_machine_learning_319"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "2 _m_ \ufffd \ufffd _\u2264_ [1] _m_ _\u2264_ [1] _m_ = [1] _m_ \ufffd _y\u2208_ Y \ufffd _y\u2208_ Y \ufffd _\u2225_ **\u03a6** ( _x_ _i_ _, y_ ) _\u2225_ [2] 2 _i_ =1 \ufffd E _**\u03c3**_ _y\u2208_ Y \ufffd\ufffd\ufffd\ufffd\ufffd _m_ \ufffd _\u03c3_ _i_ **\u03a6** ( _x_ _i_ _, y_ ) _i_ =1 \ufffd\ufffd\ufffd\ufffd 2 _m_ \ufffd 2 ~~\ufffd~~ \ufffd \ufffdE _**\u03c3**_ \ufffd ~~\ufffd~~ \ufffd \ufffd \ufffd \ufffd\ufffd\ufffd\ufffd\ufffd _m_ \ufffd \ufffd _\u03c3_ _i_ **\u03a6** ( _x_ _i_ _, y_ ) _i_ =1 \ufffd\ufffd\ufffd\ufffd 2 \ufffd _m_ \ufffd \ufffd _\u2264_ _[r]_ [2] _[c]_ _\u221am,_ where the third equality holds by definition of the dual norm, and the second inequality by Jensen\u2019s inequality. **328** **Chapter 13** **Conditional Maximum Entropy Models** The learning guarantee of the theorem for _L_ 2 -regularized conditional maxent models admits the advantage that the bound does not depend on the dimension. It can be very favorable for _r_ 2 relatively small. The algorithm can then be very effective, provided that a small error can be achieved by a non-sparse weight vector. **13.9** **Proof of the duality theorem** In this section, we give the full proof of Theorem 13.1. Proof: The proof is similar to that of Theorem 12.2 and follows by application of the Fenchel duality theorem (theorem B.39) to the optimization problem (13.4) with the functions\ufffd _f_ and _g_ defined for all \u00afp _\u2208_ (R [Y] ) [X] [1] and **u** _\u2208_ R _[N]_ by _f_ (\u00afp) = E _x\u223c_ D\ufffd [1] \ufffdD\ufffdp[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd [\ufffd], _g_ ( **u** ) = _I_ C ( **u** ) and _A_ p = [\ufffd] _x\u2208_ X \ufffd _y\u2208_ Y [D][\ufffd] [1] [(] _[x]_ [)][p][[] _[y][|][x]_ []] **[\u03a6]** [(] _[x, y]_ \ufffd _A_ E _x_ is a bounded linear map since we have _\u223c_ D\ufffd [1] \ufffdD\ufffdp[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]\ufffd [\ufffd], _g_ ( **u** ) = _I_ C ( **u** ) and _\u2225A A_ p\u00afp _\u2225\u2264\u2225_ = [\ufffd] p\u00af _x_ _\u2225_ _\u2208_ 1X sup\ufffd _yx\u2208\u2208_ YX [D] _,y_ [\ufffd] _\u2208_ [1] [(] Y _[x]_ _\u2225_ [)] **\u03a6** [p][[] ( _[y]_ _x, y_ _[|][x]_ []] **[\u03a6]** ) _\u2225_ [(] _\u221e_ _[x, y]_ _\u2264_ [).] _r\u2225_ p\u00af _\u2225_ 1 for any \u00afp _\u2208_ (R [Y] ) [X] [1] . Also, notice that the conjugate of _A_ is given for all **w** _\u2208_ R _[N]_ and ( _x, y_ ) _\u2208_ X 1 _\u00d7_ Y by ( _A_ _[\u2217]_ **w** )( _x, y_ ) = **w** _\u00b7_ \ufffd\ufffdD 1 ( _x_ ) **\u03a6** ( _x, y_ )\ufffd. Consider **u** 0 _\u2208_ R _[N]_ defined by **u** 0 = E ( _x,y_ ) _\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x, y]_ [)] =] _[ A]_ [p][\u00af] [0] [ with \u00af][p] [0] [ = (][D][(] _[\u00b7|][x]_ [))] _[x][\u2208]_ [X] [1] [.] Since \u00afp 0 is in dom( _f_ ) = \u2206 [X] [1], **u** 0 is in _A_ (dom( _f_ )). Furthermore, since _\u03bb_ is positive, **u** 0 is contained in int(C). _g_ = _I_ C equals zero over int(C) and is therefore continuous over int(C), thus _g_ is",
    "chunk_id": "foundations_machine_learning_320"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "continuous at **u** 0 and we have **u** 0 _\u2208_ _A_ (dom( _f_ )) _\u2229_ cont( _g_ ). Thus, the assumptions of Theorem B.39 hold. The conjugate function of _f_ is defined for all \u00afq = (q[ _\u00b7|x_ ]) _x\u2208_ X 1 _\u2208_ (R [Y] ) [X] [1] by _x\u2208_ X \ufffd _f_ _[\u2217]_ (\u00afq) = \u00afp _\u2208_ ( sup R [Y] ) [X][1] = sup \u00afp _\u2208_ (R [Y] ) [X][1] _\u27e8_ p _,_ q _\u27e9\u2212_ \ufffd D\ufffd [1] ( _x_ ) \ufffdD(p[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]) \ufffd _x\u2208_ X \ufffd \ufffd\ufffd _x\u2208_ X 1 \ufffd _\u2212_ D 1 [ _x_ ] \ufffd X D\ufffd [1] [ _x_ ] \ufffd _y\u2208_ Y _x\u2208_ X 1 p[ _y|x_ ]q[ _y|x_ ] D\ufffd [1] [ _x_ ] \ufffdD(p[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]) \ufffd \ufffd \ufffd _\u2208_ Y q[ _y|x_ ] _y_ \ufffd _\u2208_ Y p[ _y|x_ ]\ufffd D\ufffd 1 ( _x_ ) D\ufffd 1 ( _x_ ) = \ufffd _x\u2208_ X 1 = \ufffd _x\u2208_ X 1 D\ufffd [1] ( _x_ ) \u00afp _\u2208_ ( sup R [Y] ) [X][1] _\u2212_ D [\ufffd] (p[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]) \ufffd \ufffd _,_ \ufffd D\ufffd [1] ( _x_ ) _f_ _x_ _[\u2217]_ q[ _y|x_ ] \ufffd D\ufffd 1 ( _x_ ) where, for all _x \u2208_ X 1 and p _\u2208_ R [X] [1], _f_ _x_ is defined by _f_ _x_ (\u00afp) = D [\ufffd] (p[ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]). By Lemma B.37, the conjugate function _f_ _x_ _[\u2217]_ [is given for all \u00af][q] _[ \u2208]_ [(][R] [Y] [)] [X] [1] [ by] _[ f]_ _x_ _[ \u2217]_ \ufffd Dq\ufffd[ [1] _y_ ( _|xx_ ]) \ufffd = log _y\u2208_ Y [p] [0] [[] _[y][|][x]_ []] _[e]_ \ufffd\ufffd D\ufffdq[ _y_ [1] _|_ ( _xx_ ]) . Thus, _f_ _[\u2217]_ is given for all \u00afq _\u2208_ (R [Y] ) [X] [1] by \ufffd _f_ _[\u2217]_ (q) = E log p 0 [ _y|x_ ] _e_ _x\u223c_ D [\ufffd] [1] \ufffd \ufffd\ufffd _y\u2208_ Y D\ufffdq[ _y_ [1] _|_ ( _xx_ ]) _._ \ufffd [\ufffd] **13.9** **Proof of the duality theorem** **329** As in the proof of Theorem 12.2, the conjugate function of _g_ = _I_ C is given for all **w** _\u2208_ R _[N]_ by _g_ _[\u2217]_ ( **w** ) = E ( _x,y_ ) _\u223c_ D\ufffd [[] **[w]** _[ \u00b7]_ **[ \u03a6]** [(] _[x, y]_ [)] +] _[ \u03bb][\u2225]_ **[w]** _[\u2225]_ [1] [. In view of these identities, we] can write, for all **w** _\u2208_ R _[N]_, _\u2212_ _f_ _[\u2217]_ ( _A_ _[\u2217]_ **w** ) _\u2212_ _g_ _[\u2217]_ ( _\u2212_ **w** ) = _\u2212_ E _x\u223c_ D [\ufffd] [1] log p 0 [ _y|x_ ] _e_ **[w]** _[\u00b7]_ **[\u03a6]** [(] _[x,y]_ [)] [\ufffd\ufffd] + E [ **w** _\u00b7_ **\u03a6** ( _x, y_ )] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 \ufffd \ufffd\ufffd _y\u2208_ Y ( _x,y_ ) _\u223c_ D [\ufffd] = _\u2212_ E [1] _x\u223c_ D [\ufffd] [1] [[log] _[ Z]_ [(] **[w]** _[, x]_ [)] +] _m_ _m_ \ufffd **w** _\u00b7_ **\u03a6**",
    "chunk_id": "foundations_machine_learning_321"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "( _x_ _i_ _, y_ _i_ ) _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 _i_ =1 _Z_ ( **w** _, x_ _i_ ) _[\u2212]_ _[\u03bb][\u2225]_ **[w]** _[\u2225]_ [1] = [1] _m_ = [1] _m_ _m_ \ufffd _i_ =1 log _[e]_ _Z_ **[w]** ( _[\u00b7]_ **[\u03a6]** **w** [(] _[x]_ _, x_ _[i]_ _[,y]_ _i_ ) _[i]_ [)] _m_ \ufffd p **w** [ _y_ _i_ _|x_ _i_ ] \ufffd _i_ =1 log \ufffd p 0 [ _y_ _i_ _|x_ _i_ ] _m_ \ufffd p 0 [ _y_ _i_ _|x_ _i_ ] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 = _G_ ( **w** ) _,_ \ufffd which proves that sup **w** _\u2208_ R _N_ _G_ ( **w** ) = min \u00afp _\u2208_ (R Y ) X1 _F_ (\u00afp). The second part of the proof is similar to that of Theorem 12.2. For any **w** _\u2208_ R _[N]_, we can write _G_ ( **w** ) _\u2212_ E E _x\u223c_ D [\ufffd] [1] [[][D][(][p] _[\u2217]_ [[] _[\u00b7|][x]_ []] _[ \u2225]_ [p] [0] [[] _[\u00b7|][x]_ [])] +] _x\u223c_ D [\ufffd] [1] [[][D][(][p] _[\u2217]_ [[] _[\u00b7|][x]_ []] _[ \u2225]_ [p] **[w]** [[] _[\u00b7|][x]_ [])]] _\u2212_ _\u03bb\u2225_ **w** _\u2225_ 1 _\u2212_ E \ufffd _x\u223c_ D [\ufffd] [1] _y\u223c_ p _[\u2217]_ [ _\u00b7|x_ ] + E \ufffd _x\u223c_ D [\ufffd] [1] _y\u223c_ p _[\u2217]_ [ _\u00b7|x_ ] \ufffd log [p] _[\u2217]_ [[] _[y][|][x]_ []] \ufffd p 0 [ _y|x_ ] log [p] _[\u2217]_ [[] _[y][|][x]_ []] \ufffd p **w** [ _y|x_ ] = E ( _x,y_ ) _\u223c_ D [\ufffd] log [p] **[w]** [[] _[y][|][x]_ []] \ufffd p 0 [ _y|x_ ] _\u2212_ E \ufffd _x\u223c_ D [\ufffd] [1] _y\u223c_ p _[\u2217]_ [ _\u00b7|x_ ] log [p] **[w]** [[] _[y][|][x]_ []] \ufffd p 0 [ _y|x_ ] \ufffd = _\u2212\u03bb\u2225_ **w** _\u2225_ 1 + E ( _x,y_ ) _\u223c_ D [\ufffd] log [p] **[w]** [[] _[y][|][x]_ []] \ufffd p 0 [ _y|x_ ] = _\u2212\u03bb\u2225_ **w** _\u2225_ 1 + E [ **w** _\u00b7_ **\u03a6** ( _x, y_ ) _\u2212_ log _Z_ ( **w** _, x_ )] _\u2212_ E [ **w** _\u00b7_ **\u03a6** ( _x, y_ ) _\u2212_ log _Z_ ( **w** _, x_ )] ( _x,y_ ) _\u223c_ D [\ufffd] _x\u223c_ D [\ufffd] [1] _y\u223c_ p _[\u2217]_ [ _\u00b7|x_ ] = _\u2212\u03bb\u2225_ **w** _\u2225_ 1 + **w** _\u00b7_ E [ **\u03a6** ( _x, y_ )] _\u2212_ E [ **\u03a6** ( _x, y_ )] _._ \ufffd ( _x,y_ ) _\u223c_ D [\ufffd] _x\u223c_ D [\ufffd] [1] \ufffd _y\u223c_ p _[\u2217]_ [ _\u00b7|x_ ] E _x\u223c_ D\ufffd [1] [ **\u03a6** ( _x, y_ )] = 0, \ufffd _y\u223c_ p _[\u2217]_ [ _\u00b7|x_ ] \ufffd As the solution of the primal optimization, \u00afp _[\u2217]_ verifies _I_ C that is E _x\u223c_ D\ufffd [1] [ **\u03a6** ( _x, y_ )] _\u2212_ E ( _x,y_ ) _\u223c_ D\ufffd [[] **[\u03a6]** [(] _[x, y]_ [)]] _\u2264_ _\u03bb_ . By H\u00a8older\u2019s inequality, this \ufffd\ufffd\ufffd\ufffd _y\u223c_ p _[\u2217]_ [ _\u00b7|x_ ] \ufffd\ufffd\ufffd\ufffd _\u221e_ implies the following inequality: _\u2212\u2225_ **w** _\u2225_ 1 + **w** _\u00b7_ E [ **w** _\u00b7_ **\u03a6** ( _x, y_ )] _\u2212_ E [ **w** _\u00b7_ **\u03a6** ( _x, y_ )]",
    "chunk_id": "foundations_machine_learning_322"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2264\u2212\u2225_ **w** _\u2225_ 1 + _\u2225_ **w** _\u2225_ 1 = 0 _._ \ufffd ( _x,y_ ) _\u223c_ D [\ufffd] _x\u223c_ D [\ufffd] [1] \ufffd _y\u223c_ p _[\u2217]_ [ _\u00b7|x_ ] **330** **Chapter 13** **Conditional Maximum Entropy Models** Thus, we can write, for any **w** _\u2208_ R _[N]_, D(p _[\u2217]_ [ _\u00b7|x_ ] _\u2225_ p 0 [ _\u00b7|x_ ]) _\u2212_ _G_ ( **w** ) _._ \ufffd \ufffd E _x\u223c_ D [\ufffd] [1] D(p _[\u2217]_ [ _\u00b7|x_ ] _\u2225_ p **w** [ _\u00b7|x_ ]) _\u2264_ E \ufffd \ufffd _x\u223c_ D [\ufffd] [1] Now, assume that **w** verifies _|G_ ( **w** ) _\u2212_ sup **w** _\u2208_ R _N_ _G_ ( **w** ) _| \u2264_ _\u03f5_ for some _\u03f5 >_ 0. Then, E _x\u223c_ D\ufffd [1] [[][D][(][p] _[\u2217]_ [[] _[\u00b7|][x]_ []] _[ \u2225]_ [p] [0] [[] _[\u00b7|][x]_ [])]] _[\u2212][G]_ [(] **[w]** [) = (sup] **[w]** _[ G]_ [(] **[w]** [))] _[\u2212][G]_ [(] **[w]** [)] _[ \u2264]_ _[\u03f5]_ [ implies the inequality] E _x\u223c_ D\ufffd [1] [[][D][(][p] _[\u2217]_ [[] _[\u00b7|][x]_ []] _[ \u2225]_ [p] **[w]** [[] _[\u00b7|][x]_ [])]] _[ \u2264]_ _[\u03f5]_ [. This concludes the proof of the theorem.] **13.10** **Chapter notes** The logistic regression model is a classical model in statistics. The term _logistic_ was introduced by the Belgian mathematician Verhulst [1838, 1845]. An early reference for logistic regression is the publication of Berkson [1944] who advocated the use of the logistic function, instead of the cumulative distribution function of the standard normal distribution ( _probit model_ ). Conditional maximum entropy models in natural language processing were introduced by Berger et al. [1996] and were widely adopted for a variety of different tasks, including part-of-speech tagging, parsing, machine translation, and text categorization (see tutorial by Manning and Klein [2003]). Our presentation of the conditional Maxent principle, including their regularized variants, the duality theorem for conditional Maxent models (Theorem 13.1) and their theoretical justifications are based on [Cortes, Kuznetsov, Mohri, and Syed, 2015]. This chapter provided two types of justification for these models: one based on the conditional Maxent principle, another based on standard generalization bounds. As in the case of Maxent models for density estimation, conditional Maxent models can be extended by using other Bregman divergences [Lafferty, Pietra, and Pietra, 1997] and other regularizations. Lafferty [1999] presented a general framework for incremental algorithms based on Bregman divergences that admits logistic regression as as special case, see also [Collins et al., 2002] who showed that boosting and logistic regression were special instances of a common framework based on Bregman divergences. The regularized conditional Maxent models presented in this chapter can be extended similarly using other Bregman divergences. In the binary classification case, when coordinate descent is used to solve the optimization problem of regularized conditional Maxent models, the algorithm coincides with _L_ 1 -regularized AdaBoost modulo the use of the logistic loss instead of the exponential loss. Cortes, Kuznetsov, Mohri, and Syed [2015] presented a more general family of conditional probability models, _conditional structural Maxent models_, for which they also presented a duality theorem and gave strong learning guarantees. These Maxent models are based on feature functions",
    "chunk_id": "foundations_machine_learning_323"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "selected from a union of possibly **13.11** **Exercises** **331** very complex sub-families. The resulting algorithms coincide with the DeepBoost algorithms of Cortes, Mohri, and Syed [2014] in the binary classification case or the multi-class DeepBoost algorithm of Kuznetsov, Mohri, and Syed [2014] in the multiclass classification case, when the logistic function is used as a convex surrogate loss function. **13.11** **Exercises** 13.1 Extension to Bregman divergences. (a) Show how conditional Maxent models can be extended by using arbitrary Bregman divergences instead of the (unnormalized) relative entropy. (b) Prove a duality theorem similar to Theorem 13.1 for theses extensions. (c) Derive theoretical guarantees for these extensions. What additional property is needed for the Bregman divergence so that your learning guarantees hold? 13.2 Stability analysis for _L_ 2 -regularized conditional Maxent. (a) Give an upper bound on the stability of the _L_ 2 -regularized conditional Maxent in terms of the sample size and _\u03bb_ ( _Hint_ : use the techniques and results of Chapter 14). (b) Use the previous question to derive a stability-based generalization guarantee for the algorithm. 13.3 Maximum conditional Maxent. An alternative measure of closeness, instead of the conditional relative entropy, is the maximum relative entropy over all _x \u2208_ X 1 . (a) Write the primal optimization problem for this maximum conditional Maxent formulation. Show that it is a convex optimization problem, and discuss its feasibility and the uniqueness of its solution. (b) Prove a duality theorem for maximum conditional Maxent and write the equivalent dual problem. (c) Analyze the properties of maximum conditional Maxent and give a generalization bound for the algorithm. **332** **Chapter 13** **Conditional Maximum Entropy Models** 13.4 Conditional Maxent with other marginal distributions: discuss and analyze conditional Maxent models when using a distribution Q over X instead of D [\ufffd] [1] . Prove that a duality theorem similar to Theorem 13.1 holds. # 14 Algorithmic Stability In chapters 2\u20135 and several subsequent chapters, we presented a variety of generalization bounds based on different measures of the complexity of the hypothesis set H used for learning, including the Rademacher complexity, the growth function, and the VC-dimension. These bounds ignore the specific algorithm used, that is, they hold for any algorithm using H as a hypothesis set. One may ask if an analysis of the properties of a specific algorithm could lead to finer guarantees. Such an algorithm-dependent analysis could have the benefit of a more informative guarantee. On the other hand, it could be inapplicable to other algorithms using the same hypothesis set. Alternatively, as we shall see in this chapter, a more general property of the learning algorithm could be used to incorporate algorithm-specific properties while extending the applicability of the analysis to other learning algorithms with similar properties. This chapter uses the property of _algorithmic stability_ to derive _algorithm-dependent_ learning guarantees. We first present a generalization bound for any algorithm that is sufficiently stable. Then, we show that the wide class of kernel-based regularization algorithms enjoys this property and derive a general upper bound on their stability coefficient. Finally, we illustrate the application of these",
    "chunk_id": "foundations_machine_learning_324"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "results to the analysis of several algorithms both in the regression and classification settings, including kernel ridge regression (KRR), SVR, and SVMs. **14.1** **Definitions** We start by introducing the notation and definitions relevant to our analysis of algorithmic stability. We denote by _z_ a labeled example ( _x, y_ ) _\u2208_ X _\u00d7_ Y. The hypotheses _h_ we consider map X to a set Y _[\u2032]_ sometimes different from Y. In particular, for classification, we may have Y = _{\u2212_ 1 _,_ +1 _}_ while the hypothesis _h_ learned takes values in R. The loss functions _L_ we consider are therefore defined over Y _[\u2032]_ _\u00d7_ Y, with Y _[\u2032]_ = Y in most cases. For a loss function _L_ : Y _[\u2032]_ _\u00d7_ Y _\u2192_ R +, we denote the loss of **334** **Chapter 14** **Algorithmic Stability** a hypothesis _h_ at point _z_ by _L_ _z_ ( _h_ ) = _L_ ( _h_ ( _x_ ) _, y_ ). We denote by D the distribution according to which samples are drawn and by H the hypothesis set. The empirical error or loss of _h \u2208_ H on a sample _S_ = ( _z_ 1 _, . . ., z_ _m_ ) and its generalization error are defined, respectively, by \ufffd _R_ _S_ ( _h_ ) = [1] _m_ _m_ \ufffd _L_ _z_ _i_ ( _h_ ) and _R_ ( _h_ ) = _z\u223c_ E D [[] _[L]_ _[z]_ [(] _[h]_ [)]] _[.]_ _i_ =1 Given an algorithm _A_, we denote by _h_ _S_ the hypothesis _h_ _S_ _\u2208_ H returned by _A_ when trained on sample _S_ . We will say that the loss function _L_ is bounded by _M \u2265_ 0 if for all _h \u2208_ H and _z \u2208_ X _\u00d7_ Y, _L_ _z_ ( _h_ ) _\u2264_ _M_ . For the results presented in this chapter, a weaker condition suffices, namely that _L_ _z_ ( _h_ _S_ ) _\u2264_ _M_ for all hypotheses _h_ _S_ returned by the algorithm _A_ . We are now able to define the notion of uniform stability, the algorithmic property used in the analyses of this chapter. **Definition 14.1 (Uniform stability)** _Let S and S_ _[\u2032]_ _be any two training samples that dif-_ _fer by a single point. Then, a learning algorithm A is_ uniformly _\u03b2_ -stable _if the_ _hypotheses it returns when trained on any such samples S and S_ _[\u2032]_ _satisfy_ _\u2200z \u2208Z,_ _|L_ _z_ ( _h_ _S_ ) _\u2212_ _L_ _z_ ( _h_ _S_ _\u2032_ ) _| \u2264_ _\u03b2._ _The smallest such \u03b2 satisfying this inequality is called the_ stability coefficient _of A._ In other words, when _A_ is trained on two similar training sets, the losses incurred by the corresponding hypotheses returned by _A_ should not differ by more than _\u03b2_ . Note that a uniformly _\u03b2_ -stable algorithm is often referred to as being _\u03b2-stable_ or even just _stable_ (for some unspecified _\u03b2_ ). In general, the coefficient _\u03b2_ depends on the sample size _m_ . We will see in section 14.2 that _\u03b2_ = _o_ (1 _/_",
    "chunk_id": "foundations_machine_learning_325"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u221a]_ _m_ ~~)~~ is necessary for the convergence of the stability-based learning bounds presented in this chapter. In section 14.3, we will show that a more favorable condition holds, that is, _\u03b2_ = _O_ (1 _/m_ ), for a wide family of algorithms. **14.2** **Stability-based generalization guarantee** In this section, we show that exponential bounds can be derived for the generalization error of stable learning algorithms. The main result is presented in theo rem 14.2. **Theorem 14.2** _Assume that the loss function L is bounded by M \u2265_ 0 _. Let A be a_ _\u03b2-stable learning algorithm and let S be a sample of m points drawn i.i.d. according_ **14.2** **Stability-based generalization guarantee** **335** _to distribution_ D _. Then, with probability at least_ 1 _\u2212_ _\u03b4 over the sample S drawn,_ _the following holds:_ _R_ ( _h_ _S_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ _S_ ) + _\u03b2_ + (2 _m\u03b2_ + _M_ ) \ufffd log [1] _\u03b4_ 2 _m_ _[.]_ log [1] Proof: The proof is based on the application of McDiarmid\u2019s inequality (theorem D.8) to the function \u03a6 defined for all samples _S_ by \u03a6( _S_ ) = _R_ ( _h_ _S_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ _S_ ). Let _S_ _[\u2032]_ be another sample of size _m_ with points drawn i.i.d. according to D that differs from _S_ by exactly one point. We denote that point by _z_ _m_ in _S_, _z_ _m_ _[\u2032]_ [in] _[ S]_ _[\u2032]_ [,] i.e., _S_ = ( _z_ 1 _, . . ., z_ _m\u2212_ 1 _, z_ _m_ ) and _S_ _[\u2032]_ = ( _z_ 1 _, . . ., z_ _m\u2212_ 1 _, z_ _m_ _[\u2032]_ [)] _[.]_ By definition of \u03a6, the following inequality holds: _|_ \u03a6( _S_ _[\u2032]_ ) _\u2212_ \u03a6( _S_ ) _| \u2264|R_ ( _h_ _S_ _\u2032_ ) _\u2212_ _R_ ( _h_ _S_ ) _|_ + _|R_ [\ufffd] _S_ _\u2032_ ( _h_ _S_ _\u2032_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ _S_ ) _|._ (14.1) We bound each of these two terms separately. By the _\u03b2_ -stability of _A_, we have _|R_ ( _h_ _S_ ) _\u2212_ _R_ ( _h_ _S_ _\u2032_ ) _|_ = _|_ E _z_ [[] _[L]_ _[z]_ [(] _[h]_ _[S]_ [)]] _[ \u2212]_ [E] _z_ [[] _[L]_ _[z]_ [(] _[h]_ _[S]_ _[\u2032]_ [)]] _[| \u2264]_ [E] _z_ [[] _[|][L]_ _[z]_ [(] _[h]_ _[S]_ [)] _[ \u2212]_ _[L]_ _[z]_ [(] _[h]_ _[S]_ _[\u2032]_ [)] _[|]_ []] _[ \u2264]_ _[\u03b2.]_ Using the boundedness of _L_ along with _\u03b2_ -stability of _A_, we also have \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd _|R_ [\ufffd] _S_ ( _h_ _S_ ) _\u2212_ _R_ [\ufffd] _S_ _\u2032_ ( _h_ _S_ _\u2032_ ) _|_ = [1] _m_ _|R_ [\ufffd] _S_ ( _h_ _S_ ) _\u2212_ _R_ [\ufffd] _S_ _\u2032_ ( _h_ _S_ _\u2032_ ) _|_ = [1] _m\u2212_ 1 \ufffd \ufffd _\u2212_ \ufffd _L_ _z_ _i_ ( _h_ _S_ ) _\u2212_ _L_ _z_ _i_ ( _h_ _S_ _\u2032_ ) + _L_ _z_ _m_ ( _h_ _S_ ) _\u2212_ _L_ _z_ _m\u2032_ ( _h_ _S_ _\u2032_ ) _i_ =1 \ufffd _m\u2212_ 1 \ufffd",
    "chunk_id": "foundations_machine_learning_326"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd\ufffd _i_ =1 \ufffd _\u2264_ [1] _m_ _\u2212_ \ufffd _|L_ _z_ _i_ ( _h_ _S_ ) _\u2212_ _L_ _z_ _i_ ( _h_ _S_ _\u2032_ ) _|_ + _|L_ _z_ _m_ ( _h_ _S_ ) _\u2212_ _L_ _z_ _m\u2032_ ( _h_ _S_ _\u2032_ ) _|_ _i_ =1 \ufffd _[ \u2212]_ [1] _\u03b2_ + _[M]_ _m_ _m_ _m_ _[.]_ _\u2264_ _[m][ \u2212]_ [1] _[M]_ _m_ _[\u2264]_ _[\u03b2]_ [ +] _[ M]_ Thus, in view of (14.1), \u03a6 satisfies the condition _|_ \u03a6( _S_ ) _\u2212_ \u03a6( _S_ _[\u2032]_ ) _| \u2264_ 2 _\u03b2_ + _[M]_ Thus, in view of (14.1), \u03a6 satisfies the condition _|_ \u03a6( _S_ ) _\u2212_ \u03a6( _S_ _[\u2032]_ ) _| \u2264_ 2 _\u03b2_ + _m_ [. By] applying McDiarmid\u2019s inequality to \u03a6( _S_ ), we can bound the deviation of \u03a6 from its mean as _\u2212_ 2 _m\u03f5_ [2] P \u03a6( _S_ ) _\u2265_ _\u03f5_ + E _\u2264_ exp _,_ \ufffd _S_ [[\u03a6(] _[S]_ [)]] \ufffd \ufffd (2 _m\u03b2_ + _M_ ) [2] \ufffd or, equivalently, with probability 1 _\u2212_ _\u03b4_, \u03a6( _S_ ) _< \u03f5_ + E (14.2) _S_ [[\u03a6(] _[S]_ [)]] _[,]_ _\u2212_ 2 _m\u03f5_ [2] where _\u03b4_ = exp \ufffd (2 _m\u03b2_ + _M_ ) [2] \ufffd. If we solve for _\u03f5_ in this expression for _\u03b4_, plug into (14.2) and rearrange terms, then, with probability 1 _\u2212_ _\u03b4_, we have (2 _m\u03b2_ + _M_ ) [2] _,_ \ufffd \u03a6( _S_ ) _\u2264_ E _S\u223c_ D _[m]_ [[\u03a6(] _[S]_ [)] + (2] _[m\u03b2]_ [ +] _[ M]_ [)] ~~\ufffd~~ log [1] _\u03b4_ (14.3) 2 _m_ _[.]_ log [1] **336** **Chapter 14** **Algorithmic Stability** We now bound the expectation term, first noting that by linearity of expectation E _S_ [\u03a6( _S_ )] = E _S_ [ _R_ ( _h_ _S_ )] _\u2212_ E _S_ [ _R_ [\ufffd] _S_ ( _h_ _S_ )]. By definition of the generalization error, _S\u223c_ E D _[m]_ [[] _[R]_ [(] _[h]_ _[S]_ [)] =] _S\u223c_ E D _[m]_ \ufffd _z\u223c_ E D [[] _[L]_ _[z]_ [(] _[h]_ _[S]_ [)]] \ufffd = _S,z\u223c_ E D _[m]_ [+1] [[] _[L]_ _[z]_ [(] _[h]_ _[S]_ [)]] _[.]_ (14.4) By the linearity of expectation, E [1] _S\u223c_ D _[m]_ [[] _[R]_ [ \ufffd] _[S]_ [(] _[h]_ _[S]_ [)] =] _m_ _m_ \ufffd _S\u223c_ E D _[m]_ [[] _[L]_ _[z]_ _[i]_ [(] _[h]_ _[S]_ [)] =] _S\u223c_ E D _[m]_ [[] _[L]_ _[z]_ [1] [(] _[h]_ _[S]_ [)]] _[,]_ (14.5) _i_ =1 where the second equality follows from the fact that the _z_ _i_ are drawn i.i.d. and thus the expectations E _S\u223c_ D _m_ [ _L_ _z_ _i_ ( _h_ _S_ )], _i \u2208_ [ _m_ ], are all equal. The last expression in (14.5) is the expected loss of a hypothesis on one of its training points. We can rewrite it as E _S\u223cD_ _m_ [ _L_ _z_ 1 ( _h_ _S_ )] = E _S,z\u223c_ D _m_ +1 [ _L_ _z_ ( _h_ _S_ _\u2032_ )], where _S_ _[\u2032]_ is a sample of _m_ points containing _z_ extracted from the _m_ + 1 points formed",
    "chunk_id": "foundations_machine_learning_327"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "by _S_ and _z_ . Thus, in view of (14.4) and by the _\u03b2_ -stability of _A_, it follows that _|_ _S\u223c_ E D _[m]_ [[\u03a6(] _[S]_ [)]] _[|]_ [ =] \ufffd\ufffd _S,z\u223c_ E D _[m]_ [+1] [[] _[L]_ _[z]_ [(] _[h]_ _[S]_ [)]] _[ \u2212]_ _S,z\u223c_ E D _[m]_ [+1] [[] _[L]_ _[z]_ [(] _[h]_ _[S]_ _[\u2032]_ [)]] \ufffd\ufffd _\u2264_ _S,z\u223c_ E D _[m]_ [+1] \ufffd _|L_ _z_ ( _h_ _S_ ) _\u2212_ _L_ _z_ ( _h_ _S_ _\u2032_ ) _|_ \ufffd _\u2264_ E _S,z\u223c_ D _[m]_ [+1] [[] _[\u03b2]_ [] =] _[ \u03b2.]_ We can thus replace E _S_ [\u03a6( _S_ )] by _\u03b2_ in (14.3), which completes the proof. The bound of the theorem converges for ( _m\u03b2_ ) _/_ _[\u221a]_ _m_ = _o_ (1), that is _\u03b2_ = _o_ (1 _/_ _[\u221a]_ _m_ ~~)~~ . In particular, when the stability coefficient _\u03b2_ is in _O_ (1 _/m_ ), the theorem guarantees that _R_ ( _h_ _S_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ _S_ ) = _O_ (1 _/_ _[\u221a]_ _m_ ) with high probability. In the next section, we show that kernel-based regularization algorithms precisely admit this property under some general assumptions. **14.3** **Stability of kernel-based regularization algorithms** Let _K_ be a positive definite symmetric kernel, H the reproducing kernel Hilbert space associated to _K_, and _\u2225\u00b7 \u2225_ _K_ the norm induced by _K_ in H. A kernel-based regularization algorithm is defined by the minimization over H of an objective function _F_ _S_ based on a training sample _S_ = ( _z_ 1 _, . . ., z_ _m_ ) and defined for all _h \u2208_ H by: _F_ _S_ ( _h_ ) = _R_ [\ufffd] _S_ ( _h_ ) + _\u03bb\u2225h\u2225_ _K_ [2] _[.]_ (14.6) In this equation, _R_ [\ufffd] _S_ ( _h_ ) = _m_ [1] \ufffd _mi_ =1 _[L]_ _[z]_ _[i]_ [(] _[h]_ [) is the empirical error of hypothesis] _[ h]_ [ with] respect to a loss function _L_ and _\u03bb \u2265_ 0 a trade-off parameter balancing the emphasis on the empirical error versus the regularization term _\u2225h\u2225_ [2] _K_ [. The hypothesis set][ H] **14.3** **Stability of kernel-based regularization algorithms** **337** is the subset of H formed by the hypotheses possibly returned by the algorithm. Algorithms such as KRR, SVR and SVMs all fall under this general model. We first introduce some definitions and tools needed for a general proof of an upper bound on the stability coefficient of kernel-based regularization algorithms. Our analysis will assume that the loss function _L_ is convex and that it further verifies the following Lipschitz-like smoothness condition. **Definition 14.3 (** _\u03c3_ **-admissibility)** _A loss function L is \u03c3_ -admissible _with respect to the_ _hypothesis class_ H _if there exists \u03c3 \u2208_ R + _such that for any two hypotheses h, h_ _[\u2032]_ _\u2208_ H _and for all_ ( _x, y_ ) _\u2208_ X _\u00d7_ Y _,_ _|L_ ( _h_ _[\u2032]_ ( _x_ ) _, y_ ) _\u2212_ _L_ ( _h_ ( _x_ ) _, y_ ) _| \u2264_ _\u03c3|h_ _[\u2032]_ ( _x_ ) _\u2212_ _h_ (",
    "chunk_id": "foundations_machine_learning_328"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_x_ ) _|._ (14.7) This assumption holds for the quadratic loss and most other loss functions where the hypothesis set and the set of output labels are bounded by some _M \u2208_ R + : _\u2200h \u2208_ H _, \u2200x \u2208_ X _, |h_ ( _x_ ) _| \u2264_ _M_ and _\u2200y \u2208_ Y _, |y| \u2264_ _M_ . We will use the notion of _Bregman divergence_, B _F_ which can be defined for any convex and differentiable function _F_ : H _\u2192_ R as follows: for all _f, g \u2208_ H, B _F_ ( _f_ _\u2225g_ ) = _F_ ( _f_ ) _\u2212_ _F_ ( _g_ ) _\u2212\u27e8f \u2212_ _g, \u2207F_ ( _g_ ) _\u27e9_ _._ Section E.4 presents the properties of the Bregman divergence in more detail and also contains figure E.2 which illustrates the geometric interpretation of the Bregman divergence. We generalize the definition of Bregman divergence to cover the case of convex but non-differentiable loss functions _F_ by using the notion of _sub-_ _gradient_ . For a convex function _F_ : H _\u2192_ R, we denote by _\u2202F_ ( _h_ ) the _subdifferential_ of _F_ at _h_, which is defined as follows: _\u2202F_ ( _h_ ) = _{g \u2208_ H : _\u2200h_ _[\u2032]_ _\u2208_ H _, F_ ( _h_ _[\u2032]_ ) _\u2212_ _F_ ( _h_ ) _\u2265\u27e8h_ _[\u2032]_ _\u2212_ _h, g\u27e9}._ Thus, _\u2202F_ ( _h_ ) is the set of vectors _g_ defining a hyperplane supporting function _F_ at point _h_ (see figure 14.1). Elements of the subdifferential are called subgradients (see section B.4.1 for more discussion). Note, the subgradient found in _\u2202F_ ( _h_ ) coincides with _\u2207F_ ( _h_ ) when _F_ is differentiable at _h_, i.e. _\u2202F_ ( _h_ ) = _{\u2207F_ ( _h_ ) _}_ . Furthermore, at a point _h_ where _F_ is minimal, 0 is an element of _\u2202F_ ( _h_ ). The subgradient is additive, that is, for two convex function _F_ 1 and _F_ 2, _\u2202_ ( _F_ 1 + _F_ 2 )( _h_ ) = _{g_ 1 + _g_ 2 : _g_ 1 _\u2208_ _\u2202F_ 1 ( _h_ ) _, g_ 2 _\u2208_ _\u2202F_ 2 ( _h_ ) _}_ . For any _h \u2208_ H, we fix _\u03b4F_ ( _h_ ) to be an (arbitrary) element of _\u2202F_ ( _h_ ). For any such choice of _\u03b4F_, we can define the _generalized Bregman divergence_ associated to _F_ by: _\u2200h_ _[\u2032]_ _, h \u2208_ H _,_ B _F_ ( _h_ _[\u2032]_ _\u2225_ _h_ ) = _F_ ( _h_ _[\u2032]_ ) _\u2212_ _F_ ( _h_ ) _\u2212\u27e8h_ _[\u2032]_ _\u2212_ _h, \u03b4F_ ( _h_ ) _\u27e9_ _._ (14.8) Note that by definition of the subgradient, B _F_ ( _h_ _[\u2032]_ _\u2225_ _h_ ) _\u2265_ 0 for all _h_ _[\u2032]_ _, h \u2208_ H. Starting from (14.6), we can now define the generalized Bregman divergence of _F_ _S_ . Let _N_ denote the convex function _h \u2192\u2225h\u2225_ [2] _K_ [.] Since _N_ is differentiable, **338** **Chapter 14** **Algorithmic Stability** **Figure 14.1** **Image:** [No caption returned] Illustration of the notion of subgradient: supporting hyperplanes",
    "chunk_id": "foundations_machine_learning_329"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(shown in red, orange and green) for the function _F_ (shown in blue) at point _h_ are defined by elements of the subdifferential _\u2202F_ ( _h_ ). _\u03b4N_ ( _h_ ) = _\u2207N_ ( _h_ ) for all _h \u2208_ H, and thus _\u03b4N_ (as well as B _N_ ) is uniquely defined. To make the definition of the Bregman divergences for _F_ _S_ and _R_ [\ufffd] _S_ compatible so that B _F_ _S_ = B _R_ \ufffd _S_ [+] _[\u03bb]_ [B] _[N]_ [, we define] _[ \u03b4]_ [ \ufffd] _[R]_ _[S]_ [ in terms of] _[ \u03b4F]_ _[S]_ [ by:] _[ \u03b4]_ [ \ufffd] _[R]_ _[S]_ [(] _[h]_ [) =] _[ \u03b4F]_ _[S]_ [(] _[h]_ [)] _[\u2212]_ _[\u03bb][\u2207][N]_ [(] _[h]_ [)] for all _h \u2208_ H. Furthermore, we choose _\u03b4F_ _S_ ( _h_ ) to be 0 for any point _h_ where _F_ _S_ is minimal and let _\u03b4F_ _S_ ( _h_ ) be an arbitrary element of _\u2202F_ _S_ ( _h_ ) for all other _h \u2208_ H. We proceed in a similar way to define the Bregman divergences for _F_ _S_ _\u2032_ and _R_ [\ufffd] _S_ _\u2032_ so that B _F_ _S\u2032_ = B _R_ \ufffd _S\u2032_ [+] _[ \u03bb]_ [B] _[N]_ [.] We will use the notion of generalized Bregman divergence for the proof of the following general upper bound on the stability coefficient of kernel-based regularization algorithms. **Proposition 14.4** _Let K be a positive definite symmetric kernel such that for all_ _x \u2208_ X _, K_ ( _x, x_ ) _\u2264_ _r_ [2] _for some r \u2208_ R + _and let L be a convex and \u03c3-admissible_ _loss function. Then, the kernel-based regularization algorithm defined by the mini-_ _mization_ (14.6) _is \u03b2-stable with the following upper bound on \u03b2:_ _\u03b2 \u2264_ _[\u03c3]_ [2] _[r]_ [2] _m\u03bb_ _[.]_ Proof: Let _h_ be a minimizer of _F_ _S_ and _h_ _[\u2032]_ a minimizer of _F_ _S_ _\u2032_, where samples _S_ and _S_ _[\u2032]_ differ exactly by one point, _z_ _m_ in _S_ and _z_ _m_ _[\u2032]_ [in] _[ S]_ _[\u2032]_ [. Since the generalized Bregman] divergence is non-negative and since B _F_ _S_ = B _R_ \ufffd _S_ [+] _[ \u03bb]_ [B] _[N]_ [ and][ B] _[F]_ _S_ _[\u2032]_ [ =][ B] _R_ [ \ufffd] _S\u2032_ [+] _[ \u03bb]_ [B] _[N]_ [,] we can write B _F_ _S_ ( _h_ _[\u2032]_ _\u2225h_ ) + B _F_ _S\u2032_ ( _h\u2225h_ _[\u2032]_ ) _\u2265_ _\u03bb_ \ufffdB _N_ ( _h_ _[\u2032]_ _\u2225h_ ) + B _N_ ( _h\u2225h_ _[\u2032]_ )\ufffd _._ **14.3** **Stability of kernel-based regularization algorithms** **339** Observe that B _N_ ( _h_ _[\u2032]_ _\u2225h_ ) + B _N_ ( _h\u2225h_ _[\u2032]_ ) = _\u2212\u27e8h_ _[\u2032]_ _\u2212_ _h,_ 2 _h\u27e9\u2212\u27e8h \u2212_ _h_ _[\u2032]_ _,_ 2 _h_ _[\u2032]_ _\u27e9_ = 2 _\u2225h_ _[\u2032]_ _\u2212_ _h\u2225_ [2] _K_ [.] Let \u2206 _h_ denote _h_ _[\u2032]_ _\u2212_ _h_, then we can write 2 _\u03bb||_ \u2206 _h\u2225_ _K_ [2] _\u2264_ B _F_ _S_ ( _h_ _[\u2032]_ _\u2225_ _h_ ) + B _F_ _S\u2032_ ( _h \u2225_ _h_ _[\u2032]_ ) = _F_ _S_ ( _h_ _[\u2032]_",
    "chunk_id": "foundations_machine_learning_330"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ") _\u2212_ _F_ _S_ ( _h_ ) _\u2212\u27e8h_ _[\u2032]_ _\u2212_ _h, \u03b4F_ _S_ ( _h_ ) _\u27e9_ + _F_ _S_ _\u2032_ ( _h_ ) _\u2212_ _F_ _S_ _\u2032_ ( _h_ _[\u2032]_ ) _\u2212\u27e8h \u2212_ _h_ _[\u2032]_ _, \u03b4F_ _S_ _\u2032_ ( _h_ _[\u2032]_ ) _\u27e9_ = _F_ _S_ ( _h_ _[\u2032]_ ) _\u2212_ _F_ _S_ ( _h_ ) + _F_ _S_ _\u2032_ ( _h_ ) _\u2212_ _F_ _S_ _\u2032_ ( _h_ _[\u2032]_ ) = _R_ [\ufffd] _S_ ( _h_ _[\u2032]_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ ) + _R_ [\ufffd] _S_ _\u2032_ ( _h_ ) _\u2212_ _R_ [\ufffd] _S_ _\u2032_ ( _h_ _[\u2032]_ ) _._ The second equality follows from the definition of _h_ _[\u2032]_ and _h_ as minimizers and our choice of the subgradients for minimal points which together imply _\u03b4F_ _S_ _\u2032_ ( _h_ _[\u2032]_ ) = 0 and _\u03b4F_ _S_ ( _h_ ) = 0. The last equality follows from the definitions of _F_ _S_ and _F_ _S_ _\u2032_ . Next, we express the resulting inequality in terms of the loss function _L_ and use the fact that _S_ and _S_ _[\u2032]_ differ by only one point along with the _\u03c3_ -admissibility of _L_ to get 2 _\u03bb\u2225_ \u2206 _h\u2225_ _K_ [2] _[\u2264]_ [1] _m_ [[] _[L]_ _[z]_ _[m]_ [(] _[h]_ _[\u2032]_ [)] _[ \u2212]_ _[L]_ _[z]_ _[m]_ [(] _[h]_ [) +] _[ L]_ _[z]_ _[m][\u2032]_ [(] _[h]_ [)] _[ \u2212]_ _[L]_ _[z]_ _[m][\u2032]_ [(] _[h]_ _[\u2032]_ [)]] _\u2264_ _[\u03c3]_ _m_ [)] _[|]_ []] _[.]_ (14.9) _m_ [[] _[|]_ [\u2206] _[h]_ [(] _[x]_ _[m]_ [)] _[|]_ [ +] _[ |]_ [\u2206] _[h]_ [(] _[x]_ _[\u2032]_ By the reproducing kernel property and the Cauchy-Schwarz inequality, for all _x \u2208_ X, \u2206 _h_ ( _x_ ) = _\u27e8_ \u2206 _h, K_ ( _x, \u00b7_ ) _\u27e9\u2264\u2225_ \u2206 _h\u2225_ _K_ _\u2225K_ ( _x, \u00b7_ ) _\u2225_ _K_ = ~~\ufffd~~ _K_ ( _x, x_ ) _\u2225_ \u2206 _h\u2225_ _K_ _\u2264_ _r\u2225_ \u2206 _h\u2225_ _K_ _._ In view of (14.9), this implies _\u2225_ \u2206 _h\u2225_ _K_ _\u2264_ _\u03bbm\u03c3r_ [. By the] _[ \u03c3]_ [-admissibility of] _[ L]_ [ and the] reproducing property, the following holds: _\u2200z \u2208_ X _\u00d7_ Y _, |L_ _z_ ( _h_ _[\u2032]_ ) _\u2212_ _L_ _z_ ( _h_ ) _| \u2264_ _\u03c3|_ \u2206 _h_ ( _x_ ) _| \u2264_ _r\u03c3\u2225_ \u2206 _h\u2225_ _K_ _,_ which gives _\u2200z \u2208_ X _\u00d7_ Y _, |L_ _z_ ( _h_ _[\u2032]_ ) _\u2212_ _L_ _z_ ( _h_ ) _| \u2264_ _[\u03c3]_ [2] _[r]_ [2] _m\u03bb_ _[,]_ and concludes the proof. Thus, under the assumptions of the proposition, for a fixed _\u03bb_, the stability coefficient of kernel-based regularization algorithms is in _O_ (1 _/m_ ). **14.3.1** **Application to regression algorithms: SVR and KRR** Here, we analyze more specifically two widely used regression algorithms, Support Vector Regression (SVR) and Kernel Ridge Regression (KRR), which are both special instances of the family of kernel-based regularization algorithms. **340** **Chapter 14** **Algorithmic Stability** SVR is based on the _\u03f5_ -insensitive loss _L_ _\u03f5_ defined for all ( _y, y_ _[\u2032]_ ) _\u2208_ Y _\u00d7_ Y by: _L_ _\u03f5_",
    "chunk_id": "foundations_machine_learning_331"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "( _y_ _[\u2032]_ _, y_ ) = 0 if _|y_ _[\u2032]_ _\u2212_ _y| \u2264_ _\u03f5_ ; (14.10) \ufffd _|y_ _[\u2032]_ _\u2212_ _y| \u2212_ _\u03f5_ otherwise _._ We now present a stability-based bound for SVR assuming that _L_ _\u03f5_ is bounded for the hypotheses returned by SVR (which, as we shall later see in lemma 14.7, is indeed the case when the label set Y is bounded). **Corollary 14.5 (Stability-based learning bound for SVR)** _Assume that K_ ( _x, x_ ) _\u2264_ _r_ [2] _for_ _all x \u2208_ X _for some r \u2265_ 0 _and that L_ _\u03f5_ _is bounded by M \u2265_ 0 _. Let h_ _S_ _denote the_ _hypothesis returned by SVR when trained on an i.i.d. sample S of size m. Then,_ _for any \u03b4 >_ 0 _, the following inequality holds with probability at least_ 1 _\u2212_ _\u03b4:_ _[r]_ [2] 2 _r_ 2 _m\u03bb_ [+] \ufffd _\u03bb_ log [1] _\u03b4_ 2 _m_ _[.]_ _R_ ( _h_ _S_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ _S_ ) + _[r]_ [2] 2 _\u03bbr_ [+] _[ M]_ \ufffd\ufffd Proof: We first show that _L_ _\u03f5_ ( _\u00b7_ ) = _L_ _\u03f5_ ( _\u00b7, y_ ) is 1-Lipschitz for any _y \u2208_ Y. For any _y_ _[\u2032]_ _, y_ _[\u2032\u2032]_ _\u2208_ Y, we must consider four cases. First, if _|y_ _[\u2032]_ _\u2212_ _y| \u2264_ _\u03f5_ and _|y_ _[\u2032\u2032]_ _\u2212_ _y| \u2264_ _\u03f5_, then _|L_ _\u03f5_ ( _y_ _[\u2032\u2032]_ ) _\u2212L_ _\u03f5_ ( _y_ _[\u2032]_ ) _|_ = 0. Second, if _|y_ _[\u2032]_ _\u2212y| > \u03f5_ and _|y_ _[\u2032\u2032]_ _\u2212y| > \u03f5_, then _|L_ _\u03f5_ ( _y_ _[\u2032\u2032]_ ) _\u2212L_ _\u03f5_ ( _y_ _[\u2032]_ ) _|_ = _||y_ _[\u2032\u2032]_ _\u2212_ _y| \u2212|y_ _[\u2032]_ _\u2212_ _y|| \u2264|y_ _[\u2032\u2032]_ _\u2212_ _y_ _[\u2032]_ _|_, by the triangle inequality. Third, if _|y_ _[\u2032]_ _\u2212_ _y| \u2264_ _\u03f5_ and _|y_ _[\u2032\u2032]_ _\u2212y| > \u03f5_, then _|L_ _\u03f5_ ( _y_ _[\u2032\u2032]_ ) _\u2212L_ _\u03f5_ ( _y_ _[\u2032]_ ) _|_ = _||y_ _[\u2032\u2032]_ _\u2212y|\u2212\u03f5|_ = _|y_ _[\u2032\u2032]_ _\u2212y|\u2212\u03f5 \u2264|y_ _[\u2032\u2032]_ _\u2212y|\u2212|y_ _[\u2032]_ _\u2212y| \u2264_ _|y_ _[\u2032\u2032]_ _\u2212_ _y_ _[\u2032]_ _|_ . Fourth, if _|y_ _[\u2032\u2032]_ _\u2212_ _y| \u2264_ _\u03f5_ and _|y_ _[\u2032]_ _\u2212_ _y| > \u03f5_, by symmetry the same inequality is obtained as in the previous case. Thus, in all cases, _|L_ _\u03f5_ ( _y_ _[\u2032\u2032]_ _, y_ ) _\u2212L_ _\u03f5_ ( _y_ _[\u2032]_ _, y_ ) _| \u2264|y_ _[\u2032\u2032]_ _\u2212y_ _[\u2032]_ _|_ . This implies in particular that _L_ _\u03f5_ is _\u03c3_ -admissible with _\u03c3_ = 1 for any hypothesis set H. By proposition 14.4, under _r_ [2] the assumptions made, SVR is _\u03b2_ -stable with _\u03b2 \u2264_ _m\u03bb_ [. Plugging this expression into] the bound of theorem 14.2 yields the result. We next present a stability-based bound for KRR, which is based on the square loss _L_ 2 defined for all _y_ _[\u2032]_ _, y \u2208_ Y by: _L_ 2 ( _y_ _[\u2032]_ _, y_ ) = ( _y_ _[\u2032]_ _\u2212_ _y_ ) [2] _._ (14.11) As in the SVR setting, we assume in our analysis that _L_ 2 is bounded for the hypotheses returned by KRR (which,",
    "chunk_id": "foundations_machine_learning_332"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "as we shall later see again in lemma 14.7, is indeed the case when the label set Y is bounded). **Corollary 14.6 (Stability-based learning bound for KRR)** _Assume that K_ ( _x, x_ ) _\u2264_ _r_ [2] _for_ _all x \u2208_ X _for some r \u2265_ 0 _and that L_ 2 _is bounded by M \u2265_ 0 _. Let h_ _S_ _denote the_ _hypothesis returned by KRR when trained on an i.i.d. sample S of size m. Then,_ _for any \u03b4 >_ 0 _, the following inequality holds with probability at least_ 1 _\u2212_ _\u03b4:_ _R_ ( _h_ _S_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ _S_ ) + [4] _[Mr]_ [2] 2 + _M_ _\u03bb_ \ufffd\ufffd _[Mr]_ [2] 8 _Mr_ 2 + _\u03bbm_ \ufffd _\u03bb_ log [1] _\u03b4_ 2 _m_ _[.]_ **14.3** **Stability of kernel-based regularization algorithms** **341** Proof: For any ( _x, y_ ) _\u2208_ X _\u00d7_ Y and _h, h_ _[\u2032]_ _\u2208_ H, _|L_ 2 ( _h_ _[\u2032]_ ( _x_ ) _, y_ ) _\u2212_ _L_ 2 ( _h_ ( _x_ ) _, y_ ) _|_ = \ufffd\ufffd( _h_ _\u2032_ ( _x_ ) _\u2212_ _y_ ) 2 _\u2212_ ( _h_ ( _x_ ) _\u2212_ _y_ ) 2 \ufffd\ufffd = \ufffd\ufffd\ufffd\ufffd _h_ _[\u2032]_ ( _x_ ) _\u2212_ _h_ ( _x_ )][( _h_ _[\u2032]_ ( _x_ ) _\u2212_ _y_ ) + ( _h_ ( _x_ ) _\u2212_ _y_ )\ufffd [\ufffd] \ufffd\ufffd _\u2264_ ( _|h_ _[\u2032]_ ( _x_ ) _\u2212_ _y|_ + _|h_ ( _x_ ) _\u2212_ _y|_ ) _|h_ ( _x_ ) _\u2212_ _h_ _[\u2032]_ ( _x_ ) _|_ _\u2264_ 2 _\u221a_ _M_ _|h_ ( _x_ ) _\u2212_ _h_ _[\u2032]_ ( _x_ ) _|,_ where we used the _M_ -boundedness of the loss. Thus, _L_ 2 is _\u03c3_ -admissible with 4 _r_ [2] _M_ _\u03c3_ = 2 _\u221aM_ . Therefore, by proposition 14.4, KRR is _\u03b2_ -stable with _\u03b2 \u2264_ _m\u03bb_ [.] Plugging this expression into the bound of theorem 14.2 yields the result. The previous two corollaries assumed bounded loss functions. We now present a lemma that implies in particular that the loss functions used by SVR and KRR are bounded when the label set is bounded. **Lemma 14.7** _Assume that K_ ( _x, x_ ) _\u2264_ _r_ [2] _for all x \u2208_ X _for some r \u2265_ 0 _and that for_ _all y \u2208_ Y _, L_ (0 _, y_ ) _\u2264_ _B for some B \u2265_ 0 _. Then, the hypothesis h_ _S_ _returned by a_ _kernel-based regularization algorithm trained on a sample S is bounded as follows:_ _\u2200x \u2208_ X _, |h_ _S_ ( _x_ ) _| \u2264_ _r_ ~~\ufffd~~ _B/\u03bb._ Proof: By the reproducing kernel property and the Cauchy-Schwarz inequality, we can write _\u2200x \u2208_ X _, |h_ _S_ ( _x_ ) _|_ = _\u27e8h_ _S_ _, K_ ( _x, \u00b7_ ) _\u27e9\u2264\u2225h_ _S_ _\u2225_ _K_ \ufffd _K_ ( _x, x_ ) _\u2264_ _r\u2225h_ _S_ _\u2225_ _K_ _._ (14.12) The minimization (14.6) is over H, which includes 0. Thus, by definition of _F_ _S_ and _h_ _S_, the following inequality holds: _F_ _S_ ( _h_ _S_",
    "chunk_id": "foundations_machine_learning_333"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ") _\u2264_ _F_ _S_ (0) = [1] _m_ _m_ \ufffd _L_ (0 _, y_ _i_ ) _\u2264_ _B._ _i_ =1 Since the loss _L_ is non-negative, we have _\u03bb\u2225h_ _S_ _\u2225_ [2] _K_ _[\u2264]_ _[F]_ _[S]_ [(] _[h]_ _[S]_ [) and thus] _[ \u03bb][\u2225][h]_ _[S]_ _[\u2225]_ [2] _K_ _[\u2264]_ _[B]_ [.] Combining this inequality with (14.12) yields the result. **14.3.2** **Application to classification algorithms: SVMs** This section presents a generalization bound for SVMs, when using the standard hinge loss defined for all _y \u2208_ Y = _{\u2212_ 1 _,_ +1 _}_ and _y_ _[\u2032]_ _\u2208_ R by _L_ hinge ( _y_ _[\u2032]_ _, y_ ) = 0 if 1 _\u2212_ _yy_ _[\u2032]_ _\u2264_ 0; (14.13) \ufffd1 _\u2212_ _yy_ _[\u2032]_ otherwise _._ **Corollary 14.8 (Stability-based learning bound for SVMs)** _Assume that K_ ( _x, x_ ) _\u2264_ _r_ [2] _for_ _all x \u2208_ X _for some r \u2265_ 0 _. Let h_ _S_ _denote the hypothesis returned by SVMs when_ **342** **Chapter 14** **Algorithmic Stability** _trained on an i.i.d. sample S of size m. Then, for any \u03b4 >_ 0 _, the following inequality_ _holds with probability at least_ 1 _\u2212_ _\u03b4:_ _[r]_ [2] 2 _r_ 2 _m\u03bb_ [+] \ufffd _\u03bb_ + 1 _\u03bb_ \ufffd ~~\ufffd~~ _R_ ( _h_ _S_ ) _\u2264_ _R_ [\ufffd] _S_ ( _h_ _S_ ) + _[r]_ [2] _r_ _r_ _\u03bb_ [+] _\u221a_ log [1] _\u03b4_ 2 _m_ _[.]_ Proof: It is straightforward to verify that _L_ hinge ( _\u00b7, y_ ) is 1-Lipschitz for any _y \u2208_ Y and therefore that it is _\u03c3_ -admissible with _\u03c3_ = 1. Therefore, by proposition 14.4, SVMs _r_ [2] is _\u03b2_ -stable with _\u03b2 \u2264_ _m\u03bb_ [. Since] _[ |][L]_ [hinge] [(0] _[, y]_ [)] _[| \u2264]_ [1 for any] _[ y][ \u2208]_ [Y][, by lemma 14.7,] _\u2200x \u2208_ X _, |h_ _S_ ( _x_ ) _| \u2264_ _r/\u221a\u03bb_ . Thus, for any sample _S_ and any _x \u2208_ X and _y \u2208_ Y, the loss is bounded as follows: _L_ hinge ( _h_ _S_ ( _x_ ) _, y_ ) _\u2264_ _r/\u221a\u03bb_ + 1. Plugging this value of _M_ and the one found for _\u03b2_ into the bound of theorem 14.2 yields the result. Since the hinge loss upper bounds the binary loss, the bound of the corollary 14.8 also applies to the generalization error of _h_ _S_ measured in terms of the standard binary loss used in classification. **14.3.3** **Discussion** Note that the learning bounds presented for kernel-based regularization algorithms 1 are of the form _R_ ( _h_ _S_ ) _\u2212_ _R_ [\ufffd] _S_ ( _h_ _S_ ) _\u2264_ _O_ \ufffd _\u03bb_ ~~_[\u221a]_~~ _m_ \ufffd. Thus, these bounds are informative only when _\u03bb \u226b_ 1 _/_ _[\u221a]_ _m_ . The regularization parameter _\u03bb_ is a function of the sample size _m_ : for larger values of _m_, it is expected to be smaller, decreasing the emphasis on regularization. The magnitude of _\u03bb_ affects the norm of the linear hypotheses used for prediction, with a larger value of _\u03bb_ implying a smaller hypothesis norm. In this sense, _\u03bb_ is a",
    "chunk_id": "foundations_machine_learning_334"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "measure of the complexity of the hypothesis set and the condition required for _\u03bb_ can be interpreted as stating that a less complex hypothesis set guarantees better generalization. Note also that our analysis of stability in this chapter assumed a fixed _\u03bb_ : the regularization parameter is assumed to be invariant to the change of one point of the training sample. While this is a mild assumption, it may not hold in general. **14.4** **Chapter notes** The notion of algorithmic stability was first used by Devroye, Rogers and Wagner [Rogers and Wagner, 1978, Devroye and Wagner, 1979a,b] for the _k_ -nearest neighbor algorithm and other _k_ -local rules. Kearns and Ron [1999] later gave a formal definition of stability and used it to provide an analysis of the leave-oneout error. Much of the material presented in this chapter is based on Bousquet and Elisseeff [2002]. Our proof of proposition 14.4 is novel and generalizes the results of Bousquet and Elisseeff [2002] to the case of non-differentiable convex losses. Moreover, stability-based generalization bounds have been extended to ranking algorithms [Agarwal and Niyogi, 2005, Cortes et al., 2007b], as well as to the non-i.i.d. **14.5** **Exercises** **343** scenario of stationary \u03a6- and _\u03b2_ -mixing processes [Mohri and Rostamizadeh, 2010], and to the transductive setting [Cortes et al., 2008a]. Additionally, exercise 14.5 is based on Cortes et al. [2010b], which introduces and analyzes stability with respect to the choice of the kernel function or kernel matrix. Note that while, as shown in this chapter, uniform stability is sufficient for deriving generalization bounds, it is not a necessary condition. Some algorithms may generalize well in the supervised learning scenario but may not be uniformly stable, for example, the Lasso algorithm [Xu et al., 2008]. Shalev-Shwartz et al. [2009] have used the notion of stability to provide necessary and sufficient conditions for a technical condition of learnability related to PAC-learning, even in general scenarios where learning is possible only by using non-ERM rules. **14.5** **Exercises** 14.1 Tighter stability bounds (a) Assuming the conditions of theorem 14.2 hold, can one hope to guarantee a generalization with slack better than _O_ (1 _/_ _[\u221a]_ _m_ ) even if the algorithm is very stable, i.e. _\u03b2 \u2192_ 0? (b) Can you show an _O_ (1 _/m_ ) generalization guarantee if _L_ is bounded by _C/_ _[\u221a]_ _m_ (a very strong condition)? If so, how stable does the learning algorithm need to be? 14.2 Quadratic hinge loss stability. Let _L_ denote the quadratic hinge loss function defined for all _y \u2208{_ +1 _, \u2212_ 1 _}_ and _y_ _[\u2032]_ _\u2208_ R by _L_ ( _y_ _[\u2032]_ _, y_ ) = 0 if 1 _\u2212_ _y_ _[\u2032]_ _y \u2264_ 0; \ufffd(1 _\u2212_ _y_ _[\u2032]_ _y_ ) [2] otherwise _._ Assume that _L_ ( _h_ ( _x_ ) _, y_ ) is bounded by _M_, 1 _\u2264_ _M < \u221e_, for all _h \u2208_ H, _x \u2208_ X, and _y \u2208{_ +1 _, \u2212_ 1 _}_, which also implies a bound on _|h_ ( _x_ ) _|_ for all _h \u2208_ H and",
    "chunk_id": "foundations_machine_learning_335"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_x \u2208_ X. Derive a stability-based generalization bound for SVMs with the quadratic hinge loss. 14.3 Stability of linear regression. (a) How does the stability bound in corollary 14.6 for ridge regression (i.e. kernel ridge regression with a linear kernel) behave as _\u03bb \u2192_ 0? (b) Can you show a stability bound for linear regression (i.e. ridge regression with _\u03bb_ = 0)? If not, show a counter-example. **344** **Chapter 14** **Algorithmic Stability** 14.4 Kernel stability. Suppose an approximation of the kernel matrix **K**, denoted **K** _[\u2032]_, is used to train the hypothesis _h_ _[\u2032]_ (and let _h_ denote the non-approximate hypothesis). At test time, no approximation is made, so if we let **k** _x_ = \ufffd _K_ ( _x, x_ 1 ) _, . . .,_ _K_ ( _x, x_ _m_ )\ufffd _\u22a4_ we can write _h_ ( _x_ ) = _**\u03b1**_ _\u22a4_ **k** _x_ and _h_ _\u2032_ ( _x_ ) = _**\u03b1**_ _\u2032\u22a4_ **k** _x_ . Show that if _\u2200x, x_ _[\u2032]_ _\u2208_ X _, K_ ( _x, x_ _[\u2032]_ ) _\u2264_ _r_ then _|h_ _[\u2032]_ ( _x_ ) _\u2212_ _h_ ( _x_ ) _| \u2264_ _[rmM]_ _\u2225_ **K** _[\u2032]_ _\u2212_ **K** _\u2225_ 2 _._ _\u03bb_ [2] ( _Hint_ : Use exercise 10.3) 14.5 Stability of relative-entropy regularization. (a) Consider an algorithm that selects a distribution _g_ over a hypothesis class which is parameterized by _\u03b8 \u2208_ \u0398. Given a point _z_ = ( _x, y_ ) the expected loss is defined as _H_ ( _g, z_ ) = _L_ ( _h_ _\u03b8_ ( _x_ ) _, y_ ) _g_ ( _\u03b8_ ) _d\u03b8,_ \ufffd \u0398 with respect to a base loss function _L_ . Assuming the loss function _L_ is bounded by _M_, show that the expected loss _H_ is _M_ -admissible, i.e. show _|H_ ( _g, z_ ) _\u2212_ _H_ ( _g_ _[\u2032]_ _, z_ ) _| \u2264_ _M_ \ufffd \u0398 _[|][g]_ [(] _[\u03b8]_ [)] _[ \u2212]_ _[g]_ _[\u2032]_ [(] _[\u03b8]_ [)] _[|][ d\u03b8]_ [.] (b) Consider an algorithm that minimizes the _entropy regularized_ objective over the choice of distribution _g_ : _F_ _S_ ( _g_ ) = [1] _m_ _m_ \ufffd _H_ ( _g, z_ _i_ ) _i_ =1 + _\u03bbK_ ( _g, f_ 0 ) _._ \ufffd ~~\ufffd~~ \ufffd ~~\ufffd~~ _R_ \ufffd _S_ ( _g_ ) Here, _K_ is the Kullback-Leibler divergence (or relative entropy) between two distributions, _g_ ( _\u03b8_ ) log _[g]_ [(] _[\u03b8]_ [)] \u0398 _f_ 0 ( _\u03b8_ _K_ ( _g, f_ 0 ) = \ufffd _f_ 0 ( _\u03b8_ ) _[d\u03b8,]_ (14.14) and _f_ 0 is some fixed distribution. Show that such an algorithm is stable by performing the following steps: i. First use the fact [1] i. First use the fact [1] 2 [(] \ufffd \u0398 _[|][g]_ [(] _[\u03b8]_ [)] _[\u2212][g]_ _[\u2032]_ [(] _[\u03b8]_ [)] _[|][ d\u03b8]_ [)] [2] _[ \u2264]_ _[K]_ [(] _[g, g]_ _[\u2032]_ [) (Pinsker\u2019s inequality),] to show 2 \ufffd [\ufffd] \u0398 _|g_ _S_ ( _\u03b8_ ) _\u2212_ _g_ _S_ _[\u2032]_ ( _\u03b8_ ) _| d\u03b8_ \ufffd _\u2264_ _B_ _K_ ( _.,f_ 0 ) ( _g\u2225g_ _[\u2032]_ )",
    "chunk_id": "foundations_machine_learning_336"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "+ _B_ _K_ ( _.,f_ 0 ) ( _g_ _[\u2032]_ _\u2225g_ ) _._ [1] 2 [(] \ufffd 2 \u0398 _|g_ _S_ ( _\u03b8_ ) _\u2212_ _g_ _S_ _[\u2032]_ ( _\u03b8_ ) _| d\u03b8_ \ufffd _\u2264_ _B_ _K_ ( _.,f_ 0 ) ( _g\u2225g_ _[\u2032]_ ) + _B_ _K_ ( _.,f_ 0 ) ( _g_ _[\u2032]_ _\u2225g_ ) _._ **14.5** **Exercises** **345** ii. Next, let _g_ be the minimizer of _F_ _S_ and _g_ _[\u2032]_ the minimizer of _F_ _S_ _\u2032_, where _S_ and _S_ _[\u2032]_ differ only at the index _m_ . Show that _B_ _K_ ( _.,f_ 0 ) ( _g\u2225g_ _[\u2032]_ ) + _B_ _K_ ( _.,f_ 0 ) ( _g_ _[\u2032]_ _\u2225g_ ) 1 _\u2264_ _m\u03bb_ \ufffd\ufffd _H_ ( _g_ _\u2032_ _, z_ _m_ ) _\u2212_ _H_ ( _g, z_ _m_ ) + _H_ ( _g, z_ _m\u2032_ [)] _[ \u2212]_ _[H]_ [(] _[g]_ _[\u2032]_ _[, z]_ _m_ _[\u2032]_ [)] \ufffd\ufffd _\u2264_ [2] _[M]_ _m\u03bb_ _|g_ ( _\u03b8_ ) _\u2212_ _g_ _[\u2032]_ ( _\u03b8_ ) _| d\u03b8 ._ \ufffd \u0398 iii. Finally, combine the results above to show that the entropy regularized algorithm is [2] _m\u03bb_ _[M]_ [ 2] [-stable.] # 15 Dimensionality Reduction In settings where the data has a large number of features, it is often desirable to reduce its dimension, or to find a lower-dimensional representation preserving some of its properties. The key arguments for dimensionality reduction (or manifold learning) techniques are: _\u2022_ _Computational_ : to compress the initial data as a preprocessing step to speed up subsequent operations on the data. _\u2022_ _Visualization_ : to visualize the data for exploratory analysis by mapping the input data into two- or three-dimensional spaces. _\u2022_ _Feature extraction_ : to hopefully generate a smaller and more effective or useful set of features. The benefits of dimensionality reduction are often illustrated via simulated data, such as the Swiss roll dataset. In this example, the input data, depicted in figure 15.1a, is three-dimensional, but it lies on a two-dimensional manifold that is \u201cunfolded\u201d in two-dimensional space as shown in figure 15.1b. It is important to note, however, that exact low-dimensional manifolds are rarely encountered in practice. Hence, this idealized example is more useful to illustrate the concept of dimensionality reduction than to verify the effectiveness of dimensionality reduction algorithms. Dimensionality reduction can be formalized as follows. Consider a sample _S_ = ( _x_ 1 _, . . ., x_ _m_ ), a feature mapping **\u03a6** : X _\u2192_ R _[N]_ and the data matrix **X** _\u2208_ R _[N]_ _[\u00d7][m]_ defined as ( **\u03a6** ( _x_ 1 ) _, . . .,_ **\u03a6** ( _x_ _m_ )). The _i_ th data point is represented by **x** _i_ = **\u03a6** ( _x_ _i_ ), or the _i_ th column of **X**, which is an _N_ -dimensional vector. Dimensionality reduction techniques broadly aim to find, for _k \u226a_ _N_, a _k_ -dimensional representation of the data, **Y** _\u2208_ R _[k][\u00d7][m]_, that is in some way faithful to the original representation **X** . In this chapter we will discuss various techniques that address this problem.",
    "chunk_id": "foundations_machine_learning_337"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "We first present the most commonly used dimensionality reduction technique called _principal component analysis_ (PCA). We then introduce a kernelized version of PCA (KPCA) and show the connection between KPCA and manifold learning **348** **Chapter 15** **Dimensionality Reduction** **Image:** [No caption returned] (a) (b) **Figure 15.1** The \u201cSwiss roll\u201d dataset. (a) high-dimensional representation. (b) lower-dimensional representation. algorithms. We conclude with a presentation of the Johnson-Lindenstrauss lemma, a classical theoretical result that has inspired a variety of dimensionality reduction methods based on the concept of random projections. The discussion in this chapter relies on basic matrix properties that are reviewed in appendix A. **15.1** **Principal component analysis** Fix _k \u2208_ [ _N_ ] and let **X** be a mean-centered data matrix, that is, [\ufffd] _[m]_ _i_ =1 **[x]** _[i]_ [ =] **[ 0]** [. Define] _P_ _k_ as the set of _N_ -dimensional rank- _k_ orthogonal projection matrices. PCA consists of projecting the _N_ -dimensional input data onto the _k_ -dimensional linear subspace that minimizes _reconstruction error_, that is the sum of the squared _L_ 2 -distances between the original data and the projected data. Thus, the PCA algorithm is completely defined by the orthogonal projection matrix solution **P** _[\u2217]_ of the following minimization problem: min _F_ _[.]_ (15.1) **P** _\u2208P_ _k_ _[\u2225]_ **[PX]** _[ \u2212]_ **[X]** _[\u2225]_ [2] The following theorem shows that PCA coincides with the projection of each data point onto the _k_ top singular vectors of the sample covariance matrix, i.e., 1 **C** = _m_ **[XX]** _[\u22a4]_ [for the mean-centered data matrix] **[ X]** [.] Figure 15.2 illustrates the basic intuition behind PCA, showing how two-dimensional data points with highly correlated features can be more succinctly represented with a one-dimensional representation that captures most of the variance in the data. **Theorem 15.1** _Let_ **P** _[\u2217]_ _\u2208P_ _k_ _be the PCA solution, i.e., the orthogonal projection_ _matrix solution of_ (15.1) _. Then,_ **P** _[\u2217]_ = **U** _k_ **U** _[\u22a4]_ _k_ _[, where]_ **[ U]** _[k]_ _[ \u2208]_ [R] _[N]_ _[\u00d7][k]_ _[ is the matrix]_ **15.2** **Kernel principal component analysis (KPCA)** **349** _formed by the top k singular vectors of_ **C** = _m_ [1] **[XX]** _[\u22a4]_ _[, the sample covariance matrix]_ _corresponding to_ **X** _. Moreover, the associated k-dimensional representation of_ **X** _is_ _given by_ **Y** = **U** _[\u22a4]_ _k_ **[X]** _[.]_ Proof: Let **P** = **P** _[\u22a4]_ be an orthogonal projection matrix. By the definition of the Frobenius norm, the linearity of the trace operator and the fact that **P** is idempotent, i.e., **P** [2] = **P**, we observe that _\u2225_ **PX** _\u2212_ **X** _\u2225_ _F_ [2] [= Tr[(] **[PX]** _[ \u2212]_ **[X]** [)] _[\u22a4]_ [(] **[PX]** _[ \u2212]_ **[X]** [)] = Tr[] **[X]** _[\u22a4]_ **[P]** [2] **[X]** _[ \u2212]_ [2] **[X]** _[\u22a4]_ **[PX]** [ +] **[ X]** _[\u22a4]_ **[X]** []] = _\u2212_ Tr[ **X** _[\u22a4]_ **PX** ] + Tr[ **X** _[\u22a4]_ **X** ] _._ Since Tr[ **X** _[\u22a4]_ **X** ] is a constant with respect to **P**, we have argmin _\u2225_ **PX** _\u2212_ **X** _\u2225_ _F_ [2] [= argmax] Tr[ **X** _[\u22a4]_ **PX** ] _._ (15.2) **P** _\u2208P_ _k_",
    "chunk_id": "foundations_machine_learning_338"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**P** _\u2208P_ _k_ By definition of orthogonal projections in _P_ _k_, **P** = **UU** _[\u22a4]_ for some **U** _\u2208_ R _[N]_ _[\u00d7][k]_ containing orthogonal columns. Using the invariance of the trace operator under cyclic permutations and the orthogonality of the columns of **U**, we have Tr[ **X** _[\u22a4]_ **PX** ] = Tr[ **U** _[\u22a4]_ **XX** _[\u22a4]_ **U** ] = _k_ \ufffd **u** _[\u22a4]_ _i_ **[XX]** _[\u22a4]_ **[u]** _[i]_ _[,]_ _i_ =1 where **u** _i_ is the _i_ th column of **U** . By the Rayleigh quotient (section A.2.3), it is clear that the largest _k_ singular vectors of **XX** _[\u22a4]_ maximize the rightmost sum above. Since **XX** _[\u22a4]_ and **C** differ only by a scaling factor, they have the same singular vectors, and thus **U** _k_ maximizes this sum, which proves the first statement of the theorem. Finally, since **PX** = **U** _k_ **U** _[\u22a4]_ _k_ **[X]** [,] **[ Y]** [ =] **[ U]** _[\u22a4]_ _k_ **[X]** [ is a] _[ k]_ [-dimensional] representation of **X** with **U** _k_ as the basis vectors. By definition of the covariance matrix, the top singular vectors of **C** are the directions of maximal variance in the data, and the associated singular values are equal to these variances. Hence, PCA can also be viewed as projecting onto the subspace of maximal variance. Under this interpretation, the first principal component is derived from projection onto the direction of maximal variance, given by the top singular vector of **C** . Similarly, the _i_ th principal component, for 1 _\u2264_ _i \u2264_ _k_, is derived from projection onto the _i_ th direction of maximal variance, subject to orthogonality constraints to the previous _i \u2212_ 1 directions of maximal variance (see exercise 15.1 for more details). **15.2** **Kernel principal component analysis (KPCA)** In the previous section, we presented the PCA algorithm, which involved projecting onto the singular vectors of the sample covariance matrix **C** . In this section, we **350** **Chapter 15** **Dimensionality Reduction** **Image:** [No caption returned] **Image:** [No caption returned] (a) (b) **Figure 15.2** Example of PCA. (a) Two-dimensional data points with features capturing shoe size measured with different units. (b) One-dimensional representation that captures the most variance in the data, generated by projecting onto largest principal component (red line) of the mean-centered data points. present a kernelized version of PCA, called KPCA. In the KPCA setting, **\u03a6** is a feature mapping to an arbitrary RKHS (not necessarily to R _[N]_ ) and we work exclusively with a kernel function _K_ corresponding to the inner product in this RKHS. The KPCA algorithm can thus be defined as a generalization of PCA in which the input data is projected onto the top principle components in this RKHS. We will show the relationship between PCA and KPCA by drawing upon the deep connections among the SVDs of **X**, **C** and **K** . We then illustrate how various manifold learning algorithms can be interpreted as special instances of KPCA. Let _K_ be a PDS kernel defined over X _\u00d7_ X and define the kernel matrix as **K** = **X** _[\u22a4]_ **X**",
    "chunk_id": "foundations_machine_learning_339"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ". Since **X** admits the following singular value decomposition: **X** = **U\u03a3V** _[\u22a4]_, **C** and **K** can be rewritten as follows: **C** = [1] **K** = **V\u039bV** _[\u22a4]_ _,_ (15.3) _m_ **[U\u039bU]** _[\u22a4]_ where **\u039b** = **\u03a3** [2] is the diagonal matrix of the singular values (equivalently eigenvalues) of _m_ **C** and **U** is the matrix of the singular vectors (equivalently eigenvectors) of **C** (and _m_ **C** ). Starting with the SVD of **X**, note that right multiplying by **V\u03a3** _[\u2212]_ [1] and using the relationship between **\u039b** and **\u03a3** yields **U** = **XV\u039b** _[\u2212]_ [1] _[/]_ [2] . Thus, the singular vector **u** of **C** associated to the singular value _\u03bb/m_ coincides with **[Xv]** ~~_\u221a_~~ _\u03bb_ [, where] **[ v]** [ is the singular] vector of **K** associated to _\u03bb_ . Now fix an arbitrary feature vector **x** = **\u03a6** ( _x_ ) for _x \u2208_ X. Then, following the expression for **Y** in theorem 15.1, the one-dimensional **15.3** **KPCA and manifold learning** **351** representation of **x** derived by projection onto **P** _u_ = **uu** _[\u22a4]_ is defined by = **[k]** _x_ _[\u22a4]_ **[v]** _\u03bb_ _\u221a\u03bb_ **x** _[\u22a4]_ **u** = **x** _[\u22a4]_ **[Xv]** _\u221a\u03bb_ _,_ (15.4) _\u03bb_ where **k** _x_ = ( _K_ ( _x_ 1 _, x_ ) _, . . ., K_ ( _x_ _m_ _, x_ )) _[\u22a4]_ . If **x** is one of the data points, i.e., **x** = **x** _i_ for 1 _\u2264_ _i \u2264_ _m_, then **k** _x_ is the _i_ th column of **K** and (15.4) can be simplified as follows: **x** _[\u22a4]_ **u** = **[k]** _x_ _[\u22a4]_ **[v]** _\u221a\u03bb_ **[v]** = _[\u03bbv]_ _[i]_ _\u03bb_ _\u221a\u03bb_ _[i]_ = _\u221a_ _\u03bb_ _\u03bbv_ _i_ _,_ (15.5) where _v_ _i_ is the _i_ th component of **v** . More generally, the PCA solution of theorem 15.1 can be fully defined by the top _k_ singular vectors (or eigenvectors) of **K**, **v** 1 _, . . .,_ **v** _k_, and the corresponding singular values (or eigenvalues). This alternative derivation of the PCA solution in terms of **K** precisely defines the KPCA solution, providing a generalization of PCA via the use of PDS kernels (see chapter 6 for more details on kernel methods). **15.3** **KPCA and manifold learning** Several manifold learning techniques have been proposed as non-linear methods for dimensionality reduction. These algorithms implicitly assume that high-dimensional data lie on or near a low-dimensional non-linear manifold embedded in the input space. They aim to learn this manifold structure by finding a low-dimensional space that in some way preserves the local structure of high-dimensional input data. For instance, the Isomap algorithm aims to preserve approximate geodesic distances, or distances along the manifold, between all pairs of data points. Other algorithms, such as Laplacian eigenmaps and locally linear embedding, focus only on preserving local neighborhood relationships in the high-dimensional space. We will next describe these classical manifold learning algorithms and then interpret them as specific instances of KPCA. **15.3.1** **Isomap** _Isomap_ aims to extract a low-dimensional data representation that best preserves all pairwise distances between input points, as measured by their",
    "chunk_id": "foundations_machine_learning_340"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "geodesic distances along the underlying manifold. It approximates geodesic distance assuming that _L_ 2 distance provides good approximations for nearby points, and for faraway points it estimates distance as a series of hops between neighboring points. The Isomap algorithm works as follows: **352** **Chapter 15** **Dimensionality Reduction** 1. Find the _t_ nearest neighbors for each data point based on _L_ 2 distance and construct an undirected neighborhood graph, denoted by _G_, with points as nodes and links between neighbors as edges. 2. Compute the approximate geodesic distances, \u2206 _ij_, between all pairs of nodes ( _i, j_ ) by computing all-pairs shortest distances in _G_ using, for instance, the Floyd-Warshall algorithm. 3. Convert the squared distance matrix into a _m\u00d7m_ similarity matrix by performing double centering, i.e., compute **K** Iso = _\u2212_ 2 [1] **[H]** [\u2206] **[H]** [, where \u2206is the squared] distance matrix, **H** = **I** _m_ _\u2212_ _m_ [1] **[11]** _[\u22a4]_ [is the centering matrix,] **[ I]** _[m]_ [ is the] _[ m][ \u00d7][ m]_ identity matrix and **1** is a column vector of all ones (for more details on double centering see exercise 15.2). 4. Find the optimal _k_ -dimensional representation, **Y** = _{_ **y** _i_ _}_ _[n]_ _i_ =1 [, such that] **[ Y]** [ =] argmin **Y** _\u2032_ [\ufffd] \ufffd _\u2225_ **y** _i_ _[\u2032]_ _[\u2212]_ **[y]** _[\u2032]_ _[\u2225]_ 2 [2] _[\u2212]_ [\u2206] [2] \ufffd _._ The solution is given by, ing double centering, i.e., compute **K** Iso = _\u2212_ 2 **[H]** [\u2206] **[H]** [, where \u2206is the squared] distance matrix, **H** = **I** _m_ _\u2212_ [1] **[11]** _[\u22a4]_ [is the centering matrix,] **[ I]** _[m]_ [ is the] _[ m][ \u00d7][ m]_ _i,j_ \ufffd _\u2225_ **y** _i_ _[\u2032]_ _[\u2212]_ **[y]** _j_ _[\u2032]_ _[\u2225]_ 2 [2] _[\u2212]_ [\u2206] [2] _ij_ \ufffd _._ The solution is given by, **Y** = ( **\u03a3** Iso _,k_ ) [1] _[/]_ [2] **U** _[\u22a4]_ Iso _,k_ (15.6) where **\u03a3** Iso _,k_ is the diagonal matrix of the top _k_ singular values of **K** Iso and **U** Iso _,k_ are the associated singular vectors. **K** Iso can naturally be viewed as a kernel matrix, thus providing a simple connection between Isomap and KPCA. Note, however, that this interpretation is valid only when **K** Iso is in fact positive semidefinite, which is indeed the case in the continuum limit for a smooth manifold. **15.3.2** **Laplacian eigenmaps** The _Laplacian eigenmaps_ algorithm aims to find a low-dimensional representation that best preserves neighborhood relations as measured by a weight matrix **W** . The algorithm works as follows: 1. Find _t_ nearest neighbors for each point. 2. Construct **W**, a sparse, symmetric _m \u00d7 m_ matrix, where **W** _ij_ = exp \ufffd _\u2212\u2225_ **x** _i_ _\u2212_ **x** _j_ _\u2225_ 2 [2] _[/\u03c3]_ [2] [\ufffd] if ( **x** _i_ _,_ **x** _j_ ) are neighbors, 0 otherwise, and _\u03c3_ is a scaling parameter. 3. Construct the diagonal matrix **D**, such that **D** _ii_ = [\ufffd] _j_ **[W]** _[ij]_ [.] 4. Find the _k_ -dimensional representation by minimizing the weighted distance between neighbors as, **Y** = argmin **Y** _[\u2032]_ \ufffd **W**",
    "chunk_id": "foundations_machine_learning_341"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_ij_ _\u2225_ **y** _i_ _[\u2032]_ _[\u2212]_ **[y]** _j_ _[\u2032]_ _[\u2225]_ 2 [2] _[.]_ (15.7) _i,j_ This objective function penalizes nearby inputs for being mapped to faraway outputs, with \u201cnearness\u201d measured by the weight matrix **W** . The solution to the minimization in (15.7) is **Y** = **U** _[\u22a4]_ **L** _,k_ [, where] **[ L]** [ =] **[ D]** _[ \u2212]_ **[W]** [ is the graph Laplacian] **15.3** **KPCA and manifold learning** **353** and **U** _[\u22a4]_ **L** _,k_ [are the bottom] _[ k]_ [ singular vectors of] **[ L]** [, excluding the last singular] vector corresponding to the singular value 0 (assuming that the underlying neighborhood graph is connected). The solution to (15.7) can also be interpreted as finding the largest singular vectors of **L** _[\u2020]_, the pseudo-inverse of **L** . Defining **K** **L** = **L** _[\u2020]_ we can thus view Laplacian Eigenmaps as an instance of KPCA in which the output dimensions are normalized to have unit variance, which corresponds to setting _\u03bb_ = 1 in (15.5). Moreover, it can be shown that **K** **L** is the kernel matrix associated with the commute times of diffusion on the underlying neighborhood graph, where the commute time between nodes _i_ and _j_ in a graph is the expected time taken for a random walk to start at node _i_, reach node _j_ and then return to _i_ . **15.3.3** **Locally linear embedding (LLE)** The _locally linear embedding_ (LLE) algorithm also aims to find a low-dimensional representation that preserves neighborhood relations as measured by a weight matrix **W** . The algorithm works as follows: 1. Find _t_ nearest neighbors for each point. 2. Construct **W**, a sparse, symmetric _m\u00d7m_ matrix, whose _i_ th row sums to one and contains the linear coefficients that optimally reconstruct **x** _i_ from its _t_ neighbors. More specifically, if we assume that the _i_ th row of **W** sums to one, then the reconstruction error is 2 2 \ufffd **x** _i_ _\u2212_ \ufffd **W** _ij_ **x** _j_ \ufffd = \ufffd\ufffd **W** _ij_ ( **x** _i_ _\u2212_ **x** _j_ )\ufffd = \ufffd **W** _ij_ **W** _ik_ **C** _[\u2032]_ _jk_ (15.8) 2 **W** _ij_ ( **x** _i_ _\u2212_ **x** _j_ )\ufffd = \ufffd _j\u2208N_ _i_ _j,k\u2208N_ 2 \ufffd **W** _ij_ **x** _j_ \ufffd = \ufffd\ufffd _j\u2208N_ _i_ _j\u2208N_ \ufffd **W** _ij_ **W** _ik_ **C** _[\u2032]_ _jk_ (15.8) _j,k\u2208N_ _i_ where _N_ _i_ is the set of indices of the neighbors of point **x** _i_ and **C** _[\u2032]_ _jk_ [= (] **[x]** _[i]_ _[ \u2212]_ **x** _j_ ) _[\u22a4]_ ( **x** _i_ _\u2212_ **x** _k_ ) the local covariance matrix. Minimizing this expression with the constraint [\ufffd] _j_ **[W]** _[ij]_ [ = 1 gives the solution] \ufffd _k_ [(] **[C]** _[\u2032\u2212]_ [1] [)] _[j][k]_ \ufffd _st_ [(] **[C]** _[\u2032\u2212]_ [1] [)] _[st]_ \ufffd **W** _ij_ = _._ (15.9) _st_ [(] **[C]** _[\u2032\u2212]_ [1] [)] _[st]_ Note that the solution can be equivalently obtained by first solving the system of linear equations [\ufffd] _j_ **[C]** _[\u2032]_ _kj_ **[W]** _[ij]_ [ = 1, for] _[ k][ \u2208N]_ _[i]_ [, and then normalizing so that] the",
    "chunk_id": "foundations_machine_learning_342"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "weights sum to one. 3. Find the _k_ -dimensional representation that best obeys neighborhood relations as specified by **W**, i.e., 2 \ufffd **y** _i_ _[\u2032]_ _[\u2212]_ \ufffd **W** _ij_ **y** _j_ _[\u2032]_ \ufffd _._ (15.10) _j_ **Y** = argmin **Y** _[\u2032]_ \ufffd _i_ **354** **Chapter 15** **Dimensionality Reduction** The solution to the minimization in (15.10) is **Y** = **U** _[\u22a4]_ **M** _,k_ [, where] **[ M]** [ = (] **[I]** _[ \u2212]_ **W** _[\u22a4]_ )( **I** _\u2212_ **W** _[\u22a4]_ ) and **U** _[\u22a4]_ **M** _,k_ [are the bottom] _[ k]_ [ singular vectors of] **[ M]** [, excluding] the last singular vector corresponding to the singular value 0. As discussed in exercise 15.5, LLE coincides with KPCA used with a particular kernel matrix **K** _LLE_ whereby the output dimensions are normalized to have unit variance (as in the case of Laplacian Eigenmaps). **15.4** **Johnson-Lindenstrauss lemma** The Johnson-Lindenstrauss lemma is a fundamental result in dimensionality reduction that states that any _m_ points in high-dimensional space can be mapped to a much lower dimension, _k \u2265_ _O_ ( [lo] _\u03f5_ [g] [2] _[ m]_ [ ), without distorting pairwise distance between] any two points by more than a factor of (1 _\u00b1 \u03f5_ ). In fact, such a mapping can be found in randomized polynomial time by projecting the high-dimensional points onto randomly chosen _k_ -dimensional linear subspaces. The Johnson-Lindenstrauss lemma is formally presented in lemma 15.4. The proof of this lemma hinges on lemma 15.2 and lemma 15.3, and it is an example of the \u201cprobabilistic method\u201d, in which probabilistic arguments lead to a deterministic statement. Moreover, as we will see, the Johnson-Lindenstrauss lemma follows by showing that the squared norm of a random vector is sharply concentrated around its mean when the vector is projected onto a _k_ -dimensional random subspace. First, we prove the following property of the _\u03c7_ [2] distribution (see definition C.7 in appendix), which will be used in lemma 15.3. **Lemma 15.2** _Let Q be a random variable following a \u03c7_ [2] _distribution with k degrees_ _of freedom. Then, for any_ 0 _< \u03f5 <_ 1 _/_ 2 _, the following inequality holds:_ P[(1 _\u2212_ _\u03f5_ ) _k \u2264_ _Q \u2264_ (1 + _\u03f5_ ) _k_ ] _\u2265_ 1 _\u2212_ 2 _e_ _[\u2212]_ [(] _[\u03f5]_ [2] _[\u2212][\u03f5]_ [3] [)] _[k/]_ [4] _._ (15.11) Proof: By Markov\u2019s inequality, we can write E[exp( _\u03bbQ_ )] P[ _Q \u2265_ (1 + _\u03f5_ ) _k_ ] = P[exp( _\u03bbQ_ ) _\u2265_ exp( _\u03bb_ (1 + _\u03f5_ ) _k_ )] _\u2264_ exp( _\u03bb_ (1 + _\u03f5_ ) _k_ ) = exp((1 _\u2212\u03bb_ (1 +2 _\u03bb_ ) _[\u2212]_ _\u03f5_ _[k/]_ ) _k_ [2] ) _[,]_ where we used for the final equality the expression of the moment-generating function of a _\u03c7_ [2] distribution, E[exp( _\u03bbQ_ )], for _\u03bb <_ 1 _/_ 2 (equation (C.25)). Choosing _\u03f5_ _\u03bb_ = 2(1+ _\u03f5_ ) _[<]_ [ 1] _[/]_ [2, which minimizes the right-hand side of the final equality, and] using the inequality 1 + _\u03f5 \u2264_ exp( _\u03f5 \u2212_ ( _\u03f5_ [2] _\u2212_ _\u03f5_ [3] ) _/_",
    "chunk_id": "foundations_machine_learning_343"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "2) yield _k/_ 2 = exp _\u2212_ _[k]_ _._ \ufffd \ufffd 4 [(] _[\u03f5]_ [2] _[ \u2212]_ _[\u03f5]_ [3] [)] \ufffd 1 + _\u03f5_ P[ _Q \u2265_ (1 + _\u03f5_ ) _k_ ] _\u2264_ \ufffd exp( _\u03f5_ ) _k/_ 2 exp \ufffd _\u03f5 \u2212_ _[\u03f5]_ [2] _[\u2212]_ 2 _[\u03f5]_ [3] _\u2264_ \ufffd \ufffd exp( _\u03f5_ ) _[\u2212][\u03f5]_ 2 \ufffd exp( _\u03f5_ ) **15.4** **Johnson-Lindenstrauss lemma** **355** The statement of the lemma follows by using similar techniques to bound P[ _Q \u2264_ (1 _\u2212_ _\u03f5_ ) _k_ ] and by applying the union bound. **Lemma 15.3** _Let_ **x** _\u2208_ R _[N]_ _, define k < N and assume that entries in_ **A** _\u2208_ R _[k][\u00d7][N]_ _are_ _sampled independently from the standard normal distribution, N_ (0 _,_ 1) _. Then, for_ _any_ 0 _< \u03f5 <_ 1 _/_ 2 _,_ P (1 _\u2212_ _\u03f5_ ) _\u2225_ **x** _\u2225_ [2] _\u2264\u2225_ [1] **Ax** _\u2225_ [2] _\u2264_ (1 + _\u03f5_ ) _\u2225_ **x** _\u2225_ [2] _\u2265_ 1 _\u2212_ 2 _e_ _[\u2212]_ [(] _[\u03f5]_ [2] _[\u2212][\u03f5]_ [3] [)] _[k/]_ [4] _._ (15.12) \ufffd _\u221ak_ \ufffd Proof: Let \ufffd **x** = **Ax** and observe that _N_ E[ _x_ \ufffd [2] _j_ [] =][ E] \ufffd \ufffd\ufffd \ufffd _A_ [2] _ji_ _[x]_ _i_ [2] _i_ =1 _N_ _N_ 2 [\ufffd] \ufffd _i_ =1 _A_ _ji_ _x_ _i_ \ufffd = E \ufffd \ufffd _i_ =1 _N_ \ufffd _x_ [2] _i_ [=] _[ \u2225]_ **[x]** _[\u2225]_ [2] _[ .]_ _i_ =1 = \ufffd The second and third equalities follow from the independence and unit variance, respectively, of the _A_ _ij_ . Now, define _T_ _j_ = \ufffd _x_ _j_ _/\u2225_ **x** _\u2225_ and note that the _T_ _j_ s are independent standard normal random variables since the _A_ _ij_ are i.i.d. standard normal random variables and E[ _x_ \ufffd [2] _j_ [] =] _[ \u2225]_ **[x]** _[\u2225]_ [2] [. Thus, the variable] _[ Q]_ [ defined by] _[ Q]_ [ =][ \ufffd] _[k]_ _j_ =1 _[T]_ [ 2] _j_ follows a _\u03c7_ [2] distribution with _k_ degrees of freedom and we have P (1 _\u2212_ _\u03f5_ ) _\u2225_ **x** _\u2225_ [2] _\u2264_ _[\u2225]_ **[x]** [\ufffd] _[\u2225]_ [2] _\u2264_ (1 + _\u03f5_ ) _\u2225_ **x** _\u2225_ [2] = P (1 _\u2212_ _\u03f5_ ) _k \u2264_ \ufffd _k_ \ufffd \ufffd _k_ \ufffd _T_ _j_ [2] _[\u2264]_ [(1 +] _[ \u03f5]_ [)] _[k]_ _j_ =1 \ufffd = P (1 _\u2212_ _\u03f5_ ) _k \u2264_ _Q \u2264_ (1 + _\u03f5_ ) _k_ \ufffd \ufffd _\u2265_ 1 _\u2212_ 2 _e_ _[\u2212]_ [(] _[\u03f5]_ [2] _[\u2212][\u03f5]_ [3] [)] _[k/]_ [4] _,_ where the final inequality holds by lemma 15.2, thus proving the statement of the lemma. **Lemma 15.4 (Johnson-Lindenstrauss)** _For any_ 0 _< \u03f5 <_ 1 _/_ 2 _and any integer m >_ 4 _, let_ _k_ = [20 lo] _\u03f5_ [2] [g] _[ m]_ _. Then for any set V of m points in_ R _[N]_ _, there exists a map f_ : R _[N]_ _\u2192_ R _[k]_ _such that for all_ **u** _,_ **v** _\u2208_ _V,_ (1 _\u2212_ _\u03f5_ ) _\u2225_ **u** _\u2212_ **v** _\u2225_",
    "chunk_id": "foundations_machine_learning_344"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[2] _\u2264\u2225f_ ( **u** ) _\u2212_ _f_ ( **v** ) _\u2225_ [2] _\u2264_ (1 + _\u03f5_ ) _\u2225_ **u** _\u2212_ **v** _\u2225_ [2] _._ (15.13) 1 Proof: Let _f_ = ~~_\u221a_~~ 1 Proof: Let _f_ = ~~_\u221a_~~ _k_ **[A]** [ where] _[ k < N]_ [ and entries in] **[ A]** _[ \u2208]_ [R] _[k][\u00d7][N]_ [ are sampled inde-] pendently from the standard normal distribution, _N_ (0 _,_ 1). For fixed **u** _,_ **v** _\u2208_ _V_, we can apply lemma 15.3, with **x** = **u** _\u2212_ **v**, to lower bound the success probability by 1 _\u2212_ 2 _e_ _[\u2212]_ [(] _[\u03f5]_ [2] _[\u2212][\u03f5]_ [3] [)] _[k/]_ [4] . Applying the union bound over the O( _m_ [2] ) pairs in _V_, setting _k_ = [20] _\u03f5_ [2] [ log] _[ m]_ [ and upper bounding] _[ \u03f5]_ [ by 1] _[/]_ [2, we have] P[ _success_ ] _\u2265_ 1 _\u2212_ 2 _m_ [2] _e_ _[\u2212]_ [(] _[\u03f5]_ [2] _[\u2212][\u03f5]_ [3] [)] _[k/]_ [4] = 1 _\u2212_ 2 _m_ [5] _[\u03f5][\u2212]_ [3] _>_ 1 _\u2212_ 2 _m_ _[\u2212]_ [1] _[/]_ [2] _>_ 0 _._ Since the success probability is strictly greater than zero, a map that satisfies the desired conditions must exist, thus proving the statement of the lemma. _\u03f5_ [2] [ log] _[ m]_ [ and upper bounding] _[ \u03f5]_ [ by 1] _[/]_ [2, we have] **356** **Chapter 15** **Dimensionality Reduction** **15.5** **Chapter notes** PCA was introduced in the early 1900s by Pearson [1901]. KPCA was introduced roughly a century later, and our presentation of KPCA is a more concise derivation of results given by Mika et al. [1999]. Isomap and LLE were pioneering works on non-linear dimensionality reduction introduced by Tenenbaum et al. [2000], Roweis and Saul [2000]. Isomap itself is a generalization of a standard linear dimensionality reduction technique called Multidimensional Scaling [Cox and Cox, 2000]. Isomap and LLE led to the development of several related algorithms for manifold learning, e.g., Laplacian Eigenmaps and Maximum Variance Unfolding [Belkin and Niyogi, 2001, Weinberger and Saul, 2006]. As shown in this chapter, classical manifold learning algorithms are special instances of KPCA [Ham et al., 2004]. The JohnsonLindenstrauss lemma was introduced by Johnson and Lindenstrauss [1984], though our proof of the lemma follows Vempala [2004]. Other simplified proofs of this lemma have also been presented, including Dasgupta and Gupta [2003]. **15.6** **Exercises** 15.1 PCA and maximal variance. Let **X** be an _uncentered_ data matrix and let **x** \u00af = [1] **[x]** **[ X]** [.] _m_ \ufffd _i_ **[x]** _[i]_ [ be the sample mean of the columns of] **[ X]** [.] (a) Show that the variance of one-dimensional projections of the data onto an 1 arbitrary vector **u** equals **u** _[\u22a4]_ **Cu**, where **C** = _m_ \ufffd _i_ [(] **[x]** _[i]_ _[ \u2212]_ **[x]** [\u00af][)(] **[x]** _[i]_ _[ \u2212]_ **[x]** [\u00af][)] _[\u22a4]_ [is the] sample covariance matrix. (b) Show that PCA with _k_ = 1 projects the data onto the direction (i.e., **u** _[\u22a4]_ **u** = 1) of maximal variance. 15.2 Double centering. In this problem we will prove the correctness of",
    "chunk_id": "foundations_machine_learning_345"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the double centering step in Isomap when working with Euclidean distances. Define **X** and **x** \u00af as in exercise 15.1, and define **X** _[\u2217]_ as the centered version of **X**, that is, let **x** _[\u2217]_ _i_ [=] **[ x]** _[i]_ _[ \u2212]_ **[x]** [\u00af][ be the] _[ i]_ [th column of] **[ X]** _[\u2217]_ [. Let] **[ K]** [ =] **[ X]** _[\u22a4]_ **[X]** [, and let] **[ D]** [ denote the] Euclidean distance matrix, i.e., **D** _ij_ = _\u2225_ **x** _i_ _\u2212_ **x** _j_ _\u2225_ . (a) Show that **K** _ij_ = [1] 2 [(] **[K]** _[ii]_ [ +] **[ K]** _[jj]_ [ +] **[ D]** [2] _ij_ [).] (b) Show that **K** _[\u2217]_ = **X** _[\u2217\u22a4]_ **X** _[\u2217]_ = **K** _\u2212_ [1] [1] 1 _m_ **[11]** _[\u22a4]_ **[K]** [ +] _m_ [2] **[ 11]** _[\u22a4]_ **[K11]** _[\u22a4]_ [.] [1] _m_ **[K11]** _[\u22a4]_ _[\u2212]_ [1] (c) Using the results from (a) and (b) show that _m_ _m_ \ufffd \ufffd **D** [2] _ik_ _[\u2212]_ _m_ [1] _k_ =1 **K** _[\u2217]_ _ij_ [=] _[ \u2212]_ 2 [1] **D** [2] _ij_ _[\u2212]_ [1] \ufffd _m_ _m_ \ufffd **D** [2] _kj_ [+ \u00af] **[D]** _,_ _k_ =1 \ufffd **15.6** **Exercises** **357** where **D** [\u00af] = _m_ 1 [2] \ufffd _v_ **[D]** [2] _u,v_ [is the mean of the] _[ m]_ [2] [ entries in] **[ D]** [.] _u_ \ufffd (d) Show that **K** _[\u2217]_ = _\u2212_ [1] [1] 2 **[HDH]** [, where] **[ H]** [ =] **[ I]** _[m]_ _[ \u2212]_ [1] _m_ [1] **[11]** _[\u22a4]_ [.] 15.3 Laplacian eigenmaps. Assume _k_ = 1 and we seek a one-dimensional representation **y** . Show that (15.7) is equivalent to **y** = argmin **y** _\u2032_ **y** _[\u2032\u22a4]_ **Ly** _[\u2032]_, where **L** is the graph Laplacian. 15.4 Nystr\u00a8om method. Define the following block representation of a kernel matrix: **K** = **W** **K** _[\u22a4]_ 21 \ufffd **K** 21 **K** 22 \ufffd \ufffd and **C** = **W** \ufffd **K** 21 _._ The Nystr\u00a8om method uses **W** _\u2208_ R _[l][\u00d7][l]_ and **C** _\u2208_ R _[m][\u00d7][l]_ to generate the approximation **K** [\ufffd] = **CW** _[\u2020]_ **C** _[\u22a4]_ _\u2248_ **K** . (a) Show that **W** is SPSD and that _\u2225_ **K** _\u2212_ **K** [\ufffd] _\u2225_ _F_ = _\u2225_ **K** 22 _\u2212_ **K** 21 **W** _[\u2020]_ **K** _[\u22a4]_ 21 _[\u2225]_ _[F]_ [.] (b) Let **K** = **X** _[\u22a4]_ **X** for some **X** _\u2208_ R _[N]_ _[\u00d7][m]_, and let **X** _[\u2032]_ _\u2208_ R _[N]_ _[\u00d7][l]_ be the first _l_ columns of **X** . Show that **K** [\ufffd] = **X** _[\u22a4]_ **P** _U_ _X\u2032_ **X**, where **P** _U_ _X\u2032_ is the orthogonal projection onto the span of the left singular vectors of **X** _[\u2032]_ . (c) Is **K** [\ufffd] SPSD? (d) If rank( **K** ) = rank( **W** ) = _r \u226a_ _m_, show that **K** [\ufffd] = **K** . Note: this statement holds whenever rank( **K** ) = rank( **W** ), but is of interest mainly in the lowrank setting. (e) If _m_ = 20M and **K** is a dense matrix, how much space is required to store **K** if each entry is stored as",
    "chunk_id": "foundations_machine_learning_346"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "a double? How much space is required by the Nystr\u00a8om method if _l_ = 10K? 15.5 Expression for **K** _LLE_ . Show the connection between LLE and KPCA by deriving the expression for **K** _LLE_ . 15.6 Random projection, PCA, and nearest neighbors. (a) Download the MNIST test set of handwritten digits at: ``` http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz. ``` Create a data matrix **X** _\u2208_ R _[N]_ _[\u00d7][m]_ from the first _m_ = 2 _,_ 000 instances of this dataset (the dimension of each instance should be _N_ = 784). (b) Find the ten nearest neighbors for each point in **X**, that is, compute _N_ _i,_ 10 for 1 _\u2264_ _i \u2264_ _m_, where _N_ _i,t_ denotes the set of the _t_ nearest neighbors for the **358** **Chapter 15** **Dimensionality Reduction** _i_ th datapoint and nearest neighbors are defined with respect to the _L_ 2 norm. Also compute _N_ _i,_ 50 for all _i_ . (c) Generate **X** [\u02dc] = **AX**, where **A** _\u2208_ R _[k][\u00d7][N]_, _k_ = 100 and entries of **A** are sampled independently from the standard normal distribution. Find the ten nearest neighbors for each point in **X** [\u02dc], that is, compute _N_ [\u02dc] _i,_ 10 for 1 _\u2264_ _i \u2264_ _m_ . (d) Report the quality of approximation by computing score 10 = [1] (d) Report the quality of approximation by computing score _N_ \u02dc _i,_ 10 _|_ . Similarly, compute score 50 = [1] \ufffd _mi_ =1 _[|N]_ _[i,]_ [50] _[ \u2229]_ _N_ [\u02dc] 10 _i,_ 10 = _|_ . _m_ [1] \ufffd _mi_ =1 _[|N]_ _[i,]_ [10] _[\u2229]_ _m_ [1] \ufffd _mi_ =1 _[|N]_ _[i,]_ [50] _[ \u2229]_ _N_ [\u02dc] _i,_ 10 _|_ . (e) Generate two plots that show score 10 and score 50 as functions of _k_ (i.e., perform steps (c) and (d) for _k_ = _{_ 1 _,_ 10 _,_ 50 _,_ 100 _,_ 250 _,_ 500 _}_ ). Provide a oneor two-sentence explanation of these plots. (f) Generate similar plots as in (e) using PCA (with various values of _k_ ) to generate **X** [\u02dc] and subsequently compute nearest neighbors. Are the nearest neighbor approximations generated via PCA better or worse than those generated via random projections? Explain why. # 16 Learning Automata and Languages This chapter presents an introduction to the problem of learning languages. This is a classical problem explored since the early days of formal language theory and computer science, and there is a very large body of literature dealing with related mathematical questions. In this chapter, we present a brief introduction to this problem and concentrate specifically on the question of learning finite automata, which, by itself, has been a topic investigated in multiple forms by thousands of technical papers. We will examine two broad frameworks for learning automata, and for each, we will present an algorithm. In particular, we describe an algorithm for learning automata in which the learner has access to several types of query, and we discuss an algorithm for identifying a sub-class of the family of automata in the limit. **16.1** **Introduction** Learning languages is one of",
    "chunk_id": "foundations_machine_learning_347"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the earliest problems discussed in linguistics and computer science. It has been prompted by the remarkable faculty of humans to learn natural languages. Humans are capable of uttering well-formed new sentences at an early age, after having been exposed only to finitely many sentences. Moreover, even at an early age, they can make accurate judgments of grammaticality for new sentences. In computer science, the problem of learning languages is directly related to that of learning the representation of the computational device generating a language. Thus, for example, learning regular languages is equivalent to learning finite automata, or learning context-free languages or context-free grammars is equivalent to learning pushdown automata. There are several reasons for examining specifically the problem of learning finite automata. Automata provide natural modeling representations in a variety of different domains including systems, networking, image processing, text and speech **360** **Chapter 16** **Learning Automata and Languages** **Image:** [No caption returned] b **Image:** [No caption returned] b (a) (b) **Figure 16.1** (a) A graphical representation of a finite automaton. (b) Equivalent (minimal) deterministic automaton. processing, logic and many others. Automata can also serve as simple or efficient approximations for more complex devices. For example, in natural language processing, they can be used to approximate context-free languages. When it is possible, learning automata is often efficient, though, as we shall see, the problem is hard in a number of natural scenarios. Thus, learning more complex devices or languages is even harder. We consider two general learning frameworks: the model of _efficient exact learning_ and the model of _identification in the limit_ . For each of these models, we briefly discuss the problem of learning automata and describe an algorithm. We first give a brief review of some basic automata definitions and algorithms, then discuss the problem of efficient exact learning of automata and that of the identification in the limit. **16.2** **Finite automata** We will denote by \u03a3 a finite alphabet. The length of a string _x \u2208_ \u03a3 _[\u2217]_ over that alphabet is denoted by _|x|_ . The _empty string_ is denoted by _\u03f5_, thus _|\u03f5|_ = 0. For any string _x_ = _x_ 1 _\u00b7 \u00b7 \u00b7 x_ _k_ _\u2208_ \u03a3 _[\u2217]_ of length _k \u2265_ 0, we denote by _x_ [ _j_ ] = _x_ 1 _\u00b7 \u00b7 \u00b7 x_ _j_ its prefix of length _j \u2264_ _k_ and define _x_ [0] as _\u03f5_ . _Finite automata_ are labeled directed graphs equipped with initial and final states. The following gives a formal definition of these devices. **Definition 16.1 (Finite automata)** _A_ finite automaton _A is a 5-tuple_ (\u03a3 _, Q, I, F, E_ ) _where_ \u03a3 _is a finite alphabet, Q a finite set of states, I \u2286_ _Q a set of initial states, F \u2286_ _Q_ _a set of final states, and E \u2286_ _Q \u00d7_ (\u03a3 _\u222a{\u03f5}_ ) _\u00d7 Q a finite set of_ transitions _._ Figure 16.1a shows a simple example of a finite automaton. States are represented by circles. A bold circle indicates an initial state, a double circle a final state. Each transition is",
    "chunk_id": "foundations_machine_learning_348"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "represented by an arrow from its origin state to its destination state with its label in \u03a3 _\u222a{\u03f5}_ . **16.3** **Efficient exact learning** **361** A path from an initial state to a final state is said to be an _accepting path_ . An automaton is said to be _trim_ if all of its states are accessible from an initial state and admit a path to a final state, that is, if all of its states lie on an accepting path. A string _x \u2208_ \u03a3 _[\u2217]_ is _accepted_ by an automaton _A_ iff _x_ labels an accepting path. For convenience, we will say that _x \u2208_ \u03a3 _[\u2217]_ is _rejected_ by _A_ when it is not accepted. The set of all strings accepted by _A_ defines the _language accepted by A_ denoted by _L_ ( _A_ ). The class of languages accepted by finite automata coincides with the family of _regular languages_, that is, languages that can be described by _regular expressions_ . Any finite automaton admits an equivalent automaton with no _\u03f5-transition_, that is, no transition labeled with the empty string: there exists a general _\u03f5_ -removal algorithm that takes as input an automaton and returns an equivalent automaton with no _\u03f5_ -transition. An automaton with no _\u03f5-transition_ is said to be _deterministic_ if it admits a unique initial state and if no two transitions sharing the same label leave any given state. A deterministic finite automaton is often referred to by the acronym _DFA_, while the acronym _NFA_ is used for arbitrary automata, that is, non-deterministic finite automata. Any NFA admits an equivalent DFA: there exists a general (exponentialtime) _determinization_ algorithm that takes as input an NFA with no _\u03f5_ -transition and returns an equivalent DFA. Thus, the class of languages accepted by DFAs coincides with that of the languages accepted by NFAs, that is regular languages. For any string _x \u2208_ \u03a3 _[\u2217]_ and DFA _A_, we denote by _A_ ( _x_ ) the state reached in _A_ when reading _x_ from its unique initial state. A DFA is said to be _minimal_ if it admits no equivalent deterministic automaton with a smaller number of states. There exists a general _minimization_ algorithm taking as input a deterministic automaton and returning a minimal one that runs in _O_ ( _|E|_ log _|Q|_ ). When the input DFA is _acyclic_, that is when it admits no path forming a cycle, it can be minimized in linear time _O_ ( _|Q|_ + _|E|_ ). Figure 16.1b shows the minimal DFA equivalent to the NFA of figure 16.1a. **16.3** **Efficient exact learning** In the _efficient exact learning_ framework, the problem consists of identifying a target concept _c_ from a finite set of examples in time polynomial in the size of the representation of the concept and in an upper bound on the size of the representation of an example. Unlike the PAC-learning framework, in this model, there is no stochastic assumption, instances are not assumed to be drawn according to some unknown distribution. Furthermore, the objective is to identify the target concept **362**",
    "chunk_id": "foundations_machine_learning_349"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**Chapter 16** **Learning Automata and Languages** _exactly_, without any approximation. A concept class C is said to be efficiently exactly learnable if there is an algorithm for efficient exact learning of any _c \u2208_ C. We will consider two different scenarios within the framework of efficiently exact learning: a _passive_ and an _active learning_ scenario. The passive learning scenario is similar to the standard supervised learning scenario discussed in previous chapters but without any stochastic assumption: the learning algorithm _passively_ receives data instances as in the PAC model and returns a hypothesis, but here, instances are not assumed to be drawn from any distribution. In the active learning scenario, the learner _actively_ participates in the selection of the training samples by using various types of queries that we will describe. In both cases, we will focus more specifically on the problem of learning automata. **16.3.1** **Passive learning** The problem of learning finite automata in this scenario is known as the _minimum_ _consistent DFA learning problem_ . It can be formulated as follows: the learner receives a finite sample _S_ = (( _x_ 1 _, y_ 1 ) _, . . .,_ ( _x_ _m_ _, y_ _m_ )) with _x_ _i_ _\u2208_ \u03a3 _[\u2217]_ and _y_ _i_ _\u2208{\u2212_ 1 _,_ +1 _}_ for any _i \u2208_ [ _m_ ]. If _y_ _i_ = +1, then _x_ _i_ is an accepted string, otherwise it is rejected. The problem consists of using this sample to learn the smallest DFA _A consistent_ with _S_, that is the automaton with the smallest number of states that accepts the strings of _S_ with label +1 and rejects those with label _\u2212_ 1. Note that seeking the smallest DFA consistent with _S_ can be viewed as following Occam\u2019s razor principle. The problem just described is distinct from the standard minimization of DFAs. A minimal DFA accepting exactly the strings of _S_ labeled positively may not have the smallest number of states: in general there may be DFAs with fewer states accepting a superset of these strings and rejecting the negatively labeled sample strings. For example, in the simple case _S_ = (( _a,_ +1) _,_ ( _b, \u2212_ 1)), a minimal deterministic automaton accepting the unique positively labeled string _a_ or the unique negatively labeled string _b_ admits two states. However, the deterministic automaton accepting the language _a_ _[\u2217]_ accepts _a_ and rejects _b_ and has only one state. Passive learning of finite automata turns out to be a computationally hard problem. The following theorems present several negative results known for this prob lem. **Theorem 16.2** _The problem of finding the smallest deterministic automaton consis-_ _tent with a set of accepted or rejected strings is NP-complete._ Hardness results are known even for a polynomial approximation, as stated by the following theorem. **Theorem 16.3** _If P \u0338_ = _NP, then, no polynomial-time algorithm can be guaranteed to_ _find a DFA consistent with a set of accepted or rejected strings of size smaller than_ **16.3** **Efficient exact learning** **363** _a polynomial function of the smallest consistent DFA, even when the",
    "chunk_id": "foundations_machine_learning_350"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "alphabet is_ _reduced to just two elements._ Other strong negative results are known for passive learning of finite automata under various cryptographic assumptions. These negative results for passive learning invite us to consider alternative learning scenarios for finite automata. The next section describes a scenario leading to more positive results where the learner can actively participate in the data selection process using various types of queries. **16.3.2** **Learning with queries** The model of _learning with queries_ corresponds to that of a (minimal) teacher or oracle and an active learner. In this model, the learner can make the following two types of queries to which an oracle responds: _\u2022_ _membership queries_ : the learner requests the target label _f_ ( _x_ ) _\u2208{\u2212_ 1 _,_ +1 _}_ of an instance _x_ and receives that label; _\u2022_ _equivalence queries_ : the learner conjectures hypothesis _h_ ; it receives the response yes if _h_ = _f_, a counter-example otherwise. We will say that a concept class C is _efficiently exactly learnable with membership_ _and equivalence queries_ when it is efficiently exactly learnable within this model. This model is not realistic, since no such oracle is typically available in practice. Nevertheless, it provides a natural framework, which, as we shall see, leads to positive results. Note also that for this model to be significant, equivalence must be computationally testable. This would not be the case for some concept classes such as that of _context-free grammars_, for example, for which the equivalence problem is undecidable. In fact, equivalence must be further efficiently testable, otherwise the response to the learner cannot be supplied in a reasonable amount of time. [21] Efficient exact learning within this model of learning with queries implies the following variant of PAC-learning: we will say that a concept class C is _PAC-_ _learnable with membership queries_ if it is PAC-learnable by an algorithm that has access to a polynomial number of membership queries. **Theorem 16.4** _Let_ C _be a concept class that is efficiently exactly learnable with mem-_ _bership and equivalence queries, then_ C _is PAC-learnable using membership queries._ Proof: Let _A_ be an algorithm for efficiently exactly learning C using membership and equivalence queries. Fix _\u03f5, \u03b4 >_ 0. We replace in the execution of _A_ for learning 21 For a human oracle, answering membership queries may also become very hard in some cases when the queries are near the class boundaries. This may also make the model difficult to adopt in practice. **364** **Chapter 16** **Learning Automata and Languages** target _c \u2208_ C, each equivalence query by a test of the current hypothesis on a polynomial number of labeled examples. Let D be the distribution according to which points are drawn. To simulate the _t_ th equivalence query, we draw _m_ _t_ = 1 _\u03f5_ [(log] [1] _\u03b4_ [+] _[ t]_ [ log 2) points i.i.d. according to][ D][ to test the current hypothesis] _[ h]_ _[t]_ [.] If _h_ _t_ is consistent with all of these points, then the algorithm stops and returns _h_ _t_ . Otherwise, one of the points",
    "chunk_id": "foundations_machine_learning_351"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "drawn does not belong to _h_ _t_, which provides a counter-example. Since _A_ learns _c_ exactly, it makes at most _T_ equivalence queries, where _T_ is polynomial in the size of the representation of the target concept and in an upper bound on the size of the representation of an example. Thus, if no equivalence query is positively responded by the simulation, the algorithm will terminate after _T_ equivalence queries and return the correct concept _c_ . Otherwise, the algorithm stops at the first equivalence query positively responded by the simulation. The hypothesis it returns is not an _\u03f5_ -approximation only if the equivalence query stopping the algorithm is incorrectly responded positively. By the union bound, since for any fixed _t \u2208_ [ _T_ ], P[ _R_ ( _h_ _t_ ) _> \u03f5_ ] _\u2264_ (1 _\u2212_ _\u03f5_ ) _[m]_ _[t]_, the probability that for some _t \u2208_ [ _T_ ], _R_ ( _h_ _t_ ) _> \u03f5_ can be bounded as follows: P[ _\u2203t \u2208_ [ _T_ ]: _R_ ( _h_ _t_ ) _> \u03f5_ ] _\u2264_ _\u2264_ _T_ \ufffd P[ _R_ ( _h_ _t_ ) _> \u03f5_ ] _t_ =1 _T_ \ufffd(1 _\u2212_ _\u03f5_ ) _[m]_ _[t]_ _\u2264_ _t_ =1 _\u03b4_ 2 _[t]_ _[ \u2264]_ _T_ _e_ _[\u2212][m]_ _[t]_ _[\u03f5]_ _\u2264_ \ufffd _t_ =1 _T_ \ufffd _t_ =1 + _\u221e_ \ufffd _t_ =1 _\u03b4_ 2 _[t]_ [ =] _[ \u03b4.]_ Thus, with probability at least 1 _\u2212_ _\u03b4_, the hypothesis returned by the algorithm is an _\u03f5_ -approximation. Finally, the maximum number of points drawn is [\ufffd] _[T]_ _t_ =1 _[m]_ _[t]_ [ =] 1 _\u03f5_ [(] _[T]_ [ log] [1] _\u03b4_ [+] _[ T]_ [(] _[T]_ 2 [+][1][)] log 2), which is polynomial in 1 _/\u03f5_, 1 _/\u03b4_, and _T_ . Since the rest [1] _\u03b4_ [+] _[ T]_ [(] _[T]_ 2 [+][1][)] _\u03f5_ [(] _[T]_ [ log] _\u03b4_ [+] 2 log 2), which is polynomial in 1 _/\u03f5_, 1 _/\u03b4_, and _T_ . Since the rest of the computational cost of _A_ is also polynomial by assumption, this proves the PAC-learning of C. **16.3.3** **Learning automata with queries** In this section, we describe an algorithm for efficient exact learning of DFAs with _A_ membership and equivalence queries. We will denote by\ufffd the DFA that is the current hypothesis of the algorithm. For the discussion of _A_ the target DFA and by the algorithm, we assume without loss of generality that _A_ is a minimal DFA. The algorithm uses two sets of strings, _U_ and _V_ . _U_ is a set of _access strings_ : reading an access string _u \u2208_ _U_ from the initial state of _A_ leads to a state _A_ ( _u_ ). The algorithm ensures that the states _A_ ( _u_ ), _u \u2208_ _U_, are all distinct. To do so, it uses a set _V_ of _distinguishing strings_ . Since _A_ is minimal, for two distinct states _q_ and _q_ _[\u2032]_ of _A_, there must exist at least one string that leads to a final state from _q_ and not from _q_ _[\u2032]_, or",
    "chunk_id": "foundations_machine_learning_352"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "vice versa. That string helps _distinguish q_ and _q_ _[\u2032]_ . The set of strings _V_ **16.3** **Efficient exact learning** **365** **Image:** [No caption returned] a **Image:** [No caption returned] b a b b (a) (b) (c) **Figure 16.2** (a) Classification tree _T_, with _U_ = _{\u03f5, b, ba}_ and _V_ = _{\u03f5, a}_ . (b) Current automaton _A_ [\ufffd] constructed using _T_ . (c) Target automaton _A_ . help distinguish any pair of access strings in _U_ . They define in fact a partition of all strings of \u03a3 _[\u2217]_ . The objective of the algorithm is to find at each iteration a new access string distinguished from all previous ones, ultimately obtaining a number of access strings equal to the number of states of _A_ . It can then identify each state _A_ ( _u_ ) of _A_ with its access string _u_ . To find the destination state of the transition labeled with _a \u2208_ \u03a3 leaving state _u_, it suffices to determine, using the partition induced by _V_ the access string _u_ _[\u2032]_ that belongs to the same equivalence class as _ua_ . The finality of each state can be determined in a similar way. Both sets _U_ and _V_ are maintained by the algorithm via a binary decision tree _T_ similar to those presented in chapter 9. Figure 16.2a shows an example. _T_ defines the partition of all strings induced by the distinguishing strings _V_ . The leaves of _T_ are each labeled with a distinct _u \u2208_ _U_ and its internal nodes with a string _v \u2208_ _V_ . The decision tree question defined by _v \u2208_ _V_, given a string _x \u2208_ \u03a3 _[\u2217]_, is whether _xv_ is accepted by _A_, which is determined via a membership query. If accepted, _x_ is assigned to right sub-tree, otherwise to the left sub-tree, and the same is applied recursively with the sub-trees until a leaf is reached. We denote by _T_ ( _x_ ) the label of the leaf reached. For example, for the tree _T_ of figure 16.2a and target automaton _A_ of figure 16.2c, _T_ ( _baa_ ) = _b_ since _baa_ is not accepted by _A_ (root question) and _baaa_ is (question at node _a_ ). At its initialization step, the algorithm ensures that the root node is labeled with _\u03f5_, which is convenient to check the finality of the strings. The tentative hypothesis DFA _A_ [\ufffd] can be constructed from _T_ as follows. We denote by ConstructAutomaton() the corresponding function. A distinct state _A_ [\ufffd] ( _u_ ) is created for each leafthe sub-tree of the root node that _u \u2208_ _U_ . The finality of a state _u_ belongs to: _A_ \ufffd( _uA_ ) is made final iff [\ufffd] ( _u_ ) is determined based on _u_ belongs **366** **Chapter 16** **Learning Automata and Languages** QueryLearnAutomata() 1 _t \u2190_ MembershipQuery( _\u03f5_ ) 2 _T \u2190_ _T_ 0 3 _A_ \ufffd _\u2190_ _A_ 0 4 **while** (EquivalenceQuery( _A_ [\ufffd] ) _\u0338_ = true) **do** 5 _x \u2190_ CounterExample() 6 **if** ( _T_ =",
    "chunk_id": "foundations_machine_learning_353"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_T_ 0 ) **then** 7 _T \u2190_ _T_ 1 _\u25b7_ nil replaced with _x_ . 8 **else** _j \u2190_ argmin _k_ _A_ ( _x_ [ _k_ ]) _\u0338\u2261_ _T_ _A_ [\ufffd] ( _x_ [ _k_ ]) 9 Split( _A_ [\ufffd] ( _x_ [ _j \u2212_ 1])) 10 _A_ \ufffd _\u2190_ ConstructAutomaton( _T_ ) 11 **return** _A_ [\ufffd] **Figure 16.3** Algorithm for learning automata with membership and equivalence queries. _A_ 0 is a single-state automaton with self-loops labeled with all _a \u2208_ \u03a3. That state is initial. It is final iff _t_ = true. _T_ 0 is a tree with root node labeled with _\u03f5_ and two leaves, one labeled with _\u03f5_, the other with nil. the right leaf is labeled with _\u03f5_ labels iff _t_ = true. _T_ 1 is the tree obtained from _T_ 0 by replacing nil with _x_ . to the right sub-tree that is iff _u_ = _\u03f5u_ is accepted by _A_ . The destination of the transition labeled with _a \u2208_ \u03a3 leaving state _A_ [\ufffd] ( _u_ ) is the state _A_ [\ufffd] ( _v_ ) where _v_ = _T_ ( _ua_ ). Figure 16.2b shows the DFA _A_ [\ufffd] constructed from the decision tree of figure 16.2a. For convenience, for any _x \u2208_ \u03a3 _[\u2217]_, we denote by _U_ ( _A_ [\ufffd] ( _x_ )) the access string identifying state _A_ [\ufffd] ( _x_ ). Figure 16.3 shows the pseudocode of the algorithm. The initialization steps at lines 1\u20133 construct a tree _T_ with a single internal node labeled with _\u03f5_ and one leaf string labeled with _\u03f5_, the other left undetermined and labeled with nil. They also define a tentative DFA _A_ [\ufffd] with a single state with self-loops labeled with all elements of the alphabet. That single state is an initial state. It is made a final state only if _\u03f5_ is accepted by the target DFA _A_, which is determined via the membership query of line 1. At each iteration of the loop of lines 4\u201311, an equivalence query is used. If _A_ [\ufffd] is not equivalent to _A_, then a counter-example string _x_ is received (line 5). If _T_ is the tree constructed in the initialization step, then the leaf labeled with nil is replaced with _x_ (lines 6\u20137). Otherwise, since _x_ is a counter-example, states _A_ ( _x_ ) **16.3** **Efficient exact learning** **367** **Image:** [No caption returned] **Image:** [No caption returned] ~~_T_~~ ~~(~~ ~~_x_~~ ~~[~~ ~~_j_~~ _\u2212_ ~~1])~~ ~~_u_~~ _[\uffff]_ ~~_u_~~ _[\uffff]_ _T_ ( _x_ [ _j \u2212_ 1]) **Figure 16.4** Illustration of the splitting procedure Split( _A_ [\ufffd] ( _x_ [ _j \u2212_ 1])). _x_ [ _j \u2212_ 1] and _A_ [\ufffd] ( _x_ ) have a different finality; thus, the string _x_ defining _A_ ( _x_ ) and the access string _U_ ( _A_ [\ufffd] ( _x_ )) are assigned to different equivalence classes by _T_ . Thus, there exists a smallest _j_ such that _A_ ( _x_ [ _j_ ]) and _A_ [\ufffd] ( _x_ [ _j_ ]) are not equivalent, that is, such that the",
    "chunk_id": "foundations_machine_learning_354"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "prefix _x_ [ _j_ ] of _x_ and the access string _U_ ( _A_ [\ufffd] ( _x_ [ _j_ ])) are assigned to different leaves by _T_ . _j_ cannot be 0 since the initialization ensures that _A_ [\ufffd] ( _\u03f5_ ) is an initial state and _A_ has the same finality as the initial state\ufffd( _x_ [ _j_ ]) is tested by checking the equality of _A_ ( _\u03f5 T_ ) of( _x A_ [ _j_ . The equivalence of]) and _T_ ( _U_ ( _A_ \ufffd( _x_ [ _j_ ]))), which can _A_ ( _x_ [ _j_ ]) and be both determined using the tree _T_ and membership queries (line 8). Now, by definition, _A_ ( _x_ [ _j \u2212_ 1]) and _A_ [\ufffd] ( _x_ [ _j \u2212_ 1]) are equivalent, that is _T_ assigns _x_ [ _j_ _\u2212_ 1] to the leaf labeled with _U_ ( _A_ [\ufffd] ( _x_ [ _j_ _\u2212_ 1])). But, _x_ [ _j_ _\u2212_ 1] and _U_ ( _A_ [\ufffd] ( _x_ [ _j_ _\u2212_ 1])) must be distinguished since _A_ ( _x_ [ _j \u2212_ 1]) and _A_ [\ufffd] ( _x_ [ _j \u2212_ 1]) admit transitions labeled with the same label _x_ _j_ to two non-equivalent states. Let _v_ be a distinguishing string for _A_ ( _x_ [ _j_ ]) and _A_ [\ufffd] ( _x_ [ _j_ ]). _v_ can be obtained as the least common ancestor of the leaves labeled with _x_ [ _j_ ] and _U_ ( _A_ [\ufffd] ( _x_ [ _j_ ])). To distinguish _x_ [ _j \u2212_ 1] and _U_ ( _A_ [\ufffd] ( _x_ [ _j \u2212_ 1])), it suffices to split the leaf of _T_ labeled with _T_ ( _x_ [ _j \u2212_ 1]) to create an internal node _x_ _j_ _v_ dominating a leaf labeled with _x_ [ _j \u2212_ 1] and another one labeled with _T_ ( _x_ [ _j \u2212_ 1]) (line 9). Figure 16.4 illustrates this construction. Thus, this provides a new access string _x_ [ _j \u2212_ 1] which, by construction, is distinguished from _U_ ( _A_ [\ufffd] ( _x_ [ _j \u2212_ 1])) and all other access strings. Thus, the number of access strings (or states of _A_ [\ufffd] ) increases by one at each iteration of the loop. When it reaches the number of states of _A_, all states of _A_ are of the form _A_ ( _u_ ) for a distinct _u \u2208_ _U_ . _A_ and _A_ [\ufffd] have then the same number of states and in fact _A_ = _A_ [\ufffd] . Indeed, let ( _A_ ( _u_ ) _, a, A_ ( _u_ _[\u2032]_ )) be a transition in _A_, then by definition the equality _A_ ( _ua_ ) = _A_ ( _u_ _[\u2032]_ ) holds. The tree _T_ defines a partition of all strings in terms of their distinguishing strings in _A_ . Since in _A_, _ua_ and _u_ _[\u2032]_ lead to the same state, they are assigned to the same leaf by _T_, that is, the leaf labeled with _u_ _[\u2032]_",
    "chunk_id": "foundations_machine_learning_355"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ". The destination of the transition from _A_ [\ufffd] ( _u_ ) with label _a_ is found by ConstructAutomaton() by determining the leaf in _T_ assigned to _ua_, that is, _u_ _[\u2032]_ . Thus, by construction, the same transition ( _A_ [\ufffd] ( _u_ ) _, a,_ _A_ [\ufffd] ( _u_ _[\u2032]_ )) is created in _A_ [\ufffd] . |368|Col2|Col3|Col4|Chapter 16 Learning Automata and<br>A<br>a b a<br>b a a<br>0 1 2 3<br>b b<br>b<br>T A counter-exa|Col6|Col7|Col8|Col9|Col10|Language<br>mple x| |---|---|---|---|---|---|---|---|---|---|---| |368|368|368||a b a|a b a|a b a|a b a|a b a|a b a|a b a| |||||b a<br>0 1<br>b<br>b<br>T A|b a<br>0 1<br>b<br>b<br>T A|a<br>2 3<br>b|a<br>2 3<br>b|a<br>2 3<br>b|a<br>2 3<br>b|a<br>2 3<br>b| ||||b<br>\u03b5 a<br>!<br>\u03b5 NIL<br>! a<br>b<br>! a|b<br>\u03b5 a<br>!<br>NIL<br>! a<br>b|b<br>\u03b5 a<br>!<br>NIL<br>! a<br>b||||x = b|x = b| ||||b<br>\u03b5 a<br>!<br>\u03b5 NIL<br>! a<br>b<br>! a|b<br>\u03b5 a<br>!<br>NIL<br>! a<br>b|b<br>\u03b5 a<br>!<br>NIL<br>! a<br>b|b|b|b|b|b| ||||b|b|b|||||| ||||! b|! b||||||| |||||||||||| ||!<br>a b a b<br>b a<br>! b ba<br>b a<br>! ba<br>!<br>a<br>a a a b<br>b a a<br>! b ba ba<br>! ba b baa b b<br>Figure 16.5<br>Illustration of the execution of Algorithm QueryLearnAutomat<br>Each line shows the current decision tree T and the tentative DF<br>A is not equivalent to A, the learner receives a counter-example<br>b||!|!|!|!||||| ||!<br>a b a b<br>b a<br>! b ba<br>b a<br>! ba<br>!<br>a<br>a a a b<br>b a a<br>! b ba ba<br>! ba b baa b b<br>Figure 16.5<br>Illustration of the execution of Algorithm QueryLearnAutomat<br>Each line shows the current decision tree T and the tentative DF<br>A is not equivalent to A, the learner receives a counter-example<br>b||a b a b<br>b a<br>! b ba<br>b a<br>! ba<br>!<br>a a a b<br>b a a<br>! b ba<br>! ba b baa b b|a b a b<br>b a<br>! b ba<br>b a<br>! ba<br>!<br>a a a b<br>b a a<br>! b ba<br>! ba b baa b b|a b a b<br>b a<br>! b ba<br>b a<br>! ba<br>!<br>a a a b<br>b a a<br>! b ba<br>! ba b baa b b|a b a b<br>b a<br>! b ba<br>b a<br>! ba<br>!<br>a a a b<br>b a a<br>! b ba<br>! ba b baa b b|a<br>ba|a|a|a| ||!<br>a b a b<br>b a<br>! b ba<br>b a<br>! ba<br>!<br>a<br>a a a b<br>b a a<br>! b ba ba<br>! ba b baa b b<br>Figure 16.5<br>Illustration of the execution of Algorithm QueryLearnAutomat<br>Each line shows the current decision tree T and the tentative DF<br>A is not equivalent to A, the learner receives a counter-example<br>b|||||||||| Also, a state _A_ ( _u_ ) of _A_ is final iff _u_ accepted by _A_ that is iff _u_ is assigned to the right sub-tree of the root node by _T_, which is the criterion determining the finality of _A_ [\ufffd] ( _u_ ). Thus, the automata _A_ and _A_ [\ufffd] coincide. The following is the analysis of the running-time complexity of the algorithm. At each iteration, one new distinguished access string is found associated to a distinct state of _A_, thus, at most _|A|_ states are created. For each counter-example _x_, at most _|x|_ tree operations are performed. Constructing _A_ [\ufffd] requires _O_ ( _|_ \u03a3 _||A|_ ) tree operations. The cost of a tree operation is _O_ ( _|A|_ ) since it consists of at most _|A|_ **16.4** **Identification in the limit** **369** membership queries. Thus, the overall complexity of the algorithm is in _O_ ( _|_ \u03a3 _||A|_",
    "chunk_id": "foundations_machine_learning_356"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[2] + _n|A|_ ), where _n_ is the maximum length of a counter-example. Note that this analysis assumes that equivalence and membership queries are made in constant time. Our analysis shows the following result. **Theorem 16.5 (Learning DFAs with queries)** _The class of all DFAs is efficiently exactly_ _learnable using membership and equivalence queries._ Figure 16.5 illustrates a full execution of the algorithm in a specific case. In the next section, we examine a different learning scenario for automata. **16.4** **Identification in the limit** In the _identification in the limit framework_, the problem consists of identifying a target concept _c_ exactly after receiving a finite set of examples. A class of languages is said to be _identifiable in the limit_ if there exists an algorithm that identifies any language _L_ in that class after examining a finite number of examples and its hypothesis remains unchanged thereafter. This framework is perhaps less realistic from a computational point of view since it requires no upper bound on the number of instances or the efficiency of the algorithm. Nevertheless, it has been argued by some to be similar to the scenario of humans learning languages. In this framework as well, negative results hold for the general problem of learning DFAs. **Theorem 16.6** _Deterministic automata are not identifiable in the limit from positive_ _examples._ Some sub-classes of finite automata can however be successfully identified in the limit. Most algorithms for inference of automata are based on a _state-partitioning_ _paradigm_ . They start with an initial DFA, typically a tree accepting the finite set of sample strings available and the trivial partition: each block is reduced to one state of the tree. At each iteration, they merge partition blocks while preserving some congruence property. The iteration ends when no other merging is possible and the final partition defines the automaton inferred. Thus, the choice of the congruence fully determines the algorithm and a variety of different algorithms can be defined by varying that choice. A _state-splitting paradigm_ can be similarly defined starting from the single-state automaton accepting \u03a3 _[\u2217]_ . In this section, we present an algorithm for learning reversible automata, which is a special instance of the general state-partitioning algorithmic paradigm just described. Let _A_ = (\u03a3 _, Q, I, F, E_ ) be a DFA and let _\u03c0_ be a partition of _Q_ . The DFA defined by the partition _\u03c0_ is called the _automaton quotient of A and \u03c0_ . It is denoted by **370** **Chapter 16** **Learning Automata and Languages** _A/\u03c0_ and defined as follows: _A/\u03c0_ = (\u03a3 _, \u03c0, I_ _\u03c0_ _, F_ _\u03c0_ _, E_ _\u03c0_ ) with _I_ _\u03c0_ = _{B \u2208_ _\u03c0_ : _I \u2229_ _B \u0338_ = _\u2205}_ _F_ _\u03c0_ = _{B \u2208_ _\u03c0_ : _F \u2229_ _B \u0338_ = _\u2205}_ _E_ _\u03c0_ = _{_ ( _B, a, B_ _[\u2032]_ ): _\u2203_ ( _q, a, q_ _[\u2032]_ ) _\u2208_ _E | q \u2208_ _B, q_ _[\u2032]_ _\u2208_ _B_ _[\u2032]_ _, B \u2208_ _\u03c0, B_ _[\u2032]_ _\u2208_ _\u03c0}._ Let _S_ be a finite set of strings and let Pref(",
    "chunk_id": "foundations_machine_learning_357"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_S_ ) denote the set of prefixes of all strings of _S_ . A _prefix-tree automaton_ accepting exactly the set of strings _S_ is a particular DFA denoted by _PT_ ( _S_ ) = (\u03a3 _,_ Pref( _S_ ) _, {\u03f5}, S, E_ _S_ ) where \u03a3 is the set of alphabet symbols used in _S_ and _E_ _S_ defined as follows: _E_ _S_ = _{_ ( _x, a, xa_ ): _x \u2208_ Pref( _S_ ) _, xa \u2208_ Pref( _S_ ) _}._ Figure 16.7a shows the prefix-tree automaton of a particular set of strings _S_ . **16.4.1** **Learning reversible automata** In this section, we show that the sub-class of _reversible automata_ or _reversible_ _languages_ can be identified in the limit. In particular, we show that the language can be identified given a _positive presentation_ . A positive presentation of a language _L_ is an infinite sequence ( _x_ _n_ ) _n\u2208_ N such that _{x_ _n_ : _n \u2208_ N _}_ = _L_ . Thus, in particular, for any _x \u2208_ _L_ there exists _n \u2208_ N such that _x_ = _x_ _n_ . An algorithm identifies _L_ in the limit from a positive presentation if there exists _N \u2208_ N such that for _n \u2265_ _N_ the hypothesis it returns is _L_ . Given a DFA _A_, we define its _reverse A_ _[R]_ as the automaton derived from _A_ by making the initial state final, the final states initial, and by reversing the direction of every transition. The language accepted by the reverse of _A_ is precisely the language of the reverse (or mirror image) of the strings accepted by _A_ . **Definition 16.7 (Reversible automata)** _A finite automaton A is said to be_ reversible _iff_ _both A and A_ _[R]_ _are deterministic. A language L is said to be_ reversible _if it is the_ _language accepted by some reversible automaton._ Some direct consequences of this definition are that a reversible automaton _A_ has a unique final state and that its reverse _A_ _[R]_ is also reversible. Note also that a trim reversible automaton _A_ is minimal. Indeed, if states _q_ and _q_ _[\u2032]_ in _A_ are equivalent, then, they admit a common string _x_ leading both from _q_ and from _q_ _[\u2032]_ to a final state. But, by the reverse determinism of _A_, reading the reverse of _x_ from the final state must lead to a unique state, which implies that _q_ = _q_ _[\u2032]_ . For any _u \u2208_ \u03a3 _[\u2217]_ and any language _L \u2286_ \u03a3 _[\u2217]_, let Suff _L_ ( _u_ ) denote the set of all possible suffixes in _L_ for _u_ : Suff _L_ ( _u_ ) = _{v \u2208_ \u03a3 _[\u2217]_ : _uv \u2208_ _L}._ (16.1) **16.4** **Identification in the limit** **371** Suff _L_ ( _u_ ) is also often denoted by _u_ _[\u2212]_ [1] _L_ . Observe that if _L_ is a reversible language, then the following implication holds for any two strings _u, u_ _[\u2032]_ _\u2208_ \u03a3 _[\u2217]_ : Suff _L_ ( _u_ ) _\u2229_ Suff _L_ ( _u_ _[\u2032]_ )",
    "chunk_id": "foundations_machine_learning_358"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u0338_ = _\u2205_ = _\u21d2_ Suff _L_ ( _u_ ) = Suff _L_ ( _u_ _[\u2032]_ ) _._ (16.2) Indeed, let _A_ be a reversible automaton accepting _L_ . Let _q_ be the state of _A_ reached from the initial state when reading _u_ and _q_ _[\u2032]_ the one reached reading _u_ _[\u2032]_ . If _v \u2208_ Suff _L_ ( _u_ ) _\u2229_ Suff _L_ ( _u_ _[\u2032]_ ), then _v_ can be read both from _q_ and _q_ _[\u2032]_ to reach the final state. Since _A_ _[R]_ is deterministic, reading back the reverse of _v_ from the final state must lead to a unique state, therefore _q_ = _q_ _[\u2032]_, that is Suff _L_ ( _u_ ) = Suff _L_ ( _u_ _[\u2032]_ ). Let _A_ = (\u03a3 _, Q, {i_ 0 _}, {f_ 0 _}, E_ ) be a reversible automaton accepting a reversible language _L_ . We define a set of strings _S_ _L_ as follows: _S_ _L_ = _{d_ [ _q_ ] _f_ [ _q_ ] : _q \u2208_ _Q} \u222a{d_ [ _q_ ] _, a, f_ [ _q_ _[\u2032]_ ] : _q, q_ _[\u2032]_ _\u2208_ _Q, a \u2208_ \u03a3 _},_ where _d_ [ _q_ ] is a string of minimum length from _i_ 0 to _q_, and _f_ [ _q_ ] a string of minimum length from _q_ to _f_ 0 . As shown by the following proposition, _S_ _L_ characterizes the language _L_ in the sense that any reversible language containing _S_ _L_ must contain _L_ . **Proposition 16.8** _Let L be a reversible language. Then, L is the smallest reversible_ _language containing S_ _L_ _._ Proof: Let _L_ _[\u2032]_ be a reversible language containing _S_ _L_ and let _x_ = _x_ 1 _\u00b7 \u00b7 \u00b7 x_ _n_ be a string accepted by _L_, with _x_ _k_ _\u2208_ \u03a3 for _k \u2208_ [ _n_ ] and _n \u2265_ 1. For convenience, we also define _x_ 0 as _\u03f5_ . Let ( _q_ 0 _, x_ 1 _, q_ 1 ) _\u00b7 \u00b7 \u00b7_ ( _q_ _n\u2212_ 1 _, x_ _n_ _, q_ _n_ ) be the accepting path in _A_ labeled with _x_ . We show by recurrence that Suff _L_ _\u2032_ ( _x_ 0 _\u00b7 \u00b7 \u00b7 x_ _k_ ) = Suff _L_ _\u2032_ ( _d_ [ _q_ _k_ ]) for all _k \u2208{_ 0 _, . . ., n}_ . Since _d_ [ _q_ 0 ] = _d_ [ _i_ 0 ] = _\u03f5_, this clearly holds for _k_ = 0. Now assume that Suff _L_ _\u2032_ ( _x_ 0 _\u00b7 \u00b7 \u00b7 x_ _k_ ) = Suff _L_ _\u2032_ ( _d_ [ _q_ _k_ ]) for some _k \u2208{_ 0 _, . . ., n \u2212_ 1 _}_ . This implies immediately that Suff _L_ _[\u2032]_ ( _x_ 0 _\u00b7 \u00b7 \u00b7 x_ _k_ _x_ _k_ +1 ) = Suff _L_ _[\u2032]_ ( _d_ [ _q_ _k_ ] _x_ _k_ +1 ). By definition, _S_ _L_ contains both _d_ [ _q_ _k_ +1 ] _f_ [ _q_ _k_ +1 ] and _d_ [",
    "chunk_id": "foundations_machine_learning_359"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_q_ _k_ ] _x_ _k_ +1 _f_ [ _q_ _k_ +1 ]. Since _L_ _[\u2032]_ includes _S_ _L_, the same holds for _L_ _[\u2032]_ . Thus, _f_ [ _q_ _k_ +1 ] belongs to Suff _L_ _\u2032_ ( _d_ [ _q_ _k_ +1 ) _\u2229_ Suff _L_ _\u2032_ ( _d_ [ _q_ _k_ ] _x_ _k_ +1 ). In view of (16.2), this implies that Suff _L_ _\u2032_ ( _d_ [ _q_ _k_ ] _x_ _k_ +1 ) = Suff _L_ _\u2032_ ( _d_ [ _q_ _k_ +1 ]). Thus, we have Suff _L_ _\u2032_ ( _x_ 0 _\u00b7 \u00b7 \u00b7 x_ _k_ _x_ _k_ +1 ) = Suff _L_ _\u2032_ ( _d_ [ _q_ _k_ +1 ]). This shows that Suff _L_ _\u2032_ ( _x_ 0 _\u00b7 \u00b7 \u00b7 x_ _k_ ) = Suff _L_ _\u2032_ ( _d_ [ _q_ _k_ ]) holds for all _k \u2208{_ 0 _, . . ., n}_, in particular, for _k_ = _n_ . Note that since _q_ _n_ = _f_ 0, we have _f_ [ _q_ _n_ ] = _\u03f5_, therefore _d_ [ _q_ _n_ ] = _d_ [ _q_ _n_ ] _f_ [ _q_ _n_ ] is in _S_ _L_ _\u2286_ _L_ _[\u2032]_, which implies that Suff _L_ _[\u2032]_ ( _d_ [ _q_ _n_ ]) contains _\u03f5_ and thus that Suff _L_ _[\u2032]_ ( _x_ 0 _\u00b7 \u00b7 \u00b7 x_ _n_ ) contains _\u03f5_ . This is equivalent to _x_ = _x_ 0 _\u00b7 \u00b7 \u00b7 x_ _n_ _\u2208_ _L_ _[\u2032]_ . Figure 16.6 shows the pseudocode of an algorithm for inferring a reversible automaton from a sample _S_ of _m_ strings _x_ 1 _, . . ., x_ _m_ . The algorithm starts by creating a prefix-tree automaton _A_ for _S_ (line 1) and then iteratively defines a partition _\u03c0_ of the states of _A_, starting with the trivial partition _\u03c0_ 0 with one block per state (line 2). The automaton returned is the quotient of _A_ and the final partition _\u03c0_ defined. **372** **Chapter 16** **Learning Automata and Languages** LearnReversibleAutomata( _S_ = ( _x_ 1 _, . . ., x_ _m_ )) 1 _A_ = (\u03a3 _, Q, {i_ 0 _}, F, E_ ) _\u2190_ _PT_ ( _S_ ) 2 _\u03c0 \u2190_ _\u03c0_ 0 _\u25b7_ trivial partition. 3 list _\u2190{_ ( _f, f_ _[\u2032]_ ): _f_ _[\u2032]_ _\u2208_ _F_ _} \u25b7f_ arbitrarily chosen in _F_ . 4 **while** list _\u0338_ = _\u2205_ **do** 5 Remove(list _,_ ( _q_ 1 _, q_ 2 )) 6 **if** _B_ ( _q_ 1 _, \u03c0_ ) _\u0338_ = _B_ ( _q_ 2 _, \u03c0_ ) **then** 7 _B_ 1 _\u2190_ _B_ ( _q_ 1 _, \u03c0_ ) 8 _B_ 2 _\u2190_ _B_ ( _q_ 2 _, \u03c0_ ) 9 **for** all _a \u2208_ \u03a3 **do** 10 **if** ( _succ_ ( _B_ 1 _, a_ ) _\u0338_ = _\u2205_ ) _\u2227_ ( _succ_ ( _B_ 2 _, a_ ) _\u0338_ = _\u2205_ ) **then** 11 Add(list _,_ ( _succ_ ( _B_ 1 _, a_ ) _, succ_ ( _B_ 2 _, a_ ))) 12 **if** (",
    "chunk_id": "foundations_machine_learning_360"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_pred_ ( _B_ 1 _, a_ ) _\u0338_ = _\u2205\u2227_ ( _pred_ ( _B_ 2 _, a_ ) _\u0338_ = _\u2205_ )) **then** 13 Add(list _,_ ( _pred_ ( _B_ 1 _, a_ ) _, pred_ ( _B_ 2 _, a_ ))) 14 Update( _succ, pred, B_ 1 _, B_ 2 ) 15 _\u03c0 \u2190_ Merge( _\u03c0, B_ 1 _, B_ 2 ) 16 **return** _A/\u03c0_ **Figure 16.6** Algorithm for learning reversible automata from a set of positive strings _S_ . The algorithm maintains a list list of pairs of states whose corresponding blocks are to be merged, starting with all pairs of final states ( _f, f_ _[\u2032]_ ) for an arbitrarily chosen final state _f \u2208_ _F_ (line 3). We denote by _B_ ( _q, \u03c0_ ) the block containing _q_ based on the partition _\u03c0_ . For each block _B_ and alphabet symbol _a \u2208_ \u03a3, the algorithm also maintains a successor _succ_ ( _B, a_ ), that is, a state that can be reached by reading _a_ from a state of _B_ ; _succ_ ( _B, a_ ) = _\u2205_ if no such state exists. It maintains similarly the predecessor _pred_ ( _B, a_ ), which is a state that admits a transition labeled with _a_ leading to a state in _B_ ; _pred_ ( _B, a_ ) = _\u2205_ if no such state exists. Then, while list is not empty, a pair is removed from list and processed as follows. If the pair ( _q_ 1 _, q_ 1 _[\u2032]_ [) has not been already merged, the pairs formed by the] successors and predecessors of _B_ 1 = _B_ ( _q_ 1 _, \u03c0_ ) and _B_ 2 = _B_ ( _q_ 2 _, \u03c0_ ) are added to list **16.4** **Identification in the limit** **373** 0 a 1 b 10 a 2 b 5 a 11 b 14 a 3 a 4 {1, 3, 8, 12} {0, 2, 4, 7, 9, 13, 14} **a** **b** a 6 b 8 b 7 a 9 {6, 10} b 12 a 13 **b** **a** {5, 11} (a) (b) **Figure 16.7** Example of inference of a reversible automaton. (a) Prefix-tree _PT_ ( _S_ ) representing _S_ = ( _\u03f5, aa, bb, aaaa, abab, abba, baba_ ). (b) Automaton _A_ [\ufffd] returned by LearnReversibleAutomata() for the input _S_ . A double-direction arrow represents two transitions with the same label with opposite directions. The language accepted by _A_ [\ufffd] is that of strings with an even number of _a_ s and _b_ s. (lines 10\u201313). Before merging blocks _B_ 1 and _B_ 2 into a new block _B_ _[\u2032]_ that defines a new partition _\u03c0_ (line 15), the successor and predecessor values for the new block _B_ _[\u2032]_ are defined as follows (line 14). For each symbol _a \u2208_ \u03a3, _succ_ ( _B_ _[\u2032]_ _, a_ ) = _\u2205_ if _succ_ ( _B_ 1 _, a_ ) = _succ_ ( _B_ 2 _, a_ ) = _\u2205_, otherwise _succ_ ( _B_ _[\u2032]_ _, a_ ) is set to one of _succ_ (",
    "chunk_id": "foundations_machine_learning_361"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_B_ 1 _, a_ ) if it is non-empty, _succ_ ( _B_ 2 _, a_ ) otherwise. The predecessor values are defined in a similar way. Figure 16.7 illustrates the application of the algorithm in the case of a sample with _m_ = 7 strings. **Proposition 16.9** _Let S be a finite set of strings and let A_ = _PT_ ( _S_ ) _be the_ _prefix-tree automaton defined from S._ _Then, the final partition defined by_ LearnReversibleAutomata( _) used with input S is the finest partition \u03c0 for_ _which A/\u03c0 is reversible._ Proof: Let _T_ be the number of iterations of the algorithm for the input sample _S_ . We denote by _\u03c0_ _t_ the partition defined by the algorithm after _t \u2265_ 1 iterations of the loop, with _\u03c0_ _T_ the final partition. _A/\u03c0_ _T_ is a reversible automaton since all final states are guaranteed to be merged into the same block as a consequence of the initialization step of line 3 and, for any block _B_, by definition of the algorithm, states reachable by _a \u2208_ \u03a3 from _B_ are contained in the same block, and similarly for those admitting a transition labeled with _a_ to a state of _B_ . Let _\u03c0_ _[\u2032]_ be a partition of the states of _A_ for which _A/\u03c0_ _[\u2032]_ is reversible. We show by recurrence that _\u03c0_ _T_ refines _\u03c0_ _[\u2032]_ . Clearly, the trivial partition _\u03c0_ 0 refines _\u03c0_ _[\u2032]_ . Assume that _\u03c0_ _s_ refines _\u03c0_ _[\u2032]_ for all _s \u2264_ _t_ . _\u03c0_ _t_ +1 is obtained from _\u03c0_ by merging two blocks _B_ ( _q_ 1 _, \u03c0_ _t_ ) and _B_ ( _q_ 2 _, \u03c0_ _t_ ). Since _\u03c0_ _t_ refines _\u03c0_ _[\u2032]_, we must have _B_ ( _q_ 1 _, \u03c0_ _t_ ) _\u2286_ _B_ ( _q_ 1 _, \u03c0_ _[\u2032]_ ) **374** **Chapter 16** **Learning Automata and Languages** and _B_ ( _q_ 2 _, \u03c0_ _t_ ) _\u2286_ _B_ ( _q_ 2 _, \u03c0_ _[\u2032]_ ). To show that _\u03c0_ _t_ +1 refines _\u03c0_ _[\u2032]_, it suffices to prove that _B_ ( _q_ 1 _, \u03c0_ _[\u2032]_ ) = _B_ ( _q_ 2 _, \u03c0_ _[\u2032]_ ). A reversible automaton has only one final state, therefore, for the partition _\u03c0_ _[\u2032]_, all final states of _A_ must be placed in the same block. Thus, if the pair ( _q_ 1 _, q_ 2 ) processed at the ( _t_ + 1)th iteration is a pair of final states placed in list at the initialization step (line 3), then we must have _B_ ( _q_ 1 _, \u03c0_ _[\u2032]_ ) = _B_ ( _q_ 2 _, \u03c0_ _[\u2032]_ ). Otherwise, ( _q_ 1 _, q_ 2 ) was placed in list as a pair of successor or predecessor states of two states _q_ 1 _[\u2032]_ [and] _[ q]_ 2 _[\u2032]_ [merged at a previous iteration] _[ s][ \u2264]_ _[t]_ [. Since] _[ \u03c0]_ _[s]_ [refines] _[ \u03c0]_ _[\u2032]_ [,] _[ q]_ 1 _[\u2032]_ [and] _[ q]_ 2 _[\u2032]_ [are] in the same block of _\u03c0_ _[\u2032]_",
    "chunk_id": "foundations_machine_learning_362"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "and since _A/\u03c0_ _[\u2032]_ is reversible, _q_ 1 and _q_ 2 must also be in the same block as successors or predecessors of the same block for the same label _a \u2208_ \u03a3, thus _B_ ( _q_ 1 _, \u03c0_ _[\u2032]_ ) = _B_ ( _q_ 2 _, \u03c0_ _[\u2032]_ ). **Theorem 16.10** _Let S be a finite set of strings and let A be the automaton returned_ _by_ LearnReversibleAutomata( _) when used with input S. Then, L_ ( _A_ ) _is the_ _smallest reversible language containing S._ Proof: Let _L_ be a reversible language containing _S_, and let _A_ _[\u2032]_ be a reversible automaton with _L_ ( _A_ _[\u2032]_ ) = _L_ . Since every string of _S_ is accepted by _A_ _[\u2032]_, any _u \u2208_ Pref( _S_ ) can be read from the initial state of _A_ _[\u2032]_ to reach some state _q_ ( _u_ ) of _A_ _[\u2032]_ . Consider the automaton _A_ _[\u2032\u2032]_ derived from _A_ _[\u2032]_ by keeping only states of the form _q_ ( _u_ ) and transitions between such states. _A_ _[\u2032\u2032]_ has the unique final state of _A_ _[\u2032]_ since _q_ ( _u_ ) is final for _u \u2208_ _S_, and it has the initial state of _A_ _[\u2032]_, since _\u03f5_ is a prefix of strings of _S_ . Furthermore, _A_ _[\u2032\u2032]_ directly inherits from _A_ _[\u2032]_ the property of being deterministic and reverse deterministic. Thus, _A_ _[\u2032\u2032]_ is reversible. The states of _A_ _[\u2032\u2032]_ define a partition of Pref( _S_ ): _u, v \u2208_ Pref( _S_ ) are in the same block iff _q_ ( _u_ ) = _q_ ( _v_ ). Since by definition of the prefix-tree _PT_ ( _S_ ), its states can be identified with Pref( _S_ ), the states of _A_ _[\u2032\u2032]_ also define a partition _\u03c0_ _[\u2032]_ of the states of _PT_ ( _S_ ) and thus _A_ _[\u2032\u2032]_ = _PT_ ( _S_ ) _/\u03c0_ _[\u2032]_ . By proposition 16.9, the partition _\u03c0_ defined by algorithm LearnReversibleAutomata() run with input _S_ is the finest such that _PT_ ( _S_ ) _/\u03c0_ is reversible. Therefore, we must have _L_ ( _PT_ ( _S_ ) _/\u03c0_ ) _\u2286_ _L_ ( _PT_ ( _S_ ) _/\u03c0_ _[\u2032]_ ) = _L_ ( _A_ _[\u2032\u2032]_ ). Since _A_ _[\u2032\u2032]_ is a sub-automaton of _A_ _[\u2032]_, _L_ contains _L_ ( _A_ _[\u2032\u2032]_ ) and therefore _L_ ( _PT_ ( _S_ ) _/\u03c0_ ) = _L_ ( _A_ ), which concludes the proof. **Theorem 16.11 (Identification in the limit of reversible languages)** _Let L be a reversible_ _language, then algorithm_ LearnReversibleAutomata( _) identifies L in the limit_ _from a positive presentation._ Proof: Let _L_ be a reversible language. By proposition 16.8, _L_ admits a finite characteristic sample _S_ _L_ . Let ( _x_ _n_ ) _n\u2208_ N be a positive presentation of _L_ and let X _n_ denote the union of the first _n_ elements of the sequence. Since _S_ _L_ is finite, there exists _N \u2265_ 1 such that _S_ _L_ _\u2286_ X _N_ . By theorem 16.10, for any _n \u2265_ _N_, LearnReversibleAutomata()",
    "chunk_id": "foundations_machine_learning_363"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "run on the finite sample X _n_ returns the smallest **16.5** **Chapter notes** **375** reversible language _L_ _[\u2032]_ containing X _n_ a fortiori _S_ _L_, which, by definition of _S_ _L_, implies that _L_ _[\u2032]_ = _L_ . The main operations needed for the implementation of the algorithm for learning reversible automata are the standard find and union to determine the block a state belongs to and to merge two blocks into a single one. Using a disjoint-set data structure for these operations, the time complexity of the algorithm can be shown to be in _O_ ( _n\u03b1_ ( _n_ )), where _n_ denotes the sum of the lengths of all strings in the input sample _S_ and _\u03b1_ ( _n_ ) the inverse of the Ackermann function, which is essentially constant ( _\u03b1_ ( _n_ ) _\u2264_ 4 for _n \u2264_ 10 [80] ). **16.5** **Chapter notes** For an overview of finite automata and some related results, see Hopcroft and Ullman [1979] or the more recent Handbook chapter by Perrin [1990], as well as the series of books by M. Lothaire [Lothaire, 1982, 1990, 2005] and the even more recent book by De la Higuera [2010]. Theorem 16.2, stating that the problem of finding a minimum consistent DFA is NP-hard, is due to Gold [1978]. This result was later extended by Angluin [1978]. Pitt and Warmuth [1993] further strengthened these results by showing that even an approximation within a polynomial function of the size of the smallest automaton is NP-hard (theorem 16.3). Their hardness results apply also to the case where prediction is made using NFAs. Kearns and Valiant [1994] presented hardness results of a different nature relying on cryptographic assumptions. Their results imply that no polynomial-time algorithm can learn consistent NFAs polynomial in the size of the smallest DFA from a finite sample of accepted and rejected strings if any of the generally accepted cryptographic assumptions holds: if factoring Blum integers is hard; or if the RSA public key cryptosystem is secure; or if deciding quadratic residuosity is hard. Most recently, Chalermsook et al. [2014] improved the non-approximation guarantee of Pitt and Warmuth [1993] to a tight bound. On the positive side, Trakhtenbrot and Barzdin [1973] showed that the smallest finite automaton consistent with the input data can be learned exactly from a uniform complete sample, whose size is exponential in the size of the automaton. The worst-case complexity of their algorithm is exponential, but a better averagecase complexity can be obtained assuming that the topology and the labeling are selected randomly [Trakhtenbrot and Barzdin, 1973] or even that the topology is selected adversarially [Freund et al., 1993]. Cortes, Kontorovich, and Mohri [2007a] study an approach to the problem of learning automata based on linear separation in some appropriate high-dimensional feature space; see also Kontorovich et al. [2006, 2008]. The mapping of strings to **376** **Chapter 16** **Learning Automata and Languages** that feature space can be defined implicitly using the rational kernels presented in chapter 6, which are themselves defined via weighted automata and transducers. The model of learning with",
    "chunk_id": "foundations_machine_learning_364"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "queries was introduced by Angluin [1978], who also proved that finite automata can be learned in time polynomial in the size of the minimal automaton and that of the longest counter-example. Bergadano and Varricchio [1995] further extended this result to the problem of learning weighted automata defined over any field (see also an optimal algorithm by Bisht et al. [2006]). Using the relationship between the size of a minimal weighted automaton over a field and the rank of the corresponding Hankel matrix, the learnability of many other concepts classes such as disjoint DNF can be shown [Beimel et al., 2000]. Our description of an efficient implementation of the algorithm of Angluin [1982] using decision trees is adapted from Kearns and Vazirani [1994]. The model of identification in the limit of automata was introduced and analyzed by Gold [1967]. Deterministic finite automata were shown not to be identifiable in the limit from positive examples [Gold, 1967]. But, positive results were given for the identification in the limit of a number of sub-classes, such as the family of _k_ reversible languages Angluin [1982] considered in this chapter. Positive results also hold for learning subsequential transducers Oncina et al. [1993]. Some restricted classes of probabilistic automata such as acyclic probabilistic automata were also shown by Ron et al. [1995] to be efficiently learnable. There is a vast literature dealing with the problem of learning automata. In particular, positive results have been shown for a variety of sub-families of finite automata in the scenario of learning with queries and learning scenarios of different kinds have been introduced and analyzed for this problem. The results presented in this chapter should therefore be viewed only as an introduction to that material. **16.6** **Exercises** 16.1 Minimal DFA. Show that a minimal DFA _A_ also has the minimal number of transitions among all other DFAs equivalent to _A_ . Prove that a language _L_ is regular iff _Q_ = _{_ Suff _L_ ( _u_ ): _u \u2208_ \u03a3 _[\u2217]_ _}_ is finite. Show that the number of states of a minimal DFA _A_ with _L_ ( _A_ ) = _L_ is precisely the cardinality of _Q_ . 16.2 VC-dimension of finite automata. (a) What is the VC-dimension of the family of all finite automata? What does that imply for PAC-learning of finite automata? Does this result change if we restrict ourselves to learning acyclic automata (automata with no cycles)? **16.6** **Exercises** **377** (b) Show that the VC-dimension of the family of DFAs with at most _n_ states is bounded by _O_ ( _|_ \u03a3 _|n_ log _n_ ). 16.3 PAC learning with membership queries. Give an example of a concept class C that is efficiently PAC-learnable with membership queries but that is not efficiently exactly learnable. 16.4 Learning monotone DNF formulae with queries. Show that the class of monotone DNF formulae over _n_ variables is efficiently exactly learnable using membership and equivalence queries. ( _Hint_ : a _prime implicant t_ of a formula _f_ is a product of literals such that _t_ implies _f_ but no proper sub-term of _t_",
    "chunk_id": "foundations_machine_learning_365"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "implies _f_ . Use the fact that for monotone DNF, the number of prime implicants is at the most the number of terms of the formula.) 16.5 Learning with unreliable query responses. Consider the problem where the learner must find an integer _x_ selected by the oracle within [ _n_ ], where _n \u2265_ 1 is given. To do so, the learner can ask questions of the form ( _x \u2264_ _m_ ?) or ( _x > m_ ?) for _m \u2208_ [ _n_ ]. The oracle responds to these questions but may give an incorrect response to _k_ questions. How many questions should the learner ask to determine _x_ ? ( _Hint_ : observe that the learner can repeat each question 2 _k_ +1 times and use the majority vote.) 16.6 Algorithm for learning reversible languages. What is the DFA _A_ returned by the algorithm for learning reversible languages when applied to the sample _S_ = _{ab, aaabb, aabbb, aabbbb}_ ? Suppose we add a new string to the sample, say _x_ = _abab_ . How should _A_ be updated to compute the result of the algorithm for _S \u222a{x}_ ? More generally, describe a method for updating the result of the algorithm incrementally. 16.7 _k_ -reversible languages. A finite automaton _A_ _[\u2032]_ is said to be _k-deterministic_ if it is deterministic modulo a lookahead _k_ : if two distinct states _p_ and _q_ are both initial, or are both reached from another state _r_ by reading _a \u2208_ \u03a3, then no string _u_ of length _k_ can be read in _A_ _[\u2032]_ both from _p_ and _q_ . A finite automaton _A_ is said to be _k-reversible_ if it is deterministic and if _A_ _[R]_ is _k_ -deterministic. A language _L_ is _k-reversible_ if it is accepted by some _k_ -reversible automaton. (a) Prove that _L_ is _k_ -reversible iff for any strings _u, u_ _[\u2032]_ _, v \u2208_ \u03a3 _[\u2217]_ with _|v|_ = _k_, Suff _L_ ( _uv_ ) _\u2229_ Suff _L_ ( _u_ _[\u2032]_ _v_ ) _\u0338_ = _\u2205_ = _\u21d2_ Suff _L_ ( _uv_ ) = Suff _L_ ( _u_ _[\u2032]_ _v_ ) _._ (b) Show that a _k_ -reversible language admits a characteristic language. **378** **Chapter 16** **Learning Automata and Languages** (c) Show that the following defines an algorithm for learning _k_ -reversible automata. Proceed as in the algorithm for learning reversible automata but with the following merging rule instead: merge blocks _B_ 1 and _B_ 2 if they can be reached by the same string _u_ of length _k_ from some other block and if _B_ 1 and _B_ 2 are both final or have a common successor. # 17 Reinforcement Learning This chapter presents an introduction to reinforcement learning, a rich area of machine learning with connections to control theory, optimization, and cognitive sciences. Reinforcement learning is the study of planning and learning in a scenario where a learner actively interacts with the environment to achieve a certain goal. This active interaction justifies the terminology of _agent_ used to refer to the learner. The",
    "chunk_id": "foundations_machine_learning_366"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "achievement of the agent\u2019s goal is typically measured by the reward it receives from the environment and which it seeks to maximize. We first introduce the general scenario of reinforcement learning and then introduce the model of Markov decision processes (MDPs), which is widely adopted in this area, as well as essential concepts such as that of _policy_ or _policy value_ related to this model. The rest of the chapter presents several algorithms for the _planning_ problem, which corresponds to the case where the environment model is known to the agent, and then a series of _learning_ algorithms for the more general case of an unknown model. **17.1** **Learning scenario** The general scenario of reinforcement learning is illustrated by figure 17.1. Unlike the supervised learning scenario considered in previous chapters, here, the learner does not passively receive a labeled data set. Instead, it collects information through a course of _actions_ by interacting with the _environment_ . In response to an action, the learner or _agent_, receives two types of information: its current _state_ in the environment, and a real-valued _reward_, which is specific to the task and its corresponding goal. The objective of the agent is to maximize its reward and thus to determine the best course of actions, or _policy_, to achieve that objective. However, the information he receives from the environment is only the immediate reward related to the action just taken. No future or long-term reward feedback is provided by the environment. An important aspect of reinforcement learning is to consider delayed rewards or **380** **Chapter 17** **Reinforcement Learning** action reward **Figure 17.1** **Image:** [No caption returned] Representation of the general scenario of reinforcement learning. penalties. The agent is faced with the dilemma between exploring unknown states and actions to gain more information about the environment and the rewards, and exploiting the information already collected to optimize its reward. This is known as the _exploration versus exploitation trade-off_ inherent to reinforcement learning. Note that there are several differences between the learning scenario of reinforcement learning and that of supervised learning examined in most of the previous chapters. Unlike supervised learning, in reinforcement learning there is no fixed distribution according to which instances are drawn; it is the choice of a policy that defines the distribution over observations. In fact, slight changes to the policy may have dramatic effects on the rewards received. Furthermore, in general, the environment may not be fixed and could vary as a result of the actions selected by the agent. This may be a more realistic model for some learning problems than the standard supervised learning. Finally, note that, unlike supervised learning, in reinforcement learning, training and testing phases are intermixed. Two main settings can be distinguished here: the one where the environment model is known to the agent, in which case its objective of maximizing the reward received is reduced to a _planning problem_ ; and the one where the environment model is unknown, in which case the agent faces a _learning problem_ . In the latter setting, the agent must learn from the",
    "chunk_id": "foundations_machine_learning_367"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "state and reward information gathered to both gain information about the environment and determine the best action policy. This chapter presents algorithmic solutions for both of these settings. **17.2** **Markov decision process model** We first introduce the model of Markov decision processes (MDPs), a model of the environment and interactions with the environment widely adopted in reinforcement learning. An MDP is a Markovian process defined as follows. **Definition 17.1 (MDPs)** _A Markov decision process (MDP) is defined by:_ _\u2022_ _a set of_ states _S, possibly infinite._ **17.3** **Policy** **381** **Figure 17.2** **Image:** [No caption returned] Illustration of the states and transitions of an MDP at different times. _\u2022_ _a_ start state _or_ initial state _s_ 0 _\u2208_ _S._ _\u2022_ _a set of_ actions _A, possibly infinite._ _\u2022_ _a_ transition probability P[ _s_ _[\u2032]_ _|s, a_ ] _: distribution over destination states s_ _[\u2032]_ = _\u03b4_ ( _s, a_ ) _._ _\u2022_ _a_ reward probability P[ _r_ _[\u2032]_ _|s, a_ ] _: distribution over rewards returned r_ _[\u2032]_ = _r_ ( _s, a_ ) _._ The model is Markovian because the transition and reward probabilities depend only on the current state _s_ and not the entire history of states and actions taken. This definition of MDP can be further generalized to the case of non-discrete state and action sets. In a discrete-time model, actions are taken at a set of _decision epochs {_ 0 _, . . ., T_ _}_, and this is the model we will adopt in what follows. This model can also be straightforwardly generalized to a continuous-time one where actions are taken at arbitrary points in time. When _T_ is finite, the MDP is said to have a _finite horizon_ . Independently of the finiteness of the time horizon, an MDP is said to be _finite_ when both _S_ and _A_ are finite sets. Here, we are considering the general case where the reward _r_ ( _s, a_ ) at state _s_ when taking action _a_ is a random variable. However, in many cases, the reward is assumed to be a deterministic function the state and action pair ( _s, a_ ). Figure 17.2 illustrates the model corresponding to an MDP. At time _t \u2208{_ 0 _, . . ., T_ _}_ the state observed by the agent is _s_ _t_ and it takes action _a_ _t_ _\u2208_ _A_ . The state reached is _s_ _t_ +1 (with probability P[ _s_ _t_ +1 _|s_ _t_ _, a_ _t_ ]) and the reward received _r_ _t_ +1 _\u2208_ R (with probability P[ _r_ _t_ +1 _|s_ _t_ _, a_ _t_ ]). Many real-world tasks can be represented by MDPs. Figure 17.3 gives the example of a simple MDP for a robot picking up balls on a tennis court. **17.3** **Policy** The main problem for an agent in an MDP environment is to determine the action to take at each state, that is, an action _policy_ . **17.3.1** **Definition** **Definition 17.2 (Policy)** _A_ policy _is a mapping \u03c0_ : _S \u2192_ \u2206( _A_ ) _, where_ \u2206( _A_ ) _is the",
    "chunk_id": "foundations_machine_learning_368"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "set_ _of probability distributions over A. A policy \u03c0 is_ deterministic _if for any s, there_ _exists a unique a \u2208_ _A such that \u03c0_ ( _s_ )( _a_ ) = 1 _. In that case, we can identify \u03c0 with a_ _mapping from S to A and use \u03c0_ ( _s_ ) _to denote that action._ **382** **Chapter 17** **Reinforcement Learning** More precisely, this is the definition of a _stationary policy_ since the choice of the distribution of actions does not depend on time. More generally, we could define a _non-stationary policy_ as a sequence of mappings _\u03c0_ _t_ : _S \u2192_ \u2206( _A_ ) indexed by _t_ . In particular, in the finite horizon case, a non-stationary policy is typically necessary for optimizing rewards. The agent\u2019s objective is to find a policy that maximizes its expected (reward) _return_ . The return it receives following a deterministic policy _\u03c0_ along a specific sequence of states _s_ 0 _, . . ., s_ _T_ is defined as follows: _\u2022_ for a finite horizon ( _T < \u221e_ ): [\ufffd] _[T]_ _t_ =0 _[r]_ \ufffd _s_ _t_ _, \u03c0_ ( _s_ _t_ )\ufffd. _\u2022_ for an infinite horizon ( _T_ = _\u221e_ ): [\ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b3]_ _[t]_ _[r]_ \ufffd _s_ _t_ _, \u03c0_ ( _s_ _t_ )\ufffd, where _\u03b3 \u2208_ [0 _,_ 1) is a constant factor less than one used to discount future rewards. Note that the return is a single scalar summarizing a possibly infinite sequence of immediate rewards. In the discounted case, early rewards are viewed as more valuable than later ones. **17.3.2** **Policy value** This leads to the following definition of the value of a policy at each state. **Definition 17.3 (Policy value)** _The_ value _V_ _\u03c0_ ( _s_ ) _of a policy \u03c0 at state s \u2208_ _S is defined_ _as the expected reward returned when starting at s and following policy \u03c0:_ _T_ _\u2022_ _finite horizon: V_ _\u03c0_ ( _s_ ) = E _a_ _t_ _\u223c\u03c0_ ( _s_ _t_ ) \ufffd\ufffd _t_ =0 _[r]_ \ufffd _s_ _t_ _, a_ _t_ \ufffd [\ufffd] \ufffd\ufffd _s_ 0 = _s_ \ufffd _;_ + _\u221e_ _\u2022_ _infinite discounted horizon: V_ _\u03c0_ ( _s_ ) = E _a_ _t_ _\u223c\u03c0_ ( _s_ _t_ ) \ufffd\ufffd _t_ =0 _[\u03b3]_ _[t]_ _[r]_ \ufffd _s_ _t_ _, a_ _t_ \ufffd [\ufffd] \ufffd\ufffd _s_ 0 = _s_ \ufffd _,_ _where the expectations are over the random selection of an action a_ _t_ _according to_ _the distribution \u03c0_ ( _s_ _t_ ) _, which is explicitly indicated, and over the random states s_ _t_ _reached and the reward values r_ \ufffd _s_ _t_ _, a_ _t_ \ufffd _._ [22] _An infinite undiscounted horizon is also_ _often considered based on the limit of the average reward, when it exists._ **17.3.3** **Optimal policies** Starting from a state _s \u2208_ _S_, to maximize its reward, an agent naturally seeks a policy _\u03c0_ with the largest value _V_ _\u03c0_ ( _s_ ). In this section, we will show that, remarkably, for any finite MDP in the infinite horizon",
    "chunk_id": "foundations_machine_learning_369"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "setting, there exists a policy that is _optimal_ for _any_ start state, that is one with the following definition. **Definition 17.4 (Optimal policy)** _A policy \u03c0_ _[\u2217]_ _is_ optimal _if its value is maximal for every_ _state s \u2208_ _S, that is, for any policy \u03c0 and any state s \u2208_ _S, V_ _\u03c0_ _\u2217_ ( _s_ ) _\u2265_ _V_ _\u03c0_ ( _s_ ) _._ 22 More generally, in all that follows, the randomization with respect to the reward function and the next state will not be explicitly indicated to simplify the notation. **17.3** **Policy** **383** **Image:** [No caption returned] carry/[.5, -1] pickup/[1, R2] **Figure 17.3** Example of a simple MDP for a robot picking up balls on a tennis court. The set of actions is _A_ = _{search, carry, pickup}_ and the set of states reduced to _S_ = _{start, other}_ . Each transition is labeled with the action followed by the probability of the transition probability and the reward received after taking that action. _R_ 1, _R_ 2, and _R_ 3 are real numbers indicating the reward associated to each transition (case of deterministic reward). Moreover, we will show that for any MDP there exists a deterministic optimal policy. To do so, it is convenient to introduce the notion of _state-action value_ _function_ . **Definition 17.5 (State-action value function)** _The_ state-action value function _Q associ-_ _ated to a policy \u03c0 is defined for all_ ( _s, a_ ) _\u2208_ _S \u00d7 A as the expected return for taking_ _action a \u2208_ _A at state s \u2208_ _S and then following policy \u03c0:_ \ufffd _Q_ _\u03c0_ ( _s, a_ ) = E[ _r_ ( _s, a_ )] + E _a_ _t_ _\u223c\u03c0_ ( _s_ _t_ ) + _\u221e_ \ufffd \ufffd _t_ =1 \ufffd _\u03b3_ _[t]_ _r_ \ufffd _s_ _t_ _, a_ _t_ \ufffd [\ufffd] \ufffd\ufffd _s_ 0 = _s, a_ 0 = _a_ _t_ =1 (17.1) = E _r_ ( _s, a_ ) + _\u03b3V_ _\u03c0_ ( _s_ 1 ) _s_ 0 = _s, a_ 0 = _a_ _._ \ufffd \ufffd\ufffd\ufffd \ufffd Observe that E _a\u223c\u03c0_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd = _V_ _\u03c0_ ( _s_ ) (see also proposition 17.9) **Theorem 17.6 (Policy improvement theorem)** _For any two policies \u03c0 and \u03c0_ _[\u2032]_ _the follow-_ _ing holds:_ \ufffd _\u2200s \u2208_ _S,_ _a\u223c\u03c0_ E _[\u2032]_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd _\u2265_ _a\u223c_ E _\u03c0_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd [\ufffd] _\u21d2_ \ufffd _\u2200s \u2208_ _S, V_ _\u03c0_ _\u2032_ ( _s_ ) _\u2265_ _V_ _\u03c0_ ( _s_ )\ufffd _._ _Furthermore, a strict inequality for at least one state s in the left-hand side implies_ _a strict inequality for at least one s in the right-hand side._ \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd _\u2265_ _a\u223c_ E _\u03c0_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd [\ufffd] _\u21d2_ \ufffd _\u2200s \u2208_ _S, V_ _\u03c0_ _\u2032_ ( _s_ ) _\u2265_ _V_ _\u03c0_ ( _s_ )\ufffd _._ **384** **Chapter 17** **Reinforcement Learning** Proof: Assume that _\u03c0_ and _\u03c0_",
    "chunk_id": "foundations_machine_learning_370"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u2032]_ verify the left-hand side. For any _s \u2208_ _S_, we have _\u0338_ _\u0338_ _V_ _\u03c0_ ( _s_ ) = _a\u223c_ E _\u03c0_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd _\u0338_ _\u0338_ \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd _r_ ( _s, a_ ) + _\u03b3V_ _\u03c0_ ( _s_ 1 ) _s_ 0 = _s_ \ufffd \ufffd\ufffd\ufffd \ufffd _\u0338_ _\u0338_ _\u2264_ E _a\u223c\u03c0_ _[\u2032]_ ( _s_ ) = E _a\u223c\u03c0_ _[\u2032]_ ( _s_ ) = E _a\u223c\u03c0_ _[\u2032]_ ( _s_ ) _\u2264_ E _a\u223c\u03c0_ _[\u2032]_ ( _s_ ) _\u0338_ _\u0338_ \ufffd _Q_ _\u03c0_ ( _s_ 1 _, a_ 1 )\ufffd [\ufffd] \ufffd\ufffd _s_ 0 = _s_ \ufffd \ufffd _Q_ _\u03c0_ ( _s_ 1 _, a_ 1 )\ufffd [\ufffd] \ufffd\ufffd _s_ 0 = _s_ \ufffd _\u0338_ _\u0338_ _r_ ( _s, a_ ) + _\u03b3_ E \ufffd _a_ 1 _\u223c\u03c0_ ( _s_ 1 ) _r_ ( _s, a_ ) + _\u03b3_ E \ufffd _a_ 1 _\u223c\u03c0_ _[\u2032]_ ( _s_ 1 ) _\u0338_ _\u0338_ = E _a\u223c\u03c0_ _[\u2032]_ ( _s_ ) _a_ 1 _\u223c\u03c0_ _[\u2032]_ ( _s_ 1 ) _\u0338_ _\u0338_ _r_ ( _s, a_ ) + _\u03b3r_ ( _s_ 1 _, a_ 1 ) + _\u03b3_ [2] _V_ _\u03c0_ ( _s_ 2 ) _s_ 0 = _s_ _._ \ufffd \ufffd\ufffd\ufffd \ufffd _\u0338_ _\u0338_ Proceeding in this way shows that for any _T \u2265_ 1: _\u0338_ _\u0338_ _T_ _V_ _\u03c0_ ( _s_ ) _\u2264_ _a_ _t_ _\u223c\u03c0_ E _[\u2032]_ ( _s_ _t_ ) \ufffd \ufffd _t_ =0 _\u03b3_ _[t]_ E[ _r_ ( _s_ _t_ _, a_ _t_ )] + _\u03b3_ _[T]_ [ +1] _V_ _\u03c0_ ( _s_ _T_ +1 ) \ufffd\ufffd\ufffd _s_ 0 = _s_ \ufffd _._ _\u0338_ _\u0338_ Since _V_ _\u03c0_ ( _s_ _T_ +1 ) is bounded, taking the limit _T \u2192_ + _\u221e_ gives _\u0338_ _\u0338_ _V_ _\u03c0_ ( _s_ ) _\u2264_ E _a_ _t_ _\u223c\u03c0_ _[\u2032]_ ( _s_ _t_ ) _\u0338_ _\u0338_ + _\u221e_ \ufffd \ufffd _t_ =0 _\u03b3_ _[t]_ E[ _r_ ( _s_ _t_ _, a_ _t_ )] \ufffd\ufffd\ufffd _s_ 0 = _s_ \ufffd = _V_ _\u03c0_ _\u2032_ ( _s_ ) _._ _\u0338_ _\u0338_ Finally, any strict inequality in the left-hand side property results in a strict inequality in the chain of inequalities above. **Theorem 17.7 (Bellman\u2019s optimality condition)** _A policy \u03c0 is optimal iff for any pair_ ( _s, a_ ) _\u2208_ _S \u00d7 A with \u03c0_ ( _s_ )( _a_ ) _>_ 0 _the following holds:_ _a \u2208_ argmax _Q_ _\u03c0_ ( _s, a_ _[\u2032]_ ) _._ (17.2) _a_ _[\u2032]_ _\u2208A_ Proof: By Theorem 17.6, if the condition (17.2) does not hold for some ( _s, a_ ) with _\u03c0_ ( _s_ )( _a_ ) _>_ 0, then the policy _\u03c0_ is not optimal. This is because _\u03c0_ can then be improved by defining _\u03c0_ _[\u2032]_ such that _\u03c0_ _[\u2032]_ ( _s_ _[\u2032]_ ) = _\u03c0_ ( _s_ ) for _s_ _[\u2032]_ = _\u0338_ _s_ and _\u03c0_ _[\u2032]_ ( _s_ ) concentrated on any element of argmax _a_ _\u2032_ _\u2208A_ _Q_ _\u03c0_ ( _s, a_ _[\u2032]_ ). _\u03c0_ _[\u2032]_ verifies E _a\u223c\u03c0_ _\u2032_ ( _s_",
    "chunk_id": "foundations_machine_learning_371"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": ") \ufffd _Q_ _\u03c0_ ( _s_ _[\u2032]_ _, a_ )\ufffd = E _a\u223c\u03c0_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s_ _[\u2032]_ _, a_ )\ufffd for _s_ _[\u2032]_ = _\u0338_ _s_ and E _a\u223c\u03c0_ _\u2032_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd _>_ E _a\u223c\u03c0_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd. Thus, by Theorem 17.6, _V_ _\u03c0_ _\u2032_ ( _s_ ) _> V_ _\u03c0_ ( _s_ ) for at least one _s_ and _\u03c0_ is not optimal. Conversely, let _\u03c0_ _[\u2032]_ be a non-optimal policy. Then there exists a policy _\u03c0_ and at least one state _s_ for which _V_ _\u03c0_ _\u2032_ ( _s_ ) _< V_ _\u03c0_ ( _s_ ). By Theorem 17.6, this implies that there exists some state _s \u2208_ _S_ with E _a\u223c\u03c0_ _\u2032_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd _<_ E _a\u223c\u03c0_ ( _s_ ) \ufffd _Q_ _\u03c0_ ( _s, a_ )\ufffd. Thus, _\u03c0_ _[\u2032]_ cannot satisfy the condition (17.2). **17.3** **Policy** **385** **Theorem 17.8 (Existence of an optimal deterministic policy)** _Any finite MDP admits an_ _optimal deterministic policy._ Proof: Let _\u03c0_ _[\u2217]_ be a deterministic policy maximizing [\ufffd] _s\u2208S_ _[V]_ _[\u03c0]_ [(] _[s]_ [).] _[ \u03c0]_ _[\u2217]_ [exists since] there are only finitely many deterministic policies. If _\u03c0_ _[\u2217]_ were not optimal, by Theorem 17.7, there would exist a state _s_ with _\u03c0_ ( _s_ ) _\u0338\u2208_ argmax _a_ _\u2032_ _\u2208A_ _Q_ _\u03c0_ ( _s, a_ _[\u2032]_ ). By theorem 17.6, _\u03c0_ _[\u2217]_ could then be improved by choosing a policy _\u03c0_ with _\u03c0_ ( _s_ ) _\u2208_ argmax _a_ _\u2032_ _\u2208A_ _Q_ _\u03c0_ ( _s, a_ _[\u2032]_ ) and _\u03c0_ coinciding with _\u03c0_ _[\u2217]_ for all other states. But then _\u03c0_ would verify _V_ _\u03c0\u2217_ ( _s_ ) _\u2264_ _V_ _\u03c0_ ( _s_ ) with a strict inequality at least for one state. This would contradict the fact that _\u03c0_ _[\u2217]_ maximizes [\ufffd] _s\u2208S_ _[V]_ _[\u03c0]_ [(] _[s]_ [).] In view of the existence of a deterministic optimal policy, in what follows, to simplify the discussion, we will consider only deterministic policies. Let _\u03c0_ _[\u2217]_ denote a (deterministic) optimal policy, and let _Q_ _[\u2217]_ and _V_ _[\u2217]_ denote its corresponding stateaction value function and value function. By Theorem 17.7, we can write _\u2200s \u2208_ _S, \u03c0_ _[\u2217]_ ( _s_ ) = argmax _Q_ _[\u2217]_ ( _s, a_ ) _._ (17.3) _a\u2208A_ Thus, the knowledge of the state-action value function _Q_ _[\u2217]_ is sufficient for the agent to determine the optimal policy, without any direct knowledge of the reward or transition probabilities. Replacing _Q_ _[\u2217]_ by its definition gives the following system of equations for the optimal policy values _V_ _[\u2217]_ ( _s_ ) = _Q_ _[\u2217]_ ( _s, \u03c0_ _[\u2217]_ ( _s_ )): _\u2200s \u2208_ _S, V_ _[\u2217]_ ( _s_ ) = max _a\u2208A_ \ufffd E[ _r_ ( _s, a_ )] + _\u03b3_ \ufffd P[ _s_ _[\u2032]_ _|s, a_ ] _V_ _[\u2217]_ ( _s_ _[\u2032]_ )\ufffd _,_ (17.4) _s_ _[\u2032]_ _\u2208S_ also known as _Bellman equations_ . Note that this system of equations is",
    "chunk_id": "foundations_machine_learning_372"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "not linear due to the presence of the max operator. **17.3.4** **Policy evaluation** The value of a policy at state _s_ can be expressed in terms of its values at other states, forming a system of linear equations. **Proposition 17.9 (** _Bellman equations_ **)** _The values V_ _\u03c0_ ( _s_ ) _of policy \u03c0 at states s \u2208_ _S_ _for an infinite horizon MDP obey the following system of linear equations:_ _\u2200s \u2208_ _S, V_ _\u03c0_ ( _s_ ) = _a_ 1 _\u223c_ E _\u03c0_ ( _s_ ) [[] _[r]_ [(] _[s, a]_ [1] [)] +] _[ \u03b3]_ \ufffd P[ _s_ _[\u2032]_ _|s, \u03c0_ ( _s_ )] _V_ _\u03c0_ ( _s_ _[\u2032]_ ) _._ (17.5) _s_ _[\u2032]_ **386** **Chapter 17** **Reinforcement Learning** Proof: We can decompose the expression of the policy value as a sum of the first term and the rest of the terms, which admit _\u03b3_ as a multiplier: \ufffd _V_ _\u03c0_ ( _s_ ) = E + _\u221e_ \ufffd \ufffd _t_ =0 \ufffd _t_ =0 _\u03b3_ _[t]_ _r_ \ufffd _s_ _t_ _, \u03c0_ ( _s_ _t_ )\ufffd [\ufffd] \ufffd\ufffd\ufffd _s_ 0 = _s_ _._ \ufffd = E[ _r_ ( _s, \u03c0_ ( _s_ ))] + _\u03b3_ E = E[ _r_ ( _s, \u03c0_ ( _s_ ))] + _\u03b3_ E + _\u221e_ \ufffd \ufffd _t_ =0 + _\u221e_ \ufffd \ufffd _t_ =0 \ufffd _t_ =0 _\u03b3_ _[t]_ _r_ \ufffd _s_ _t_ +1 _, \u03c0_ ( _s_ _t_ +1 )\ufffd [\ufffd] \ufffd\ufffd\ufffd _s_ 0 = _s_ \ufffd _t_ =0 _\u03b3_ _[t]_ _r_ \ufffd _s_ _t_ +1 _, \u03c0_ ( _s_ _t_ +1 )\ufffd [\ufffd] \ufffd\ufffd\ufffd _s_ 1 = _\u03b4_ ( _s, \u03c0_ ( _s_ )) \ufffd = E[ _r_ ( _s, \u03c0_ ( _s_ )] + _\u03b3_ E[ _V_ _\u03c0_ ( _\u03b4_ ( _s, \u03c0_ ( _s_ )))] _._ This completes the proof. This a linear system of equations, also known as Bellman equations, that is distinct from the non-linear system (17.4). The system can be rewritten as **V** = **R** + _\u03b3_ **PV** _,_ (17.6) using the following notation: **P** denotes the transition probability matrix defined by **P** _s,s_ _\u2032_ = P[ _s_ _[\u2032]_ _|s, \u03c0_ ( _s_ )] for all _s, s_ _[\u2032]_ _\u2208_ _S_ ; **V** is the value column matrix whose _s_ th component is **V** _s_ = _V_ _\u03c0_ ( _s_ ); and **R** the reward column matrix whose _s_ th component is **R** _s_ = E[ _r_ ( _s, \u03c0_ ( _s_ )]. **V** is typically the unknown variable in the Bellman equations and is determined by solving for it. The following theorem shows that, for a finite MDP, this system of linear equations admits a unique solution. **Theorem 17.10** _For a finite MDP, Bellman\u2019s equations admit a unique solution_ _given by_ **V** 0 = ( **I** _\u2212_ _\u03b3_ **P** ) _[\u2212]_ [1] **R** _._ (17.7) Proof: The Bellman equations (17.6) can be equivalently written as ( **I** _\u2212_ _\u03b3_ **P** ) **V** = **R** _._ Thus, to prove the theorem it suffices to show that ( **I** _\u2212_ _\u03b3_ **P** ) is invertible. To",
    "chunk_id": "foundations_machine_learning_373"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "do so, note that the infinity of **P** can be computed using its stochasticity properties: _\u2225_ **P** _\u2225_ _\u221e_ = max _s_ \ufffd _|_ **P** _ss_ _\u2032_ _|_ = max _s_ _s_ _[\u2032]_ \ufffd P[ _s_ _[\u2032]_ _|s, \u03c0_ ( _s_ )] = 1 _._ _s_ _[\u2032]_ \ufffd This implies that _\u2225\u03b3_ **P** _\u2225_ _\u221e_ = _\u03b3 <_ 1. The eigenvalues of _\u03b3_ **P** are thus all less than one, and ( **I** _\u2212_ _\u03b3_ **P** ) is invertible. **17.4** **Planning algorithms** **387** Thus, for a finite MDP, when the transition probability matrix **P** and the reward expectations **R** are known, the value of policy _\u03c0_ at all states can be determined by inverting a matrix. **17.4** **Planning algorithms** In this section, we assume that the environment model is known. That is, the transition probability P[ _s_ _[\u2032]_ _|s, a_ ] and the expected reward E[ _r_ ( _s, a_ )] for all _s, s_ _[\u2032]_ _\u2208_ _S_ and _a \u2208_ _A_ are assumed to be given. The problem of finding the optimal policy then does not require learning the parameters of the environment model or estimating other quantities helpful in determining the best course of actions, it is purely a _planning_ problem. This section discusses three algorithms for this planning problem: the value iteration algorithm, the policy iteration algorithm, and a linear programming formulation of the problem. **17.4.1** **Value iteration** The _value iteration algorithm_ seeks to determine the optimal policy values _V_ _[\u2217]_ ( _s_ ) at each state _s \u2208_ _S_, and thereby the optimal policy. The algorithm is based on the Bellman equations (17.4). As already indicated, these equations do not form a system of linear equations and require a different technique to determine the solution. The main idea behind the design of the algorithm is to use an iterative method to solve them: the new values of _V_ ( _s_ ) are determined using the Bellman equations and the current values. This process is repeated until a convergence condition is met. For a vector **V** in R _[|][S][|]_, we denote by _V_ ( _s_ ) its _s_ th coordinate, for any _s \u2208_ _S_ . Let **\u03a6** : R _[|][S][|]_ _\u2192_ R _[|][S][|]_ be the mapping defined based on Bellman\u2019s equations (17.4): _\u2200s \u2208_ _S,_ [ **\u03a6** ( **V** )]( _s_ ) = max _a\u2208A_ \ufffd E[ _r_ ( _s, a_ )] + _\u03b3_ \ufffd P[ _s_ _[\u2032]_ _|s, a_ ] _V_ ( _s_ _[\u2032]_ )\ufffd _._ (17.8) _s_ _[\u2032]_ _\u2208S_ The maximizing actions _a \u2208_ _A_ in these equations define an action to take at each state _s \u2208_ _S_, that is a policy _\u03c0_ . We can thus rewrite these equations in matrix terms as follows: **\u03a6** ( **V** ) = max _\u03c0_ _[{]_ **[R]** _[\u03c0]_ [ +] _[ \u03b3]_ **[P]** _[\u03c0]_ **[V]** _[}][,]_ (17.9) where **P** _\u03c0_ is the transition probability matrix defined by ( **P** _\u03c0_ ) _ss_ _\u2032_ = P[ _s_ _[\u2032]_ _|s, \u03c0_ ( _s_ )] for all _s, s_ _[\u2032]_ _\u2208_ _S_, and **R** _\u03c0_ the reward vector defined by (",
    "chunk_id": "foundations_machine_learning_374"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**R** _\u03c0_ ) _s_ = E[ _r_ ( _s, \u03c0_ ( _s_ )], for all _s \u2208_ _S_ . The algorithm is directly based on (17.9). The pseudocode is given above. Starting from an arbitrary policy value vector **V** 0 _\u2208_ R _[|][S][|]_, the algorithm iteratively applies **388** **Chapter 17** **Reinforcement Learning** ValueIteration( **V** 0 ) 1 **V** _\u2190_ **V** 0 _\u25b7_ **V** 0 arbitrary value 2 **while** _\u2225_ **V** _\u2212_ **\u03a6** ( **V** ) _\u2225\u2265_ [(][1] _[\u2212]_ _\u03b3_ _[\u03b3]_ [)] _[\u03f5]_ **do** 3 **V** _\u2190_ **\u03a6** ( **V** ) 4 **return \u03a6** ( **V** ) **Figure 17.4** Value iteration algorithm. **\u03a6** to the current **V** to obtain a new policy value vector until _\u2225_ **V** _\u2212_ **\u03a6** ( **V** ) _\u2225_ _<_ [(][1] _[\u2212]_ _\u03b3_ _[\u03b3]_ [)] _[\u03f5]_, where _\u03f5 >_ 0 is a desired approximation. The following theorem proves the convergence of the algorithm to the optimal policy values. **Theorem 17.11** _For any initial value_ **V** 0 _, the sequence defined by_ **V** _n_ +1 = **\u03a6** ( **V** _n_ ) _converges to_ **V** _[\u2217]_ _._ Proof: We first show that **\u03a6** is _\u03b3_ -Lipschitz for the _\u2225\u00b7 \u2225_ _\u221e_ . [23] For any _s \u2208_ _S_ and **V** _\u2208_ R _[|][S][|]_, let _a_ _[\u2217]_ ( _s_ ) be the maximizing action defining **\u03a6** ( **V** )( _s_ ) in (17.8). Then, for any _s \u2208_ _S_ and any **U** _\u2208_ R _[|][S][|]_, **\u03a6** ( **V** )( _s_ ) _\u2212_ **\u03a6** ( **U** )( _s_ ) _\u2264_ **\u03a6** ( **V** )( _s_ ) _\u2212_ \ufffd E[ _r_ ( _s, a_ _[\u2217]_ ( _s_ ))] + _\u03b3_ \ufffd P[ _s_ _[\u2032]_ _| s, a_ _[\u2217]_ ( _s_ )] **U** ( _s_ _[\u2032]_ )\ufffd _s_ _[\u2032]_ _\u2208S_ = _\u03b3_ \ufffd P[ _s_ _[\u2032]_ _|s, a_ _[\u2217]_ ( _s_ )][ **V** ( _s_ _[\u2032]_ ) _\u2212_ **U** ( _s_ _[\u2032]_ )] _s_ _[\u2032]_ _\u2208S_ _\u2264_ _\u03b3_ \ufffd P[ _s_ _[\u2032]_ _|s, a_ _[\u2217]_ ( _s_ )] _\u2225_ **V** _\u2212_ **U** _\u2225_ _\u221e_ = _\u03b3\u2225_ **V** _\u2212_ **U** _\u2225_ _\u221e_ _._ _s_ _[\u2032]_ _\u2208S_ Proceeding similarly with **\u03a6** ( **U** )( _s_ ) _\u2212_ **\u03a6** ( **V** )( _s_ ), we obtain **\u03a6** ( **U** )( _s_ ) _\u2212_ **\u03a6** ( **V** )( _s_ ) _\u2264_ _\u03b3\u2225_ **V** _\u2212_ **U** _\u2225_ _\u221e_ . Thus, _|_ **\u03a6** ( **V** )( _s_ ) _\u2212_ **\u03a6** ( **U** )( _s_ ) _| \u2264_ _\u03b3\u2225_ **V** _\u2212_ **U** _\u2225_ _\u221e_ for all _s_, which implies _\u2225_ **\u03a6** ( **V** ) _\u2212_ **\u03a6** ( **U** ) _\u2225_ _\u221e_ _\u2264_ _\u03b3\u2225_ **V** _\u2212_ **U** _\u2225_ _\u221e_ _,_ 23 A _\u03b2_ -Lipschitz function with _\u03b2 <_ 1 is also called _\u03b2-contracting_ . In a _complete metric space_, that is a metric space where any Cauchy sequence converges to a point of that space, a _\u03b2_ -contracting function _f_ admits a _fixed point_ : any sequence ( _f_ ( _x_ _n_ )) _n\u2208_ N converges to some _x_ with _f_ ( _x_ ) = _x_ . R _[N]_, _N \u2265_ 1, or, more generally, any finite-dimensional vector space, is a complete metric",
    "chunk_id": "foundations_machine_learning_375"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "space. **17.4** **Planning algorithms** **389** a/[3/4, 2] c/[1, 2] **Image:** [No caption returned] **Figure 17.5** Example of MDP with two states. The state set is reduced to _S_ = _{_ 1 _,_ 2 _}_ and the action set to _A_ = _{a, b, c, d}_ . Only transitions with non-zero probabilities are represented. Each transition is labeled with the action taken followed by a pair [ _p, r_ ] after a slash separator, where _p_ is the probability of the transition and _r_ the expected reward for taking that transition. that is the _\u03b3_ -Lipschitz property of **\u03a6** . Now, by Bellman equations (17.4), **V** _[\u2217]_ = **\u03a6** ( **V** _[\u2217]_ ), thus for any _n \u2208_ N, _\u2225_ **V** _[\u2217]_ _\u2212_ **V** _n_ +1 _\u2225_ _\u221e_ = _\u2225_ **\u03a6** ( **V** _[\u2217]_ ) _\u2212_ **\u03a6** ( **V** _n_ ) _\u2225_ _\u221e_ _\u2264_ _\u03b3\u2225_ **V** _[\u2217]_ _\u2212_ **V** _n_ _\u2225_ _\u221e_ _\u2264_ _\u03b3_ _[n]_ [+1] _\u2225_ **V** _[\u2217]_ _\u2212_ **V** 0 _\u2225_ _\u221e_ _,_ which proves the convergence of the sequence to **V** _[\u2217]_ since _\u03b3 \u2208_ (0 _,_ 1). The _\u03f5_ -optimality of the value returned by the algorithm can be shown as follows. By the triangle inequality and the _\u03b3_ -Lipschitz property of **\u03a6**, for any _n \u2208_ N, _\u2225_ **V** _[\u2217]_ _\u2212_ **V** _n_ +1 _\u2225_ _\u221e_ _\u2264\u2225_ **V** _[\u2217]_ _\u2212_ **\u03a6** ( **V** _n_ +1 ) _\u2225_ _\u221e_ + _\u2225_ **\u03a6** ( **V** _n_ +1 ) _\u2212_ **V** _n_ +1 _\u2225_ _\u221e_ = _\u2225_ **\u03a6** ( **V** _[\u2217]_ ) _\u2212_ **\u03a6** ( **V** _n_ +1 ) _\u2225_ _\u221e_ + _\u2225_ **\u03a6** ( **V** _n_ +1 ) _\u2212_ **\u03a6** ( **V** _n_ ) _\u2225_ _\u221e_ _\u2264_ _\u03b3\u2225_ **V** _[\u2217]_ _\u2212_ **V** _n_ +1 _\u2225_ _\u221e_ + _\u03b3\u2225_ **V** _n_ +1 _\u2212_ **V** _n_ _\u2225_ _\u221e_ _._ Thus, if **V** _n_ +1 is the policy value returned by the algorithm, we have _\u2225_ **V** _[\u2217]_ _\u2212_ **V** _n_ +1 _\u2225_ _\u221e_ _\u2264_ _\u03b3_ 1 _\u2212_ _\u03b3_ _[\u2225]_ **[V]** _[n]_ [+1] _[ \u2212]_ **[V]** _[n]_ _[\u2225]_ _[\u221e]_ _[\u2264]_ _[\u03f5.]_ The convergence of the algorithm is in _O_ (log [1] _\u03f5_ [) number of iterations.] Indeed, observe that _\u2225_ **V** _n_ +1 _\u2212_ **V** _n_ _\u2225_ _\u221e_ = _\u2225_ **\u03a6** ( **V** _n_ ) _\u2212_ **\u03a6** ( **V** _n\u2212_ 1 ) _\u2225_ _\u221e_ _\u2264_ _\u03b3\u2225_ **V** _n_ _\u2212_ **V** _n\u2212_ 1 _\u2225_ _\u221e_ _\u2264_ _\u03b3_ _[n]_ _\u2225_ **\u03a6** ( **V** 0 ) _\u2212_ **V** 0 _\u2225_ _\u221e_ _._ Thus, if _n_ is the largest integer such that [(][1] _[\u2212][\u03b3]_ [)] _[\u03f5]_ Thus, if _n_ is the largest integer such that _[\u2212]_ _\u03b3_ _[\u03b3]_ _[\u03f5]_ _\u2264\u2225_ **V** _n_ +1 _\u2212_ **V** _n_ _\u2225_ _\u221e_, it must verify (1 _\u2212\u03b3_ ) _\u03f5_ _[n]_ [1] [24] _\u03b3\u03b3_ _\u03f5_ _\u2264_ _\u03b3_ _[n]_ _\u2225_ **\u03a6** ( **V** 0 ) _\u2212_ **V** 0 _\u2225_ _\u221e_ and therefore _n \u2264_ _O_ \ufffd log [1] _\u03f5_ [1] _\u03f5_ \ufffd. [24] 24 Here, the _O_ -notation hides the dependency on the discount factor _\u03b3_ . As a function of _\u03b3_, the running time is not polynomial. **390** **Chapter 17** **Reinforcement Learning**",
    "chunk_id": "foundations_machine_learning_376"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "PolicyIteration( _\u03c0_ 0 ) 1 _\u03c0 \u2190_ _\u03c0_ 0 _\u25b7\u03c0_ 0 arbitrary policy 2 _\u03c0_ _[\u2032]_ _\u2190_ nil 3 **while** ( _\u03c0 \u0338_ = _\u03c0_ _[\u2032]_ ) **do** 4 **V** _\u2190_ **V** _\u03c0_ _\u25b7_ policy evaluation: solve ( **I** _\u2212_ _\u03b3_ **P** _\u03c0_ ) **V** = **R** _\u03c0_ _._ 5 _\u03c0_ _[\u2032]_ _\u2190_ _\u03c0_ 6 _\u03c0 \u2190_ argmax _\u03c0_ _{_ **R** _\u03c0_ + _\u03b3_ **P** _\u03c0_ **V** _}_ _\u25b7_ greedy policy improvement _._ 7 **return** _\u03c0_ **Figure 17.6** Policy iteration algorithm. Figure 17.5 shows a simple example of MDP with two states. The iterated values of these states calculated by the algorithm for that MDP are given by 3 **V** _n_ +1 (1) = max 2 + _\u03b3_ \ufffd \ufffd 4 3 4 **[V]** _[n]_ [(1) + 1] 4 _,_ 2 + _\u03b3_ **V** _n_ (2) 4 **[V]** _[n]_ [(2)] \ufffd \ufffd **V** _n_ +1 (2) = max 3 + _\u03b3_ **V** _n_ (1) _,_ 2 + _\u03b3_ **V** _n_ (2) _._ \ufffd \ufffd For **V** 0 (1) = _\u2212_ 1, **V** 0 (2) = 1, and _\u03b3_ = 1 _/_ 2, we obtain **V** 1 (1) = **V** 1 (2) = 5 _/_ 2. Thus, both states seem to have the same policy value initially. However, by the fifth iteration, **V** 5 (1) = 4 _._ 53125, **V** 5 (2) = 5 _._ 15625 and the algorithm quickly converges to the optimal values **V** _[\u2217]_ (1) = 14 _/_ 3 and **V** _[\u2217]_ (2) = 16 _/_ 3 showing that state 2 has a higher optimal value. **17.4.2** **Policy iteration** An alternative algorithm for determining the best policy consists of using policy evaluations, which can be achieved via a matrix inversion, as shown by theorem 17.10. The pseudocode of the algorithm known as _policy iteration algorithm_ is given in figure 17.6. Starting with an arbitrary action policy _\u03c0_ 0, the algorithm repeatedly computes the value of the current policy _\u03c0_ via that matrix inversion and greedily selects the new policy as the one maximizing the right-hand side of the Bellman equations (17.9). The following theorem proves the convergence of the policy iteration algorithm. **17.4** **Planning algorithms** **391** **Theorem 17.12** _Let_ ( **V** _n_ ) _n\u2208_ N _be the sequence of policy values computed by the algo-_ _rithm, then, for any n \u2208_ N _, the following inequalities hold:_ **V** _n_ _\u2264_ **V** _n_ +1 _\u2264_ **V** _[\u2217]_ _._ (17.10) Proof: Let _\u03c0_ _n_ +1 be the policy improvement at the _n_ th iteration of the algorithm. We first show that ( **I** _\u2212_ _\u03b3_ **P** _\u03c0_ _n_ +1 ) _[\u2212]_ [1] preserves ordering, that is, for any column matrices **X** and **Y** in R _[|][S][|]_, if ( **Y** _\u2212_ **X** ) _\u2265_ **0**, then ( **I** _\u2212_ _\u03b3_ **P** _\u03c0_ _n_ +1 ) _[\u2212]_ [1] ( **Y** _\u2212_ **X** ) _\u2265_ **0** . As shown in the proof of theorem 17.10, _\u2225\u03b3_ **P** _\u2225_ _\u221e_ = _\u03b3 <_ 1. Since the radius of convergence of the power series (1 _\u2212_ _x_ ) _[\u2212]_ [1] is one, we can use its expansion",
    "chunk_id": "foundations_machine_learning_377"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "and write ( **I** _\u2212_ _\u03b3_ **P** _\u03c0_ _n_ +1 ) _[\u2212]_ [1] = _\u221e_ \ufffd( _\u03b3_ **P** _\u03c0_ _n_ +1 ) _[k]_ _._ _k_ =0 Thus, if **Z** = ( **Y** _\u2212_ **X** ) _\u2265_ **0**, then ( **I** _\u2212_ _\u03b3_ **P** _\u03c0_ _n_ +1 ) _[\u2212]_ [1] **Z** = [\ufffd] _[\u221e]_ _k_ =0 [(] _[\u03b3]_ **[P]** _[\u03c0]_ _n_ +1 [)] _[k]_ **[Z]** _[ \u2265]_ **[0]** [, since] the entries of matrix **P** _\u03c0_ _n_ +1 and its powers are all non-negative as well as those of **Z** . Now, by definition of _\u03c0_ _n_ +1, we have **R** _\u03c0_ _n_ +1 + _\u03b3_ **P** _\u03c0_ _n_ +1 **V** _n_ _\u2265_ **R** _\u03c0_ _n_ + _\u03b3_ **P** _\u03c0_ _n_ **V** _n_ = **V** _n_ _,_ which shows that **R** _\u03c0_ _n_ +1 _\u2265_ ( **I** _\u2212_ _\u03b3_ **P** _\u03c0_ _n_ +1 ) **V** _n_ . Since ( **I** _\u2212_ _\u03b3_ **P** _\u03c0_ _n_ +1 ) _[\u2212]_ [1] preserves ordering, this implies that **V** _n_ +1 = ( **I** _\u2212_ _\u03b3_ **P** _\u03c0_ _n_ +1 ) _[\u2212]_ [1] **R** _\u03c0_ _n_ +1 _\u2265_ **V** _n_, which concludes the proof of the theorem. Note that two consecutive policy values can be equal only at the last iteration of the algorithm. The total number of possible policies is _|A|_ _[|][S][|]_, thus this constitutes a straightforward upper bound on the maximal number of iterations. Better upper _|A|_ _|S|_ bounds of the form _O_ \ufffd _S_ \ufffd are known for this algorithm. bounds of the form _O_ \ufffd _|S|_ \ufffd are known for this algorithm. For the simple MDP shown by figure 17.5, let the initial policy _\u03c0_ 0 be defined by _\u03c0_ 0 (1) = _b, \u03c0_ 0 (2) = _c_ . Then, the system of linear equations for evaluating this policy is _V_ _\u03c0_ 0 (1) = 1 + _\u03b3V_ _\u03c0_ 0 (2) \ufffd _V_ _\u03c0_ 0 (2) = 2 + _\u03b3V_ _\u03c0_ 0 (2) _,_ _V_ _\u03c0_ 0 (2) = 2 + _\u03b3V_ _\u03c0_ 0 (2) _,_ which gives _V_ _\u03c0_ 0 (1) = 1 [1] _\u2212_ [+] _[\u03b3]_ _\u03b3_ [and] _[ V]_ _[\u03c0]_ [0] [(2) =] 1 _\u2212_ 2 _\u03b3_ [.] **Theorem 17.13** _Let_ ( **U** _n_ ) _n\u2208_ N _be the sequence of policy values generated by the value_ _iteration algorithm, and_ ( **V** _n_ ) _n\u2208_ N _the one generated by the policy iteration algo-_ _rithm. If_ **U** 0 = **V** 0 _, then,_ _\u2200n \u2208_ N _,_ **U** _n_ _\u2264_ **V** _n_ _\u2264_ **V** _[\u2217]_ _._ (17.11) Proof: We first show that the function **\u03a6** previously introduced is monotonic. Let **U** and **V** be such that **U** _\u2264_ **V** and let _\u03c0_ be the policy such that **\u03a6** ( **U** ) = **R** _\u03c0_ + _\u03b3_ **P** _\u03c0_ **U** . **392** **Chapter 17** **Reinforcement Learning** Then, **\u03a6** ( **U** ) _\u2264_ **R** _\u03c0_ + _\u03b3_ **P** _\u03c0_ **V** _\u2264_ max _\u03c0_ _[\u2032]_ _[ {]_ **[R]** _[\u03c0]_ _[\u2032]_ [ +] _[ \u03b3]_ **[P]** _[\u03c0]_ _[\u2032]_ **[V]** _[}]_ [ =] **[ \u03a6]** [(] **[V]** [)] _[.]_ The proof is by induction",
    "chunk_id": "foundations_machine_learning_378"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "on _n_ . Assume that **U** _n_ _\u2264_ **V** _n_, then by the monotonicity of **\u03a6**, we have **U** _n_ +1 = **\u03a6** ( **U** _n_ ) _\u2264_ **\u03a6** ( **V** _n_ ) = max _\u03c0_ _[{]_ **[R]** _[\u03c0]_ [ +] _[ \u03b3]_ **[P]** _[\u03c0]_ **[V]** _[n]_ _[}][.]_ Let _\u03c0_ _n_ +1 be the maximizing policy, that is, _\u03c0_ _n_ +1 = argmax _\u03c0_ _{_ **R** _\u03c0_ + _\u03b3_ **P** _\u03c0_ **V** _n_ _}_ . Then, **\u03a6** ( **V** _n_ ) = **R** _\u03c0_ _n_ +1 + _\u03b3_ **P** _\u03c0_ _n_ +1 **V** _n_ _\u2264_ **R** _\u03c0_ _n_ +1 + _\u03b3_ **P** _\u03c0_ _n_ +1 **V** _n_ +1 = **V** _n_ +1 _,_ and thus **U** _n_ +1 _\u2264_ **V** _n_ +1 . The theorem shows that the policy iteration algorithm converges in a smaller number of iterations than the value iteration algorithm due to the optimal policy. But, each iteration of the policy iteration algorithm requires computing a policy value, that is, solving a system of linear equations, which is more expensive to compute than an iteration of the value iteration algorithm. **17.4.3** **Linear programming** An alternative formulation of the optimization problem defined by the Bellman equations (17.4) or the proof of Theorem 17.8 is via linear programming (LP), that is an optimization problem with a linear objective function and linear constraints. LPs admit (weakly) polynomial-time algorithmic solutions. There exist a variety of different methods for solving relatively large LPs in practice, using the simplex method, interior-point methods, or a variety of special-purpose solutions. All of these methods could be applied in this context. By definition, the equations (17.4) are each based on a maximization. These maximizations are equivalent to seeking to minimize all elements of _{V_ ( _s_ ): _s \u2208_ _S}_ under the constraints _V_ ( _s_ ) _\u2265_ E[ _r_ ( _s, a_ )] + _\u03b3_ [\ufffd] _s_ _[\u2032]_ _\u2208S_ [P][[] _[s]_ _[\u2032]_ _[|][s, a]_ []] _[V]_ [ (] _[s]_ _[\u2032]_ [), (] _[s][ \u2208]_ _[S]_ [). Thus,] this can be written as the following LP for any set of fixed positive weights _\u03b1_ ( _s_ ) _>_ 0, ( _s \u2208_ _S_ ): min **V** \ufffd _\u03b1_ ( _s_ ) _V_ ( _s_ ) (17.12) _s\u2208S_ subject to _\u2200s \u2208_ _S, \u2200a \u2208_ _A, V_ ( _s_ ) _\u2265_ E[ _r_ ( _s, a_ )] + _\u03b3_ \ufffd P[ _s_ _[\u2032]_ _|s, a_ ] _V_ ( _s_ _[\u2032]_ ) _,_ _s_ _[\u2032]_ _\u2208S_ **17.5** **Learning algorithms** **393** where _**\u03b1**_ _>_ **0** is the vector with the _s_ th component equal to _\u03b1_ ( _s_ ). [25] To make each coefficient _\u03b1_ ( _s_ ) interpretable as a probability, we can further add the constraints that [\ufffd] _s\u2208S_ _[\u03b1]_ [(] _[s]_ [) = 1.] The number of rows of this LP is _|S||A|_ and its number of columns _|S|_ . The complexity of the solution techniques for LPs is typically more favorable in terms of the number of rows than the number of columns. This motivates a solution based on the equivalent dual formulation of this LP which",
    "chunk_id": "foundations_machine_learning_379"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "can be written as max **x** \ufffd E[ _r_ ( _s, a_ )] _x_ ( _s, a_ ) (17.13) _s\u2208S,a\u2208A_ \ufffd _x_ ( _s_ _[\u2032]_ _, a_ ) = _\u03b1_ ( _s_ _[\u2032]_ ) + _\u03b3_ \ufffd _a\u2208A_ _s\u2208S,a_ subject to _\u2200s \u2208_ _S,_ \ufffd \ufffd P[ _s_ _[\u2032]_ _|s, a_ ] _x_ ( _s_ _[\u2032]_ _, a_ ) _s\u2208S,a\u2208A_ _\u2200s \u2208_ _S, \u2200a \u2208_ _A, x_ ( _s, a_ ) _\u2265_ 0 _,_ and for which the number of rows is only _|S|_ and the number of columns _|S||A|_ . Here _x_ ( _s, a_ ) can be interpreted as the probability of being in state _s_ and taking action _a_ . **17.5** **Learning algorithms** This section considers the more general scenario where the environment model of an MDP, that is the transition and reward probabilities, is unknown. This matches many realistic applications of reinforcement learning where, for example, a robot is placed in an environment that it needs to explore in order to reach a specific goal. How can an agent determine the best policy in this context? Since the environment models are not known, it may seek to learn them by estimating transition or reward probabilities. To do so, as in the standard case of supervised learning, the agent needs some amount of training information. In the context of reinforcement learning with MDPs, the training information is the sequence of immediate rewards the agent receives based on the actions it has taken. There are two main learning approaches that can be adopted. One known as the _model-free approach_ consists of learning an action policy directly. Another one, a _model-based_ approach, consists of first learning the environment model, and then of using that to learn a policy. The Q-learning algorithm we present for this problem is widely adopted in reinforcement learning and belongs to the family of model-free approaches. 25 Let us emphasize that the LP is only in terms of the variables _V_ ( _s_ ), as indicated by the subscript of the minimization operator, and not in terms of _V_ ( _s_ ) and _\u03b1_ ( _s_ ). **394** **Chapter 17** **Reinforcement Learning** The estimation and algorithmic methods adopted for learning in reinforcement learning are closely related to the concepts and techniques in _stochastic approxi-_ _mation_ . Thus, we start by introducing several useful results of this field that will be needed for the proofs of convergence of the reinforcement learning algorithms presented. **17.5.1** **Stochastic approximation** Stochastic approximation methods are iterative algorithms for solving optimization problems whose objective function is defined as the expectation of some random variable, or to find the fixed point of a function _H_ that is accessible only through noisy observations. These are precisely the type of optimization problems found in reinforcement learning. For example, for the Q-learning algorithm we will describe, the optimal state-action value function _Q_ _[\u2217]_ is the fixed point of some function _H_ that is defined as an expectation and thus not directly accessible. We start with a basic result whose proof and related algorithm show the flavor of more",
    "chunk_id": "foundations_machine_learning_380"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "complex ones found in stochastic approximation. The theorem is a generalization of a result known as the _strong law of large numbers_ . It shows that under some conditions on the coefficients, an iterative sequence of estimates _\u00b5_ _m_ converges almost surely (a.s.) to the mean of a bounded random variable. **Theorem 17.14 (Mean estimation)** _Let X be a random variable taking values in_ [0 _,_ 1] _and let x_ 0 _, . . ., x_ _m_ _be i.i.d. values of X. Define the sequence_ ( _\u00b5_ _m_ ) _m\u2208_ N _by_ _\u00b5_ _m_ +1 = (1 _\u2212_ _\u03b1_ _m_ ) _\u00b5_ _m_ + _\u03b1_ _m_ _x_ _m_ _,_ (17.14) _with \u00b5_ 0 = _x_ 0 _, \u03b1_ _m_ _\u2208_ [0 _,_ 1] _,_ [\ufffd] _m\u2265_ 0 _[\u03b1]_ _[m]_ [ = +] _[\u221e]_ _[and]_ [ \ufffd] _m\u2265_ 0 _[\u03b1]_ _m_ [2] _[<]_ [ +] _[\u221e][. Then,]_ _\u00b5_ _m_ _\u2212\u2212\u2192_ _a.s._ E[ _X_ ] _._ (17.15) Proof: We give the proof of the _L_ 2 convergence. The a.s. convergence is shown later for a more general theorem. By the independence assumption, for _m \u2265_ 0, Var[ _\u00b5_ _m_ +1 ] = (1 _\u2212_ _\u03b1_ _m_ ) [2] Var[ _\u00b5_ _m_ ] + _\u03b1_ _m_ [2] [Var[] _[x]_ _[m]_ []] _[ \u2264]_ [(1] _[ \u2212]_ _[\u03b1]_ _[m]_ [) Var[] _[\u00b5]_ _[m]_ [] +] _[ \u03b1]_ _m_ [2] _[.]_ [ (17.16)] Let _\u03f5 >_ 0 and suppose that there exists _N \u2208_ N such that for all _m \u2265_ _N_, Var[ _\u00b5_ _m_ ] _\u2265_ _\u03f5_ . Then, for _m \u2265_ _N_, Var[ _\u00b5_ _m_ +1 ] _\u2264_ Var[ _\u00b5_ _m_ ] _\u2212_ _\u03b1_ _m_ Var[ _\u00b5_ _m_ ] + _\u03b1_ _m_ [2] _[\u2264]_ [Var[] _[\u00b5]_ _[m]_ []] _[ \u2212]_ _[\u03b1]_ _[m]_ _[\u03f5]_ [ +] _[ \u03b1]_ _m_ [2] _[,]_ which implies, by reapplying this inequality, that Var[ _\u00b5_ _N_ ] _\u2212_ _\u03f5_ \ufffd _\u03b1_ _n_ + \ufffd _\u03b1_ _n_ [2] _,_ _n_ = _N_ _n_ = _N_ ~~\ufffd~~ ~~\ufffd\ufffd~~ ~~\ufffd~~ _\u2192\u2212\u221e_ when _m\u2192\u221e_ Var[ _\u00b5_ _m_ + _N_ ] _\u2264_ Var[ _\u00b5_ _N_ ] _\u2212_ _\u03f5_ _m_ + _N_ \ufffd _\u03b1_ _n_ + _n_ = _N_ _m_ + _N_ \ufffd _,_ **17.5** **Learning algorithms** **395** contradicting Var[ _\u00b5_ _m_ + _N_ ] _\u2265_ 0. Thus, this contradicts the existence of such an integer _N_ . Therefore, for all _N \u2208_ N, there exists _m_ 0 _\u2265_ _N_ such that Var[ _\u00b5_ _m_ 0 ] _\u2264_ _\u03f5_ . Choose _N_ large enough so that for all _m \u2265_ _N_, the inequality _\u03b1_ _m_ _\u2264_ _\u03f5_ holds. This is possible since the sequence ( _\u03b1_ _m_ [2] [)] _[m][\u2208]_ [N] [and thus (] _[\u03b1]_ _[m]_ [)] _[m][\u2208]_ [N] [converges to zero in view] of [\ufffd] _m\u2265_ 0 _[\u03b1]_ _m_ [2] _[<]_ [ +] _[\u221e]_ [. We will show by induction that for any] _[ m][ \u2265]_ _[m]_ [0] [, Var[] _[\u00b5]_ _[m]_ []] _[ \u2264]_ _[\u03f5]_ [,] which implies the statement of the theorem. Assume that Var[ _\u00b5_ _m_ ] _\u2264_ _\u03f5_ for some _m \u2265_ _m_ 0 . Then, using this assumption, inequality 17.16, and",
    "chunk_id": "foundations_machine_learning_381"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the fact that _\u03b1_ _m_ _\u2264_ _\u03f5_, the following inequality holds: Var[ _\u00b5_ _m_ +1 ] _\u2264_ (1 _\u2212_ _\u03b1_ _m_ ) _\u03f5_ + _\u03f5\u03b1_ _m_ = _\u03f5._ Thus, this proves that lim _m\u2192_ + _\u221e_ Var[ _\u00b5_ _m_ ] = 0, that is the _L_ 2 convergence of _\u00b5_ _m_ to E[ _X_ ]. Note that the hypotheses of the theorem related to the sequence ( _\u03b1_ _m_ ) _m\u2208_ N hold in particular when _\u03b1_ _m_ = _m_ [1] [. The special case of the theorem with this choice of] _[ \u03b1]_ _[m]_ coincides with the strong law of large numbers. This result has tight connections with the general problem of stochastic optimization. Stochastic optimization is the general problem of finding the solution to the equation **x** = _H_ ( **x** ) _,_ where **x** _\u2208_ R _[N]_, when _\u2022_ _H_ ( _x_ ) cannot be computed, for example, because _H_ is not accessible or because the cost of its computation is prohibitive; _\u2022_ but an i.i.d. sample of _m_ noisy observations _H_ ( **x** _i_ ) + **w** _i_ are available, _i \u2208_ [ _m_ ], where the noise random variable **w** has expectation zero: E[ **w** ] = **0** . This problem arises in a variety of different contexts and applications. As we shall see, it is directly related to the learning problem for MDPs. One general idea for solving this problem is to use an iterative method and define a sequence ( **x** _t_ ) _t\u2208_ N in a way similar to what is suggested by theorem 17.14: **x** _t_ +1 = (1 _\u2212_ _\u03b1_ _t_ ) **x** _t_ + _\u03b1_ _t_ [ _H_ ( **x** _t_ ) + **w** _t_ ] (17.17) = **x** _t_ + _\u03b1_ _t_ [ _H_ ( **x** _t_ ) + **w** _t_ _\u2212_ **x** _t_ ] _,_ (17.18) where ( _\u03b1_ _t_ ) _t\u2208_ N follow conditions similar to those assumed in theorem 17.14. More generally, we consider sequences defined via **x** _t_ +1 = **x** _t_ + _\u03b1_ _t_ _D_ ( **x** _t_ _,_ **w** _t_ ) _,_ (17.19) where _D_ is a function mapping R _[N]_ _\u00d7_ R _[N]_ to R _[N]_ . There are many different theorems guaranteeing the convergence of this sequence under various assumptions. We will present one of the most general forms of such theorems, which relies on the following result. **396** **Chapter 17** **Reinforcement Learning** **Theorem 17.15 (Supermartingale convergence)** _Let_ ( _X_ _t_ ) _t\u2208_ N _,_ ( _Y_ _t_ ) _t\u2208_ N _, and_ ( _Z_ _t_ ) _t\u2208_ N _be_ _sequences of non-negative random variables such that_ [\ufffd] [+] _t_ =0 _[\u221e]_ _[Y]_ _[t]_ _[ <]_ [ +] _[\u221e][.]_ _Let F_ _t_ _denote all the information for t_ _[\u2032]_ _\u2264_ _t: F_ _t_ = _{_ ( _X_ _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t_ _,_ ( _Y_ _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t_ _,_ ( _Z_ _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t_ _}. Then, if_ E \ufffd _X_ _t_ +1 \ufffd\ufffd _F_ _t_ \ufffd _\u2264_ _X_ _t_ + _Y_ _t_ _\u2212_ _Z_ _t_",
    "chunk_id": "foundations_machine_learning_382"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_, the following holds:_ _\u2022_ _X_ _t_ _converges to a limit (with probability one)._ _\u2022_ [\ufffd] [+] _t_ =0 _[\u221e]_ _[Z]_ _[t]_ _[ <]_ [ +] _[\u221e][.]_ The following is one of the most general forms of such theorems. **Theorem 17.16** _Let D be a function mapping_ R _[N]_ _\u00d7_ R _[N]_ _to_ R _[N]_ _,_ ( **w** _t_ ) _t\u2208_ N _a sequence_ _of random variables in_ R _[N]_ _,_ ( _\u03b1_ _t_ ) _t\u2208_ N _a sequence of real numbers, and_ ( **x** _t_ ) _t\u2208_ N _a_ _sequence defined by_ **x** _t_ +1 = **x** _t_ + _\u03b1_ _t_ _D_ ( **x** _t_ _,_ **w** _t_ ) _with_ **x** 0 _\u2208_ R _[N]_ _. Let F_ _t_ _denote the_ _entire history up to t, that is: F_ _t_ = _{_ ( **x** _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t_ _,_ ( **w** _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t\u2212_ 1 _,_ ( _\u03b1_ _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t_ _}, and let_ \u03a8 _denote the function_ **x** _\u2192_ [1] 2 _[\u2225]_ **[x]** _[\u2212]_ **[x]** _[\u2217]_ _[\u2225]_ 2 [2] _[for some]_ **[ x]** _[\u2217]_ _[\u2208]_ [R] _[N]_ _[. Assume that][ D][ and]_ [ (] _[\u03b1]_ [)] _[t][\u2208]_ [N] _verify the following conditions:_ _\u2022_ _\u2203K_ 1 _, K_ 2 _\u2208_ R : E \ufffd _\u2225D_ ( **x** _t_ _,_ **w** _t_ ) _\u2225_ 2 [2] \ufffd\ufffd _F_ _t_ \ufffd _\u2264_ _K_ 1 + _K_ 2 \u03a8( **x** _t_ ) _;_ _\u2022_ _\u2203c \u2265_ 0: _\u2207_ \u03a8( **x** _t_ ) _[\u22a4]_ E \ufffd _D_ ( **x** _t_ _,_ **w** _t_ ) \ufffd\ufffd _F_ _t_ \ufffd _\u2264\u2212c_ \u03a8( **x** _t_ ) _;_ _\u2022_ _\u03b1_ _t_ _>_ 0 _,_ [\ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _[t]_ [ = +] _[\u221e][,]_ [ \ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _t_ [2] _[<]_ [ +] _[\u221e][.]_ _Then, the sequence_ **x** _t_ _converges almost surely to_ **x** _[\u2217]_ _:_ _a.s._ **x** _t_ _\u2212\u2212\u2192_ **x** _[\u2217]_ _._ (17.20) Proof: Since function \u03a8 is quadratic, a Taylor expansion gives \u03a8( **x** _t_ +1 ) = \u03a8( **x** _t_ ) + _\u2207_ \u03a8( **x** _t_ ) _[\u22a4]_ ( **x** _t_ +1 _\u2212_ **x** _t_ ) + [1] 2 [(] **[x]** _[t]_ [+1] _[ \u2212]_ **[x]** _[t]_ [)] _[\u22a4]_ _[\u2207]_ [2] [\u03a8(] **[x]** _[t]_ [)(] **[x]** _[t]_ [+1] _[ \u2212]_ **[x]** _[t]_ [)] _[.]_ Thus, E \ufffd\u03a8( **x** _t_ +1 )\ufffd\ufffd _F_ _t_ \ufffd = \u03a8( **x** _t_ ) + _\u03b1_ _t_ _\u2207_ \u03a8( **x** _t_ ) _[\u22a4]_ E \ufffd _D_ ( **x** _t_ _,_ **w** _t_ )\ufffd\ufffd _F_ _t_ \ufffd + _[\u03b1]_ 2 _t_ [2] [E] \ufffd _\u2225D_ ( **x** _t_ _,_ **w** _t_ ) _\u2225_ [2] [\ufffd\ufffd] _F_ _t_ \ufffd _t_ _\u2264_ \u03a8( **x** _t_ ) _\u2212_ _\u03b1_ _t_ _c_ \u03a8( **x** _t_ ) + _[\u03b1]_ [2] 2 [(] _[K]_ [1] [ +] _[ K]_ [2] [\u03a8(] **[x]** _[t]_ [))] _t_ _[K]_ [1] = \u03a8( **x** _t_ ) + _[\u03b1]_ [2] _[K]_ [1] _\u2212_ _\u03b1_ _t_ _c \u2212_ _[\u03b1]_ _t_ [2] _[K]_ [2] 2 \ufffd 2 2 \u03a8( **x** _t_ ) _._ \ufffd Since by assumption the series [\ufffd] [+] _t_ =0",
    "chunk_id": "foundations_machine_learning_383"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u221e]_ _[\u03b1]_ _t_ [2] [is convergent, (] _[\u03b1]_ _t_ [2] [)] _[t]_ [and thus (] _[\u03b1]_ _[t]_ [)] _[t]_ [con-] _t_ _[K]_ [2] verges to zero. Therefore, for _t_ sufficiently large, the term \ufffd _\u03b1_ _t_ _c \u2212_ _[\u03b1]_ [2] 2 \ufffd\u03a8( **x** _t_ ) _t_ [2] verges to zero. Therefore, for _t_ sufficiently large, the term \ufffd _\u03b1_ _t_ _c \u2212_ _[\u03b1]_ 2 \ufffd\u03a8( **x** _t_ ) has the sign of _\u03b1_ _t_ _c_ \u03a8( **x** _t_ ) and is non-negative, since _\u03b1_ _t_ _>_ 0, \u03a8( **x** _t_ ) _\u2265_ 0, and _c >_ 0. Thus, by the supermartingale convergence theorem 17.15, \u03a8( **x** _t_ ) converges and [\ufffd] [+] _t_ =0 _[\u221e]_ \ufffd _\u03b1_ _t_ _c_ _\u2212_ _[\u03b1]_ _t_ [2] 2 _[K]_ [2] \ufffd\u03a8( **x** _t_ ) _<_ + _\u221e_ . Since \u03a8( **x** _t_ ) converges and [\ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _t_ [2] _[<]_ verges and [\ufffd] [+] _t_ =0 _[\u221e]_ \ufffd _\u03b1_ _t_ _c_ _\u2212_ _[\u03b1]_ _t_ 2 _[K]_ [2] \ufffd\u03a8( **x** _t_ ) _<_ + _\u221e_ . Since \u03a8( **x** _t_ ) converges and [\ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _t_ [2] _[<]_ + _\u221e_, we have [\ufffd] [+] _t_ =0 _[\u221e]_ _\u03b1_ [2] _t_ 2 _[K]_ [2] \u03a8( **x** _t_ ) _<_ + _\u221e_ . But, since [\ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _[t]_ [ = +] _[\u221e]_ [, if the limit] + _\u221e_, we have [\ufffd] [+] _t_ =0 _[\u221e]_ _\u03b1_ _t_ 2 _[K]_ [2] \u03a8( **x** _t_ ) _<_ + _\u221e_ . But, since [\ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _[t]_ [ = +] _[\u221e]_ [, if the limit] of \u03a8( **x** _t_ ) were non-zero, we would have [\ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _[t]_ _[c]_ [\u03a8(] **[x]** _[t]_ [) = +] _[\u221e]_ [.] This implies **17.5** **Learning algorithms** **397** that the limit of \u03a8( **x** _t_ ) is zero, that is lim _t\u2192\u221e_ _\u2225_ **x** _t_ _\u2212_ **x** _[\u2217]_ _\u2225_ 2 _\u2192_ 0, which implies a.s. **x** _t_ _\u2212\u2212\u2192_ **x** _[\u2217]_ . The following is another related result for which we do not present the full proof. **Theorem 17.17** _Let_ **H** _be a function mapping_ R _[N]_ _to_ R _[N]_ _,_ ( **w** _t_ ) _t\u2208_ N _a sequence of ran-_ _dom variables in_ R _[N]_ _,_ ( _\u03b1_ _t_ ) _t\u2208_ N _a sequence of real numbers, and_ ( **x** _t_ ) _t\u2208_ N _a sequence_ _defined by_ _\u2200s \u2208_ [ _N_ ] _,_ **x** _t_ +1 ( _s_ ) = **x** _t_ ( _s_ ) + _\u03b1_ _t_ ( _s_ )\ufffd **H** ( **x** _t_ )( _s_ ) _\u2212_ **x** _t_ ( _s_ ) + **w** _t_ ( _s_ )\ufffd _,_ _for some_ **x** 0 _\u2208_ R _[N]_ _. Define F_ _t_ _by F_ _t_ = _{_ ( **x** _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t_ _,_ ( **w** _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t\u2212_ 1 ( _\u03b1_ _t_ _\u2032_ ) _t_ _\u2032_ _\u2264t_ _}, that is the_ _entire history up to t, and assume that the following conditions are met:_ _\u2022_ _\u2203K_ 1 _, K_ 2 _\u2208_",
    "chunk_id": "foundations_machine_learning_384"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "R : E \ufffd _\u2225_ **w** _t_ _\u2225_ [2] ( _s_ ) \ufffd\ufffd _F_ _t_ \ufffd _\u2264_ _K_ 1 + _K_ 2 _\u2225_ **x** _t_ _\u2225_ [2] _for some norm \u2225\u00b7 \u2225;_ _\u2022_ E \ufffd **w** _t_ \ufffd\ufffd _F_ _t_ \ufffd = 0 _;_ _\u2022_ _\u2200s \u2208_ [ _N_ ] _,_ [\ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _[t]_ [(] _[s]_ [) = +] _[\u221e][,]_ [ \ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _t_ [2] [(] _[s]_ [)] _[ <]_ [ +] _[\u221e][; and]_ _\u2022_ **H** _is a \u2225\u00b7 \u2225_ _\u221e_ _-contraction with fixed point_ **x** _[\u2217]_ _._ _Then, the sequence_ **x** _t_ _converges almost surely to_ **x** _[\u2217]_ _:_ _a.s._ **x** _t_ _\u2212\u2212\u2192_ **x** _[\u2217]_ _._ (17.21) The next sections present several learning algorithms for MDPs with an unknown model. **17.5.2** **TD(0) algorithm** This section presents an algorithm, TD(0) algorithm, for evaluating a policy in the case where the environment model is unknown. The algorithm is based on Bellman\u2019s linear equations giving the value of a policy _\u03c0_ (see proposition 17.9): _V_ _\u03c0_ ( _s_ ) = E[ _r_ ( _s, \u03c0_ ( _s_ )] + _\u03b3_ \ufffd P[ _s_ _[\u2032]_ _|s, \u03c0_ ( _s_ )] _V_ _\u03c0_ ( _s_ _[\u2032]_ ) _s_ _[\u2032]_ = E \ufffd _r_ ( _s, \u03c0_ ( _s_ )) + _\u03b3V_ _\u03c0_ ( _s_ _[\u2032]_ ) _|s_ \ufffd _._ _s_ _[\u2032]_ However, here the probability distribution according to which this last expectation is defined is not known. Instead, the TD(0) algorithm consists of _\u2022_ sampling a new state _s_ _[\u2032]_ ; and _\u2022_ updating the policy values according to the following, which justifies the name of the algorithm: _V_ ( _s_ ) _\u2190_ (1 _\u2212_ _\u03b1_ ) _V_ ( _s_ ) + _\u03b1_ [ _r_ ( _s, \u03c0_ ( _s_ )) + _\u03b3V_ ( _s_ _[\u2032]_ )] = _V_ ( _s_ ) + _\u03b1_ [ _r_ ( _s, \u03c0_ ( _s_ )) + _\u03b3V_ ( _s_ _[\u2032]_ ) _\u2212_ _V_ ( _s_ )] _._ (17.22) \ufffd ~~\ufffd\ufffd~~ ~~\ufffd~~ temporal difference of _V_ values Here, the parameter _\u03b1_ is a function of the number of visits to the state _s_ . **398** **Chapter 17** **Reinforcement Learning** TD(0)() 1 **V** _\u2190_ **V** 0 _\u25b7_ initialization. 2 **for** _t \u2190_ 0 **to** _T_ **do** 3 _s \u2190_ SelectState() 4 **for** each step of epoch _t_ **do** 5 _r_ _[\u2032]_ _\u2190_ Reward( _s, \u03c0_ ( _s_ )) 6 _s_ _[\u2032]_ _\u2190_ NextState( _\u03c0, s_ ) 7 _V_ ( _s_ ) _\u2190_ (1 _\u2212_ _\u03b1_ ) _V_ ( _s_ ) + _\u03b1_ [ _r_ _[\u2032]_ + _\u03b3V_ ( _s_ _[\u2032]_ )] 8 _s \u2190_ _s_ _[\u2032]_ 9 **return V** The pseudocode of the algorithm is given above. The algorithm starts with an arbitrary policy value vector **V** 0 . An initial state is returned by SelectState at the beginning of each epoch. Within each epoch, the iteration continues until a final state is found. Within each iteration, action _\u03c0_ ( _s_ ) is taken from the current state _s_ following policy _\u03c0_ . The new state _s_ _[\u2032]_ reached and the reward _r_",
    "chunk_id": "foundations_machine_learning_385"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u2032]_ received are observed. The policy value of state _s_ is then updated according to the rule (17.22) and current state set to be _s_ _[\u2032]_ . The convergence of the algorithm can be proven using theorem 17.17. We will give instead the full proof of the convergence of the Q-learning algorithm, for which that of TD(0) can be viewed as a special case. **17.5.3** **Q-learning algorithm** This section presents an algorithm for estimating the optimal state-action value function _Q_ _[\u2217]_ in the case of an unknown model. Note that the optimal policy or policy value can be straightforwardly derived from _Q_ _[\u2217]_ via: _\u03c0_ _[\u2217]_ ( _s_ ) = argmax _a\u2208A_ _Q_ _[\u2217]_ ( _s, a_ ) and _V_ _[\u2217]_ ( _s_ ) = max _a\u2208A_ _Q_ _[\u2217]_ ( _s, a_ ). To simplify the presentation, we will assume a deterministic reward function. The Q-learning algorithm is based on the equations giving the optimal stateaction value function _Q_ _[\u2217]_ (17.1): _Q_ _[\u2217]_ ( _s, a_ ) = E[ _r_ ( _s, a_ )] + _\u03b3_ \ufffd P[ _s_ _[\u2032]_ _| s, a_ ] _V_ _[\u2217]_ ( _s_ _[\u2032]_ ) _s_ _[\u2032]_ _\u2208S_ = E _s_ _[\u2032]_ [[] _[r]_ [(] _[s, a]_ [) +] _[ \u03b3]_ [ max] _a\u2208A_ _[Q]_ _[\u2217]_ [(] _[s]_ _[\u2032]_ _[, a]_ [)]] _[.]_ **17.5** **Learning algorithms** **399** Q-Learning( _\u03c0_ ) 1 _Q \u2190_ _Q_ 0 _\u25b7_ initialization, e.g., _Q_ 0 = 0 _._ 2 **for** _t \u2190_ 0 **to** _T_ **do** 3 _s \u2190_ SelectState() 4 **for** each step of epoch _t_ **do** 5 _a \u2190_ SelectAction( _\u03c0, s_ ) _\u25b7_ policy _\u03c0_ derived from _Q_, e.g., _\u03f5_ -greedy. 6 _r_ _[\u2032]_ _\u2190_ Reward( _s, a_ ) 7 _s_ _[\u2032]_ _\u2190_ NextState( _s, a_ ) 8 _Q_ ( _s, a_ ) _\u2190_ _Q_ ( _s, a_ ) + _\u03b1_ \ufffd _r_ _[\u2032]_ + _\u03b3_ max _a_ _\u2032_ _Q_ ( _s_ _[\u2032]_ _, a_ _[\u2032]_ ) _\u2212_ _Q_ ( _s, a_ )\ufffd 9 _s \u2190_ _s_ _[\u2032]_ 10 **return** _Q_ As for the policy values in the previous section, the distribution model is not known. Thus, the Q-learning algorithm consists of the following main steps: _\u2022_ sampling a new state _s_ _[\u2032]_ ; and _\u2022_ updating the policy values according to the following: _Q_ ( _s, a_ ) _\u2190_ (1 _\u2212_ _\u03b1_ ) _Q_ ( _s, a_ ) + _\u03b1_ [ _r_ ( _s, a_ ) + _\u03b3_ max (17.23) _a_ _[\u2032]_ _\u2208A_ _[Q]_ [(] _[s]_ _[\u2032]_ _[, a]_ _[\u2032]_ [)]] _[.]_ where the parameter _\u03b1_ is a function of the number of visits to the state _s_ . The algorithm can be viewed as a stochastic formulation of the value iteration algorithm presented in the previous section. The pseudocode is given above. Within each epoch, an action is selected from the current state _s_ using a policy _\u03c0_ derived from _Q_ . The choice of the policy _\u03c0_ is arbitrary so long as it guarantees that every pair ( _s, a_ ) is visited infinitely many times. The reward received and the state _s_ _[\u2032]_ observed are",
    "chunk_id": "foundations_machine_learning_386"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "then used to update _Q_ following (17.23). **Theorem 17.18** _Consider a finite MDP. Assume that for all s \u2208_ _S and a \u2208_ _A,_ \ufffd + _t_ =0 _\u221e_ _[\u03b1]_ _[t]_ [(] _[s, a]_ [) = +] _[\u221e][, and]_ [ \ufffd] [+] _t_ =0 _[\u221e]_ _[\u03b1]_ _t_ [2] [(] _[s, a]_ [)] _[ <]_ [ +] _[\u221e]_ _[with][ \u03b1]_ _[t]_ [(] _[s, a]_ [)] _[ \u2208]_ [[0] _[,]_ [ 1]] _[. Then, the]_ _Q-learning algorithm converges to the optimal value Q_ _[\u2217]_ _(with probability one)._ Note that the conditions on _\u03b1_ _t_ ( _s, a_ ) impose that each state-action pair is visited infinitely many times. **400** **Chapter 17** **Reinforcement Learning** Proof: Let ( _Q_ _t_ ( _s, a_ )) _t\u2265_ 0 denote the sequence of state-action value functions at ( _s, a_ ) _\u2208_ _S \u00d7_ _A_ generated by the algorithm. By definition of the Q-learning updates, _Q_ _t_ +1 ( _s_ _t_ _, a_ _t_ ) = _Q_ _t_ ( _s_ _t_ _, a_ _t_ ) + _\u03b1_ \ufffd _r_ ( _s_ _t_ _, a_ _t_ ) + _\u03b3_ max \ufffd _._ _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[s]_ _[t]_ [+1] _[, a]_ _[\u2032]_ [)] _[ \u2212]_ _[Q]_ _[t]_ [(] _[s]_ _[t]_ _[, a]_ _[t]_ [)] This can be rewritten as the following for all _s \u2208_ _S_ and _a \u2208_ _A_ : _Q_ _t_ +1 ( _s, a_ ) = _Q_ _t_ ( _s, a_ ) + _\u03b1_ _t_ ( _s, a_ ) _r_ ( _s, a_ ) + _\u03b3_ E max _\u2212_ _Q_ _t_ ( _s, a_ ) \ufffd _u\u223c_ P[ _\u00b7|s,a_ ] \ufffd _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[u, a]_ _[\u2032]_ [)] \ufffd \ufffd + _\u03b3\u03b1_ _t_ ( _s, a_ ) max E max _,_ (17.24) \ufffd _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[s]_ _[\u2032]_ _[, a]_ _[\u2032]_ [)] _[ \u2212]_ _u\u223c_ P[ _\u00b7|s,a_ ] \ufffd _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[u, a]_ _[\u2032]_ [)] \ufffd [\ufffd] if we define _s_ _[\u2032]_ = NextState( _s, a_ ) and _\u03b1_ _t_ ( _s, a_ ) as 0 if ( _s, a_ ) _\u0338_ = ( _s_ _t_ _, a_ _t_ ) and _\u03b1_ _t_ ( _s_ _t_ _, a_ _t_ ) otherwise. Now, let **Q** _t_ denote the vector with components _Q_ _t_ ( _s, a_ ), **w** _t_ the vector whose _s_ _[\u2032]_ th entry is _w_ _t_ ( _s_ ) = max E _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[s]_ _[\u2032]_ _[, a]_ _[\u2032]_ [)] _[ \u2212]_ _u\u223c_ P[ _\u00b7|s,a_ ] max _,_ \ufffd _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[u, a]_ _[\u2032]_ [)] \ufffd and **H** ( **Q** _t_ ) the vector with components **H** ( **Q** _t_ )( _s, a_ ) defined by **H** ( **Q** _t_ )( _s, a_ ) = _r_ ( _s, a_ ) + _\u03b3_ E _u\u223c_ P[ _\u00b7|s,a_ ] Then, in view of (17.24), max _._ \ufffd _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[u, a]_ _[\u2032]_ [)] \ufffd _\u2200_ ( _s, a_ ) _\u2208_ _S \u00d7_ _A,_ **Q** _t_ +1 ( _s, a_ ) = **Q** _t_",
    "chunk_id": "foundations_machine_learning_387"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "( _s, a_ )+ _\u03b1_ _t_ ( _s, a_ )\ufffd **H** ( **Q** _t_ )( _s, a_ ) _\u2212_ **Q** _t_ ( _s, a_ )+ _\u03b3_ **w** _t_ ( _s_ )\ufffd _._ We now show that the hypotheses of theorem 17.17 hold for **Q** _t_ and **w** _t_, which will imply the convergence of **Q** _t_ to **Q** _[\u2217]_ . The conditions on _\u03b1_ _t_ hold by assumption. By definition of **w** _t_, E[ **w** _t_ \ufffd\ufffd _F_ _t_ ] = 0. Also, for any _s_ _\u2032_ _\u2208_ _S_, _|_ **w** _t_ ( _s_ ) _| \u2264_ max E _a_ _[\u2032]_ _[ |][Q]_ _[t]_ [(] _[s]_ _[\u2032]_ _[, a]_ _[\u2032]_ [)] _[|]_ [ +] \ufffd\ufffd\ufffd\ufffd _u\u223c_ P[ _\u00b7|s,a_ ] max \ufffd _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[u, a]_ _[\u2032]_ [)] \ufffd [\ufffd] \ufffd\ufffd\ufffd _\u2264_ 2 max _|_ max _s_ _[\u2032]_ _a_ _[\u2032]_ _[ Q]_ _[t]_ [(] _[s]_ _[\u2032]_ _[, a]_ _[\u2032]_ [)] _[|]_ [ = 2] _[\u2225]_ **[Q]** _[t]_ _[\u2225]_ _[\u221e]_ _[.]_ **17.5** **Learning algorithms** **401** Thus, E \ufffd **w** _t_ [2] [(] _[s]_ [)] \ufffd\ufffd _F_ _t_ \ufffd _\u2264_ 4 _\u2225_ **Q** _t_ _\u2225_ [2] _\u221e_ [. Finally,] **[ H]** [ is a] _[ \u03b3]_ [-contraction for] _[ \u2225\u00b7 \u2225]_ _[\u221e]_ [since for] any **Q** 1 _,_ **Q** 2 _\u2208_ R _[|][S][|\u00d7|][A][|]_, and ( _s, a_ ) _\u2208_ _S \u00d7 A_, we can write _|_ **H** ( **Q** 2 )( _x, a_ ) _\u2212_ **H** ( **Q** 1 )( _x, a_ ) _|_ = _\u03b3_ E \ufffd\ufffd\ufffd\ufffd _u\u223c_ P[ _\u00b7|s,a_ ] max \ufffd _a_ _[\u2032]_ _[ Q]_ [2] [(] _[u, a]_ _[\u2032]_ [)] _[ \u2212]_ [max] _a_ _[\u2032]_ _[ Q]_ [1] [(] _[u, a]_ _[\u2032]_ [)] \ufffd [\ufffd] \ufffd\ufffd\ufffd _\u2264_ _\u03b3_ E _u\u223c_ P[ _\u00b7|s,a_ ] max \ufffd\ufffd\ufffd\ufffd _a_ _[\u2032]_ _[ Q]_ [2] [(] _[u, a]_ _[\u2032]_ [)] _[ \u2212]_ [max] _a_ _[\u2032]_ _[ Q]_ [1] [(] _[u, a]_ _[\u2032]_ [)] \ufffd\ufffd\ufffd\ufffd _\u2264_ _\u03b3_ E _u\u223c_ P[ _\u00b7|s,a_ ] [max] _a_ _[\u2032]_ [ [] _[|][Q]_ [2] [(] _[u, a]_ _[\u2032]_ [)] _[ \u2212]_ _[Q]_ [1] [(] _[u, a]_ _[\u2032]_ [)] _[|]_ []] _\u2264_ _\u03b3_ max max _u_ _a_ _[\u2032]_ [ [] _[|][Q]_ [2] [(] _[u, a]_ _[\u2032]_ [)] _[ \u2212]_ _[Q]_ [1] [(] _[u, a]_ _[\u2032]_ [)] _[|]_ []] = _\u03b3\u2225_ **Q** 2 _\u2212_ **Q** 1 _\u2225_ _\u221e_ _._ Since **H** is a contraction, it admits a fixed point **Q** _[\u2217]_ : **H** ( **Q** _[\u2217]_ ) = **Q** _[\u2217]_ . The choice of the policy _\u03c0_ according to which an action _a_ is selected (line 5) is not specified by the algorithm and, as already indicated, the theorem guarantees the convergence of the algorithm for an arbitrary policy so long as it ensures that every pair ( _s, a_ ) is visited infinitely many times. In practice, several natural choices are considered for _\u03c0_ . One possible choice is the policy determined by the state-action value at time _t_, _Q_ _t_ . Thus, the action selected from state _s_ is argmax _a\u2208A_ _Q_ _t_ ( _s, a_ ). But this choice typically does not guarantee that all actions are",
    "chunk_id": "foundations_machine_learning_388"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "taken or that all states are visited. Instead, a standard choice in reinforcement learning is the socalled _\u03f5-greedy policy_, which consists of selecting with probability (1 _\u2212_ _\u03f5_ ) the greedy action from state _s_, that is, argmax _a\u2208A_ _Q_ _t_ ( _s, a_ ), and with probability _\u03f5_ a random action from _s_, for some _\u03f5 \u2208_ (0 _,_ 1). Another possible choice is the so-called _Boltzmann_ _exploration_, which, given the current state-action value _Q_, epoch _t \u2208{_ 0 _, . . ., T_ _}_, and current state _s_, consists of selecting action _a_ with the following probability: _Q_ ( _s,a_ ) _e_ _\u03c4t_ _p_ _t_ ( _a|s, Q_ ) = \ufffd _a_ _[\u2032]_ _\u2208A_ _[e]_ _Q_ ( _s,a_ _[\u2032]_ ) _,_ _\u03c4t_ where _\u03c4_ _t_ is the _temperature_ . _\u03c4_ _t_ must be defined so that _\u03c4_ _t_ _\u2192_ 0 as _t \u2192_ + _\u221e_, which ensures that for large values of _t_, the greedy action based on _Q_ is selected. This is natural, since as _t_ increases, we can expect _Q_ to be close to the optimal function. On the other hand, _\u03c4_ _t_ must be chosen so that it does not tend to 0 too fast to ensure that all actions are visited infinitely often. It can be chosen, for instance, as 1 _/_ log( _n_ _t_ ( _s_ )), where _n_ _t_ ( _s_ ) is the number of times _s_ has been visited up to epoch _t_ . Reinforcement learning algorithms include two components: a _learning policy_, which determines the action to take, and an _update rule_, which defines the new estimate of the optimal value function. For an _off-policy algorithm_, the update rule does not necessarily depend on the learning policy. Q-learning is an off-policy algorithm since its update rule (line 8 of the pseudocode) is based on the max **402** **Chapter 17** **Reinforcement Learning** operator and the comparison of all possible actions _a_ _[\u2032]_, that is the greedy action, which may not coincide with the action recommended by the current the policy _\u03c0_ . More generally, an off-policy algorithm evaluates or improves one policy, while acting based on another policy. In contrast, the algorithm presented in the next section, SARSA, is an _on-policy_ _algorithm_ . An on-policy algorithm evaluates and improves the current policy used for control. It evaluates the return based on the algorithm\u2019s policy. **17.5.4** **SARSA** SARSA is also an algorithm for estimating the optimal state-action value function in the case of an unknown model. The pseudocode is given in figure 17.7. The algorithm is in fact very similar to Q-learning, except that its update rule (line 9 of the pseudocode) is based on the action _a_ _[\u2032]_ selected by the learning policy. Thus, SARSA is an on-policy algorithm, and its convergence therefore crucially depends on the learning policy. In particular, the convergence of the algorithm requires, in addition to all actions being selected infinitely often, that the learning policy becomes greedy in the limit. The proof of the convergence of the algorithm is nevertheless close to that of Q-learning. The name",
    "chunk_id": "foundations_machine_learning_389"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "of the algorithm derives from the sequence of instructions defining successively _s_, _a_, _r_ _[\u2032]_, _s_ _[\u2032]_, and _a_ _[\u2032]_, and the fact that the update to the function _Q_ depends on the quintuple ( _s, a, r_ _[\u2032]_ _, s_ _[\u2032]_ _, a_ ). **17.5.5** **TD(** _\u03bb_ **) algorithm** Both TD(0) and Q-learning algorithms are only based on immediate rewards. The idea of TD( _\u03bb_ ) consists instead of using multiple steps ahead. Thus, for _n >_ 1 steps, we would have the update _V_ ( _s_ ) _\u2190_ _V_ ( _s_ ) + _\u03b1_ ( _R_ _t_ _[n]_ _[\u2212]_ _[V]_ [ (] _[s]_ [))] _[,]_ where _R_ _t_ _[n]_ [is defined by] _R_ _t_ _[n]_ [=] _[ r]_ _[t]_ [+1] [+] _[ \u03b3r]_ _[t]_ [+2] [+] _[ . . .]_ [ +] _[ \u03b3]_ _[n][\u2212]_ [1] _[r]_ _[t]_ [+] _[n]_ [+] _[ \u03b3]_ _[n]_ _[V]_ [ (] _[s]_ _[t]_ [+] _[n]_ [)] _[.]_ How should _n_ be chosen? Instead of selecting a specific _n_, TD( _\u03bb_ ) is based on a geometric distribution over all rewards _R_ _t_ _[n]_ [, that is, it uses] _[ R]_ _t_ _[\u03bb]_ [= (1] _[\u2212][\u03bb]_ [)][ \ufffd] [+] _n_ =0 _[\u221e]_ _[\u03bb]_ _[n]_ _[R]_ _t_ _[n]_ instead of _R_ _t_ _[n]_ [where] _[ \u03bb][ \u2208]_ [[0] _[,]_ [ 1]. Thus, the main update becomes] _V_ ( _s_ ) _\u2190_ _V_ ( _s_ ) + _\u03b1_ ( _R_ _t_ _[\u03bb]_ _[\u2212]_ _[V]_ [ (] _[s]_ [))] _[.]_ The pseudocode of the algorithm is given above. For _\u03bb_ = 0, the algorithm coincides with TD(0). _\u03bb_ = 1 corresponds to the total future reward. **17.5** **Learning algorithms** **403** SARSA( _\u03c0_ ) 1 _Q \u2190_ _Q_ 0 _\u25b7_ initialization, e.g., _Q_ 0 = 0 _._ 2 **for** _t \u2190_ 0 **to** _T_ **do** 3 _s \u2190_ SelectState() 4 _a \u2190_ SelectAction( _\u03c0_ ( _Q_ ) _, s_ ) _\u25b7_ policy _\u03c0_ derived from _Q_, e.g., _\u03f5_ -greedy. 5 **for** each step of epoch _t_ **do** 6 _r_ _[\u2032]_ _\u2190_ Reward( _s, a_ ) 7 _s_ _[\u2032]_ _\u2190_ NextState( _s, a_ ) 8 _a_ _[\u2032]_ _\u2190_ SelectAction( _\u03c0_ ( _Q_ ) _, s_ _[\u2032]_ ) _\u25b7_ policy _\u03c0_ derived from _Q_, e.g., _\u03f5_ -greedy. 9 _Q_ ( _s, a_ ) _\u2190_ _Q_ ( _s, a_ ) + _\u03b1_ _t_ ( _s, a_ )\ufffd _r_ _[\u2032]_ + _\u03b3Q_ ( _s_ _[\u2032]_ _, a_ _[\u2032]_ ) _\u2212_ _Q_ ( _s, a_ )\ufffd 10 _s \u2190_ _s_ _[\u2032]_ 11 _a \u2190_ _a_ _[\u2032]_ 12 **return** _Q_ **Figure 17.7** The SARSA algorithm. In the previous sections, we presented learning algorithms for an agent navigating in an unknown environment. The scenario faced in many practical applications is more challenging; often, the information the agent receives about the environment is uncertain or unreliable. Such problems can be modeled as partially observable Markov decision processes (POMDPs). POMDPs are defined by augmenting the definition of MDPs with an observation probability distribution depending on the action taken, the state reached, and the observation. The presentation of their model and solution techniques are beyond the scope of this",
    "chunk_id": "foundations_machine_learning_390"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "material. **17.5.6** **Large state space** In some cases in practice, the number of states or actions to consider for the environment may be very large. For example, the number of states in the game of backgammon is estimated to be over 10 [20] . Thus, the algorithms presented in the previous section can become computationally impractical for such applications. More importantly, generalization becomes extremely difficult. Suppose we wish to estimate the policy value _V_ _\u03c0_ ( _s_ ) at each state _s_ using experience obtained using policy _\u03c0_ . To cope with the case of large state spaces, we **404** **Chapter 17** **Reinforcement Learning** TD( _\u03bb_ )() 1 **V** _\u2190_ **V** 0 _\u25b7_ initialization. 2 **e** _\u2190_ **0** 3 **for** _t \u2190_ 0 **to** _T_ **do** 4 _s \u2190_ SelectState() 5 **for** each step of epoch _t_ **do** 6 _s_ _[\u2032]_ _\u2190_ NextState( _\u03c0, s_ ) 7 _\u03b4 \u2190_ _r_ ( _s, \u03c0_ ( _s_ )) + _\u03bbV_ ( _s_ _[\u2032]_ ) _\u2212_ _V_ ( _s_ ) 8 _e_ ( _s_ ) _\u2190_ _\u03bbe_ ( _s_ ) + 1 9 **for** _u \u2208_ _S_ **do** 10 **if** _u \u0338_ = _s_ **then** 11 _e_ ( _u_ ) _\u2190_ _\u03b3\u03bbe_ ( _u_ ) 12 _V_ ( _u_ ) _\u2190_ _V_ ( _u_ ) + _\u03b1\u03b4e_ ( _u_ ) 13 _s \u2190_ _s_ _[\u2032]_ 14 **return V** can map each state of the environment to R _[N]_ via a mapping **\u03a6** : _S \u2192_ R _[N]_, with _N_ relatively small ( _N \u2248_ 200 has been used for backgammon) and approximate _V_ _\u03c0_ ( _s_ ) by a function _f_ **w** ( _s_ ) parameterized by some vector **w** . For example, _f_ **w** could be a linear function defined by _f_ **w** ( _s_ ) = **w** _\u00b7_ **\u03a6** ( _s_ ) for all _s \u2208_ _S_, or some more complex non-linear function of **w** . The problem then consists of approximating _V_ _\u03c0_ with _f_ **w** and can be formulated as a regression problem. Note, however, that the empirical data available is not i.i.d. Suppose that at each time step _t_ the agent receives the exact policy value _V_ _\u03c0_ ( _s_ _t_ ). Then, if the family of functions _f_ **w** is differentiable, a gradient descent method applied to the empirical squared loss can be used to sequentially update the weight vector **w** via: 1 **w** _t_ +1 = **w** _t_ _\u2212_ _\u03b1\u2207_ **w** _t_ 2 [[] _[V]_ _[\u03c0]_ [(] _[s]_ _[t]_ [)] _[ \u2212]_ _[f]_ **[w]** _[t]_ [(] _[s]_ _[t]_ [)]] [2] [ =] **[ w]** _[t]_ [ +] _[ \u03b1]_ [[] _[V]_ _[\u03c0]_ [(] _[s]_ _[t]_ [)] _[ \u2212]_ _[f]_ **[w]** _[t]_ [(] _[s]_ _[t]_ [)]] _[\u2207]_ **[w]** _[t]_ _[f]_ **[w]** _[t]_ [(] _[s]_ _[t]_ [)] _[.]_ It is worth mentioning, however, that for large action spaces, there are simple cases where the methods used do not converge and instead cycle. **17.6** **Chapter notes** **405** **17.6** **Chapter notes** Reinforcement learning is an important area of machine learning with a large body of literature. This chapter presents only a brief",
    "chunk_id": "foundations_machine_learning_391"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "introduction to this area. For a more detailed study, the reader could consult the book of Sutton and Barto [1998], whose mathematical content is short, or those of Puterman [1994] and Bertsekas [1987], which discuss in more depth several aspects, as well as the more recent book of Szepesv\u00b4ari [2010]. The Ph.D. theses of Singh [1993] and Littman [1996] are also excellent sources. Some foundational work on MDPs and the introduction of the temporal difference (TD) methods are due to Sutton [1984]. Q-learning was introduced and analyzed by Watkins [1989], though it can be viewed as a special instance of TD methods. The first proof of the convergence of Q-learning was given by Watkins and Dayan [1992]. Many of the techniques used in reinforcement learning are closely related to those of stochastic approximation which originated with the work of Robbins and Monro [1951], followed by a series of results including Dvoretzky [1956], Schmetterer [1960], Kiefer and Wolfowitz [1952], and Kushner and Clark [1978]. For a recent survey of stochastic approximation, including a discussion of powerful proof techniques based on ODE (ordinary differential equations), see Kushner [2010] and the references therein. The connection with stochastic approximation was emphasized by Tsitsiklis [1994] and Jaakkola et al. [1994], who gave a related proof of the convergence of Q-learning. For the convergence rate of Q-learning, consult Even-Dar and Mansour [2003]. For recent results on the convergence of the policy iteration algorithm, see Ye [2011], which shows that the algorithm is strongly polynomial for a fixed discount factor. Reinforcement learning has been successfully applied to a variety of problems including robot control, board games such as backgammon in which Tesauro\u2019s TDGammon reached the level of a strong master [Tesauro, 1995] (see also chapter 11 of Sutton and Barto [1998]), chess, elevator scheduling problems [Crites and Barto, 1996], telecommunications, inventory management, dynamic radio channel assignment [Singh and Bertsekas, 1997], and a number of other problems (see chapter 1 of Puterman [1994]). **Conclusion** We described a large variety of machine learning algorithms and techniques and discussed their theoretical foundations as well as their use and applications. While this is not a fully comprehensive presentation, it should nevertheless offer the reader some idea of the breadth of the field and its multiple connections with a variety of other domains, including statistics, information theory, optimization, game theory, and automata and formal language theory. The fundamental concepts, algorithms, and proof techniques we presented should supply the reader with the necessary tools for analyzing other learning algorithms, including variants of the algorithms analyzed in this book. They are also likely to be helpful for devising new algorithms or for studying new learning schemes. We strongly encourage the reader to explore both and more generally to seek enhanced solutions for all theoretical, algorithmic, and applied learning problems. The exercises included at the end of each chapter, as well as the full solutions we provide separately, should help the reader become more familiar with the techniques and concepts described. Some of them could also serve as a starting point for research work and the investigation",
    "chunk_id": "foundations_machine_learning_392"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "of new questions. Many of the algorithms we presented as well as their variants can be directly used in applications to derive effective solutions to real-world learning problems. Our detailed description of the algorithms and discussion should help with their implementation or their adaptation to other learning scenarios. Machine learning is a relatively recent field and yet probably one of the most active ones in computer science. Given the wide accessibility of digitized data and its many applications, we can expect it to continue to grow at a very fast pace over the next few decades. Learning problems of different nature, some arising due to the substantial increase of the scale of the data, which already requires processing billions of records in some applications, others related to the introduction of completely new learning frameworks, are likely to pose new research challenges and require novel algorithmic solutions. In all cases, learning theory, algorithms, **408** **Conclusion** and applications form an exciting area of computer science and mathematics, which we hope this book could at least partly communicate. # A Linear Algebra Review In this appendix, we introduce some basic notions of linear algebra relevant to the material presented in this book. This appendix does not represent an exhaustive tutorial, and it is assumed that the reader has some prior knowledge of the subject. **A.1** **Vectors and norms** We will denote by H a vector space whose dimension may be infinite. **A.1.1** **Norms** **Definition A.1** _A mapping_ \u03a6: H _\u2192_ R + _is said to define a_ norm _on_ H _if it verifies the following_ _axioms:_ _\u2022_ _definiteness: \u2200_ **x** _\u2208_ H _,_ \u03a6( **x** ) = 0 _\u21d4_ **x** = **0** _;_ _\u2022_ _homogeneity: \u2200_ **x** _\u2208_ H _, \u2200\u03b1 \u2208_ R _,_ \u03a6( _\u03b1_ **x** ) = _|\u03b1|_ \u03a6( **x** ) _;_ _\u2022_ _triangle inequality: \u2200_ **x** _,_ **y** _\u2208_ H _,_ \u03a6( **x** + **y** ) _\u2264_ \u03a6( **x** ) + \u03a6( **y** ) _._ A norm is typically denoted by _\u2225\u00b7 \u2225_ . Examples of vector norms are the absolute value on R and the Euclidean (or _L_ 2 ) norm on R _[N]_ . More generally, for any _p \u2265_ 1 the _L_ _p_ norm is defined on R _[N]_ as _\u2200_ **x** _\u2208_ R _[N]_ _,_ _\u2225_ **x** _\u2225_ _p_ = \ufffd \ufffd _[N]_ _|x_ _j_ _|_ _[p]_ [\ufffd] [1] _[/p]_ _._ (A.1) _j_ =1 The _L_ 1, _L_ 2, and _L_ _\u221e_ norms are some of the most commonly used norms, where _\u2225_ **x** _\u2225_ _\u221e_ = max _j\u2208_ [ _N_ ] _|x_ _j_ _|_ . Two norms _\u2225\u00b7 \u2225_ and _\u2225\u00b7 \u2225_ _[\u2032]_ are said to be _equivalent_ iff there exists _\u03b1, \u03b2 >_ 0 such that for all **x** _\u2208_ H, _\u03b1\u2225_ **x** _\u2225\u2264\u2225_ **x** _\u2225_ _[\u2032]_ _\u2264_ _\u03b2\u2225_ **x** _\u2225._ (A.2) The following general inequalities relating these norms can be proven straightforwardly: _\u2225_ **x** _\u2225_ 2 _\u2264\u2225_ **x** _\u2225_ 1 _\u2264_ _\u221a_ _N_ _\u2225_ **x** _\u2225_ 2 (A.3) _\u2225_ **x** _\u2225_ _\u221e_ _\u2264\u2225_ **x** _\u2225_ 2 _\u2264_ _\u221a_ _N_ _\u2225_ **x** _\u2225_ _\u221e_ (A.4) _\u2225_",
    "chunk_id": "foundations_machine_learning_393"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**x** _\u2225_ _\u221e_ _\u2264\u2225_ **x** _\u2225_ 1 _\u2264_ _N_ _\u2225_ **x** _\u2225_ _\u221e_ _._ (A.5) The second inequality of the first line can be shown using the _Cauchy-Schwarz inequality_ presented later while the other inequalities are clear. These inequalities show the equivalence of these three norms. More generally, all norms on a finite-dimensional space are equivalent. The following additional properties hold for the _L_ _\u221e_ norm: for all **x** _\u2208_ H, _\u2200p \u2265_ 1 _, \u2225_ **x** _\u2225_ _\u221e_ _\u2264\u2225_ **x** _\u2225_ _p_ _\u2264_ _N_ [1] _[/p]_ _\u2225_ **x** _\u2225_ _\u221e_ (A.6) lim (A.7) _p\u2192_ + _\u221e_ _[\u2225]_ **[x]** _[\u2225]_ _[p]_ [ =] _[ \u2225]_ **[x]** _[\u2225]_ _[\u221e]_ _[.]_ **410** **Appendix A** **Linear Algebra Review** The inequalities of the first line are straightforward and imply the limit property of the second line. **Definition A.2 (Hilbert space)** _A_ Hilbert space _is a vector space equipped with an inner product_ _\u27e8\u00b7, \u00b7\u27e9_ _and that is_ complete _(all Cauchy sequences are convergent). The inner product induces a_ _norm defined as follows:_ _\u2200_ **x** _\u2208_ H _,_ _\u2225_ **x** _\u2225_ H = ~~\ufffd~~ _\u27e8_ **x** _,_ **x** _\u27e9._ (A.8) **A.1.2** **Dual norms** **Definition A.3** _Let \u2225\u00b7 \u2225_ _be a norm on_ R _[N]_ _. Then, the_ dual norm _\u2225\u00b7 \u2225_ _\u2217_ _associated to \u2225\u00b7 \u2225_ _is the_ _norm defined by_ _\u2200_ **y** _\u2208_ R _[N]_ _,_ _\u2225_ **y** _\u2225_ _\u2217_ = sup _| \u27e8_ **y** _,_ **x** _\u27e9| ._ (A.9) _\u2225_ **x** _\u2225_ =1 _p_ [1] [+] [1] _q_ For any _p, q \u2265_ 1 that are _conjugate_ that is such that [1] For any _p, q \u2265_ 1 that are _conjugate_ that is such that _p_ [+] _q_ [= 1, the] _[ L]_ _[p]_ [ and] _[ L]_ _[q]_ [ norms are dual] norms of each other. In particular, the dual norm of _L_ 2 is the _L_ 2 norm, and the dual norm of the _L_ 1 norm is the _L_ _\u221e_ norm. **Proposition A.4 (H\u00a8older\u2019s inequality)** _Let p, q \u2265_ 1 _be conjugate:_ _p_ 1 [+] [1] _q_ [= 1] _[.]_ _Then, for all_ _x, y \u2208_ R _[N]_ _,_ _| \u27e8_ **x** _,_ **y** _\u27e9| \u2264\u2225_ **x** _\u2225_ _p_ _\u2225_ **y** _\u2225_ _q_ _,_ (A.10) _with equality when |y_ _i_ _|_ = _|x_ _i_ _|_ _[p][\u2212]_ [1] _for all i \u2208_ [ _N_ ] _._ **Proposition A.4 (H\u00a8older\u2019s inequality)** _Let p, q \u2265_ 1 _be conjugate:_ _p_ 1 [+] [1] _q_ Proof: The statement holds trivially for **x** = **0** or **y** = **0** ; thus, we can assume **x** _\u0338_ = **0** and **y** _\u0338_ = **0** . Let _a, b >_ 0. By the concavity of log (see definition B.7), we can write 1 log \ufffd _p_ _\u2265_ [1] _q_ _[b]_ _[q]_ \ufffd _p_ 1 _p_ _[a]_ _[p]_ [ + 1] [1] _p_ [log(] _[a]_ _[p]_ [) + 1] _q_ _q_ [log(] _[b]_ _[q]_ [) = log(] _[a]_ [) + log(] _[b]_ [) = log(] _[ab]_ [)] _[.]_ Taking the exponential of the left- and right-hand sides gives 1 _p_ _[a]_ _[p]_ [ + 1] _q_ _[b]_ _[q]_ _[ \u2265]_",
    "chunk_id": "foundations_machine_learning_394"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[ab,]_ which is known as _Young\u2019s inequality_ . Using this inequality with _a_ = _|x_ _j_ _|/\u2225_ **x** _\u2225_ _p_ and _b_ = _|y_ _j_ _|/\u2225_ **y** _\u2225_ _q_ for _j \u2208_ [ _N_ ] and summing up gives _N_ \ufffd _j_ =1 _[|][x]_ _[j]_ _[y]_ _[j]_ _[|]_ [1] _\u2225_ **x** _\u2225_ _[p]_ _\u2225_ **y** _\u2225_ _[q]_ [1] [1] _\u2225_ **x** _\u2225_ _[p]_ _p_ _\u2225_ **x** _\u2225_ _[p]_ _\u2225_ **y** _\u2225_ _[q]_ _q_ _\u2225_ **y** _\u2225_ _[q]_ _p_ [1] [+ 1] _q_ _q_ [= 1] _[.]_ _j_ =1 _[x]_ _[j]_ _[y]_ _[j]_ _\u2264_ [1] _\u2225_ **x** _\u2225_ _p_ _\u2225_ **y** _\u2225_ _q_ _p_ _\u2225_ **x** _\u2225_ _[p]_ _\u2225_ **x** _\u2225_ _[p]_ [ + 1] _q_ _\u2225_ **y** _\u2225_ _[q]_ [1] _\u2225_ **y** _\u2225_ _[q]_ [ =] _p_ Since _| \u27e8_ **x** _,_ **y** _\u27e9| \u2264_ [\ufffd] _[N]_ _j_ =1 _[|][x]_ _[j]_ _[y]_ _[j]_ _[|]_ [, the inequality claim follows.] The equality case can be verified straightforwardly. Taking _p_ = _q_ = 2 immediately yields the following result known as the _Cauchy-Schwarz inequality_ . **Corollary A.5 (Cauchy-Schwarz inequality)** _For all_ **x** _,_ **y** _\u2208_ R _[N]_ _,_ _| \u27e8_ **x** _,_ **y** _\u27e9| \u2264\u2225_ **x** _\u2225_ 2 _\u2225_ **y** _\u2225_ 2 _,_ (A.11) _with equality iff_ **x** _and_ **y** _are collinear._ Let _H_ be the hyperplane in R _[N]_ whose equation is given by **w** _\u00b7_ **x** + _b_ = 0 _,_ for some normal vector **w** _\u2208_ R _[N]_ and offset _b \u2208_ R. Let _d_ _p_ ( **x** _, H_ ) denote the distance of **x** to the hyperplane _H_, that is, _d_ _p_ ( **x** _, H_ ) = inf (A.12) **x** _[\u2032]_ _\u2208H_ _[\u2225]_ **[x]** _[\u2032]_ _[ \u2212]_ **[x]** _[\u2225]_ _[p]_ _[.]_ Then, the following identity holds for all _p \u2265_ 1: _d_ _p_ ( **x** _, H_ ) = _[|]_ **[w]** _[ \u00b7]_ **[ x]** [ +] _[ b][|]_ _,_ (A.13) _\u2225_ **w** _\u2225_ _q_ 1 where _q_ is the conjugate of _p_ : _p_ [+] [1] _q_ [= 1. (A.13) can be shown by a straightforward application] of the results of appendix B to the constrained optimization problem (A.12). **A.2** **Matrices** **411** **A.1.3** **Relationship between norms** A general form for the inequalities seen in equations (A.3), (A.4) and (A.5), which holds for all _L_ _p_ norms, is shown in the following proposition. **Proposition A.6** _Let_ 1 _\u2264_ _p \u2264_ _q. Then the following inequalities hold for all_ **x** _\u2208_ R _[N]_ _:_ 1 _\u2225x\u2225_ _q_ _\u2264\u2225x\u2225_ _p_ _\u2264_ _N_ _p_ _[\u2212]_ _q_ [1] _\u2225x\u2225_ _q_ _._ (A.14) Proof: First, assume **x** _\u0338_ = **0**, otherwise the inequalities hold trivially. Then the first inequality holds using 1 _\u2264_ _p \u2264_ _q_ as follows: \ufffd _\u2225\u2225_ **xx** _\u2225\u2225_ _pq_ \ufffd _p_ = \ufffd _N_ \ufffd _\u2225_ **x** _x\u2225_ _i_ _q_ \ufffd _p_ _\u2265_ \ufffd _N_ \ufffd _\u2225_ **x** _x\u2225_ _i_ _q_ \ufffd _q_ = 1 _._ _x_ _i_ \ufffd _\u2225_ **x** _\u2225_ _q_ ~~\ufffd\ufffd~~ \ufffd ~~\ufffd~~ _\u2264_ 1 _N_ \ufffd _i_ =1 _N_ \ufffd _i_ =1 _x_ _i_ \ufffd _\u2225_ **x** _\u2225_ **x** _\u2225_ _q_ _p_ _\u2265_ \ufffd _\u2225_",
    "chunk_id": "foundations_machine_learning_395"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "**x** _\u2225_ _q_ _p_ = \ufffd _q_ = 1 _._ \ufffd Finally, the second inequality follows by using H\u00a8older\u2019s inequality (proposition A.4) _N_ _q_ 1 _\u2212_ _[p]_ _q_ \ufffd (1) _q\u2212p_ _i_ =1 \ufffd \uf8fb 1 _p_ _N_ _q_ _q_ \ufffd ( _|x_ _i_ _|_ _[p]_ ) _p_ _i_ =1 \ufffd _[p]_ _q_ [\uf8f9] _q_ [\ufffd] _N_ \ufffd _i_ =1 \uf8ee \uf8ee _N_ \ufffd \uf8f0\ufffd _i_ =1 _\u2225_ **x** _\u2225_ _p_ = _N_ \ufffd \ufffd _i_ =1 _N_ _p_ \ufffd _|x_ _i_ _|_ _[p]_ _i_ =1 \ufffd [1] _p_ _\u2264_ 1 = _\u2225_ **x** _\u2225_ _q_ _N_ _p_ _[\u2212]_ _q_ [1] _,_ which completes the proof. **A.2** **Matrices** For a matrix **M** _\u2208_ R _[m][\u00d7][n]_ with _m_ rows and _n_ columns, we denote by **M** _ij_ its _ij_ th entry, for all _i \u2208_ [ _m_ ] and _j \u2208_ [ _n_ ]. For any _m \u2265_ 1, we denote by **I** _m_ the _m_ -dimensional identity matrix, and refer to it as **I** when the dimension is clear from the context. The _transpose_ of **M** is denoted by **M** _[\u22a4]_ and defined by ( **M** _[\u22a4]_ ) _ij_ = **M** _ji_ for all ( _i, j_ ). For any two matrices **M** _\u2208_ R _[m][\u00d7][n]_ and **N** _\u2208_ R _[n][\u00d7][p]_, ( **MN** ) _[\u22a4]_ = **N** _[\u22a4]_ **M** _[\u22a4]_ . **M** is said to be _symmetric_ iff **M** _ij_ = **M** _ji_ for all ( _i, j_ ), that is, iff **M** = **M** _[\u22a4]_ . The _trace_ of a square matrix **M** is denoted by Tr[ **M** ] and defined as Tr[ **M** ] = [\ufffd] _[N]_ _i_ =1 **[M]** _[ii]_ [. For any] two matrices **M** _\u2208_ R _[m][\u00d7][n]_ and **N** _\u2208_ R _[n][\u00d7][m]_, the following identity holds: Tr[ **MN** ] = Tr[ **NM** ]. More generally, the following cyclic property holds with the appropriate dimensions for the matrices **M**, **N**, and **P** : Tr[ **MNP** ] = Tr[ **PMN** ] = Tr[ **NPM** ] _._ (A.15) The inverse of a square matrix **M**, which exists when **M** has full rank, is denoted by **M** _[\u2212]_ [1] and is the unique matrix satisfying **MM** _[\u2212]_ [1] = **M** _[\u2212]_ [1] **M** = **I** . **A.2.1** **Matrix norms** A _matrix norm_ is a norm defined over R _[m][\u00d7][n]_ where _m_ and _n_ are the dimensions of the matrices considered. Many matrix norms, including those discussed below, satisfy the following _submulti-_ _plicative property_ : _\u2225_ **MN** _\u2225\u2264\u2225_ **M** _\u2225\u2225_ **N** _\u2225._ (A.16) The _matrix norm induced_ by the vector norm _\u2225\u00b7 \u2225_ _p_ or the _operator norm induced_ by that norm is also denoted by _\u2225\u00b7 \u2225_ _p_ and defined by _\u2225_ **M** _\u2225_ _p_ = sup _\u2225_ **Mx** _\u2225_ _p_ _._ (A.17) _\u2225_ **x** _\u2225_ _p_ _\u2264_ 1 The norm induced for _p_ = 2 is known as the _spectral norm_, which equals the largest singular value of **M** (see section A.2.2), or the square-root of the largest eigenvalue of **M** _[\u22a4]_ **M** : _\u2225_ **M** _\u2225_ 2 = _\u03c3_ 1 ( **M** ) = ~~\ufffd~~ _\u03bb_ max ( **M**",
    "chunk_id": "foundations_machine_learning_396"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[\u22a4]_ **M** ) _._ (A.18) **412** **Appendix A** **Linear Algebra Review** Not all matrix norms are induced by vector norms. The _Frobenius norm_ denoted by _\u2225\u00b7 \u2225_ _F_ is the most notable of such norms and is defined by: _m_ _\u2225_ **M** _\u2225_ _F_ = \ufffd \ufffd _i_ =1 _n_ **M** [2] \ufffd _ij_ _j_ =1 _n_ \ufffd 1 _/_ 2 _._ \ufffd The Frobenius norm can be interpreted as the _L_ 2 norm of a vector when treating **M** as a vector of size _mn_ . It also coincides with the norm induced by the _Frobenius product_, which is the inner product defined for all **M** _,_ **N** _\u2208_ R _[m][\u00d7][n]_ by _\u27e8_ **M** _,_ **N** _\u27e9_ _F_ = Tr[ **M** _[\u22a4]_ **N** ] _._ (A.19) This relates the Frobenius norm to the singular values of **M** : _\u2225_ **M** _\u2225_ [2] _F_ [= Tr[] **[M]** _[\u22a4]_ **[M]** [] =] _r_ \ufffd _\u03c3_ _i_ ( **M** ) [2] _,_ _i_ =1 where _r_ = rank( **M** ). The second equality follows from properties of SPSD matrices (see section A.2.3). For any _j \u2208_ [ _n_ ], let **M** _j_ denote the _j_ th column of **M**, that is **M** = [ **M** 1 _\u00b7 \u00b7 \u00b7_ **M** _n_ ]. Then, for any _p, r \u2265_ 1, the _L_ _p,r_ _group norm_ of **M** is defined by _n_ _\u2225_ **M** _\u2225_ _p,r_ = \ufffd _\u2225_ **M** _i_ _\u2225_ _[r]_ _p_ \ufffd _j_ =1 _n_ _\u2225_ **M** _\u2225_ _p,r_ = \ufffd \ufffd 1 _/r_ _._ \ufffd One of the most commonly used group norms is the _L_ 2 _,_ 1 norm defined by _\u2225_ **M** _\u2225_ 2 _,_ 1 = _n_ \ufffd _\u2225_ **M** _i_ _\u2225_ 2 _._ _i_ =1 **A.2.2** **Singular value decomposition** The compact _singular value decomposition (SVD)_ of **M**, with _r_ = rank( **M** ) _\u2264_ min( _m, n_ ), can be written as follows: **M** = **U** _M_ **\u03a3** _M_ **V** _M_ _[\u22a4]_ _[.]_ The _r \u00d7 r_ matrix **\u03a3** _M_ = diag( _\u03c3_ 1 _, . . ., \u03c3_ _r_ ) is diagonal and contains the non-zero _singular values_ of **M** sorted in decreasing order, that is _\u03c3_ 1 _\u2265_ _. . . \u2265_ _\u03c3_ _r_ _>_ 0. The matrices **U** _M_ _\u2208_ R _[m][\u00d7][r]_ and **V** _M_ _\u2208_ R _[n][\u00d7][r]_ have orthonormal columns that contain the _left_ and _right singular vectors_ of **M** corresponding to the sorted singular values. We denote by **U** _k_ _\u2208_ R _[m][\u00d7][k]_ the top _k \u2264_ _r_ left singular vectors of **M** . The _orthogonal projection_ onto the span of **U** _k_ can be written as **P** _U_ _k_ = **U** _k_ **U** _[\u22a4]_ _k_ [, where] **[ P]** _[U]_ _k_ [is] SPSD and idempotent, i.e., **P** [2] _U_ _k_ [=] **[ P]** _[U]_ _k_ [. Moreover, the orthogonal projection onto the subspace] orthogonal to **U** _k_ is defined as **P** _U_ _k_ _,\u22a5_ . Similar definitions, i.e., **V** _k_ _,_ **P** _V_ _k_ _,_ **P** _V_ _k_ _,\u22a5_, hold for the right singular vectors. The _generalized inverse_, or _Moore-Penrose pseudo-inverse_ of",
    "chunk_id": "foundations_machine_learning_397"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "a matrix **M** is denoted by **M** _[\u2020]_ and defined by **M** _[\u2020]_ = **U** _M_ **\u03a3** _[\u2020]_ _M_ **[V]** _M_ _[\u22a4]_ _[,]_ (A.20) where **\u03a3** _[\u2020]_ _M_ [= diag(] _[\u03c3]_ 1 _[\u2212]_ [1] _, . . ., \u03c3_ _r_ _[\u2212]_ [1] ). For any square _m \u00d7 m_ matrix **M** with full rank, i.e., _r_ = _m_, the pseudo-inverse coincides with the matrix inverse: **M** _[\u2020]_ = **M** _[\u2212]_ [1] . **A.2.3** **Symmetric positive semidefinite (SPSD) matrices** **Definition A.7** _A symmetric matrix_ **M** _\u2208_ R _[m][\u00d7][m]_ _is said to be_ positive semidefinite _iff_ **x** _[\u22a4]_ **Mx** _\u2265_ 0 (A.21) _for all_ **x** _\u2208_ R _[m]_ _._ **M** _is said to be_ positive definite _if the inequality is strict._ Kernel matrices (see chapter 6) and orthogonal projection matrices are two examples of SPSD matrices. It is straightforward to show that a matrix **M** is SPSD iff its eigenvalues are all nonnegative. Furthermore, the following properties hold for any SPSD matrix **M** : **A.2** **Matrices** **413** _\u2022_ **M** admits a decomposition **M** = **X** _[\u22a4]_ **X** for some matrix **X** and the _Cholesky decomposition_ provides one such decomposition in which **X** is an upper triangular matrix. _\u2022_ The left and right singular vectors of **M** are the same and the SVD of **M** is also its eigenvalue decomposition. _\u2022_ The SVD of an arbitrary matrix **X** = **U** _X_ **\u03a3** _X_ **V** _X_ _[\u22a4]_ [defines the SVD of two related SPSD matrices:] the left singular vectors ( **U** _X_ ) are the eigenvectors of **XX** _[\u22a4]_, the right singular vectors ( **V** _X_ ) are the eigenvectors of **X** _[\u22a4]_ **X** and the non-zero singular values of **X** are the square roots of the non-zero eigenvalues of **XX** _[\u22a4]_ and **X** _[\u22a4]_ **X** . _\u2022_ The trace of **M** is the sum of its singular values, i.e., Tr[ **M** ] = [\ufffd] _[r]_ _i_ =1 _[\u03c3]_ _[i]_ [(] **[M]** [), where rank(] **[M]** [) =] _[ r]_ [.] _\u2022_ The top singular vector of **M**, **u** 1, maximizes the _Rayleigh quotient_, which is defined as _r_ ( **x** _,_ **M** ) = **[x]** _[\u22a4]_ **[Mx]** _._ **x** _[\u22a4]_ **x** In other words, **u** 1 = argmax **x** _r_ ( **x** _,_ **M** ) and _r_ ( **u** _,_ **M** ) = _\u03c3_ 1 ( **M** ). Similarly, if **M** _[\u2032]_ = **P** _U_ _i_ _,\u22a5_ **M**, that is, the projection of **M** onto the subspace orthogonal to **U** _i_, then **u** _i_ +1 = argmax **x** _r_ ( **x** _,_ **M** _[\u2032]_ ), where **u** _i_ +1 is the ( _i_ + 1)st singular vector of **M** . # B Convex Optimization In this appendix, we introduce the main definitions and results of convex optimization needed for the analysis of the learning algorithms presented in this book. **B.1** **Differentiation and unconstrained optimization** We start with some basic definitions for differentiation needed to present Fermat\u2019s theorem and to describe some properties of convex functions. **Definition B.1 (Gradient)** _Let f_ : X _\u2286_ R _[N]_ _\u2192_ R _be a differentiable function. Then, the_ gradient _of",
    "chunk_id": "foundations_machine_learning_398"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "f at_ **x** _\u2208_ X _is the vector in_ R _[N]_ _denoted by \u2207f_ ( **x** ) _and defined by_ \uf8f9 _._ \uf8fa\uf8fa\uf8fa\uf8fb _\u2207f_ ( **x** ) = \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 _\u2202f_ _\u2202_ **x** 1 [(] **[x]** [)] _..._ _\u2202f_ _\u2202_ **x** _N_ [(] **[x]** [)] **Definition B.2 (Hessian)** _Let f_ : X _\u2286_ R _[N]_ _\u2192_ R _be a twice differentiable function._ _Then, the_ Hessian _of f at_ **x** _\u2208_ X _is the matrix in_ R _[N]_ _[\u00d7][N]_ _denoted by \u2207_ [2] _f_ ( **x** ) _and defined by_ _\u2202_ [2] _f_ _\u2207_ [2] _f_ ( **x** ) = ( **x** ) \ufffd _\u2202_ **x** _i_ _,_ **x** _j_ \ufffd 1 _\u2264i,j\u2264N_ _[.]_ Next, we present a classic result for unconstrained optimization. **Theorem B.3 (Fermat\u2019s theorem)** _Let f_ : X _\u2286_ R _[N]_ _\u2192_ R _be a differentiable function. If f admits_ _a local extremum at_ **x** _[\u2217]_ _\u2208_ X _, then \u2207f_ ( **x** _[\u2217]_ ) = 0 _, that is,_ **x** _[\u2217]_ _is a_ stationary point _._ **B.2** **Convexity** This section introduces the notions of _convex sets_ and _convex functions_ . Convex functions play an important role in the design and analysis of learning algorithms, in part because a local minimum of a convex function is necessarily also a global minimum. Thus, the properties of a hypothesis that is learned by finding a local minimum of a convex optimization are often well understood, while for some non-convex optimization problems there may be a very large number of local minima for which no clear characterization of the learned hypothesis can be given. **Definition B.4 (Convex set)** _A set_ X _\u2286_ R _[N]_ _is said to be_ convex _if for any two points_ **x** _,_ **y** _\u2208_ X _the segment_ [ **x** _,_ **y** ] _lies in_ X _, that is_ _{\u03b1_ **x** + (1 _\u2212_ _\u03b1_ ) **y** : 0 _\u2264_ _\u03b1 \u2264_ 1 _} \u2286_ X _._ **416** **Appendix B** **Convex Optimization** **Figure B.1** **Image:** [No caption returned] **Image:** [No caption returned] Examples of a convex (left) and a concave (right) functions. Note that any line segment drawn between two points on the convex function lies entirely above the graph of the function while any line segment drawn between two points on the concave function lies entirely below the graph of the function. The following lemma illustrates several operations on convex sets that preserve convexity. These will be useful for proving several subsequent results of this section. **Lemma B.5 (Operations that preserve convexity of sets)** _The following operations on convex_ _sets preserve convexity:_ _\u2022_ _Let {_ C _i_ _}_ _i\u2208I_ _be any family of sets where for all i \u2208_ _I the set_ C _i_ _is convex. Then the intersection_ _of these sets_ [\ufffd] _i\u2208I_ [C] _[i]_ _[ is also convex.]_ _\u2022_ _Let_ C 1 _and_ C 2 _be convex sets, then their sum_ C 1 + C 2 = _{x_ 1 + _x_ 2 : _x_ 1 _\u2208_ C 1 _, x_ 2 _\u2208_ C 2 _}, when_ _defined, is convex._ _\u2022_ _Let_ C 1 _and_ C 2 _be",
    "chunk_id": "foundations_machine_learning_399"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "convex sets, then their cross-product_ (C 1 _\u00d7_ C 2 ) _is also convex._ _\u2022_ _Any projection of a convex set_ C _is also convex._ Proof: The first property holds since for any _x, y \u2208_ [\ufffd] _i\u2208I_ [C] _[i]_ [ and any] _[ \u03b1][ \u2208]_ [[0] _[,]_ [ 1], we have] _[ \u03b1x]_ [ +] (1 _\u2212_ _\u03b1_ ) _y \u2208_ C _i_ for any _i \u2208_ _I_ by the convexity of C _i_ . The second property holds since for any ( _x_ 1 + _x_ 2 ) _,_ ( _y_ 1 + _y_ 2 ) _\u2208_ (C 1 + C 2 ) we have _\u03b1_ ( _x_ 1 + _x_ 2 ) + (1 _\u2212_ _\u03b1_ )( _y_ 1 + _y_ 2 ) = ( _\u03b1x_ 1 + (1 _\u2212_ _\u03b1_ ) _y_ 1 + _\u03b1x_ 2 + (1 _\u2212_ _\u03b1_ ) _y_ 2 ) _\u2208_ (C 1 + C 2 ), which follows since _\u03b1x_ 1 + (1 _\u2212_ _\u03b1_ ) _y_ 1 _\u2208_ C 1 and _\u03b1x_ 2 + (1 _\u2212_ _\u03b1_ ) _y_ 2 _\u2208_ C 2 . The third property holds since for ( _x_ 1 _, x_ 2 ) _,_ ( _y_ 1 _, y_ 2 ) _\u2208_ (C 1 _\u00d7_ C 2 ) we have _\u03b1_ ( _x_ 1 _, x_ 2 ) + (1 _\u2212_ _\u03b1_ )( _y_ 1 _, y_ 2 ) = ( _\u03b1x_ 1 + (1 _\u2212_ _\u03b1_ ) _y_ 1 _, \u03b1x_ 2 + (1 _\u2212_ _\u03b1_ ) _y_ 2 ) _\u2208_ (C 1 _\u00d7_ C 2 ), where the membership holds due to the assumption that C 1 and C 2 are convex. Finally, the fourth property holds by noting that for any decomposition of the convex set C into projections C 1 and C 2, such that C = (C 1 _\u00d7_ C 2 ), it must be the case that C 1 is convex. If C 2 is empty, then the result is trivially true. Otherwise, fix an element _x_ 2 _\u2208_ C 2, then for any _x, y \u2208_ C 1 and any _\u03b1 \u2208_ [0 _,_ 1] we have _\u03b1_ ( _x, x_ 2 ) + (1 _\u2212_ _\u03b1_ )( _y, x_ 2 ) _\u2208_ C, which implies _\u03b1x_ + (1 _\u2212_ _\u03b1_ ) _y \u2208_ C 1 . Since C 1 was chosen arbitrarily, this fact holds for any projection of C. Note that many set operations may not preserve convexity. Consider the union of disjoint intervals on R: [ _a, b_ ] _\u222a_ [ _c, d_ ] where _a < b < c < d_ . Clearly [ _a, b_ ] and [ _c, d_ ] are convex, however we have 1 2 _[b]_ [ + (1] _[ \u2212]_ 2 [1] [)] _[c][ \u0338\u2208]_ [([] _[a, b]_ []] _[ \u222a]_ [[] _[c, d]_ []).] **Definition B.6 (Convex hull)** _The_ convex hull conv(X) _of a set of points_ X _\u2286_ R _[N]_ _is the minimal_ _convex set containing_ X _and can be equivalently defined as follows:_ conv(X) = \ufffd \ufffd _[m]_ _\u03b1_",
    "chunk_id": "foundations_machine_learning_400"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_i_ **x** _i_ : _m \u2265_ 1 _, \u2200i \u2208_ [ _m_ ] _,_ **x** _i_ _\u2208_ X _, \u03b1_ _i_ _\u2265_ 0 _,_ _i_ =1 _m_ \ufffd _\u03b1_ _i_ = 1\ufffd _._ (B.1) _i_ =1 Let Epi _f_ denote the _epigraph_ of function _f_ : X _\u2192_ R, that is the set of points lying above its graph: _{_ ( _x, y_ ): _x \u2208_ X _, y \u2265_ _f_ ( _x_ ) _}_ . **B.2** **Convexity** **417** **Figure B.2** **Image:** [No caption returned] Illustration of the first-order property satisfied by all convex functions. **Definition B.7 (Convex function)** _Let_ X _be a convex set. A function f_ : X _\u2192_ R _is said to be_ convex _iff_ Epi _f is a convex set, or, equivalently, if for all_ **x** _,_ **y** _\u2208_ X _and \u03b1 \u2208_ [0 _,_ 1] _,_ _f_ ( _\u03b1_ **x** + (1 _\u2212_ _\u03b1_ ) **y** ) _\u2264_ _\u03b1f_ ( **x** ) + (1 _\u2212_ _\u03b1_ ) _f_ ( **y** ) _._ (B.2) _f_ is said to be _strictly convex_ if inequality (B.2) is strict for all **x** _,_ **y** _\u2208_ X where **x** _\u0338_ = **y** and _\u03b1 \u2208_ (0 _,_ 1). _f_ is said to be (strictly) _concave_ when _\u2212f_ is (strictly) convex. Figure B.1 shows simple examples of convex and concave functions. Convex functions can also be characterized in terms of their first- or second-order differential. **Theorem B.8** _Let f be a differentiable function, then f is convex if and only if_ dom( _f_ ) _is convex_ _and the following inequalities hold:_ _\u2200_ **x** _,_ **y** _\u2208_ dom( _f_ ) _, f_ ( **y** ) _\u2212_ _f_ ( **x** ) _\u2265\u2207f_ ( **x** ) _\u00b7_ ( **y** _\u2212_ **x** ) _._ (B.3) The property (B.3) is illustrated by figure B.2: for a convex function, the hyperplane tangent at **x** is always below the graph. **Theorem B.9** _Let f be a twice differentiable function, then f is convex iff_ dom( _f_ ) _is convex and_ _its Hessian is positive semidefinite:_ _\u2200_ **x** _\u2208_ dom( _f_ ) _, \u2207_ [2] _f_ ( **x** ) _\u2ab0_ 0 _._ Recall that a symmetric matrix is positive semidefinite if all of its eigenvalues are non-negative. Further, note that when _f_ is scalar, this theorem states that _f_ is convex if and only if its second derivative is always non-negative, that is, for all _x \u2208_ dom( _f_ ) _, f_ _[\u2032\u2032]_ ( _x_ ) _\u2265_ 0. **Example B.10 (Linear functions)** Any linear function _f_ is both convex and concave, since equation (B.2) holds with equality for both _f_ and _\u2212f_ by the definition of linearity. **Example B.11 (Quadratic function)** The function _f_ : _x \ufffd\u2192_ _x_ [2] defined over R is convex since it is twice differentiable and for all _x \u2208_ R, _f_ _[\u2032\u2032]_ ( _x_ ) = 2 _>_ 0. **Example B.12 (Norms)** Any norm _\u2225\u00b7\u2225_ defined over a convex set X is convex since by the triangle inequality and the homogeneity property of the norm, for all _\u03b1 \u2208_ [0 _,_ 1] _,_ **x** _,_ **y**",
    "chunk_id": "foundations_machine_learning_401"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_\u2208_ X, we can write _\u2225\u03b1_ **x** + (1 _\u2212_ _\u03b1_ ) **y** _\u2225\u2264\u2225\u03b1_ **x** _\u2225_ + _\u2225_ (1 _\u2212_ _\u03b1_ ) **y** _\u2225_ = _\u03b1\u2225_ **x** _\u2225_ + (1 _\u2212_ _\u03b1_ ) _\u2225_ **y** _\u2225_ _._ **418** **Appendix B** **Convex Optimization** **Example B.13 (Maximum function)** The max function defined for all **x** _\u2208_ R _[N]_, by **x** _\ufffd\u2192_ max _j\u2208_ [ _N_ ] **x** _j_ is convex. For all _\u03b1 \u2208_ [0 _,_ 1] _,_ **x** _,_ **y** _\u2208_ R _[N]_, by the sub-additivity of max, we can write max( _\u03b1_ **x** _j_ + (1 _\u2212_ _\u03b1_ ) **y** _j_ ) _\u2264_ max( _\u03b1_ **x** _j_ ) + max((1 _\u2212_ _\u03b1_ ) **y** _j_ ) = _\u03b1_ max( **x** _j_ ) + (1 _\u2212_ _\u03b1_ ) max( **y** _j_ ) _._ _j_ _j_ _j_ _j_ _j_ One useful approach for proving convexity or concavity of functions is to make use of composition rules. For simplicity of presentation, we will assume twice differentiability, although the results can also be proven without this assumption. **Lemma B.14 (Composition of convex/concave functions)** _Assume h_ : R _\u2192_ R _and g_ : R _[N]_ _\u2192_ R _are twice differentiable functions and for all_ **x** _\u2208_ R _[N]_ _, define f_ ( **x** ) = _h_ ( _g_ ( **x** )) _. Then the following_ _implications are valid:_ _\u2022_ _h is convex and non-decreasing, and g is convex_ = _\u21d2_ _f is convex._ _\u2022_ _h is convex and non-increasing, and g is concave_ = _\u21d2_ _f is convex._ _\u2022_ _h is concave and non-decreasing, and g is concave_ = _\u21d2_ _f is concave._ _\u2022_ _h is concave and non-increasing, and g is convex_ = _\u21d2_ _f is concave._ Proof: We restrict ourselves to _N_ = 1, since it suffices to prove convexity (concavity) along all arbitrary lines that intersect the domain. Now, consider the second derivative of _f_ : _f_ _[\u2032\u2032]_ ( _x_ ) = _h_ _[\u2032\u2032]_ ( _g_ ( _x_ )) _g_ _[\u2032]_ ( _x_ ) [2] + _h_ _[\u2032]_ ( _g_ ( _x_ )) _g_ _[\u2032\u2032]_ ( _x_ ) _._ (B.4) Note that if _h_ is convex and non-decreasing, we have _h_ _[\u2032\u2032]_ _\u2265_ 0 and _h_ _[\u2032]_ _\u2265_ 0. Furthermore, if _g_ is convex we also have _g_ _[\u2032\u2032]_ _\u2265_ 0, and it follows that _f_ _[\u2032\u2032]_ ( _x_ ) _\u2265_ 0, which proves the first statement. The remainder of the statements are proven in a similar manner. **Example B.15 (Composition of functions)** The previous lemma shows the convexity or concavity of the following composed functions: _\u2022_ If _f_ : R _[N]_ _\u2192_ R is convex, then exp( _f_ ) is convex. _\u2022_ Any squared norm _\u2225\u00b7 \u2225_ [2] is convex. _\u2022_ For all **x** _\u2208_ R _[N]_ the function **x** _\ufffd\u2192_ log( [\ufffd] _[N]_ _j_ =1 _[x]_ _[j]_ [) is concave.] The following two lemmas give examples of two other operations preserving convexity. **Lemma B.16 (Pointwise supremum or maximum of convex functions)** _Let_ ( _f_ _i_ ) _i\u2208_ I _be a fam-_ _ily of convex functions defined over a convex set_ C _.",
    "chunk_id": "foundations_machine_learning_402"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Then, their_ pointwise supremum _f defined_ _for all x \u2208_ C _by f_ ( _x_ ) = sup _i\u2208_ I _f_ _i_ ( _x_ ) _(resp. their_ pointwise maximum _if |_ I _| <_ + _\u221e) is a convex_ _function._ Proof: Observe that Epi _f_ = _\u2229_ _i\u2208_ I Epi _f_ _i_ and is therefore convex as an intersection of convex sets. **Example B.17 (Pointwise supremum of convex functions)** The lemma shows in particular the convexity of the following functions: _\u2022_ A piecewise linear function _f_ defined for all _x \u2208_ R _[N]_ by _f_ ( _x_ ) = max _i\u2208_ [ _m_ ] **w** _i_ _[\u22a4]_ **[x]** [ +] _[ b]_ _[i]_ [ is convex as] a pointwise maximum of affine (and thus convex) functions. _\u2022_ The maximum eigenvalue _\u03bb_ max ( **M** ) is a convex function over the set of symmetric matrices **M** since the set of symmetric matrices is convex and since _\u03bb_ max ( **M** ) = sup _\u2225_ **x** _\u2225_ 2 _\u2264_ 1 **x** _[\u22a4]_ **Mx** is defined as the supremum of the linear (and thus convex) functions **M** _\ufffd\u2192_ **x** _[\u22a4]_ **Mx** . _\u2022_ More generally, let _\u03bb_ 1 ( **M** ) _, . . ., \u03bb_ _k_ ( **M** ) denote the top _k \u2264_ _n_ eigenvalues of a symmetric _n \u00d7_ _n_ matrix **M** . Then, by a similar argument, **M** _\ufffd\u2192_ [\ufffd] _[k]_ _i_ =1 _[\u03bb]_ _[i]_ [(] **[M]** [) is a convex function since] \ufffd _ki_ =1 _[\u03bb]_ _[i]_ [(] **[M]** [) = sup] dim( **V** )= _k_ \ufffd _ki_ =1 **[u]** _[\u22a4]_ _i_ **[Mu]** _[i]_ [, where] **[ u]** [1] _[, . . .,]_ **[ u]** _[k]_ [ is an orthonormal basis of] **[ V]** [.] _\u2022_ Using the previous property, along with the fact that Tr( **M** ) is linear in **M**, also shows that **M** _\ufffd\u2192_ [\ufffd] _[n]_ _i_ = _k_ +1 _[\u03bb]_ _[i]_ [(] **[M]** [) = Tr(] **[M]** [)] _[ \u2212]_ [\ufffd] _[k]_ _i_ =1 _[\u03bb]_ _[i]_ [(] **[M]** [) or] **[ M]** _[ \ufffd\u2192]_ [\ufffd] _[n]_ _i_ = _n\u2212k_ +1 _[\u03bb]_ _[i]_ [(] **[M]** [) =] _[ \u2212]_ [\ufffd] _[k]_ _i_ =1 _[\u03bb]_ _[i]_ [(] _[\u2212]_ **[M]** [)] are concave functions. **B.3** **Constrained optimization** **419** **Lemma B.18 (Partial infimum)** _Let f be a convex function defined over a convex set_ C _\u2286_ X _\u00d7_ Y _and let_ B _\u2286_ Y _be a convex set such that_ A = _{x \u2208_ X : _\u2203y \u2208_ B _|_ ( _x, y_ ) _\u2208_ C _} is non-empty. Then,_ A _is a convex set and the function g defined for all x \u2208_ A _by g_ ( _x_ ) = inf _y\u2208_ B _f_ ( _x, y_ ) _is convex._ Proof: First note that the intersection of the convex sets C and (X _\u00d7_ B) is convex. Thus, A is convex since it is the projection of the convex set C _\u2229_ (X _\u00d7_ B) onto X. Let _x_ 1 and _x_ 2 be in A. By definition of _g_, for any _\u03f5 >_ 0, there exist _y_",
    "chunk_id": "foundations_machine_learning_403"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "1 _, y_ 2 _\u2208_ B with ( _x_ 1 _, y_ 1 ) _,_ ( _x_ 2 _, y_ 2 ) _\u2208_ C such that _f_ ( _x_ 1 _, y_ 1 ) _\u2264_ _g_ ( _x_ 1 ) + _\u03f5_ and _f_ ( _x_ 2 _, y_ 2 ) _\u2264_ _g_ ( _x_ 2 ) + _\u03f5_ . Then, for any _\u03b1 \u2208_ [0 _,_ 1], _g_ ( _\u03b1x_ 1 + (1 _\u2212_ _\u03b1_ ) _x_ 2 ) = inf _y\u2208_ B _[f]_ [(] _[\u03b1x]_ [1] [ + (1] _[ \u2212]_ _[\u03b1]_ [)] _[x]_ [2] _[, y]_ [)] _\u2264_ _f_ ( _\u03b1x_ 1 + (1 _\u2212_ _\u03b1_ ) _x_ 2 _, \u03b1y_ 1 + (1 _\u2212_ _\u03b1_ ) _y_ 2 ) _\u2264_ _\u03b1f_ ( _x_ 1 _, y_ 1 ) + (1 _\u2212_ _\u03b1_ ) _f_ ( _x_ 2 _, y_ 2 ) _\u2264_ _\u03b1g_ ( _x_ 1 ) + (1 _\u2212_ _\u03b1_ ) _g_ ( _x_ 2 ) + _\u03f5._ Since the inequality holds for all _\u03f5 >_ 0, it implies _g_ ( _\u03b1x_ 1 + (1 _\u2212_ _\u03b1_ ) _x_ 2 ) _\u2264_ _\u03b1g_ ( _x_ 1 ) + (1 _\u2212_ _\u03b1_ ) _g_ ( _x_ 2 ) _,_ which completes the proof. **Example B.19** The lemma shows in particular that the distance to a convex set B, _d_ ( _x,_ B) = inf _y\u2208_ B _\u2225x \u2212_ _y\u2225_, is a convex function of _x_ in any normed vector space, since ( _x, y_ ) _\ufffd\u2192\u2225x \u2212_ _y\u2225_ is jointly convex in _x_ and _y_ for any norm _\u2225\u00b7 \u2225_ . The following is a useful inequality applied in a variety of contexts. It is in fact a quasi-direct consequence of the definition of convexity. **Theorem B.20 (Jensen\u2019s inequality)** _Let X be a random variable taking values in a non-empty_ _convex set_ C _\u2286_ R _[N]_ _with a finite expectation_ E[ _X_ ] _, and f a measurable convex function defined_ _over_ C _. Then,_ E[ _X_ ] _is in_ C _,_ E[ _f_ ( _X_ )] _is finite, and the following inequality holds:_ _f_ (E[ _X_ ]) _\u2264_ E[ _f_ ( _X_ )] _._ Proof: We give a sketch of the proof, which essentially follows from the definition of convexity. Note that for any finite set of elements _x_ 1 _, . . ., x_ _n_ in C and any positive reals _\u03b1_ 1 _, . . ., \u03b1_ _n_ such that \ufffd _ni_ =1 _[\u03b1]_ _[i]_ [ = 1, we have] _n_ _f_ \ufffd \ufffd _\u03b1_ _i_ _x_ _i_ \ufffd _\u2264_ _i_ =1 _n_ \ufffd _\u03b1_ _i_ _f_ ( _x_ _i_ ) _._ _i_ =1 This follows straightforwardly by induction from the definition of convexity. Since the _\u03b1_ _i_ s can be interpreted as probabilities, this immediately proves the inequality for any distribution with a finite support defined by _**\u03b1**_ = ( _\u03b1_ 1 _, . . ., \u03b1_ _n_ ): _f_ ( _**\u03b1**_ E [[] _[X]_ [])] _[ \u2264]_ _**\u03b1**_ [E][[] _[f]_ [(] _[X]_ [)]] _[ .]_ Extending this to arbitrary distributions can be shown",
    "chunk_id": "foundations_machine_learning_404"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "via the continuity of _f_ on any open set, which is guaranteed by the convexity of _f_, and the weak density of distributions with finite support in the family of all probability measures. **B.3** **Constrained optimization** We now define a general constrained optimization problem and the specific properties associated to convex constrained optimization problems. **420** **Appendix B** **Convex Optimization** **Definition B.21 (Constrained optimization problem)** _Let_ X _\u2286_ R _[N]_ _and f, g_ _i_ : X _\u2192_ R _, for all_ _i \u2208_ [ _m_ ] _. Then, a_ constrained optimization problem _has the form:_ min **x** _\u2208_ X _[f]_ [(] **[x]** [)] _subject to: g_ _i_ ( **x** ) _\u2264_ 0 _, \u2200i \u2208{_ 1 _, . . ., m}._ This general formulation does not make any convexity assumptions and can be augmented with equality constraints. It is referred to as the _primal problem_ in contrast with a related problem introduced later. We will denote by _p_ _[\u2217]_ the optimal value of the objective. For any **x** _\u2208_ X, we will denote by _g_ ( **x** ) the vector ( _g_ 1 ( **x** ) _, . . ., g_ _m_ ( **x** )) _[\u22a4]_ . Thus, the constraints can be written as _g_ ( **x** ) _\u2264_ **0** . To any constrained optimization problem, we can associate a _Lagrange_ _function_ that plays an important role in the analysis of the problem and its relationship with another related optimization problem. **Definition B.22 (Lagrangian)** _The_ Lagrange function _or the_ Lagrangian _associated to the general_ _constrained optimization problem defined in (B.21) is the function defined over_ X _\u00d7_ R + _by:_ _\u2200_ **x** _\u2208_ X _, \u2200_ _**\u03b1**_ _\u2265_ 0 _,_ _L_ ( **x** _,_ _**\u03b1**_ ) = _f_ ( **x** ) + _m_ \ufffd _\u03b1_ _i_ _g_ _i_ ( **x** ) _,_ _i_ =1 _where the variables \u03b1_ _i_ _are known as the_ Lagrange _or_ dual _variables with_ _**\u03b1**_ = ( _\u03b1_ 1 _, . . ., \u03b1_ _m_ ) _[\u22a4]_ _._ Any equality constraint of the form _g_ ( **x** ) = 0 for a function _g_ can be equivalently expressed by two inequalities: _\u2212g_ ( **x** ) _\u2264_ 0 and + _g_ ( **x** ) _\u2264_ 0. Let _\u03b1_ _\u2212_ _\u2265_ 0 be the Lagrange variable associated to the first constraint and _\u03b1_ + _\u2265_ 0 the one associated to the second constraint. The sum of the terms corresponding to these constraints in the definition of the Lagrange function can therefore be written as _\u03b1g_ ( **x** ) with _\u03b1_ = ( _\u03b1_ + _\u2212_ _\u03b1_ _\u2212_ ). Thus, in general, for an equality constraint _g_ ( **x** ) = 0 the Lagrangian is augmented with a term _\u03b1g_ ( **x** ) but with _\u03b1 \u2208_ R not constrained to be non-negative. Note that in the case of a convex optimization problem, equality constraints _g_ ( **x** ) are required to be affine since both _g_ ( **x** ) and _\u2212g_ ( **x** ) are required to be convex. **Definition B.23 (Dual function)** _The_ (Lagrange) dual function _associated to the constrained",
    "chunk_id": "foundations_machine_learning_405"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "op-_ _timization problem is defined by_ _\u2200_ _**\u03b1**_ _\u2265_ 0 _, F_ ( _**\u03b1**_ ) = inf **x** _\u2208_ X _[L]_ [(] **[x]** _[,]_ _**[ \u03b1]**_ [) = inf] **x** _\u2208_ X \ufffd _f_ ( **x** ) + _m_ \ufffd _\u03b1_ _i_ _g_ _i_ ( **x** )\ufffd _._ (B.5) _i_ =1 Note that _F_ is always concave, since the Lagrangian is linear with respect to _**\u03b1**_ and since the infimum preserves concavity. We further observe that _\u2200_ _**\u03b1**_ _\u2265_ 0 _,_ _F_ ( _**\u03b1**_ ) _\u2264_ _p_ _[\u2217]_ _,_ (B.6) since for any feasible **x**, _f_ ( **x** ) + [\ufffd] _[m]_ _i_ =1 _[\u03b1]_ _[i]_ _[g]_ _[i]_ [(] **[x]** [)] _[ \u2264]_ _[f]_ [(] **[x]** [). The dual function naturally leads to the] following optimization problem. **Definition B.24 (Dual problem)** _The_ dual (optimization) problem _associated to the constrained_ _optimization problem is_ max _**\u03b1**_ _F_ ( _**\u03b1**_ ) _subject to:_ _**\u03b1**_ _\u2265_ 0 _._ The dual problem is always a convex optimization problem (as a maximization of a concave problem). Let _d_ _[\u2217]_ denote an optimal value. By (B.6), the following inequality always holds: _d_ _[\u2217]_ _\u2264_ _p_ _[\u2217]_ ( _weak duality_ ) _._ The difference ( _p_ _[\u2217]_ _\u2212_ _d_ _[\u2217]_ ) is known as the _duality gap_ . The equality case _d_ _[\u2217]_ = _p_ _[\u2217]_ ( _strong duality_ ) does not hold in general. However, strong duality does hold when convex problems satisfy a _constraint qualification_ . We will denote by int(X) the interior of the set X. **B.3** **Constrained optimization** **421** **Definition B.25 (Strong constraint qualification)** _Assume that_ int(X) _\u0338_ = _\u2205. Then, the_ strong constraint qualification _or_ Slater\u2019s condition _is defined as_ _\u2203_ **x** _\u2208_ int(X): _g_ ( **x** ~~)~~ _<_ 0 _._ (B.7) A function _h_ : X _\u2192_ R is said to be _affine_ if it can be defined for all **x** _\u2208_ X by _h_ ( **x** ) = **w** _\u00b7_ **x** + _b_, for some **w** _\u2208_ R _[N]_ and _b \u2208_ R. **Definition B.26 (Weak constraint qualification)** _Assume that_ int(X) _\u0338_ = _\u2205. Then, the_ weak constraint qualification _or_ weak Slater\u2019s condition _is defined as_ _\u2203_ **x** _\u2208_ int(X): _\u2200i \u2208_ [ _m_ ] _,_ \ufffd _g_ _i_ ( **x** ~~)~~ _<_ 0\ufffd _\u2228_ \ufffd _g_ _i_ ( **x** ~~)~~ = 0 _\u2227_ _g_ _i_ _affine_ \ufffd _._ (B.8) We next present sufficient and necessary conditions for solutions to constrained optimization problems, based on the saddle point of the Lagrangian and Slater\u2019s condition. **Theorem B.27 (Saddle point \u2014 sufficient condition)** _Let P be a constrained optimization prob-_ _lem over_ X = R _[N]_ _. If_ ( **x** _[\u2217]_ _,_ _**\u03b1**_ _[\u2217]_ ) _is a saddle point of the associated Lagrangian, that is,_ _\u2200_ **x** _\u2208_ R _[N]_ _, \u2200_ _**\u03b1**_ _\u2265_ 0 _,_ _L_ ( **x** _[\u2217]_ _,_ _**\u03b1**_ ) _\u2264L_ ( **x** _[\u2217]_ _,_ _**\u03b1**_ _[\u2217]_ ) _\u2264L_ ( **x** _,_ _**\u03b1**_ _[\u2217]_ ) _,_ (B.9) _then_ **x** _[\u2217]_ _is a solution of the problem P_ _._ Proof: By the first inequality, the following holds: _\u2200_ _**\u03b1**_ _\u2265_ 0 _, L_",
    "chunk_id": "foundations_machine_learning_406"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "( **x** _[\u2217]_ _,_ _**\u03b1**_ ) _\u2264L_ ( **x** _[\u2217]_ _,_ _**\u03b1**_ _[\u2217]_ ) _\u21d2\u2200_ _**\u03b1**_ _\u2265_ 0 _,_ _**\u03b1**_ _\u00b7 g_ ( **x** _[\u2217]_ ) _\u2264_ _**\u03b1**_ _[\u2217]_ _\u00b7 g_ ( **x** _[\u2217]_ ) _\u21d2_ _g_ ( **x** _[\u2217]_ ) _\u2264_ 0 _\u2227_ _**\u03b1**_ _[\u2217]_ _\u00b7 g_ ( **x** _[\u2217]_ ) = 0 _,_ (B.10) where _g_ ( **x** _[\u2217]_ ) _\u2264_ 0 in (B.10) follows by letting _**\u03b1**_ _\u2192_ + _\u221e_ and _**\u03b1**_ _[\u2217]_ _\u00b7g_ ( **x** _[\u2217]_ ) = 0 follows by letting _**\u03b1**_ _\u2192_ 0. In view of (B.10), the second inequality in (B.9) gives, _\u2200_ **x** _, L_ ( **x** _[\u2217]_ _,_ _**\u03b1**_ _[\u2217]_ ) _\u2264L_ ( **x** _,_ _**\u03b1**_ _[\u2217]_ ) _\u21d2\u2200_ **x** _, f_ ( **x** _[\u2217]_ ) _\u2264_ _f_ ( **x** ) + _**\u03b1**_ _[\u2217]_ _\u00b7 g_ ( **x** ) _._ Thus, for all **x** satisfying the constraints, that is _g_ ( **x** ) _\u2264_ 0, we have _f_ ( **x** _[\u2217]_ ) _\u2264_ _f_ ( **x** ) _,_ which completes the proof. **Theorem B.28 (Saddle point \u2014 necessary condition)** _Assume that f and g_ _i_ _, i \u2208_ [ _m_ ] _, are_ convex functions _and that Slater\u2019s condition holds. Then, if_ **x** _is a solution of the constrained opti-_ _mization problem, there exists_ _**\u03b1**_ _\u2265_ 0 _such that_ ( **x** _,_ _**\u03b1**_ ) _is a saddle point of the Lagrangian._ **Theorem B.29 (Saddle point \u2014 necessary condition)** _Assume that f and g_ _i_ _, i \u2208_ [ _m_ ] _, are_ convex differentiable functions _and that the weak Slater\u2019s condition holds. If_ **x** _is a solution of the_ _constrained optimization problem, then there exists_ _**\u03b1**_ _\u2265_ 0 _such that_ ( **x** _,_ _**\u03b1**_ ) _is a saddle point of_ _the Lagrangian._ We conclude with a theorem providing necessary and sufficient optimality conditions when the problem is convex, the objective function differentiable, and the constraints qualified. **Theorem B.30 (Karush-Kuhn-Tucker\u2019s theorem)** _Assume that f, g_ _i_ : X _\u2192_ R _, \u2200i \u2208_ [ _m_ ] _are con-_ _vex and differentiable and that the constraints are qualified. Then_ **x** _is a solution of the constrained_ _program if and if only there exists_ _**\u03b1**_ _\u2265_ 0 _such that,_ _\u2207_ **x** _L_ ( **x** _,_ _**\u03b1**_ ~~)~~ = _\u2207_ **x** _f_ ( **x** ~~)~~ + _**\u03b1**_ _\u00b7 \u2207_ **x** _g_ ( **x** ~~)~~ = 0 (B.11) _\u2207_ _**\u03b1**_ _L_ ( **x** _,_ _**\u03b1**_ ~~)~~ = _g_ ( **x** ~~)~~ _\u2264_ 0 (B.12) _**\u03b1**_ _\u00b7 g_ ( **x** ~~)~~ = _m_ \ufffd _i_ =1 _**\u03b1**_ _i_ _g_ _i_ ( **x** ~~)~~ = 0 _._ (B.13) _The conditions B.11\u2013B.13 are known as the_ KKT conditions _. Note that the last two KKT con-_ _ditions are equivalent to_ _g_ ( **x** ~~)~~ _\u2264_ 0 _\u2227_ ( _\u2200i \u2208{_ 1 _, . . ., m},_ \u00af _\u03b1_ _i_ _g_ _i_ ( **x** ~~)~~ = 0) _._ (B.14) _These equalities are known as_ complementarity conditions _._ **422** **Appendix B** **Convex Optimization** Proof: For the forward direction, since the constraints are qualified, if **x** is a solution, then there exists _**\u03b1**_ such",
    "chunk_id": "foundations_machine_learning_407"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "that the ( **x** _,_ _**\u03b1**_ ~~)~~ is a saddle point of the Lagrangian and all three conditions are satisfied (the first condition follows by definition of a saddle point, and the second two conditions follow from (B.10)). In the opposite direction, if the conditions are met, then for any **x** such that _g_ ( **x** ) _\u2264_ 0, we can write _f_ ( **x** ) _\u2212_ _f_ ( **x** ~~)~~ _\u2265\u2207_ **x** _f_ ( **x** ~~)~~ _\u00b7_ ( **x** _\u2212_ **x** ~~)~~ (convexity of _f_ ) = _\u2212_ _\u2265\u2212_ = _\u2212_ _m_ \ufffd _i_ =1 _m_ \ufffd _i_ =1 _m_ \ufffd _i_ =1 _**\u03b1**_ _i_ _\u2207_ **x** _g_ _i_ ( **x** ~~)~~ _\u00b7_ ( **x** _\u2212_ **x** ~~)~~ (first condition) _**\u03b1**_ _i_ [ _g_ _i_ ( **x** ) _\u2212_ _g_ _i_ ( **x** ~~)~~ ] (convexity of _g_ _i_ s) _**\u03b1**_ _i_ _g_ _i_ ( **x** ) _\u2265_ 0 _,_ (third condition) which shows that _f_ ( **x** ~~)~~ is the minimum of _f_ over the set of points satisfying the constraints. **B.4** **Fenchel duality** In this section, we present an alternative theory of convex optimization or convex analysis where the functions _f_ considered may be non-differentiable and take infinite values. Throughout, this section, the set X denotes a Hilbert space with the inner product denoted by _\u27e8\u00b7, \u00b7\u27e9_ . However, the results presented can be straightforwardly extended to the case of a Banach space. We consider functions taking values in [ _\u2212\u221e,_ + _\u221e_ ]. The _domain of a function f_ : X _\u2192_ [ _\u2212\u221e,_ + _\u221e_ ] is defined as the set dom( _f_ ) = _{x \u2208_ X : _f_ ( _x_ ) _<_ + _\u221e}._ (B.15) We extend the definition of convexity and say that _f_ : X _\u2192_ [ _\u2212\u221e,_ + _\u221e_ ] is convex if it is convex over dom( _f_ ), that is if for all _x, x_ _[\u2032]_ _\u2208_ dom( _f_ ) and all _t \u2208_ [0 _,_ 1], _f_ ( _tx_ + (1 _\u2212_ _t_ ) _x_ _[\u2032]_ ) _\u2264_ _tu_ + (1 _\u2212_ _t_ ) _v,_ (B.16) for all ( _u, v_ ) _\u2208_ R [2] with _u \u2265_ _f_ ( _x_ ) and _v \u2265_ _f_ ( _x_ _[\u2032]_ ). A convex function is said to be _proper_ if it takes values in ( _\u2212\u221e,_ + _\u221e_ ] and if it is not uniformly equal to + _\u221e_ . It is said to be _closed_ when its epigraph is closed. **B.4.1** **Subgradients** **Definition B.31** _Let f_ : X _\u2192_ ( _\u2212\u221e,_ + _\u221e_ ] _be a convex function. Then, a vector g \u2208_ X _is a_ subgradient _of f at a point x \u2208_ dom( _f_ ) _if the following inequality holds for all z \u2208_ X _:_ _f_ ( _z_ ) _\u2265_ _f_ ( _x_ ) + _\u27e8z \u2212_ _x, g\u27e9._ (B.17) _The set of all subgradients at x is called the_ subdifferential of _f_ at _x and is denoted by \u2202f_ ( _x_ ) _with_ _\u2202f_ ( _x_ ) = _\u2205_ _for x \u0338\u2208_ dom( _f_ )",
    "chunk_id": "foundations_machine_learning_408"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_._ Thus, _g_ is a subgradient at _x_ iff the hyperplane with normal vector _g_ passing through the point ( _x, f_ ( _x_ )) is below the graph of _f_, that is iff it is supporting the graph of _f_ . Figure 14.1 illustrates these definitions. The following lemma shows that if _f_ is differentiable at _x \u2208_ dom( _f_ ), then its subdifferential is reduced to its gradient at _x_ . **Lemma B.32** _If f is differentiable at x \u2208_ dom( _f_ ) _, then \u2202f_ ( _x_ ) = _{\u2207f_ ( _x_ ) _}._ **B.4** **Fenchel duality** **423** Proof: Clearly, the gradient _\u2207f_ ( _x_ ) is always a subgradient at _x_ . Now, let _g_ be in _\u2202f_ ( _x_ ). Then, by definition of a subgradient, for any _\u03f5 \u2208_ R, _f_ ( _x_ + _\u03f5_ ( _\u2207f_ ( _x_ ) _\u2212_ _g_ )) _\u2265_ _f_ ( _x_ ) + _\u03f5\u27e8\u2207f_ ( _x_ ) _\u2212_ _g, g\u27e9._ A first-order Taylor series expansion gives _f_ ( _x_ + _\u03f5_ ( _\u2207f_ ( _x_ ) _\u2212_ _g_ )) _\u2212_ _f_ ( _x_ ) = _\u03f5\u27e8\u2207f_ ( _x_ ) _, \u2207f_ ( _x_ ) _\u2212_ _g\u27e9_ + _o_ ( _\u03f5\u2225\u2207f_ ( _x_ ) _\u2212_ _g\u2225_ ) _._ In view of that, the first inequality can be rewritten as _\u03f5\u2225\u2207f_ ( _x_ ) _\u2212_ _g\u2225_ [2] _\u2264_ _o_ ( _\u03f5\u2225\u2207f_ ( _x_ ) _\u2212_ _g\u2225_ ) _,_ which implies _\u2225\u2207f_ ( _x_ ) _\u2212_ _g\u2225_ = _o_ (1) and _\u2207f_ ( _x_ ) = _g_ . **Proposition B.33** _Let f_ : X _\u2192_ ( _\u2212\u221e,_ + _\u221e_ ] _be a proper function. Then, x_ _[\u2217]_ _is a global minimizer_ _of f iff \u2202f_ ( _x_ _[\u2217]_ ) _contains_ 0 _._ Proof: Since _f_ is proper, if _x_ _[\u2217]_ is a minimizer, _f_ ( _x_ _[\u2217]_ ) cannot be + _\u221e_ . Thus _x_ _[\u2217]_ must be in dom( _f_ ) (and thus _\u2202f_ ( _x_ ) is not defined to be empty). Now, _x_ _[\u2217]_ is a global minimizer iff for all _z \u2208_ X, _f_ ( _z_ ) _\u2265_ _f_ ( _x_ _[\u2217]_ ), that is iff 0 is a subgradient of _f_ at _x_ _[\u2217]_ . **B.4.2** **Core** The _core_ of a set C _\u2286_ X is denoted by core(C) and defined as follows: core(C) = _{x \u2208_ C : _\u2200u \u2208_ X _, \u2203\u03f5 >_ 0 _| \u2200t \u2208_ [0 _, \u03f5_ ] _,_ ( _x_ + _tu_ ) _\u2208_ C _}._ (B.18) Thus, for _x \u2208_ core(C), for any direction _u_, ( _x_ + _tu_ ) is in C for _t_ sufficiently small. In view of this definition, core(C) clearly contains the interior of C, int(C). **Proposition B.34** _Let h_ : X _\u2192_ [ _\u2212\u221e,_ + _\u221e_ ] _be a convex function. If there exists x_ 0 _\u2208_ core(dom( _h_ )) _such that h_ ( _x_ 0 ) _> \u2212\u221e, then h_ ( _x_ ) _> \u2212\u221e_ _for all x \u2208_ X _._ Proof: Let _x_ be in X. Since _x_ 0 is in core(dom( _h_ )),",
    "chunk_id": "foundations_machine_learning_409"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "there exists _t >_ 0 such that _x_ _[\u2032]_ 0 [=] _[ x]_ [0] [ +] _[t]_ [(] _[x]_ [0] _[ \u2212]_ _[x]_ [)] is in dom( _h_ ), that is such that _h_ ( _x_ _[\u2032]_ 0 [)] _[ <]_ [ +] _[\u221e]_ [. Since] _[ x]_ [0] [ =] 1+1 _t_ _[x]_ 0 _[\u2032]_ [+] 1+ _t_ _t_ _[x]_ [, by convexity, the following] holds: 1 _t_ 1 _h_ ( _x_ 0 ) _\u2264_ 0 [) +] _h_ ( _x_ 0 ) _\u2212_ 0 [)] (B.19) 1 + _t_ _[h]_ [(] _[x]_ _[\u2032]_ 1 + _t_ _[v][ \u21d0\u21d2]_ _[v][ \u2265]_ [1 +] _t_ _[ t]_ \ufffd 1 + _t_ _[h]_ [(] _[x]_ _[\u2032]_ \ufffd _t_ 1 _h_ ( _x_ 0 ) _\u2212_ 0 [)] (B.19) \ufffd 1 + _t_ _[h]_ [(] _[x]_ _[\u2032]_ \ufffd for all _v \u2265_ _h_ ( _x_ ). This implies _h_ ( _x_ ) _\u2265_ [1+] _[t]_ _[t]_ 1 _t_ [[] _[h]_ [(] _[x]_ [0] [)] _[ \u2212]_ 1+ _t_ _[h]_ [(] _[x]_ 0 _[\u2032]_ [)]] _[ >][ \u2212\u221e]_ [, which concludes the proof.][ \u25a1] The proof of the following result is left as an exercise (Exercise B.3). **Proposition B.35** _Let h_ : X _\u2192_ ( _\u2212\u221e,_ + _\u221e_ ] _be a convex function. Then, h admits a subdifferential_ _at any x \u2208_ core(dom( _h_ )) _._ **B.4.3** **Conjugate functions** **Definition B.36 (Conjugate functions)** _Let_ X _be a Hilbert space._ _The_ conjugate function _or_ Fenchel conjugate _of a function f_ : X _\ufffd\u2192_ [ _\u2212\u221e,_ + _\u221e_ ] _is the function f_ _[\u2217]_ : X _\ufffd\u2192_ [ _\u2212\u221e,_ + _\u221e_ ] _defined by_ _f_ _[\u2217]_ ( _u_ ) = sup _x\u2208_ X \ufffd _\u27e8u, x\u27e9\u2212_ _f_ ( _x_ )\ufffd _._ (B.20) Note that _f_ _[\u2217]_ is convex as the pointwise supremum of the set of affine and thus convex functions _u \ufffd\u2192\u27e8x, u\u27e9\u2212_ _f_ ( _x_ ). Also, if there exists _x_ such that _f_ ( _x_ ) _<_ + _\u221e_, then _f > \u2212\u221e_ . Conjugation is order-reversing: for any _f_ and _g_, if _f \u2264_ _g_, then _g_ _[\u2217]_ _\u2264_ _f_ _[\u2217]_ . Also, it is straightforward to see that if _f_ is closed proper convex, then _f_ _[\u2217\u2217]_ = _f_ . Figure B.3 illustrates the definition of conjugate functions. As shown by the figure, conjugate functions correspond to a dual description of the epigraph of a function in terms of supporting hyperplanes and their crossing points. **424** **Appendix B** **Convex Optimization** **Figure B.3** **Image:** [No caption returned] Illustration of the conjugate _f_ _[\u2217]_ of a function _f_ . Given _y_, _x_ _[\u2217]_ is the point at which the distance between the hyperplane of equation _z_ = _\u27e8x, y\u27e9_ with normal _y_ (slope _y_ in dimension one) and the plot of _f_ ( _x_ ) is the largest. This largest distance is equal to _f_ _[\u2217]_ ( _y_ ). The parallel hyperplane _z_ = _\u27e8x \u2212_ _x_ _[\u2217]_ _, y\u27e9_ + _f_ ( _x_ _[\u2217]_ ) with normal _y_ and passing through the point ( _x_ _[\u2217]_ _, f_ ( _x_ _[\u2217]_ ) is",
    "chunk_id": "foundations_machine_learning_410"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "shown. This is a supporting hyperplane of the plot of _f_ ( _x_ ). The point at which it intercepts the _y_ -axis (crossing point) has _y_ -coordinate _\u2212f_ _[\u2217]_ ( _y_ ). **Lemma B.37 (Conjugate of extended relative entropy)** _Let p_ 0 _\u2208_ \u2206 _be a distribution over_ X _such that p_ 0 ( _x_ ) _>_ 0 _for all x \u2208_ X _. Define f_ : R [X] _\u2192_ R _by_ _f_ ( _p_ ) = D( _p\u2225p_ 0 ) _if p \u2208_ \u2206 + _\u221e_ _otherwise._ \ufffd _Then, the conjugate function f_ _[\u2217]_ : R [X] _\u2192_ R _of f is defined by_ _\u2200q \u2208_ R [X] _, f_ _[\u2217]_ ( _q_ ) = log _p_ 0 ( _x_ ) _e_ _[q]_ [(] _[x]_ [)] _._ \ufffd\ufffd _x\u2208_ X \ufffd Proof: By definition of _f_, for any _q \u2208_ R [X], we can write \ufffd _\u27e8p, q\u27e9\u2212_ D( _p\u2225p_ 0 )\ufffd _._ (B.21) sup _p\u2208_ R [X] \ufffd _\u27e8p, q\u27e9\u2212_ D( _p\u2225p_ 0 )\ufffd = sup _p\u2208_ \u2206 Fix _q \u2208_ R [X] and let \u00af _q \u2208_ \u2206be defined for all _x \u2208_ X by \u00af _p_ 0 ( _x_ ) _e_ _[q]_ [(] _[x]_ [)] _q_ ( _x_ ) = _._ (B.22) \ufffd _x\u2208_ X _[p]_ [0] [(] _[x]_ [)] _[e]_ _[q]_ [(] _[x]_ [)] [ =] _[ p]_ [0] E [(] _p_ _[x]_ 0 [)] [ _[e]_ _e_ _[q][q]_ [(] ] _[x]_ [)] Then, the following holds for all _p \u2208_ \u2206: = E \ufffd _p_ log _[p]_ [0] _[e]_ _[q]_ \ufffd _p_ = _\u2212_ D( _p\u2225q_ \u00af) + log E \ufffd _p_ 0 [[] _[e]_ _[q]_ []] _[.]_ _\u27e8p, q\u27e9\u2212_ D( _p\u2225p_ 0 ) = E _p_ [[log(] _[e]_ _[q]_ [)]] _[ \u2212]_ [E] _p_ log _[p]_ \ufffd _p_ 0 Since D( _p\u2225q_ \u00af) _\u2265_ 0 and D( _p\u2225q_ \u00af) = 0 for _p_ = \u00af _q_, this shows that sup _p\u2208_ \u2206 \ufffd _p\u00b7q\u2212_ D( _p\u2225p_ 0 )\ufffd = log \ufffd E _p_ 0 [ _e_ _[q]_ ]\ufffd and concludes the proof. Table B.1 gives a series of other examples of functions and their conjugates. The following is an immediate consequence of the definition of the conjugate functions. **B.4** **Fenchel duality** **425** **Table B.1** Examples of functions _g_ and their conjugates _g_ _[\u2217]_ . _**g**_ **(** _**x**_ **)** **dom(** _**g**_ **)** _**g**_ _**[\u2217]**_ **(** _**y**_ **)** **dom(** _**g**_ _**[\u2217]**_ **)** _f_ ( _ax_ ) ( _a \u0338_ = 0) X _f_ _[\u2217]_ [\ufffd] _a_ _[y]_ \ufffd X _[\u2217]_ _f_ ( _x_ + _b_ ) X _f_ _[\u2217]_ ( _y_ ) _\u2212\u27e8b, y\u27e9_ X _[\u2217]_ _af_ ( _x_ ) ( _a >_ 0) X _af_ _[\u2217]_ [\ufffd] _a_ _[y]_ \ufffd X _[\u2217]_ _\u03b1x_ + _\u03b2_ R _\u2212\u03b2_ if _y_ = _\u03b1_ R + _\u221e_ otherwise \ufffd _|x|_ _[p]_ _p|_ _[p]_ ( _p >_ 1) R _|yq|_ _[q]_ _q_ ( _p_ [1] _p_ [1] [+] [1] _q_ _q_ [= 1)] R _px_ _[p]_ (0 _< p <_ 1) R + _\u2212_ ( _\u2212qy_ ) _[q]_ _p_ [1] [+] [1] _q_ _\u2212x_ _[p]_ _qy_ ( _p_ [1]",
    "chunk_id": "foundations_machine_learning_411"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "(0 _< p <_ 1) R + _\u2212_ _\u2212qy_ ( _p_ [+] _q_ [= 1)] R _\u2212_ _\u221a_ 1 + _x_ [2] R _\u2212_ ~~\ufffd~~ 1 _\u2212_ ( _y_ ) [2] [ _\u2212_ 1 1 + _x_ [2] R _\u2212_ ~~\ufffd~~ 1 _\u2212_ ( _y_ ) [2] [ _\u2212_ 1 _,_ 1] _\u2212_ log( _x_ ) R + _/ {_ 0 _}_ _\u2212_ (1 + log( _\u2212y_ )) R _\u2212_ _/ {_ 0 _}_ _e_ _[x]_ R log (1 + _e_ _[x]_ ) R _\u2212_ log (1 _\u2212_ _e_ _[x]_ ) R _y_ log( _y_ ) _\u2212_ _y,_ if _y >_ 0 R + \ufffd0 _,_ if _y_ = 0 _y_ log( _y_ ) + (1 _\u2212_ _y_ ) log(1 _\u2212_ _y_ ) _,_ if 0 _< y <_ 1 [0 _,_ 1] \ufffd0 _,_ if _y_ = 0 _,_ 1 _y_ log( _y_ ) _\u2212_ (1 + _y_ ) log(1 + _y_ ) _,_ if _y >_ 0 R + \ufffd0 _,_ if _y_ = 0 **Proposition B.38 (Fenchel\u2019s inequality)** _Let_ X _be a Hilbert space. For any function f_ : X _\ufffd\u2192_ [ _\u2212\u221e,_ + _\u221e_ ] _and any x \u2208_ dom( _f_ ) _and u \u2208_ X _, the following inequality holds:_ _f_ ( _x_ ) + _f_ _[\u2217]_ ( _u_ ) _\u2265\u27e8u, x\u27e9_ _._ (B.23) Equality holds iff _u_ is a subgradient of _f_ at _x_ . We will denote by _A_ _[\u2217]_ the adjoint operator of a bounded (or continuous) linear map _A_ : X _\u2192_ Y. Also, we denote by cont( _f_ ) the set of points _x \u2208_ X at which _f_ : X _\u2192_ [ _\u2212\u221e,_ + _\u221e_ ] is finite and continuous. **Theorem B.39 (Fenchel duality theorem)** _Let_ X _and_ Y _be two Hilbert spaces, f_ : X _\u2192_ ( _\u2212\u221e,_ + _\u221e_ ] _and g_ : Y _\u2192_ ( _\u2212\u221e,_ + _\u221e_ ] _two convex functions and A_ : X _\u2192_ Y _a bounded linear map. Then, the_ _following two optimization problems (_ Fenchel problems _)_ _p_ _[\u2217]_ = inf _x\u2208_ X _[{][f]_ [(] _[x]_ [) +] _[ g]_ [(] _[Ax]_ [)] _[}]_ _d_ _[\u2217]_ = sup _{\u2212f_ _[\u2217]_ ( _A_ _[\u2217]_ _y_ ) _\u2212_ _g_ _[\u2217]_ ( _\u2212y_ ) _}_ _y\u2208_ Y _satisfy the_ weak duality _p_ _[\u2217]_ _\u2265_ _d_ _[\u2217]_ _. If further f and g satisfy the condition_ 0 _\u2208_ core \ufffd dom( _g_ ) _\u2212_ _A_ (dom( _f_ ))\ufffd _,_ _or the stronger condition_ _A_ (dom( _f_ )) _\u2229_ cont( _g_ ) _\u0338_ = _\u2205,_ _then_ strong duality _holds, that is p_ _[\u2217]_ = _d_ _[\u2217]_ _and the supremum in the dual problem is attained if_ _d_ _[\u2217]_ _\u2208_ R _._ **426** **Appendix B** **Convex Optimization** Proof: By Fenchel\u2019s inequality (proposition B.38) applied to both _f_ and _g_, for any _x \u2208_ X and _y \u2208_ Y, the following inequalities hold: _f_ ( _x_ ) + _f_ _[\u2217]_ ( _A_ _[\u2217]_ _y_ ) _\u2265\u27e8A_ _[\u2217]_ _y, x\u27e9_ = _\u27e8y, Ax\u27e9_ = _\u2212\u27e8\u2212y, Ax\u27e9\u2265\u2212g_ ( _Ax_ ) _\u2212_ _g_ _[\u2217]_ ( _\u2212y_ ) _._ Comparing the leftmost and the",
    "chunk_id": "foundations_machine_learning_412"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "rightmost terms gives _f_ ( _x_ ) + _f_ _[\u2217]_ ( _A_ _[\u2217]_ _y_ ) _\u2265\u2212g_ ( _Ax_ ) _\u2212_ _g_ _[\u2217]_ ( _\u2212y_ ) _\u21d0\u21d2_ _f_ ( _x_ ) + _g_ ( _Ax_ ) _\u2265\u2212f_ _[\u2217]_ ( _A_ _[\u2217]_ _y_ ) _\u2212_ _g_ _[\u2217]_ ( _\u2212y_ ) _._ Taking the infimum over _x \u2208_ X of the left-hand side and the supremum over _y \u2208_ Y of the right-hand side of the last inequality yields _p_ _[\u2217]_ _\u2265_ _d_ _[\u2217]_ . Consider now the function _h_ : Y _\u2192_ [ _\u2212\u221e,_ + _\u221e_ ] defined for all _u \u2208_ Y by _h_ ( _u_ ) = inf (B.24) _x\u2208_ X _[{][f]_ [(] _[x]_ [) +] _[ g]_ [(] _[Ax]_ [ +] _[ u]_ [)] _[}][.]_ Since ( _x, u_ ) _\ufffd\u2192_ _f_ ( _x_ ) + _g_ ( _Ax_ + _u_ ) is convex, _h_ is convex as the infimum over one argument of that function. _u_ is in dom( _h_ ) iff there exists _x \u2208_ X such that _f_ ( _x_ ) + _g_ ( _Ax_ + _u_ ) _<_ + _\u221e_, that is iff there exists _x \u2208_ X such that _f_ ( _x_ ) _<_ + _\u221e_ and _g_ ( _Ax_ + _u_ ) _<_ + _\u221e_, that is iff there exists _x \u2208_ dom( _f_ ) such that ( _Ax_ + _u_ ) _\u2208_ dom( _g_ ). Thus, we have dom( _h_ ) = dom( _g_ ) _\u2212_ _A_ dom( _f_ ). If _p_ _[\u2217]_ = _\u2212\u221e_, then strong duality clearly holds. Otherwise, _p_ _[\u2217]_ _> \u2212\u221e_ . If 0 _\u2208_ core \ufffd dom( _g_ ) _\u2212_ _A_ (dom( _f_ ))\ufffd = core(dom( _h_ )), then 0 is in dom( _h_ ) and _p_ _[\u2217]_ _<_ + _\u221e_ . Thus, _p_ _[\u2217]_ = _h_ (0) is in R. By proposition B.34, since _h_ (0) _> \u2212\u221e_ and 0 _\u2208_ core(dom( _h_ )), _h_ takes values in ( _\u2212\u221e,_ + _\u221e_ ]. Thus, by proposition B.35, _h_ admits a subgradient _\u2212y_ at 0. By definition of _y_, for all _x \u2208_ X and _u \u2208_ Y, _h_ (0) _\u2264_ _h_ ( _u_ ) + _\u27e8y, u\u27e9_ _\u2264_ _f_ ( _x_ ) + _g_ ( _Ax_ + _u_ ) + _\u27e8y, u\u27e9_ = _{f_ ( _x_ ) _\u2212\u27e8A_ _[\u2217]_ _y, x\u27e9}_ + _{g_ ( _Ax_ + _u_ ) + _\u27e8y, u\u27e9_ + _\u27e8A_ _[\u2217]_ _y, x\u27e9}_ = _{f_ ( _x_ ) _\u2212\u27e8A_ _[\u2217]_ _y, x\u27e9}_ + _{g_ ( _Ax_ + _u_ ) + _\u27e8y, Ax_ + _u\u27e9}._ Taking the infimum over _u_ and the supremum over _x_ yields _h_ (0) _\u2264\u2212f_ _[\u2217]_ ( _A_ _[\u2217]_ _y_ ) _\u2212_ _g_ _[\u2217]_ ( _\u2212y_ ) _\u2264_ _d_ _[\u2217]_ _\u2264_ _p_ _[\u2217]_ = _h_ (0) _,_ which proves _d_ _[\u2217]_ = _p_ _[\u2217]_ and that the supremum defining _d_ _[\u2217]_ is reached at _y_ . Finally, assume that _A_ (dom( _f_ )) _\u2229_ cont( _g_ ) _\u0338_ = _\u2205_ and let _u \u2208_ _A_ (dom( _f_ )) _\u2229_ cont( _g_ ). Then, _u_ = _Ax_ with _x \u2208_ dom( _f_ )",
    "chunk_id": "foundations_machine_learning_413"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "and _u \u2208_ cont( _g_ ) _\u2286_ dom( _g_ ). Thus, we have 0 = _u \u2212_ _Ax \u2208_ dom( _g_ ) _\u2212_ _A_ dom( _f_ ). Since _g_ is continuous at _u_ and _g_ ( _u_ ) is finite, for any _v \u2208_ X, there exists _\u03f5 >_ 0 such that _g_ ( _u_ + _tv_ ) is finite for all _t \u2208_ [0 _, \u03f5_ ], thus _w_ _t_ = ( _u_ + _tv_ ) _\u2208_ dom( _g_ ). Therefore, for any _t \u2208_ [0 _, \u03f5_ ], _tv_ = _w_ _t_ _\u2212_ _u_ = _w_ _t_ _\u2212_ _Ax \u2208_ dom( _g_ ) _\u2212_ _A_ dom( _f_ ), which shows that 0 _\u2208_ core \ufffd dom( _g_ ) _\u2212_ _A_ (dom( _f_ ))\ufffd. To illustrate the theorem, consider the case where _A_ is the identity operator. The primal optimization problem is then min _x_ \ufffd _f_ ( _x_ ) + _g_ ( _x_ )\ufffd. Figure B.4 illustrates the Fenchel duality theorem in that case. The primal problem consists of finding the point _x_ _[\u2217]_ at which the distance between the plots of _f_ ( _x_ ) and _\u2212g_ ( _x_ ) is minimal since _f_ ( _x_ ) + _g_ ( _x_ ) = _f_ ( _x_ ) _\u2212_ ( _\u2212g_ ( _x_ )). As shown by the figure, under the conditions of the theorem, this coincides with seeking _y_ _[\u2217]_ at which the difference of the conjugate values of _f_ ( _x_ ) and _\u2212g_ ( _x_ ), that is the difference _\u2212f_ _[\u2217]_ ( _y_ ) _\u2212_ _g_ _[\u2217]_ ( _\u2212y_ ) is maximal. **B.5** **Chapter notes** The results presented in this appendix are based on several key theorems: theorem B.3 due to Fermat (1629); theorem B.27 due to Lagrange (1797), theorem B.30 due to Karush [1939] and Kuhn and Tucker [1951], and theorem B.39 due to Werner Fenchel, based on the notion of conjugate functions or Legendre transformations. For a more extensive material on convex optimization, we strongly recommend the books of Boyd and Vandenberghe [2004], Bertsekas, Nedi\u00b4c, and Ozdaglar [2003], Rockafellar [1997], Borwein and Lewis [2000] and Borwein and Zhu [2005] which have formed the basis for part of the material presented in this appendix. In particular, our table of conjugate functions is extracted from [Borwein and Lewis, 2000]. **B.6** **Exercises** **427** **Figure B.4** |Col1|Col2|Col3|Col4|Col5|Col6| |---|---|---|---|---|---| |||(x)<br>g(x)||norm|al y\u2217| ||f<br>\u2212|(x)<br>g(x)|(x)<br>g(x)|(x)<br>g(x)|(x)<br>g(x)| ||\u2212f \u2217(|y)||norm|al y| ||\u2212f \u2217(y)\u2212<br>g\u2217(\u2212|g\u2217(\u2212y)<br>y)|||| |\u2212f|\u2217(y\u2217)\u2212g|\u2217(\u2212y\u2217)|||| |||(0, 0)|x\u2217||x| ||||||| Illustration of Fenchel Duality; min _x_ \ufffd _f_ ( _x_ ) + _g_ ( _x_ )\ufffd = max _y_ \ufffd _\u2212_ _f_ _[\u2217]_ ( _y_ ) _\u2212_ _g_ _[\u2217]_ ( _\u2212y_ )\ufffd. **B.6** **Exercises** B.1 Give the conjugate of the function _f_ defined by _f_ ( _x_ ) = _|x|_ for all _x \u2208_ R. B.2 Prove the correctness of the conjugate function _g_ _[\u2217]_ for each function _g_ of Table B.1. B.3 Give the proof of Proposition B.35. # C Probability Review In this appendix, we give a brief review of some basic notions of probability and will also define the notation that is used throughout the",
    "chunk_id": "foundations_machine_learning_414"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "textbook. **C.1** **Probability** A _probability space_ is a tuple consisting of three components: a _sample space_, an _events set_, and a _probability distribution_ : _\u2022_ _sample space_ \u2126: \u2126is the set of all elementary events or outcomes possible in a trial, for example, each of the six outcomes in _{_ 1 _, . . .,_ 6 _}_ when casting a die. _\u2022_ _events set F_ : _F_ is a _\u03c3-algebra_, that is a set of subsets of \u2126containing \u2126that is closed under complementation and countable union (therefore also countable intersection). An example of an event may be \u201cthe die lands on an odd number\u201d. _\u2022_ _probability distribution_ : P is a mapping from the set of all events _F_ to [0 _,_ 1] such that P[\u2126] = 1, P[ _\u2205_ ] = 1, and, for all mutually exclusive events _A_ 1 _, . . ., A_ _n_, P[ _A_ 1 _\u222a_ _. . . \u222a_ _A_ _n_ ] = _n_ \ufffd P[ _A_ _i_ ] _._ _i_ =1 The discrete probability distribution associated with a fair die can be defined by P[ _A_ _i_ ] = 1 _/_ 6 for _i \u2208{_ 1 _. . ._ 6 _}_, where _A_ _i_ is the event that the die lands on value _i_ . **C.2** **Random variables** **Definition C.1 (Random variables)** _A_ random variable _X is a function X_ : \u2126 _\u2192_ R _that is_ measurable _, that is such that for any interval I, the subset of the sample space {\u03c9 \u2208_ \u2126: _X_ ( _\u03c9_ ) _\u2208_ _I} is_ _an event._ The _probability mass function_ of a discrete random variable _X_ is defined as the function _x \ufffd\u2192_ P[ _X_ = _x_ ]. The _joint probability mass function_ of discrete random variables _X_ and _Y_ is defined as the function ( _x, y_ ) _\ufffd\u2192_ P[ _X_ = _x \u2227_ _Y_ = _y_ ]. A probability distribution is said to be _absolutely continuous_ when it admits a _probability density_ _function_, that is a function _f_ associated to a real-valued random variable _X_ that satisfies for all _a, b \u2208_ R _b_ P[ _a \u2264_ _X \u2264_ _b_ ] = _f_ ( _x_ ) _dx ._ (C.1) \ufffd _a_ **430** **Appendix C** **Probability Review** 0.15 0.1 0.05 0 **Image:** [No caption returned] 0 10 20 30 **Figure C.1** Approximation of the binomial distribution (in red) by a normal distribution (in blue). **Definition C.2 (Binomial distribution)** _A random variable X is said to follow a_ binomial distribution _B_ ( _n, p_ ) _with n \u2208_ N _and p \u2208_ [0 _,_ 1] _if for any k \u2208{_ 0 _,_ 1 _, . . ., n},_ _n_ P[ _X_ = _k_ ] = \ufffd _k_ \ufffd _p_ _[k]_ (1 _\u2212_ _p_ ) _[n][\u2212][k]_ _._ **Definition C.3 (Normal distribution)** _A random variable X is said to follow a_ normal _(or_ Gaussian _)_ distribution _N_ ( _\u00b5, \u03c3_ [2] ) _with \u00b5 \u2208_ R _and \u03c3 >_ 0 _if its probability density function is given by,_ 1 _\u2212_ [(] _[x][ \u2212]_ _[\u00b5]_ [)] [2] 2 _\u03c0\u03c3_ [2]",
    "chunk_id": "foundations_machine_learning_415"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[ exp] \ufffd 2 _\u03c3_ [2] _._ \ufffd 1 _f_ ( _x_ ) = _\u221a_ 2 2 _\u03c3_ [2] _The_ standard normal _distribution N_ (0 _,_ 1) _is the normal distribution with zero mean and unit_ _variance._ The normal distribution is often used to approximate a binomial distribution. Figure C.1 illustrates that approximation. **Definition C.4 (Laplace distribution)** _A random variable X is said to follow a_ Laplace distribution _with location parameter \u00b5 \u2208_ R _and scale parameter b >_ 0 _if its probability density function_ _is given by,_ [1] _\u2212_ _[|][x][ \u2212]_ _[\u00b5][|]_ 2 _b_ [exp] \ufffd _b_ _f_ ( _x_ ) = [1] _b_ _._ \ufffd **Definition C.5 (Gibbs distributions)** _Given a set_ X _and_ feature function \u03a6: X _\u2192_ R _[N]_ _, a random_ _variable X is said to follow a_ Gibbs distribution _with parameter_ **w** _\u2208_ R _[N]_ _if for any x \u2208_ X _,_ exp( **w** _\u00b7_ \u03a6( _x_ )) P[ _X_ = _x_ ] = \ufffd _x\u2208_ X [exp(] **[w]** _[ \u00b7]_ [ \u03a6(] _[x]_ [))] _[ .]_ _The normalizing quantity in the denominator Z_ = [\ufffd] _x\u2208_ X [exp(] **[w]** _[\u00b7]_ [\u03a6(] _[x]_ [))] _[ is also called the]_ [ partition] function _._ **Definition C.6 (Poisson distribution)** _A random variable X is said to follow a_ Poisson distribution _with \u03bb >_ 0 _if for any k \u2208_ N _,_ P[ _X_ = _k_ ] = _[\u03bb]_ _[k]_ _[e]_ _[\u2212][\u03bb]_ _._ _k_ ! The definition of the following family of distributions uses the notion of independence of random variables defined in the next section. **Definition C.7 (** _\u03c7_ [2] **distribution)** _The \u03c7_ [2] distribution _(or_ chi-squared distribution _) with k_ degrees of freedom _is the distribution of the sum of the squares of k independent random variables, each_ _following a standard normal distribution._ **C.3** **Conditional probability and independence** **431** **C.3** **Conditional probability and independence** **Definition C.8 (Conditional probability)** _The_ conditional probability _of event A given event B_ _is defined by_ P[ _A | B_ ] = [P][[] _[A][ \u2229]_ _[B]_ []] _when_ P[ _B_ ] _\u0338_ = 0 _._ _,_ (C.2) P[ _B_ ] **Definition C.9 (Independence)** _Two events A and B are said to be_ independent _if_ P[ _A \u2229_ _B_ ] = P[ _A_ ] P[ _B_ ] _._ (C.3) _Equivalently, A and B are independent iff_ P[ _A | B_ ] = P[ _A_ ] _when_ P[ _B_ ] _\u0338_ = 0 _._ A sequence of random variables is said to be _independent and identically distributed (i.i.d.)_ when the random variables are mutually independent and follow the same distribution. The following are basic probability formulae related to the notion of conditional probability. They hold for any events _A_, _B_, and _A_ 1 _, . . ., A_ _n_, with the additional constraint P[ _B_ ] _\u0338_ = 0 needed for the Bayes formula to be well defined: P[ _A \u222a_ _B_ ] = P[ _A_ ] + P[ _B_ ] _\u2212_ P[ _A \u2229_ _B_ ] ( _sum rule_ ) (C.4) _n_ \ufffd P[ _A_ _i_ ] ( _union bound_ ) (C.5) _i_ =1",
    "chunk_id": "foundations_machine_learning_416"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "P[ _n_ \ufffd _A_ _i_ ] _\u2264_ _i_ =1 _[|][ A]_ []][ P][[] _[A]_ []] P[ _A | B_ ] = [P][[] _[B]_ ( _Bayes formula_ ) (C.6) P[ _B_ ] _n\u2212_ 1 \ufffd _A_ _i_ ] ( _chain rule_ ) _._ (C.7) _i_ =1 P[ _n_ \ufffd _A_ _i_ ] = P[ _A_ 1 ] P[ _A_ 2 _| A_ 1 ] _\u00b7 \u00b7 \u00b7_ P[ _A_ _n_ _|_ _i_ =1 The sum rule follows immediately from the decomposition of _A \u222a_ _B_ as the union of the disjoint sets _A_ and ( _B \u2212_ _A \u2229_ _B_ ). The union bound is a direct consequence of the sum rule. The Bayes formula follows immediately from the definition of conditional probability and the observation that: P[ _A|B_ ] P[ _B_ ] = P[ _B|A_ ] P[ _A_ ] = P[ _A \u2229_ _B_ ]. Similarly, the chain rule follows the observation that P[ _A_ 1 ] P[ _A_ 2 _|A_ 1 ] = P[ _A_ 1 _\u2229_ _A_ 2 ]; using the same argument shows recursively that the product of the first _k_ terms of the right-hand side equals P[ [\ufffd] _[k]_ _i_ =1 _[A]_ _[i]_ [].] Finally, assume that \u2126= _A_ 1 _\u222a_ _A_ 2 _\u222a_ _. . ._ _\u222a_ _A_ _n_ with _A_ _i_ _\u2229_ _A_ _j_ = _\u2205_ for _i \u0338_ = _j_, i.e., the _A_ _i_ s are mutually disjoint. Then, the following formula is valid for any event _B_ : P[ _B_ ] = _n_ \ufffd P[ _B | A_ _i_ ] P[ _A_ _i_ ] ( _theorem of total probability_ ) _._ (C.8) _i_ =1 This follows the observation that P[ _B | A_ _i_ ] P[ _A_ _i_ ] = P[ _B \u2229_ _A_ _i_ ] by definition of the conditional probability and the fact that the events _B \u2229_ _A_ _i_ are mutually disjoint. **C.4** **Expectation and Markov\u2019s inequality** **Definition C.10 (Expectation)** _The_ expectation _or_ mean _of a random variable X is denoted by_ E[ _X_ ] _and defined by_ E[ _X_ ] = \ufffd _x_ P[ _X_ = _x_ ] _._ (C.9) _x_ When _X_ follows a probability distribution D, we will also write E _x\u223c_ D [ _x_ ] instead of E[ _X_ ] to explicitly indicate the distribution. A fundamental property of expectation, which is straightforward to verify using its definition, is that it is linear, that is, for any two random variables _X_ and _Y_ and any _a, b \u2208_ R, the following holds: E[ _aX_ + _bY_ ] = _a_ E[ _X_ ] + _b_ E[ _Y_ ] _._ (C.10) **432** **Appendix C** **Probability Review** Furthermore, when _X_ and _Y_ are independent random variables, then the following identity holds: E[ _XY_ ] = E[ _X_ ] E[ _Y_ ] _._ (C.11) Indeed, by definition of expectation and of independence, we can write \ufffd _xy_ P[ _X_ = _x \u2227_ _Y_ = _y_ ] = \ufffd _x,y_ _x,y_ E[ _XY_ ] = \ufffd _xy_ P[ _X_ = _x_ ] P[ _Y_ = _y_ ] _x,y_ = \ufffd\ufffd _x_ P[",
    "chunk_id": "foundations_machine_learning_417"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_X_ = _x_ ] \ufffd\ufffd\ufffd _x_ _y_ P[ _Y_ = _y_ ] _,_ \ufffd _y_ where in the last step we used Fubini\u2019s theorem . The following provides a simple bound for a non-negative random variable in terms of its expectation, known as _Markov\u2019s inequality_ . **Theorem C.11 (Markov\u2019s inequality)** _Let X be a non-negative random variable with_ E[ _X_ ] _< \u221e._ _Then for all t >_ 0 _,_ P \ufffd _X \u2265_ _t_ E[ _X_ ]\ufffd _\u2264_ [1] (C.12) _t_ _[.]_ Proof: The proof steps are as follows: P[ _X \u2265_ _t_ E[ _X_ ]] = \ufffd P[ _X_ = _x_ ] (by definition) _x\u2265t_ E[ _X_ ] _x_ _\u2264_ _x\u2265_ \ufffd _t_ E[ _X_ ] P[ _X_ = _x_ ] _t_ E[ _X_ ] _x_ using \ufffd _t_ E[ _X_ ] _[\u2265]_ [1] \ufffd _x_ _\u2264_ \ufffd _x_ P[ _X_ = _x_ ] _t_ E[ _X_ ] (extending non-negative sum) _X_ = E \ufffd _t_ E[ _X_ ] = [1] (linearity of expectation) _._ \ufffd _t_ This concludes the proof. **C.5** **Variance and Chebyshev\u2019s inequality** **Definition C.12 (Variance \u2014 Standard deviation)** _The_ variance _of a random variable X is de-_ _noted by_ Var[ _X_ ] _and defined by_ Var[ _X_ ] = E[( _X \u2212_ E[ _X_ ]) [2] ] _._ (C.13) _The_ standard deviation _of a random variable X is denoted by \u03c3_ _X_ _and defined by_ _\u03c3_ _X_ = ~~\ufffd~~ Var[ _X_ ] _._ (C.14) For any random variable _X_ and any _a \u2208_ R, the following basic properties hold for the variance, which can be proven straightforwardly: Var[ _X_ ] = E[ _X_ [2] ] _\u2212_ E[ _X_ ] [2] (C.15) Var[ _aX_ ] = _a_ [2] Var[ _X_ ] _._ (C.16) Furthermore, when _X_ and _Y_ are independent, then Var[ _X_ + _Y_ ] = Var[ _X_ ] + Var[ _Y_ ] _._ (C.17) Indeed, using the linearity of expectation and the identity E[ _X_ ] E[ _Y_ ] _\u2212_ E[ _XY_ ] = 0 which holds by the independence of _X_ and _Y_, we can write Var[ _X_ + _Y_ ] = E[( _X_ + _Y_ ) [2] ] _\u2212_ E[ _X_ + _Y_ ] [2] = E[ _X_ [2] + _Y_ [2] + 2 _XY_ ] _\u2212_ (E[ _X_ ] [2] + E[ _Y_ ] [2] + 2 E[ _XY_ ]) = (E[ _X_ [2] ] _\u2212_ E[ _X_ ] [2] ) + (E[ _Y_ [2] ] _\u2212_ E[ _Y_ ] [2] ) + 2(E[ _X_ ] E[ _Y_ ] _\u2212_ E[ _XY_ ]) = Var[ _X_ ] + Var[ _Y_ ] _._ **C.5** **Variance and Chebyshev\u2019s inequality** **433** The following inequality known as _Chebyshev\u2019s inequality_ bounds the deviation of a random variable from its expectation in terms of its standard deviation. **Theorem C.13 (Chebyshev\u2019s inequality)** _Let X be a random variable with_ Var[ _X_ ] _<_ + _\u221e. Then,_ _for all t >_ 0 _, the following inequality holds:_ P \ufffd _|X \u2212_ E[ _X_ ] _| \u2265_ _t\u03c3_ _X_ \ufffd _\u2264_ [1] (C.18) _t_ [2] _[ .]_ Proof: Observe that: P",
    "chunk_id": "foundations_machine_learning_418"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd _|X \u2212_ E[ _X_ ] _| \u2265_ _t\u03c3_ _X_ \ufffd = P[( _X \u2212_ E[ _X_ ]) [2] _\u2265_ _t_ [2] _\u03c3_ _X_ [2] []] _[.]_ The result follows by application of Markov\u2019s inequality to ( _X \u2212_ E[ _X_ ]) [2] . We will use Chebyshev\u2019s inequality to prove the following theorem. **Theorem C.14 (Weak law of large numbers)** _Let_ ( _X_ _n_ ) _n\u2208_ N _be a sequence of independent ran-_ _dom variables with the same mean \u00b5 and variance \u03c3_ [2] _< \u221e. Let X_ _n_ = _n_ [1] \ufffd _ni_ =1 _[X]_ _[i]_ _[, then, for any]_ _\u03f5 >_ 0 _,_ _n_ lim _\u2192\u221e_ [P][[] _[|][X]_ _[n]_ _[ \u2212]_ _[\u00b5][| \u2265]_ _[\u03f5]_ [] = 0] _[.]_ (C.19) Proof: Since the variables are independent, we can write _n_ Var[ _X_ _n_ ] = _n_ \ufffd \ufffd _i_ =1 Var \ufffd _Xn_ _i_ = _[n\u03c3]_ [2] \ufffd _n_ [2] [2] = _[\u03c3]_ [2] _n_ [2] _n_ _[.]_ Thus, by Chebyshev\u2019s inequality (with _t_ = _\u03f5/_ (Var[ _X_ _n_ ]) [1] _[/]_ [2] ), the following holds: P[ _|X_ _n_ _\u2212_ _\u00b5| \u2265_ _\u03f5_ ] _\u2264_ _[\u03c3]_ [2] _n\u03f5_ [2] _[,]_ which implies (C.19). **Example C.15 (Applying Chebyshev\u2019s inequality)** Suppose we roll a pair of fair dice _n_ times. Can we give a good estimate of the total value of the _n_ rolls? If we compute the mean and variance, we find _\u00b5_ = 7 _n_ and _\u03c3_ [2] = 35 _/_ 6 _n_ (we leave it to the reader to verify these expressions). 35 Thus, applying Chebyshev\u2019s inequality, we see that the final sum will lie within 7 _n \u00b1_ 10 ~~\ufffd~~ 6 _[n]_ [ in] 35 Thus, applying Chebyshev\u2019s inequality, we see that the final sum will lie within 7 _n \u00b1_ 10 6 _[n]_ [ in] at least 99 percent of all experiments. Therefore, the odds are better than 99 to 1 that the sum will be between 6 _._ 975M and 7 _._ 025M after 1M rolls. **Definition C.16 (Covariance)** _The_ covariance _of two random variables X and Y is denoted by_ Cov( _X, Y_ ) _and defined by_ Cov( _X, Y_ ) = E \ufffd( _X \u2212_ E[ _X_ ])( _Y \u2212_ E[ _Y_ ])\ufffd _._ (C.20) Two random variables _X_ and _Y_ are said to be _uncorrelated_ when Cov( _X, Y_ ) = 0. It is straightforward to see that if two random variables _X_ and _Y_ are independent then they are uncorrelated, but the converse does not hold in general. The covariance defines a positive semidefinite and symmetric bilinear form: _\u2022_ symmetry: Cov( _X, Y_ ) = Cov( _Y, X_ ) for any two random variables _X_ and _Y_ ; _\u2022_ bilinearity: Cov( _X_ + _X_ _[\u2032]_ _, Y_ ) = Cov( _X, Y_ ) + Cov( _X_ _[\u2032]_ _, Y_ ) and Cov( _aX, Y_ ) = _a_ Cov( _X, Y_ ) for any random variables _X_, _X_ _[\u2032]_, and _Y_ and _a \u2208_ R; _\u2022_ positive semidefiniteness: Cov( _X, X_ ) = Var[ _X_ ] _\u2265_ 0 for any random variable",
    "chunk_id": "foundations_machine_learning_419"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_X_ . The following Cauchy-Schwarz inequality holds for random variables _X_ and _Y_ with Var[ _X_ ] _<_ + _\u221e_ and Var[ _Y_ ] _<_ + _\u221e_ : _|_ Cov( _X, Y_ ) _| \u2264_ ~~\ufffd~~ Var[ _X_ ] Var[ _Y_ ] _._ (C.21) The following definition extends the notion of covariance to a vector of random variables. **434** **Appendix C** **Probability Review** **Definition C.17** _The_ covariance matrix _of a vector of random variables_ **X** = ( _X_ 1 _, . . ., X_ _N_ ) _is the_ _matrix in_ R _[N]_ _[\u00d7][N]_ _denoted by_ **C** ( **X** ) _and defined by_ **C** ( **X** ) = E \ufffd( **X** _\u2212_ E[ **X** ])( **X** _\u2212_ E[ **X** ]) _[\u22a4]_ [\ufffd] _._ (C.22) Thus, **C** ( **X** ) = (Cov( _X_ _i_ _, X_ _j_ )) _ij_ . It is straightforward to show that **C** ( **X** ) = E[ **XX** _[\u22a4]_ ] _\u2212_ E[ **X** ] E[ **X** ] _[\u22a4]_ _._ (C.23) We close this appendix with the following well-known theorem of probability. **Theorem C.18 (Central limit theorem)** _Let X_ 1 _, . . ., X_ _n_ _be a sequence of i.i.d. random variables_ _with mean \u00b5 and standard deviation \u03c3. Let X_ _n_ = _n_ [1] \ufffd _ni_ =1 _[X]_ _[i]_ _[ and][ \u03c3]_ _n_ ~~[2]~~ [=] _[ \u03c3]_ [2] _[/n][. Then,]_ [ (] _[X]_ _[n]_ _[\u2212][\u00b5]_ [)] _[/\u03c3]_ _[n]_ converges to the _N_ (0 _,_ 1) in distribution _, that is for any t \u2208_ R _,_ _t_ lim _n\u2192\u221e_ [P][[(] _[X]_ _[n]_ _[ \u2212]_ _[\u00b5]_ [)] _[/\u03c3]_ _[n]_ _[ \u2264]_ _[t]_ [] =] \ufffd _\u2212\u221e_ **C.6** **Moment-generating functions** 1 _e_ _[\u2212]_ _[x]_ 2 [2] 2 _\u03c0_ 1 _\u221a_ 2 _dx ._ The expectation E[ _X_ _[p]_ ] is called the _pth-moment_ of the random variable _X_ . The momentgenerating function of a random variable _X_ is a key function from which its different moments can be straightforwardly computed via differentiation at zero. It can therefore be crucial for specifying the distribution of _X_ or analyzing its properties. **Definition C.19 (Moment-generating function)** _The_ moment-generating function _of a random_ _variable X is the function M_ _X_ : _t \ufffd\u2192_ E[ _e_ _[tX]_ ] _defined over the set of t \u2208_ R _for which the expectation_ _is finite._ If _M_ _X_ is differentiable at zero, then the _p_ th-moment of _X_ is given by E[ _X_ _[p]_ ] = _M_ _X_ [(] _[p]_ [)] [(0). We will] present in the next chapter a general bound on the moment-generating function of a zero-mean bounded random variable (Lemma D.1). Here, we illustrate its computation in two special cases. **Example C.20 (Standard normal distribution)** Let _X_ be a random variable following a normal distribution with mean 0 and variance 1. Then, _M_ _X_ is defined for all _t \u2208_ R by [2] _\u221e_ 2 \ufffd _\u2212\u221e_ 2 _,_ (C.24) _\u221e_ _M_ _X_ ( _t_ ) = \ufffd _\u2212\u221e_ 1 _\u221a_ 2 _x_ [2] _e_ _[\u2212]_ 2 2 _\u03c0_ [2] _t_ [2] 2 _e_ _[tx]_ _dx_ = _e_ 2 [1] _t_ [2] 2 [(] _[x][\u2212][t]_",
    "chunk_id": "foundations_machine_learning_420"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[)] [2] _dx_ = _e_ 2 1 _\u221a_ 2 _e_ _[\u2212]_ [1] 2 2 _\u03c0_ by recognizing that the last integrand is the probability density function of a normal distribution with mean _t_ and variance 1. **Example C.21 (** _\u03c7_ [2] **distribution)** Let _X_ be a random variable following a _\u03c7_ [2] distribution with _k_ degrees of freedom. We can write _X_ = [\ufffd] _[k]_ _i_ =1 _[X]_ _i_ [2] [where the] _[ X]_ _[i]_ [s are independent and follow a] standard normal distribution. Let _t <_ 1 _/_ 2. By the i.i.d. assumption about the variables _X_ _i_, we can write _k_ E[ _e_ _[tX]_ ] = E \ufffd \ufffd _e_ _[tX]_ _i_ [2] \ufffd = _i_ =1 _k_ \ufffd E \ufffd _e_ _[tX]_ _i_ [2] [\ufffd] = E \ufffd _e_ _[tX]_ 1 [2] [\ufffd] _[k]_ _._ _i_ =1 By definition of the standard normal distribution, we have 1 E[ _e_ _[tX]_ 1 [2] ] = _\u221a_ 2 _\u03c0_ + _\u221e_ _\u2212x_ [2] _e_ _[tx]_ [2] _e_ 2 _\u2212\u221e_ _x_ [2] 1 2 _dx_ = _\u221a_ 2 2 _dx_ 2 _\u03c0_ \ufffd + _\u221e_ + _\u221e_ _e_ [(1] _[\u2212]_ [2] _[t]_ [)] _[\u2212]_ 2 _[x]_ [2] _\u2212\u221e_ _e_ 2 = (1 _\u2212_ 2 _t_ ) _[\u2212]_ 2 [1] _\u221a_ 1 _\u2212_ 2 _tdu_ _\u2212u_ [2] _e_ 2 1 = _\u221a_ 2 _\u03c0_ \ufffd _\u2212\u221e_ + _\u221e_ \ufffd _\u2212\u221e_ + _\u221e_ \ufffd + _\u221e_ 2 _,_ where we used the change of variable _u_ = _[\u221a]_ 1 _\u2212_ 2 _t x_ . In view of that, the moment-generating function of the _\u03c7_ [2] distribution is given by _\u2200t <_ 1 _/_ 2 _, M_ _X_ ( _t_ ) = E[ _e_ _[tX]_ ] = (1 _\u2212_ 2 _t_ ) _[\u2212]_ _[k]_ 2 _._ (C.25) **C.7** **Exercises** **435** **C.7** **Exercises** C.1 Let _f_ : (0 _,_ + _\u221e_ ) _\u2192_ R + be a function admitting an inverse _f_ _[\u2212]_ [1] and let _X_ be a random variable. Show that if for any _t >_ 0, P[ _X > t_ ] _\u2264_ _f_ ( _t_ ), then, for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, _X \u2264_ _f_ _[\u2212]_ [1] ( _\u03b4_ ). C.2 Let _X_ be a discrete random variable taking non-negative integer values. Show that E[ _X_ ] = \ufffd _n\u2265_ 1 [P][[] _[X][ \u2265]_ _[n]_ [] (] _[Hint]_ [: rewrite][ P][[] _[X]_ [ =] _[ n]_ [] as][ P][[] _[X][ \u2265]_ _[n]_ []] _[ \u2212]_ [P][[] _[X][ \u2265]_ _[n]_ [ + 1]).] # D Concentration Inequalities In this appendix, we present several _concentration inequalities_ used in the proofs given in this book. Concentration inequalities give probability bounds for a random variable to be concentrated around its mean, or for it to deviate from its mean or some other value. **D.1** **Hoeffding\u2019s inequality** We first present Hoeffding\u2019s inequality, whose proof makes use of the general _Chernoff bounding_ _technique_ . Given a random variable _X_ and _\u03f5 >_ 0, this technique consists of proceeding as follows to bound P[ _X \u2265_ _\u03f5_ ]. For any _t >_ 0,",
    "chunk_id": "foundations_machine_learning_421"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "first Markov\u2019s inequality is used to bound P[ _X \u2265_ _\u03f5_ ]: P[ _X \u2265_ _\u03f5_ ] = P[ _e_ _[tX]_ _\u2265_ _e_ _[t\u03f5]_ ] _\u2264_ _e_ _[\u2212][t\u03f5]_ E[ _e_ _[tX]_ ] _._ (D.1) Then, an upper bound _g_ ( _t_ ) is found for E[ _e_ _[tX]_ ] and _t_ is selected to minimize _e_ _[\u2212][t\u03f5]_ _g_ ( _t_ ). For Hoeffding\u2019s inequality, the following lemma provides an upper bound on E[ _e_ _[tX]_ ]. **Lemma D.1 (Hoeffding\u2019s lemma)** _Let X be a random variable with E_ [ _X_ ] = 0 _and a \u2264_ _X \u2264_ _b_ _with b > a. Then, for any t >_ 0 _, the following inequality holds:_ _t_ [2] ( _b\u2212a_ ) [2] E[ _e_ _[tX]_ ] _\u2264_ _e_ 8 _._ (D.2) Proof: By the convexity of _x \ufffd\u2192_ _e_ _[x]_, for all _x \u2208_ [ _a, b_ ], the following holds: _e_ _[tx]_ _\u2264_ _[b][ \u2212]_ _[x]_ _[ \u2212]_ _b \u2212_ _a_ _[e]_ _[tb]_ _[ .]_ _[ \u2212]_ _[x]_ _[x][ \u2212]_ _[a]_ _b \u2212_ _a_ _[e]_ _[ta]_ [ +] _b \u2212_ _a_ Thus, using E[ _X_ ] = 0, _b \u2212_ _X_ E[ _e_ _[tX]_ ] _\u2264_ E \ufffd _b \u2212_ _b_ _\u2212a_ _[ \u2212]_ _[a]_ = _b \u2212_ _a_ _[e]_ _[tb]_ \ufffd _b \u2212_ _a_ _[e]_ _[ta]_ [ +] _b \u2212_ _a_ _[e]_ _[tb]_ [ =] _[ e]_ _[\u03c6]_ [(] _[t]_ [)] _[,]_ _\u2212_ _X_ _b \u2212_ _a_ _[e]_ _[ta]_ [ +] _[ X]_ _b \u2212_ _[ \u2212]_ _a_ _[a]_ where, _b_ _\u2212a_ _b_ _\u2212a_ _\u03c6_ ( _t_ ) = log = _ta_ + log _._ \ufffd _b \u2212_ _a_ _[e]_ _[ta]_ [ +] _b \u2212_ _a_ _[e]_ _[tb]_ \ufffd \ufffd _b \u2212_ _a_ [+] _b \u2212_ _a_ _[e]_ _[t]_ [(] _[b][\u2212][a]_ [)] \ufffd For any _t >_ 0, the first and second derivative of _\u03c6_ are given below: _ae_ _[t]_ [(] _[b][\u2212][a]_ [)] _a_ _\u03c6_ _[\u2032]_ ( _t_ ) = _a \u2212_ _b_ _a_ _b_ _a_ _,_ _b\u2212a_ _[\u2212]_ _b\u2212a_ _[e]_ _[t]_ [(] _[b][\u2212][a]_ [)] [ =] _[ a][ \u2212]_ _b\u2212a_ _[e]_ _[\u2212][t]_ [(] _[b][\u2212][a]_ [)] _[ \u2212]_ _b\u2212a_ _\u2212abe_ _[\u2212][t]_ [(] _[b][\u2212][a]_ [)] _\u03c6_ _[\u2032\u2032]_ ( _t_ ) = _b_ _a_ [ _b\u2212a_ _[e]_ _[\u2212][t]_ [(] _[b][\u2212][a]_ [)] _[ \u2212]_ _b\u2212a_ []] [2] = _[\u03b1]_ [(][1] _[ \u2212]_ _[\u03b1]_ [)] _[e]_ _[\u2212][t]_ [(] _[b][\u2212][a]_ [)] [(] _[b][ \u2212]_ _[a]_ [)] [2] [(1 _\u2212_ _\u03b1_ ) _e_ _[\u2212][t]_ [(] _[b][\u2212][a]_ [)] + _\u03b1_ ] [2] = _\u03b1_ (1 _\u2212_ _\u03b1_ ) _e_ _[\u2212][t]_ [(] _[b][\u2212][a]_ [)] [(1 _\u2212_ _\u03b1_ ) _e_ _[\u2212][t]_ [(] _[b][\u2212][a]_ [)] + _\u03b1_ ] [(1 _\u2212_ _\u03b1_ ) _e_ _[\u2212][t]_ [(] _[b][\u2212][a]_ [)] + _\u03b1_ ] [(] _[b][ \u2212]_ _[a]_ [)] [2] _[ .]_ **438** **Appendix D** **Concentration Inequalities** where _\u03b1_ denotes _b\u2212\u2212aa_ [. Note that] _[ \u03c6]_ [(0) =] _[ \u03c6]_ _[\u2032]_ [(0) = 0 and that] _[ \u03c6]_ _[\u2032\u2032]_ [(] _[t]_ [) =] _[ u]_ [(1] _[ \u2212]_ _[u]_ [)(] _[b][ \u2212]_ _[a]_ [)] [2] [ where] _u_ = [(1 _\u2212\u03b1_ ) _e_ _[\u2212]_ _\u03b1_ _[t]_ [(] _[b][\u2212][a]_ [)] + _\u03b1_ ] [. Since] _[ u]_",
    "chunk_id": "foundations_machine_learning_422"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[ is in [0] _[,]_ [ 1],] _[ u]_ [(1] _[ \u2212]_ _[u]_ [) is upper bounded by 1] _[/]_ [4 and] _[ \u03c6]_ _[\u2032\u2032]_ [(] _[t]_ [)] _[ \u2264]_ [(] _[b][\u2212]_ 4 _[a]_ [)] [2] . Thus, by the second order expansion of function _\u03c6_, there exists _\u03b8 \u2208_ [0 _, t_ ] such that: _\u03c6_ ( _t_ ) = _\u03c6_ (0) + _t\u03c6_ _[\u2032]_ (0) + _[t]_ [2] [2] [(] _[b][ \u2212]_ _[a]_ [)] [2] 2 _[\u03c6]_ _[\u2032\u2032]_ [(] _[\u03b8]_ [)] _[ \u2264]_ _[t]_ [2] 8 _,_ (D.3) 8 which completes the proof. The lemma can be used to prove the following result known as _Hoeffding\u2019s inequality_ . **Theorem D.2 (Hoeffding\u2019s inequality)** _Let X_ 1 _, . . ., X_ _m_ _be independent random variables with_ _X_ _i_ _taking values in_ [ _a_ _i_ _, b_ _i_ ] _for all i \u2208_ [ _m_ ] _. Then, for any \u03f5 >_ 0 _, the following inequalities hold for_ _S_ _m_ = [\ufffd] _[m]_ _i_ =1 _[X]_ _[i]_ _[:]_ P[ _S_ _m_ _\u2212_ E[ _S_ _m_ ] _\u2265_ _\u03f5_ ] _\u2264_ _e_ _[\u2212]_ [2] _[\u03f5]_ [2] _[/]_ [ \ufffd] _i_ _[m]_ =1 [(] _[b]_ _[i]_ _[\u2212][a]_ _[i]_ [)] [2] (D.4) P[ _S_ _m_ _\u2212_ E[ _S_ _m_ ] _\u2264\u2212\u03f5_ ] _\u2264_ _e_ _[\u2212]_ [2] _[\u03f5]_ [2] _[/]_ [ \ufffd] _i_ _[m]_ =1 [(] _[b]_ _[i]_ _[\u2212][a]_ _[i]_ [)] [2] _._ (D.5) Proof: Using the Chernoff bounding technique and lemma D.1, we can write: P[ _S_ _m_ _\u2212_ E[ _S_ _m_ ] _\u2265_ _\u03f5_ ] _\u2264_ _e_ _[\u2212][t\u03f5]_ E[ _e_ _[t]_ [(] _[S]_ _[m]_ _[\u2212]_ [E][[] _[S]_ _[m]_ [])] ] = _e_ _[\u2212][t\u03f5]_ \u03a0 _[m]_ _i_ =1 [E][[] _[e]_ _[t]_ [(] _[X]_ _[i]_ _[\u2212]_ [E][[] _[X]_ _[i]_ [])] []] (independence of _X_ _i_ s) _\u2264_ _e_ _[\u2212][t\u03f5]_ \u03a0 _[m]_ _i_ =1 _[e]_ _[t]_ [2] [(] _[b]_ _[i]_ _[\u2212][a]_ _[i]_ [)] [2] _[/]_ [8] (lemma D.1) = _e_ _[\u2212][t\u03f5]_ _e_ _[t]_ [2] [ \ufffd] _i_ _[m]_ =1 [(] _[b]_ _[i]_ _[\u2212][a]_ _[i]_ [)] [2] _[/]_ [8] _\u2264_ _e_ _[\u2212]_ [2] _[\u03f5]_ [2] _[/]_ [ \ufffd] _i_ _[m]_ =1 [(] _[b]_ _[i]_ _[\u2212][a]_ _[i]_ [)] [2] _,_ where we chose _t_ = 4 _\u03f5/_ [\ufffd] _[m]_ _i_ =1 [(] _[b]_ _[i]_ _[ \u2212]_ _[a]_ _[i]_ [)] [2] [ to minimize the upper bound.] This proves the first statement of the theorem, and the second statement is shown in a similar way. When the variance _\u03c3_ _X_ [2] _i_ [of each random variable] _[ X]_ _[i]_ [ is known and the] _[ \u03c3]_ _X_ [2] _i_ [s are relatively small,] better concentration bounds can be derived (see _Bennett\u2019s_ and _Bernstein\u2019s inequalities_ proven in exercise D.6). **D.2** **Sanov\u2019s theorem** Here, we present a finer upper bound than Hoeffding\u2019s inequality expressed in terms of the binary relative entropy. **Theorem D.3 (Sanov\u2019s theorem)** _Let X_ 1 _, . . ., X_ _m_ _be independent random variables drawn ac-_ _cording to some distribution_ D _with mean p and support included in_ [0 _,_ 1] _. Then, for any q \u2208_ [0 _,_ 1] _,_ _the following inequality holds for_",
    "chunk_id": "foundations_machine_learning_423"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\ufffd _p_ = _m_ 1 \ufffd _mi_ =1 _[X]_ _[i]_ _[:]_ P[ _p_ \ufffd _\u2265_ _q_ ] _\u2264_ _e_ _[\u2212][m]_ [D][(] _[q][\u2225][p]_ [)] _,_ _where_ D( _q\u2225p_ ) = _q_ log _[q]_ [+ (1] _[ \u2212]_ _[q]_ [) log] [1] _\u2212_ _[\u2212][q]_ _p_ _[q]_ [+ (1] _[ \u2212]_ _[q]_ [) log] 1 [1] _\u2212_ _[\u2212]_ _p_ _[q]_ _[\u2212][q]_ 1 _\u2212p_ _[is the binary relative entropy of][ p][ and][ q][.]_ **D.3** **Multiplicative Chernoff bounds** **439** Proof: For any _t >_ 0, by convexity of the function _x \ufffd\u2192_ _e_ _[tx]_, the following inequality holds for all _x \u2208_ [0 _,_ 1]: _e_ _[tx]_ = _e_ _[t]_ [[(1] _[\u2212][x]_ [)] _[\u00b7]_ [0+] _[x][\u00b7]_ [1]] _\u2264_ 1 _\u2212_ _x_ + _e_ _[t]_ _x_ . In view of that, for any _t >_ 0, we can write P[ _p_ \ufffd _\u2265_ _q_ ] = P[ _e_ _[tm][p]_ [\ufffd] _\u2265_ _e_ _[tmq]_ ] = P[ _e_ _[tm][p]_ [\ufffd] _\u2265_ _e_ _[tmq]_ ] _\u2264_ _e_ _[\u2212][tmq]_ E[ _e_ _[tm][p]_ [\ufffd] ] (by Markov\u2019s inequality) = _e_ _[\u2212][tmq]_ E[ _e_ _[t]_ [ \ufffd] _i_ _[m]_ =1 _[X]_ _[i]_ ] _m_ = _e_ _[\u2212][tmq]_ \ufffd E[ _e_ _[tX]_ _[i]_ ] _i_ =1 _m_ _\u2264_ _e_ _[\u2212][tmq]_ \ufffd E[1 _\u2212_ _X_ _i_ + _e_ _[t]_ _X_ _i_ ] ( _\u2200x \u2208_ [0 _,_ 1] _, e_ _[tx]_ _\u2264_ 1 _\u2212_ _x_ + _e_ _[t]_ _x_ ) _i_ =1 = [ _e_ _[\u2212][tq]_ (1 _\u2212_ _p_ + _e_ _[t]_ _p_ )] _[m]_ _._ Now, the function _f_ : _t \ufffd\u2192_ _e_ _[\u2212][tq]_ (1 _\u2212_ _p_ + _e_ _[t]_ _p_ ) = (1 _\u2212_ _p_ ) _e_ _[\u2212][tq]_ + _pe_ _[t]_ [(1] _[\u2212][q]_ [)] reaches its minimum at _t_ = log _p_ _[q]_ [(] (1 [1] _[\u2212]_ _\u2212_ _[p]_ _q_ [)] ) [. Plugging in this value of] _[ t]_ [ in the inequality above yields][ P][[] _[p]_ [\ufffd] _[ \u2265]_ _[q]_ []] _[ \u2264]_ _[e]_ _[\u2212][m]_ [D][(] _[q][\u2225][p]_ [)] [.][ \u25a1] Note that for any _\u03f5 >_ 0, _\u03f5 \u2264_ 1 _\u2212_ _p_, with the choice _q_ = _p_ + _\u03f5_, the theorem implies P[ _p_ \ufffd _\u2265_ _p_ + _\u03f5_ ] _\u2264_ _e_ _[\u2212][m]_ [D][(] _[p]_ [+] _[\u03f5][\u2225][p]_ [)] _._ (D.6) This is a finer bound than Hoeffding\u2019s inequality (Theorem D.2) since, by Pinsker\u2019s inequality (Proposition E.7), D( _p_ + _\u03f5\u2225p_ ) _\u2265_ [1] 2 [(2] _[\u03f5]_ [)] [2] [ = 2] _[\u03f5]_ [2] [. Similarly, we can derive a symmetric bound by] applying the theorem to the random variables _Y_ _i_ = 1 _\u2212_ _X_ _i_ . Then, for any _\u03f5 >_ 0, _\u03f5 \u2264_ _p_, with the choice _q_ = _p \u2212_ _\u03f5_, the theorem implies P[ _p_ \ufffd _\u2264_ _p \u2212_ _\u03f5_ ] _\u2264_ _e_ _[\u2212][m]_ [D][(] _[p][\u2212][\u03f5][\u2225][p]_ [)] _._ (D.7) **D.3** **Multiplicative Chernoff bounds** Sanov\u2019s theorem can be used to prove the following _multiplicative Chernoff bounds_ . **Theorem D.4 (Multiplicative Chernoff bounds)** _Let X_ 1 _, . . ., X_ _m_ _be independent random vari-_ _ables drawn according to some distribution_ D _with mean p and support included in_ [0 _,_ 1] _. Then,_ _for any \u03b3",
    "chunk_id": "foundations_machine_learning_424"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\u2208_ \ufffd0 _,_ _p_ [1] _[\u2212]_ [1] \ufffd _, the following inequality holds for_ \ufffd _p_ = _m_ 1 \ufffd _mi_ =1 _[X]_ _[i]_ _[:]_ P[ _p_ \ufffd _\u2265_ (1 + _\u03b3_ ) _p_ ] _\u2264_ _e_ _[\u2212]_ _[m][p\u03b3]_ 3 [2] P[ _p_ \ufffd _\u2264_ (1 _\u2212_ _\u03b3_ ) _p_ ] _\u2264_ _e_ _[\u2212]_ _[m][p\u03b3]_ 2 [2] _._ Proof: The proof consists of deriving in each case a finer lower bound for the binary relative entropy than Pinsker\u2019s inequality. Using the inequalities log(1 + _x_ ) _\u2265_ 1+ _x_ _[x]_ 2 [and log(1 +] _[ x]_ [)] _[ < x]_ [,] we can write _\u2212_ D((1 + _\u03b3_ ) _p\u2225p_ ) = (1 + _\u03b3_ ) _p_ log (1 + _p \u03b3_ ) _p_ [+ (1] _[ \u2212]_ [(1 +] _[ \u03b3]_ [)] _[p]_ [) log] \ufffd 1 _\u2212_ 1(1 + _\u2212_ _p \u03b3_ ) _p_ \ufffd 1 _\u03b3p_ = (1 + _\u03b3_ ) _p_ log 1 + _\u03b3_ [+ (1] _[ \u2212]_ _[p][ \u2212]_ _[\u03b3p]_ [) log] \ufffd1 + 1 _\u2212_ _p \u2212_ _\u03b3p_ \ufffd _\u2264_ (1 + _\u03b3_ ) _p_ _[\u2212][\u03b3]_ _\u03b3p_ + (1 _\u2212_ _p \u2212_ _\u03b3p_ ) _[\u03b3]_ 2 1 _\u2212_ _p \u2212_ _\u03b3p_ 1 + _[\u03b3]_ = _[\u2212]_ _[\u03b3]_ 2 [2] _[p]_ 2 + _\u03b3_ = _\u03b3p_ \ufffd _[\u03b3]_ 1 _\u2212_ [1 +] 1 + _[\u03b3]_ 2 \ufffd 2 1 + _[\u03b3]_ _[p]_ = _[\u2212][\u03b3]_ [2] _[p]_ _[\u03b3]_ 2 2 + _\u03b3_ _\u2264_ _[\u2212][\u03b3]_ [2] _[p]_ _[\u2212][\u03b3]_ [2] _[p]_ 2 + 1 [=] _[ \u2212][\u03b3]_ 3 [2] _[p]_ 3 _._ **440** **Appendix D** **Concentration Inequalities** Similarly, using the inequalities (1 _\u2212x_ ) log(1 _\u2212x_ ) _\u2265\u2212x_ + _[x]_ 2 [2] [valid for] _[ x][ \u2208]_ [(0] _[,]_ [ 1) and log(1] _[\u2212][x]_ [)] _[ <][ \u2212][x]_ [,] we can write _\u2212_ D((1 _\u2212_ _\u03b3_ ) _p\u2225p_ ) = (1 _\u2212_ _\u03b3_ ) _p_ log _p_ 1 _\u2212_ _p_ (1 _\u2212_ _\u03b3_ ) _p_ [+ (1] _[ \u2212]_ [(1] _[ \u2212]_ _[\u03b3]_ [)] _[p]_ [) log] \ufffd 1 _\u2212_ (1 _\u2212_ _\u03b3_ ) _p_ \ufffd 1 _\u03b3p_ = (1 _\u2212_ _\u03b3_ ) _p_ log 1 _\u2212_ 1 _\u2212_ _\u03b3_ [+ (1] _[ \u2212]_ _[p]_ [ +] _[ \u03b3p]_ [) log] \ufffd 1 _\u2212_ _p_ + _\u03b3p_ \ufffd _\u2264_ _\u03b3 \u2212_ _[\u03b3]_ [2] \ufffd 2 _\u2212\u03b3p_ _p_ + (1 _\u2212_ _p_ + _\u03b3p_ ) _._ \ufffd 1 _\u2212_ _p_ + _\u03b3p_ [=] _[ \u2212][\u03b3]_ 2 [2] _[p]_ _\u2212\u03b3p_ _p_ + (1 _\u2212_ _p_ + _\u03b3p_ ) \ufffd 1 _\u2212_ _p_ + _\u03b3p_ [=] _[ \u2212][\u03b3]_ 2 [2] _[p]_ This completes the proof. **D.4** **Binomial distribution tails: Upper bounds** Let _X_ 1 _, . . ., X_ _m_ be independent random variables taking values in _{_ 0 _,_ 1 _}_ with P[ _X_ _i_ = 1] = _p \u2208_ [0 _,_ 1] for _i_ = 1 _, . . ., m_ . Then, [\ufffd] _[m]_ _i_ =1 _[X]_ _[i]_ [ follows the binomial distribution] _[ B]_ [(] _[m, p]_ [). We will denote] by _X_ the average _X_ = _m_ 1 \ufffd _mi_ =1 _[X]_ _[i]_ [. Then, the following",
    "chunk_id": "foundations_machine_learning_425"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "equality and inequalities hold:] P _X \u2212_ _p > \u03f5_ = \ufffd \ufffd _m_ \ufffd _k_ = _\u2308_ ( _p_ + _\u03f5_ ) _m\u2309_ _m_ _p_ _[k]_ (1 _\u2212_ _p_ ) _[m][\u2212][k]_ (Binomial formula) \ufffd _k_ \ufffd \ufffd _mk_ P _X \u2212_ _p > \u03f5_ _\u2264_ _e_ _[\u2212]_ [2] _[m\u03f5]_ [2] (Hoeffding\u2019s inequality) \ufffd \ufffd _X \u2212_ _p > \u03f5_ _\u2264_ _e_ _\u2212_ 2 _\u03c3m\u03f5_ [2] + [2] \ufffd P \ufffd 2 _\u03c3_ [2] + [2] _[\u03f5]_ 3 (Bernstein\u2019s inequality) _\u03f5_ P _X \u2212_ _p > \u03f5_ _\u2264_ _e_ _[\u2212][m\u03c3]_ [2] _[\u03b8]_ \ufffd _\u03c3_ [2] \ufffd (Bennett\u2019s inequality) \ufffd \ufffd P _X \u2212_ _p > \u03f5_ _\u2264_ _e_ _[\u2212][m]_ [D][(] _[p]_ [+] _[\u03f5][\u2225][p]_ [)] (Sanov\u2019s inequality) _,_ \ufffd \ufffd where _\u03c3_ [2] = _p_ (1 _\u2212_ _p_ ) = Var[ _X_ _i_ ] and _\u03b8_ ( _x_ ) = (1 + _x_ ) log(1 + _x_ ) _\u2212_ _x_ . The last three inequalities are shown in exercises D.6 and D.7. Using Bernstein\u2019s inequality, for example, we can see that for _\u03f5_ relatively small, that is _\u03f5 \u226a_ 2 _\u03c3_ [2], the upper bound is approximately of the form _e_ _[\u2212]_ _[m\u03f5]_ 2 _\u03c3_ [2][2] relatively small, that is _\u03f5 \u226a_ 2 _\u03c3_, the upper bound is approximately of the form _e_ 2 _\u03c3_ [2] and thus admits a Gaussian behavior. For _\u03f5 \u226b_ 2 _\u03c3_ [2], _e_ _[\u2212]_ [3] _[m\u03f5]_ 2, the upper bound admits a Poisson behavior. Figure D.1 shows a comparison of these bounds for different values of the variance _\u03c3_ [2] = _p_ (1 _\u2212p_ ): small variance ( _p_ = _._ 05), large variance ( _p_ = _._ 5). **D.5** **Binomial distribution tails: Lower bound** Let _X_ be a random variable following the binomial distribution _B_ ( _m, p_ ) and let _k_ be an integer such that _p \u2264_ [1] 4 [and] _[ k][ \u2265]_ _[mp]_ [ or] _[ p][ \u2264]_ [1] 2 [and] _[ mp][ \u2264]_ _[k][ \u2264]_ _[m]_ [(1] _[ \u2212]_ _[p]_ [). Then, the following inequality] known as _Slud\u2019s inequality_ holds: P[ _X \u2265_ _k_ ] _\u2265_ P where _N_ is in standard normal form. \ufffd _k \u2212_ _mp_ _N \u2265_ ~~\ufffd~~ _mp_ (1 _\u2212_ _mp_ (1 _\u2212_ _p_ ) \ufffd _,_ (D.8) **D.6** **Azuma\u2019s inequality** **441** **Figure D.1** **Image:** [No caption returned] **Image:** [No caption returned] Comparison of tail bounds for a binomial random variable for _\u03f5_ = _._ 3 and _p_ = _._ 05 (small variance) or _p_ = _._ 5 (maximal variance) as a function of the sample size _m_ . **D.6** **Azuma\u2019s inequality** This section presents a concentration inequality that is more general than Hoeffding\u2019s inequality. Its proof makes use of a Hoeffding\u2019s inequality for _martingale differences_ . **Definition D.5 (Martingale difference)** _A sequence of random variables V_ 1 _, V_ 2 _, . . . is a martin-_ _gale difference sequence with respect to X_ 1 _, X_ 2 _, . . . if for all i >_ 0 _, V_ _i_ _is a function of X_ 1 _, . . ., X_ _i_ _and_ E[ _V_ _i_",
    "chunk_id": "foundations_machine_learning_426"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "+1 _|X_ 1 _, . . ., X_ _i_ ] = 0 _._ (D.9) The following result is similar to Hoeffding\u2019s lemma. **Lemma D.6** _Let V and Z be random variables satisfying_ E[ _V |Z_ ] = 0 _and, for some function f_ _and constant c \u2265_ 0 _, the inequalities:_ _f_ ( _Z_ ) _\u2264_ _V \u2264_ _f_ ( _Z_ ) + _c ._ (D.10) _Then, for all t >_ 0 _, the following upper bound holds:_ E[ _e_ _[tV]_ _|Z_ ] _\u2264_ _e_ _[t]_ [2] _[c]_ [2] _[/]_ [8] _._ (D.11) Proof: The proof follows using the same steps as in that of lemma D.1 with conditional expectations used instead of expectations: conditioned on _Z_, _V_ takes values in [ _a, b_ ] with _a_ = _f_ ( _Z_ ) and _b_ = _f_ ( _Z_ ) + _c_ and its expectation vanishes. The lemma is used to prove the following theorem, which is one of the main results of this section. **Theorem D.7 (Azuma\u2019s inequality)** _Let V_ 1 _, V_ 2 _, . . . be a martingale difference sequence with re-_ _spect to the random variables X_ 1 _, X_ 2 _, . . ., and assume that for all i >_ 0 _there is a constant c_ _i_ _\u2265_ 0 _and random variable Z_ _i_ _, which is a function of X_ 1 _, . . ., X_ _i\u2212_ 1 _, that satisfy_ _Z_ _i_ _\u2264_ _V_ _i_ _\u2264_ _Z_ _i_ + _c_ _i_ _._ (D.12) _Then, for all \u03f5 >_ 0 _and m, the following inequalities hold:_ _m_ _\u2212_ 2 _\u03f5_ [2] P \ufffd _V_ _i_ _\u2265_ _\u03f5_ _\u2264_ exp _m_ \ufffd _i_ =1 \ufffd \ufffd \ufffd _i_ =1 _[c]_ [2] _i_ _m_ _\u2212_ 2 _\u03f5_ [2] P \ufffd _V_ _i_ _\u2264\u2212\u03f5_ _\u2264_ exp _m_ \ufffd _i_ =1 \ufffd \ufffd \ufffd _i_ =1 _[c]_ [2] _i_ (D.13) \ufffd _._ (D.14) \ufffd **442** **Appendix D** **Concentration Inequalities** Proof: For any _k \u2208_ [ _m_ ], let _S_ _k_ = [\ufffd] _[k]_ _i_ =1 _[V]_ _[i]_ [. Then, using Chernoff\u2019s bounding technique, for any] _t >_ 0, we can write P \ufffd _S_ _m_ _\u2265_ _\u03f5_ \ufffd _\u2264_ _e_ _[\u2212][t\u03f5]_ E[ _e_ _[tS]_ _[m]_ ] = _e_ _[\u2212][t\u03f5]_ E \ufffd _e_ _[tS]_ _[m][\u2212]_ [1] E[ _e_ _[tV]_ _[m]_ _|X_ 1 _, . . ., X_ _m\u2212_ 1 ]\ufffd _\u2264_ _e_ _[\u2212][t\u03f5]_ E[ _e_ _[tS]_ _[m][\u2212]_ [1] ] _e_ _[t]_ [2] _[c]_ _m_ [2] _[/]_ [8] (lemma D.6) _\u2264_ _e_ _[\u2212][t\u03f5]_ _e_ _[t]_ [2] [ \ufffd] _i_ _[m]_ =1 _[c]_ [2] _i_ _[/]_ [8] (iterating previous argument) = _e_ _[\u2212]_ [2] _[\u03f5]_ [2] _[/]_ [ \ufffd] _i_ _[m]_ =1 _[c]_ [2] _i_ _,_ where we chose _t_ = 4 _\u03f5/_ [\ufffd] _[m]_ _i_ =1 _[c]_ [2] _i_ [to minimize the upper bound. This proves the first statement of] the theorem, and the second statement is shown in a similar way. **D.7** **McDiarmid\u2019s inequality** The following is the main result of this section. Its proof makes use of Azuma\u2019s inequality. **Theorem D.8 (McDiarmid\u2019s inequality)** _Let X_ 1 _, . .",
    "chunk_id": "foundations_machine_learning_427"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "., X_ _m_ _\u2208_ X _[m]_ _be a set of m \u2265_ 1 _independent_ _random variables and assume that there exist c_ 1 _, . . ., c_ _m_ _>_ 0 _such that f_ : X _[m]_ _\u2192_ R _satisfies the_ _following conditions:_ _\u2032_ \ufffd\ufffd _f_ ( _x_ 1 _, . . ., x_ _i_ _, . . ., x_ _m_ ) _\u2212_ _f_ ( _x_ 1 _, . . ., x_ _i_ _[, . . . x]_ _[m]_ [)] \ufffd\ufffd _\u2264_ _c_ _i_ _,_ (D.15) _for all i \u2208_ [ _m_ ] _and any points x_ 1 _, . . ., x_ _m_ _, x_ _[\u2032]_ _i_ _[\u2208]_ [X] _[. Let][ f]_ [(] _[S]_ [)] _[ denote][ f]_ [(] _[X]_ [1] _[, . . ., X]_ _[m]_ [)] _[, then, for all]_ _\u03f5 >_ 0 _, the following inequalities hold:_ _\u2212_ 2 _\u03f5_ [2] P[ _f_ ( _S_ ) _\u2212_ E[ _f_ ( _S_ )] _\u2265_ _\u03f5_ ] _\u2264_ exp _m_ \ufffd \ufffd _i_ =1 _[c]_ [2] _i_ _\u2212_ 2 _\u03f5_ [2] P[ _f_ ( _S_ ) _\u2212_ E[ _f_ ( _S_ )] _\u2264\u2212\u03f5_ ] _\u2264_ exp _m_ \ufffd \ufffd _i_ =1 _[c]_ [2] _i_ (D.16) \ufffd _._ (D.17) \ufffd Proof: Define a sequence of random variables _V_ _k_, _k \u2208_ [ _m_ ], as follows: _V_ = _f_ ( _S_ ) _\u2212_ E[ _f_ ( _S_ )], _V_ 1 = E[ _V |X_ 1 ] _\u2212_ E[ _V_ ], and for _k >_ 1, _V_ _k_ = E[ _V |X_ 1 _, . . ., X_ _k_ ] _\u2212_ E[ _V |X_ 1 _, . . ., X_ _k\u2212_ 1 ] _._ Note that _V_ = [\ufffd] _[m]_ _k_ =1 _[V]_ _[k]_ [. Furthermore, the random variable][ E][[] _[V][ |][X]_ [1] _[, . . ., X]_ _[k]_ [] is a function of] _X_ 1 _, . . ., X_ _k_ . Conditioning on _X_ 1 _, . . ., X_ _k\u2212_ 1 and taking its expectation is therefore: E \ufffd E[ _V |X_ 1 _, . . ., X_ _k_ ] _|X_ 1 _, . . ., X_ _k\u2212_ 1 \ufffd = E[ _V |X_ 1 _, . . ., X_ _k\u2212_ 1 ] _,_ which implies E[ _V_ _k_ _|X_ 1 _, . . ., X_ _k\u2212_ 1 ] = 0. Thus, the sequence ( _V_ _k_ ) _k\u2208_ [ _m_ ] is a martingale difference sequence. Next, observe that, since E[ _f_ ( _S_ )] is a scalar, _V_ _k_ can be expressed as follows: _V_ _k_ = E[ _f_ ( _S_ ) _|X_ 1 _, . . ., X_ _k_ ] _\u2212_ E[ _f_ ( _S_ ) _|X_ 1 _, . . ., X_ _k\u2212_ 1 ] _._ Thus, we can define an upper bound _W_ _k_ and lower bound _U_ _k_ for _V_ _k_ by: _W_ _k_ = sup E[ _f_ ( _S_ ) _|X_ 1 _, . . ., X_ _k\u2212_ 1 _, x_ ] _\u2212_ E[ _f_ ( _S_ ) _|X_ 1 _, . . ., X_ _k\u2212_ 1 ] _x_ _U_ _k_ =",
    "chunk_id": "foundations_machine_learning_428"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "inf _x_ [E][[] _[f]_ [(] _[S]_ [)] _[|][X]_ [1] _[, . . ., X]_ _[k][\u2212]_ [1] _[, x]_ []] _[ \u2212]_ [E][[] _[f]_ [(] _[S]_ [)] _[|][X]_ [1] _[, . . ., X]_ _[k][\u2212]_ [1] []] _[.]_ Now, by (D.15), for any _k \u2208_ [ _m_ ], the following holds: _W_ _k_ _\u2212_ _U_ _k_ = sup (D.18) _x,x_ _[\u2032]_ [ E][[] _[f]_ [(] _[S]_ [)] _[|][X]_ [1] _[, . . ., X]_ _[k][\u2212]_ [1] _[, x]_ []] _[ \u2212]_ [E][[] _[f]_ [(] _[S]_ [)] _[|][X]_ [1] _[, . . ., X]_ _[k][\u2212]_ [1] _[, x]_ _[\u2032]_ []] _[ \u2264]_ _[c]_ _[k]_ _[,]_ thus, _U_ _k_ _\u2264_ _V_ _k_ _\u2264_ _U_ _k_ + _c_ _k_ . In view of these inequalities, we can apply Azuma\u2019s inequality to _V_ = [\ufffd] _[m]_ _k_ =1 _[V]_ _[k]_ [, which yields exactly (D.16) and (D.17).] McDiarmid\u2019s inequality is used in several of the proofs in this book. It can be understood in terms of stability: if changing any of its argument affects _f_ only in a limited way, then, its deviations from its mean can be exponentially bounded. Note also that Hoeffding\u2019s in **D.8** **Normal distribution tails: Lower bound** **443** equality (theorem D.2) is a special instance of McDiarmid\u2019s inequality where _f_ is defined by 1 _m_ _f_ : ( _x_ 1 _, . . ., x_ _m_ ) _\ufffd\u2192_ _m_ \ufffd _i_ =1 _[x]_ _[i]_ [.] **D.8** **Normal distribution tails: Lower bound** If _N_ is a random variable following the standard normal distribution, then for _u >_ 0, _\u0338_ _\u0338_ \ufffd1 _\u2212_ ~~\ufffd~~ 1 _\u2212_ _e_ _[\u2212][u]_ [2] [\ufffd] _._ (D.19) _\u0338_ _\u0338_ P[ _N \u2265_ _u_ ] _\u2265_ [1] 2 _\u0338_ _\u0338_ P[ _N \u2265_ _u_ ] _\u2265_ [1] _\u0338_ _\u0338_ **D.9** **Khintchine-Kahane inequality** The following inequality is useful in a variety of different contexts, including in the proof of a lower bound for the empirical Rademacher complexity of linear hypotheses (chapter 6). **Theorem D.9 (Khintchine-Kahane inequality)** _Let_ (H _, \u2225\u00b7 \u2225_ ) _be a normed vector space and let_ **x** 1 _, . . .,_ **x** _m_ _be m \u2265_ 1 _elements of_ H _. Let_ _**\u03c3**_ = ( _\u03c3_ 1 _, . . ., \u03c3_ _m_ ) _[\u22a4]_ _with \u03c3_ _i_ _s independent uniform random_ _variables taking values in {\u2212_ 1 _,_ +1 _} (Rademacher variables). Then, the following inequalities hold:_ _\u0338_ _\u0338_ \ufffd\ufffd\ufffd\ufffd _\u0338_ _\u0338_ _m_ 2 \ufffd _i_ =1 _\u03c3_ _i_ **x** _i_ \ufffd\ufffd\ufffd\ufffd\ufffd _\u2264_ E _**\u03c3**_ _\u0338_ _\u0338_ \ufffd\ufffd\ufffd\ufffd _\u0338_ _\u0338_ _m_ 2 [\ufffd] \ufffd _\u03c3_ _i_ **x** _i_ \ufffd\ufffd\ufffd _._ (D.20) _i_ =1 _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ 1 2 [E] _**\u03c3**_ _\u0338_ _\u0338_ \ufffd\ufffd\ufffd\ufffd _\u0338_ _\u0338_ _m_ 2 [\ufffd] \ufffd _i_ =1 _\u03c3_ _i_ **x** _i_ \ufffd\ufffd\ufffd _\u2264_ \ufffd E _**\u03c3**_ _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ Proof: The second inequality is a direct consequence of the convexity of _x \ufffd\u2192_ _x_ [2] and Jensen\u2019s inequality (theorem B.20). To prove the left-hand side inequality, first note that for any _\u03b2_ 1 _, . . ., \u03b2_ _m_ _\u2208_ R, expanding the product [\ufffd] _[m]_ _i_",
    "chunk_id": "foundations_machine_learning_429"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "=1 [(1 +] _[ \u03b2]_ _[i]_ [) leads exactly to the sum of all monomials] _[ \u03b2]_ 1 _[\u03b4]_ [1] _[\u00b7 \u00b7 \u00b7][ \u03b2]_ _m_ _[\u03b4]_ _[m]_ [, with exponents] _\u03b4_ 1 _, . . ., \u03b4_ _m_ in _{_ 0 _,_ 1 _}_ . We will use the notation _\u03b2_ 1 _[\u03b4]_ [1] _[\u00b7 \u00b7 \u00b7][ \u03b2]_ _m_ _[\u03b4]_ _[m]_ = _\u03b2_ _**[\u03b4]**_ and _|_ _**\u03b4**_ _|_ = [\ufffd] _[m]_ _i_ =1 _[\u03b4]_ _[m]_ [ for any] _**\u03b4**_ = ( _\u03b4_ 1 _, . . ., \u03b4_ _m_ ) _\u2208{_ 0 _,_ 1 _}_ _[m]_ . In view of that, for any ( _\u03b1_ 1 _, . . ., \u03b1_ _m_ ) _\u2208_ R _[m]_ and _t >_ 0, the following equality holds: _\u0338_ _\u0338_ _m_ _t_ [2] \ufffd _\u0338_ _\u0338_ \ufffd _\u03b1_ _**[\u03b4]**_ _/t_ _[|]_ _**[\u03b4]**_ _[|]_ = \ufffd _**\u03b4**_ _\u2208{_ 0 _,_ 1 _}_ _[m]_ _**\u03b4**_ _\u2208{_ 0 _,_ 1 _\u0338_ _\u0338_ \ufffd(1 + _\u03b1_ _i_ _/t_ ) = _t_ [2] \ufffd _i_ =1 _**\u03b4**_ 0 1 _\u0338_ _\u0338_ \ufffd _t_ [2] _[\u2212|]_ _**[\u03b4]**_ _[|]_ _\u03b1_ _**[\u03b4]**_ _._ _**\u03b4**_ _\u2208{_ 0 _,_ 1 _}_ _[m]_ _\u0338_ _\u0338_ Differentiating both sides with respect to _t_ and setting _t_ = 1 yields _\u0338_ _\u0338_ \ufffd(1 + _\u03b1_ _i_ ) = \ufffd _i_ = _\u0338_ _j_ _**\u03b4**_ _\u2208{_ 0 _,_ 1 _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ \ufffd _\u03b1_ _j_ \ufffd _j_ =1 _i_ = _\u0338_ _j_ _\u0338_ \ufffd (2 _\u2212|_ _**\u03b4**_ _|_ ) _\u03b1_ _**[\u03b4]**_ _._ (D.21) _\u0338_ _**\u03b4**_ _\u2208{_ 0 _,_ 1 _}_ _[m]_ _\u0338_ 2 _\u0338_ _\u0338_ _m_ \ufffd(1 + _\u03b1_ _i_ ) _\u2212_ _i_ =1 _\u0338_ _\u0338_ _\u0338_ For any _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_, let _S_ _**\u03c3**_ be defined by _S_ _**\u03c3**_ = _\u2225s_ _**\u03c3**_ _\u2225_ with _s_ _**\u03c3**_ = [\ufffd] _[m]_ _i_ =1 _[\u03c3]_ _[i]_ **[x]** _[i]_ [. Then, setting] _\u03b1_ _i_ = _\u03c3_ _i_ _\u03c3_ _i_ _[\u2032]_ [, multiplying both sides of (D.21) by] _[ S]_ _**[\u03c3]**_ _[S]_ _**[\u03c3]**_ _[\u2032]_ [, and taking the sum over all] _**[ \u03c3]**_ _[,]_ _**[ \u03c3]**_ _[\u2032]_ _[ \u2208]_ _{\u2212_ 1 _,_ +1 _}_ _[m]_ yields _\u0338_ _\u0338_ _m_ \ufffd _\u0338_ _\u0338_ \ufffd _\u03c3_ _j_ _\u03c3_ _j_ _[\u2032]_ \ufffd _j_ =1 _i_ = _\u0338_ _j_ _\u0338_ \ufffd(1 + _\u03c3_ _i_ _\u03c3_ _i_ _[\u2032]_ [)] _S_ _**\u03c3**_ _S_ _**\u03c3**_ _\u2032_ _i_ = _\u0338_ _j_ \ufffd _\u0338_ \ufffd 2 _**\u03c3**_ _,_ _**\u03c3**_ _[\u2032]_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ \ufffd _\u0338_ _\u0338_ _m_ \ufffd(1 + _\u03c3_ _i_ _\u03c3_ _i_ _[\u2032]_ [)] _[ \u2212]_ _i_ =1 _\u0338_ _\u0338_ _\u0338_ = \ufffd _**\u03c3**_ _,_ _**\u03c3**_ _[\u2032]_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _\u0338_ _\u0338_ \ufffd (2 _\u2212|_ _**\u03b4**_ _|_ ) _\u03c3_ _**[\u03b4]**_ _\u03c3_ _[\u2032]_ _**[\u03b4]**_ _S_ _**\u03c3**_ _S_ _**\u03c3**_ _\u2032_ _**\u03b4**_ _\u2208{_ 0 _,_ 1 _}_ _[m]_ _\u0338_ _\u0338_ \ufffd (2 _\u2212|_ _**\u03b4**_ _|_ ) \ufffd _**\u03b4**_ _\u2208{_ 0 _,_ 1 _}_ _[m]_ _**\u03c3**_ _,_ _**\u03c3**_ _[\u2032]_ _\u2208{\u2212_ 1 \ufffd (2 _\u2212|_ _**\u03b4**_ _|_ )\ufffd \ufffd _**\u03b4**_ _\u2208{_ 0 _,_ 1 _}_ _[m]_ _**\u03c3**_ _\u2208{\u2212_ 1 _,_ _\u0338_ _\u0338_ = \ufffd _\u0338_ _\u0338_ \ufffd _\u03c3_ _**[\u03b4]**_ _\u03c3_ _[\u2032]_ _**[\u03b4]**_ _S_ _**\u03c3**_ _S_",
    "chunk_id": "foundations_machine_learning_430"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_**\u03c3**_ _\u2032_ _**\u03c3**_ _,_ _**\u03c3**_ _[\u2032]_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _\u0338_ _\u0338_ (D.22) _\u0338_ _\u0338_ = \ufffd _\u0338_ _\u0338_ 2 \ufffd _\u03c3_ _**[\u03b4]**_ _S_ _**\u03c3**_ \ufffd _._ _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _\u0338_ _\u0338_ Note that the terms of the right-hand sum with _|_ _**\u03b4**_ _| \u2265_ 2 are non-positive. The terms with _|_ _**\u03b4**_ _|_ = 1 are null: since _S_ _**\u03c3**_ = _S_ _\u2212_ _**\u03c3**_, we have [\ufffd] _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _[ \u03c3]_ _**[\u03b4]**_ _[S]_ _**[\u03c3]**_ [ = 0 in that case. Thus, the right-hand] 2 side can be upper bounded by the term with _**\u03b4**_ = 0, that is, 2\ufffd\ufffd _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _[ S]_ _**[\u03c3]**_ \ufffd . The left-hand **444** **Appendix D** **Concentration Inequalities** side of (D.22) can be rewritten as follows: \ufffd (2 _[m]_ [+1] _\u2212_ _m_ 2 _[m][\u2212]_ [1] ) _S_ _**\u03c3**_ [2] [+ 2] _[m]_ \ufffd (2 _[m]_ [+1] _\u2212_ _m_ 2 _[m][\u2212]_ [1] ) _S_ _**\u03c3**_ [2] [+ 2] _[m][\u2212]_ [1] \ufffd _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _**\u03c3**_ _\u2208{\u2212_ 1 _,_ _S_ _**\u03c3**_ _S_ _**\u03c3**_ _\u2032_ _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _**\u03c3**_ _[\u2032]_ _\u2208B_ ( _**\u03c3**_ _,_ 1) _,_ (D.23) \ufffd = 2 _[m]_ \ufffd \ufffd _S_ _**\u03c3**_ _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ \ufffd _S_ _**\u03c3**_ [2] [+ 2] _[m][\u2212]_ [1] \ufffd _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _**\u03c3**_ _\u2208{\u2212_ 1 _,_ \ufffd _S_ _**\u03c3**_ _\u2032_ _\u2212_ ( _m \u2212_ 2) _S_ _**\u03c3**_ \ufffd _**\u03c3**_ _[\u2032]_ _\u2208B_ ( _**\u03c3**_ _,_ 1) \ufffd \ufffd where _B_ ( _**\u03c3**_ _,_ 1) denotes the set of _**\u03c3**_ _[\u2032]_ that differ from _**\u03c3**_ in exactly one coordinate _j \u2208_ [ _m_ ], that is the set of _**\u03c3**_ _[\u2032]_ with Hamming distance one from _**\u03c3**_ . Note that for any such _**\u03c3**_ _[\u2032]_, _s_ _**\u03c3**_ _\u2212_ _s_ _**\u03c3**_ _\u2032_ = 2 _\u03c3_ _j_ **x** _j_ for one coordinate _j \u2208_ [ _m_ ], thus, [\ufffd] _**\u03c3**_ _[\u2032]_ _\u2208B_ ( _**\u03c3**_ _,_ 1) _[s]_ _**[\u03c3]**_ _[ \u2212]_ _[s]_ _**[\u03c3]**_ _[\u2032]_ [ = 2] _[s]_ _**[\u03c3]**_ [. In light of that and using the triangle] inequality, we can write \ufffd _s_ _**\u03c3**_ \ufffd\ufffd\ufffd _\u2212_ \ufffd\ufffd\ufffd \ufffd _**\u03c3**_ _[\u2032]_ _\u2208B_ ( _**\u03c3**_ _,_ 1) _**\u03c3**_ _[\u2032]_ _\u2208B_ ( \ufffd _s_ _**\u03c3**_ _\u2032_ \ufffd\ufffd\ufffd _\u2264_ \ufffd _**\u03c3**_ _[\u2032]_ _\u2208B_ ( _**\u03c3**_ _,_ 1) _**\u03c3**_ _[\u2032]_ _\u2208B_ ( _**\u03c3**_ ( _m \u2212_ 2) _S_ _**\u03c3**_ = _\u2225ms_ _**\u03c3**_ _\u2225\u2212\u2225_ 2 _s_ _**\u03c3**_ _\u2225_ = \ufffd\ufffd\ufffd \ufffd \ufffd _s_ _**\u03c3**_ _\u2212_ _s_ _**\u03c3**_ _\u2032_ \ufffd\ufffd\ufffd _**\u03c3**_ _[\u2032]_ _\u2208B_ ( _**\u03c3**_ _,_ 1) _\u2264_ \ufffd\ufffd\ufffd \ufffd \ufffd _S_ _**\u03c3**_ _\u2032_ _._ _**\u03c3**_ _[\u2032]_ _\u2208B_ ( _**\u03c3**_ _,_ 1) Thus, the second sum of (D.23) is non-negative and the left-hand side of (D.22) can be lower bounded by the first sum 2 _[m]_ [ \ufffd] _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _[ S]_ _**\u03c3**_ [2] [. Combining this with the upper bound found for] (D.22) gives \ufffd _S_ _**\u03c3**_ [2] _[\u2264]_ [2] \ufffd \ufffd _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ _**\u03c3**_ _\u2208{\u2212_ 1 _,_ 2",
    "chunk_id": "foundations_machine_learning_431"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[m]_ \ufffd 2 \ufffd _S_ _**\u03c3**_ \ufffd _._ _**\u03c3**_ _\u2208{\u2212_ 1 _,_ +1 _}_ _[m]_ Dividing both sides by 2 [2] _[m]_ and using P[ _**\u03c3**_ ] = 1 _/_ 2 _[m]_ gives E _**\u03c3**_ [ _S_ _**\u03c3**_ [2] []] _[ \u2264]_ [2(][E] _**[\u03c3]**_ [[] _[S]_ _**[\u03c3]**_ [])] [2] [ and completes the] proof. The constant 1 _/_ 2 appearing in the first inequality of (D.20) is optimal. To see this, consider the case where _m_ = 2 and **x** 1 = **x** 2 = **x** for some non-zero vector **x** _\u2208_ H. Then, the left-hand side of the first inequality is [1] 2 \ufffd _mi_ =1 _[\u2225]_ **[x]** _[i]_ _[\u2225]_ [2] [ =] _[ \u2225]_ **[x]** _[\u2225]_ [2] [ and the right-hand side] \ufffd E _**\u03c3**_ \ufffd _\u2225_ ( _\u03c3_ 1 + _\u03c3_ 2 ) **x** _\u2225_ \ufffd\ufffd 2 = _\u2225_ **x** _\u2225_ [2] (E _**\u03c3**_ [ _|\u03c3_ 1 + _\u03c3_ 2 _|_ ]) [2] = _\u2225_ **x** _\u2225_ [2] . Note that when the norm _\u2225\u00b7 \u2225_ corresponds to an inner product, as in the case of a Hilbert space H, we can write _m_ \ufffd _\u2225_ **x** _i_ _\u2225_ [2] _,_ _i_ =1 _m_ \ufffd E _**\u03c3**_ _i,j_ =1 _m_ 2 [\ufffd] \ufffd _\u03c3_ _i_ **x** _i_ \ufffd\ufffd\ufffd = _i_ =1 _\u03c3_ _i_ _\u03c3_ _j_ ( **x** _i_ _\u00b7_ **x** _j_ ) = \ufffd \ufffd _m_ \ufffd E _**\u03c3**_ [[] _[\u03c3]_ _[i]_ _[\u03c3]_ _[j]_ [](] **[x]** _[i]_ _[ \u00b7]_ **[ x]** _[j]_ [) =] _i,j_ =1 E _**\u03c3**_ \ufffd\ufffd\ufffd\ufffd since by the independence of the random variables _\u03c3_ _i_, for _i \u0338_ = _j_, E _**\u03c3**_ [ _\u03c3_ _i_ _\u03c3_ _j_ ] = E _**\u03c3**_ [ _\u03c3_ _i_ ] E _**\u03c3**_ [ _\u03c3_ _j_ ] = 0. Thus, (D.20) can then be rewritten as follows: \ufffd\ufffd\ufffd\ufffd _m_ 2 \ufffd _i_ =1 _\u03c3_ _i_ **x** _i_ \ufffd\ufffd\ufffd\ufffd\ufffd _\u2264_ _m_ \ufffd _\u2225_ **x** _i_ _\u2225_ [2] _._ (D.24) _i_ =1 1 2 _m_ \ufffd _i_ =1 _\u2225_ **x** _i_ _\u2225_ [2] _\u2264_ \ufffd E _**\u03c3**_ _m_ \ufffd **D.10** **Maximal inequality** The following gives an upper bound on the expectation of the maximum of a finite set of random variables that is useful in several contexts. **Theorem D.10 (Maximal inequality)** _Let X_ 1 _. . . X_ _n_ _be n \u2265_ 1 _real-valued random variables such_ _t_ [2] _r_ [2] _that for all j \u2208_ [ _n_ ] _and t >_ 0 _,_ E[ _e_ _[tX]_ _[j]_ ] _\u2264_ _e_ 2 _that for all j \u2208_ [ _n_ ] _and t >_ 0 _,_ E[ _e_ _[j]_ ] _\u2264_ _e_ 2 _for some r >_ 0 _. Then, the following inequality_ _holds:_ E \ufffd _j_ max _\u2208_ [ _n_ ] _[X]_ _[j]_ \ufffd _\u2264_ _r_ ~~\ufffd~~ 2 log _n._ 2 log _n._ Proof: For any _t >_ 0, by the convexity of exp and Jensen\u2019s inequality, the following holds: _t_ [2] _r_ [2] _e_ _[tX]_ _[j]_ _\u2264_ _ne_ 2 \ufffd _j\u2208_ [ _n_ ] _e_ _[t]_ [ E][[max] _[j][\u2208]_ [[] _[n]_ []] _[ X]_ _[j]_ []] _\u2264_ E[ _e_ _[t]_ [ max]",
    "chunk_id": "foundations_machine_learning_432"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_[j][\u2208]_ [[] _[n]_ []] _[ X]_ _[j]_ ] = E max _\u2264_ E \ufffd _j\u2208_ [ _n_ ] _[e]_ _[tX]_ _[j]_ \ufffd \ufffd\ufffd 2 _._ **D.11** **Chapter notes** **445** Taking the log of both sides yields [g] _[ n]_ + _[tr]_ [2] _t_ 2 E \ufffd _j_ max _\u2208_ [ _n_ ] _[X]_ _[j]_ \ufffd _\u2264_ [lo][g] _t_ _[ n]_ (D.25) 2 _[.]_ Choosing _t_ = _\u221a_ 2 lo _r_ g _n_, which minimizes the right-hand side, gives the upper bound _r_ _[\u221a]_ 2 log _n_ . \u25a1 Note that, in view of the expression of their moment-generating function (equation (C.24)), for standard Gaussian random variables _X_ _j_, the assumptions of the theorem hold as equalities: _t_ [2] E[ _e_ _[tX]_ _[j]_ ] = _e_ 2 2 . **Corollary D.11 (Maximal inequality)** _Let X_ 1 _. . . X_ _n_ _be n \u2265_ 1 _real-valued random variables such_ _that for all j \u2208_ [ _n_ ] _, X_ _j_ = [\ufffd] _[m]_ _i_ =1 _[Y]_ _[ij]_ _[ where, for each fixed][ j][ \u2208]_ [[] _[n]_ []] _[,][ Y]_ _[ij]_ _[ are independent zero mean]_ _random variables taking values in_ [ _\u2212r_ _i_ _,_ + _r_ _i_ ] _, for some r_ _i_ _>_ 0 _. Then, the following inequality_ _holds:_ E \ufffd _j_ max _\u2208_ [ _n_ ] _[X]_ _[j]_ \ufffd _\u2264_ _r_ ~~\ufffd~~ 2 log _n,_ _m_ _with r_ = ~~\ufffd~~ \ufffd _i_ =1 _[r]_ _i_ [2] _[.]_ Proof: By the independence of the _Y_ _ij_ s for fixed _j_ and Hoeffding\u2019s lemma (Lemma D.1), the following inequality holds for all _j \u2208_ [ _n_ ]: _t_ [2] _rj_ [2] _t_ [2] _r_ [2] 2 = _e_ 2 _,_ (D.26) _m_ _e_ \ufffd _i_ =1 E[ _e_ _[tX]_ _[j]_ ] = E \ufffd \ufffd _[m]_ _e_ _[tY]_ _[ij]_ \ufffd = _i_ =1 _m_ \ufffd E[ _e_ _[tY]_ _[ij]_ ] _\u2264_ _i_ =1 with _r_ [2] = [\ufffd] _[m]_ _i_ =1 _[r]_ _i_ [2] [. The result then follows immediately by Theorem D.10.] **D.11** **Chapter notes** Several of the concentration inequalities presented in this chapter are based on a bounding technique due to Chernoff [1952]. Theorem D.3 is due to Sanov [1957]. For the exponential inequality of exercise D.7, which is an alternative form of Sanov\u2019s inequality, see [Hagerup and R\u00a8ub, 1990] and the references therein. The multiplicative Chernoff bounds presented in this chapter (Theorem D.4) were given by Angluin and Valiant [1979]. Hoeffding\u2019s inequality and lemma (Lemma D.1 and Theorem D.2) are due to Hoeffding [1963]. The improved version of Azuma\u2019s inequality [Hoeffding, 1963, Azuma, 1967] presented in this chapter is due to McDiarmid [1989]. The improvement is a reduction of the exponent by a factor of 4. This also appears in McDiarmid\u2019s inequality, which is derived from the inequality for bounded martingale sequences. The inequalities presented in exercise D.6 are due to Bernstein [1927] and Bennett [1962]; the exercise is from Devroye and Lugosi [1995]. The binomial inequality of section D.5 is due to Slud [1977]. The tail bound of section D.8 is due to Tate [1953] (see also Anthony",
    "chunk_id": "foundations_machine_learning_433"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "and Bartlett [1999]). The Khintchine-Kahane inequality was first studied in the case of real-valued variables _x_ 1 _, . . ., x_ _m_ by Khintchine [1923], with better constants and simpler proofs later provided by Szarek [1976], Haagerup [1982], and Tomaszewski [1982]. The inequality was extended to normed vector spaces by Kahane [1964]. The proof presented here is due to Lata\ufffdla and Oleszkiewicz [1994] and provides the best possible constants. **D.12** **Exercises** D.1 Twins paradox. Professor Mamoru teaches at a university whose computer science and math building has _F_ = 30 floors. (1) Assume that the floors are independent and that they have the same probability to be selected by someone taking the elevator. How many people should take the elevator in order to make it likely (probability more than half) that two of them go to the same floor? **446** **Appendix D** **Concentration Inequalities** ( _Hint_ : use the Taylor series expansion of _e_ _[\u2212][x]_ = 1 _\u2212_ _x_ + _. . ._ and give an approximate general expression of the solution.) (2) Professor Mamoru is popular, and his floor is in fact more likely to be selected than others. Assuming that all other floors are equiprobable, derive the general expression of the probability that two people go to the same floor, using the same approximation as before. How many people should take the elevator in order to make it likely that two of them go to the same floor when the probability of Professor Mamoru\u2019s floor is _._ 25, _._ 35, or _._ 5? When _q_ = _._ 5, would the answer change if the number of floors were instead _F_ = 1 _,_ 000? (3) The probability models assumed in (1) and (2) are both naive. If you had access to the data collected by the elevator guard, how would you define a more faithful model? D.2 Estimating label bias. Let D be a distribution over X and let _f_ : X _\u00d7 {\u2212_ 1 _,_ +1 _}_ be a labeling function. Suppose we wish to find a good approximation of the label bias of the distribution D, that is of _p_ + defined by: _p_ + = _x\u223c_ P D [[] _[f]_ [(] _[x]_ [) = +1]] _[.]_ (D.27) Let S be a finite labeled sample of size _m_ drawn i.i.d. according to D. Use S to derive an estimate \ufffd _p_ + of _p_ + . Show that for any _\u03b4 >_ 0, with probability at least 1 _\u2212_ _\u03b4_, _|p_ + _\u2212_ _p_ \ufffd + _| \u2264_ ~~\ufffd~~ log(2 _m_ 2 _/\u03b4_ ) . log(2 _/\u03b4_ ) 2 _m_ . D.3 Biased coins. Professor Moent has two coins in his pocket, coin _x_ _A_ and coin _x_ _B_ . Both coins are slightly biased, i.e., P[ _x_ _A_ = 0] = 1 _/_ 2 _\u2212_ _\u03f5/_ 2 and P[ _x_ _B_ = 0] = 1 _/_ 2 + _\u03f5/_ 2, where 0 _< \u03f5 <_ 1 is a small positive number, 0 denotes heads and 1 denotes tails. He likes to play the following game with",
    "chunk_id": "foundations_machine_learning_434"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "his students. He picks a coin _x \u2208{x_ _A_ _, x_ _B_ _}_ from his pocket uniformly at random, tosses it _m_ times, reveals the sequence of 0s and 1s he obtained and asks which coin was tossed. Determine how large _m_ needs to be for a student\u2019s coin prediction error to be at most _\u03b4 >_ 0. (a) Let _S_ be a sample of size _m_ . Professor Moent\u2019s best student, Oskar, plays according to the decision rule _f_ _o_ : _{_ 0 _,_ 1 _}_ _[m]_ _\u2192{x_ _A_ _, x_ _B_ _}_ defined by _f_ _o_ ( _S_ ) = _x_ _A_ iff _N_ ( _S_ ) _< m/_ 2, where _N_ ( _S_ ) is the number of 0\u2019s in sample _S_ . Suppose _m_ is even, then show that _error_ ( _f_ _o_ ) _\u2265_ [1] 2 _N_ ( _S_ ) _\u2265_ _[m]_ 2 [P] \ufffd 2 _x_ = _x_ _A_ _._ (D.28) \ufffd\ufffd\ufffd \ufffd (b) Assuming _m_ even, show that _error_ ( _f_ _o_ ) _>_ [1] 4 _\u2212_ _[m\u03f5]_ [2] 1 _\u2212_ 1 _\u2212_ _e_ 1 _\u2212\u03f5_ \ufffd \ufffd 2 [\ufffd] _._ (D.29) 1 _[m\u03f5]_ _\u2212\u03f5_ [2][2] [\ufffd] 2 [1] (c) Argue that if _m_ is odd, the probability can be lower bounded by using _m_ +1 in the bound in (a) and conclude that for both odd and even _m_, _\u2212_ [2] _[\u2308][m][/]_ [2] _[\u2309][\u03f5]_ [2] 1 _\u2212_ 1 _\u2212_ _e_ 1 _\u2212\u03f5_ [2] \ufffd \ufffd 2 [\ufffd] _._ (D.30) _error_ ( _f_ _o_ ) _>_ [1] 4 _error_ ( _f_ _o_ ) _>_ [1] _[m][/]_ [2] _[\u2309][\u03f5]_ 1 1 _\u2212\u03f5_ [2] \ufffd 2 (d) Using this bound, how large must _m_ be if Oskar\u2019s error is at most _\u03b4_, where 0 _< \u03b4 <_ 1 _/_ 4. What is the asymptotic behavior of this lower bound as a function of _\u03f5_ ? (e) Show that no decision rule _f_ : _{_ 0 _,_ 1 _}_ _[m]_ _\u2192{x_ _A_ _, x_ _B_ _}_ can do better than Oskar\u2019s rule _f_ _o_ . Conclude that the lower bound of the previous question applies to all rules. D.4 Concentration bounds. Let _X_ be a non-negative random variable satisfying P[ _X > t_ ] _\u2264_ _ce_ _[\u2212]_ [2] _[mt]_ [2] for all _t >_ 0 and some _c >_ 0. Show that E[ _X_ [2] ] _\u2264_ [lo][g(] 2 _m_ _[ce]_ [)] ( _Hint_ : to do that, use the **D.12** **Exercises** **447** identity E[ _X_ [2] ] = \ufffd 0+ _\u221e_ P[ _X_ [2] _> t_ ] _dt_, write \ufffd 0+ _\u221e_ = \ufffd 0 _u_ [+] \ufffd _u_ + _\u221e_, bound the first term by _u_ and find the best _u_ to minimize the upper bound). D.5 Comparison of Hoeffding\u2019s and Chebyshev\u2019s inequalities. Let _X_ 1 _, . . ., X_ _m_ be a sequence of random variables taking values in [0 _,_ 1] with the same mean _\u00b5_ and variance _\u03c3_ [2] _< \u221e_ and let _X_ = _m_ 1 \ufffd _mi_ =1 _[X]_ _[i]_ [.] (a) For any _\u03f5 >_ 0, give a bound",
    "chunk_id": "foundations_machine_learning_435"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "on P[ _|X \u2212_ _\u00b5| > \u03f5_ ] using Chebyshev\u2019s inequality, then Hoeffding\u2019s inequality. For what values of _\u03c3_ is Chebyshev\u2019s inequality tighter? (b) Assume that the random variables _X_ _i_ take values in _{_ 0 _,_ 1 _}_ . Show that _\u03c3_ [2] _\u2264_ 14 [.] Use this to simplify Chebyshev\u2019s inequality. Choose _\u03f5_ = _._ 05 and plot Chebyshev\u2019s inequality thereby modified and Hoeffding\u2019s inequality as a function of _m_ (you can use your preferred program for generating the plots). D.6 Bennett\u2019s and Bernstein\u2019s inequalities. The objective of this problem is to prove these two inequalities. (a) Show that for any _t >_ 0, and any random variable _X_ with E[ _X_ ] = 0, E[ _X_ [2] ] = _\u03c3_ [2], and _X \u2264_ _c_, E[ _e_ _[tX]_ ] _\u2264_ _e_ _[f]_ [(] _[\u03c3]_ [2] _[/c]_ [2] [)] _,_ (D.31) where 1 _x_ _f_ ( _x_ ) = log _._ \ufffd 1 + _x_ _[e]_ _[\u2212][ctx]_ [ +] 1 + _x_ _[e]_ _[ct]_ [\ufffd] (b) Show that _f_ _[\u2032\u2032]_ ( _x_ ) _\u2264_ 0 for _x \u2265_ 0. (c) Using Chernoff\u2019s bounding technique, show that 1 P \ufffd _m_ _m_ \ufffd _X_ _i_ _\u2265_ _\u03f5_ _\u2264_ _e_ _[\u2212][tm\u03f5]_ [+][\ufffd] _i_ _[m]_ =1 _[f]_ [(] _[\u03c3]_ _Xi_ [2] _[/c]_ [2] [)] _,_ _i_ =1 \ufffd where ( _\u03c3_ _X_ [2] _i_ [is the variance of] _[ X]_ _[i]_ [.] (d) Show that _f_ ( _x_ ) _\u2264_ _f_ (0) + _xf_ _[\u2032]_ (0) = ( _e_ _[ct]_ _\u2212_ 1 _\u2212_ _ct_ ) _x_ . (e) Using the bound derived in (4), find the optimal value of _t_ . (f) _Bennett\u2019s inequality_ . Let _X_ 1 _, . . ., X_ _m_ be independent real-valued random variables with zero mean such that for _i_ = 1 _, . . ., m_, _X_ _i_ _\u2264_ _c_ . Let _\u03c3_ [2] = _m_ 1 \ufffd _mi_ =1 _[\u03c3]_ _X_ [2] _i_ [. Show that] [2] _\u03f5c_ _c_ [2] _[ \u03b8]_ \ufffd _\u03c3_ [2] _\u03c3_ [2] 1 P \ufffd _m_ _m_ \ufffd _m_ \ufffd _i_ =1 _X_ _i_ _> \u03f5_ \ufffd _\u2264_ exp \ufffd _\u2212_ _[m\u03c3]_ _c_ [2] [2] _,_ (D.32) \ufffd\ufffd where _\u03b8_ ( _x_ ) = (1 + _x_ ) log(1 + _x_ ) _\u2212_ _x_ . (g) _Bernstein\u2019s inequality_ . Show that under the same conditions as Bennett\u2019s inequality 2 _\u03c3_ [2] + 2 _c\u03f5/_ 3 1 P \ufffd _m_ _m_ \ufffd _m_ _m\u03f5_ [2] \ufffd _i_ =1 _X_ _i_ _> \u03f5_ \ufffd _\u2264_ exp \ufffd _\u2212_ 2 _\u03c3_ [2] + 2 _._ (D.33) \ufffd ( _Hint_ : show that for all _x \u2265_ 0, _\u03b8_ ( _x_ ) _\u2265_ _h_ ( _x_ ) = [3] 2 _xx_ +3 [2] [.)] (h) Write Hoeffding\u2019s inequality assuming the same conditions. For what values of _\u03c3_ is Bernstein\u2019s inequality better than Hoeffding\u2019s inequality? **448** **Appendix D** **Concentration Inequalities** D.7 Exponential inequality. Let _X_ be a random variable following a binomial distribution _B_ ( _m, p_ ). (a) Use Sanov\u2019s inequality to show that the following _exponential inequality_ holds for any _\u03f5 >_ 0:",
    "chunk_id": "foundations_machine_learning_436"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_Xm_ _[\u2212]_ _[p > \u03f5]_ \ufffd _\u2264_ \ufffd\ufffd _p_ + _p_ _\u03f5_ \ufffd _p_ + _\u03f5_ \ufffd 1 _\u2212_ 1( _\u2212p_ + _p_ _\u03f5_ ) \ufffd 1 _\u2212_ ( _p_ + _\u03f5_ ) [\ufffd] _m_ _._ (D.34) _X_ P \ufffd _m_ (b) Use that to show that the following holds: P \ufffd _Xm_ _[\u2212]_ _[p > \u03f5]_ \ufffd _\u2264_ \ufffd _p_ + _p_ _\u03f5_ \ufffd _m_ ( _p_ + _\u03f5_ ) _e_ _[m\u03f5]_ _._ (D.35) (c) Prove that _X_ P \ufffd _m_ _X_ _[\u03f5]_ _\u2264_ _e_ _[\u2212][mp\u03b8]_ [(] _p_ _m_ _[\u2212]_ _[p > \u03f5]_ \ufffd _p_ _[\u03f5]_ [)] _,_ (D.36) where _\u03b8_ is defined as in exercise D.6. # E Notions of Information Theory This chapter introduces some basic notions of information theory useful for the presentation of several learning algorithms and their properties. The definitions and theorems are given in the case of discrete random variables or distributions, but they can be straightforwardly extended to the continuous case. We start with the notion of _entropy_, which can be viewed as a measure of the uncertainty of a random variable. **E.1** **Entropy** **Definition E.1 (Entropy)** _The_ entropy _of a discrete random variable X with probability mass_ _function p_ ( _x_ ) = P[ _X_ = _x_ ] _is denoted by_ H( _X_ ) _and defined by_ H( _X_ ) = _\u2212_ E [log( _p_ ( _X_ ))] = _\u2212_ \ufffd _p_ ( _x_ ) log( _p_ ( _x_ )) _._ (E.1) _x\u2208_ X _We define by the same expression the_ entropy of a distribution _p and abusively denote that by_ H( _p_ ) _._ The base of the logarithm is not critical in this definition since it only affects the value by a multiplicative constant. Thus, unless otherwise specified, we will consider the natural logarithm (base _e_ ). If we use base 2, then _\u2212_ log 2 ( _p_ ( _x_ )) is the number of bits needed to represent _p_ ( _x_ ). Thus, by definition, the entropy of _X_ can be viewed as the average number of bits (or amount of information) needed for the description of the random variable _X_ . By the same property, the entropy is always non-negative: H( _X_ ) _\u2265_ 0 _._ (E.2) As an example, the entropy of a biased coin _X_ _p_ taking value 1 with probability _p_ and 0 with probability 1 _\u2212_ _p_ is given by H( _X_ _p_ ) = _\u2212p_ log _p \u2212_ (1 _\u2212_ _p_ ) log(1 _\u2212_ _p_ ) _._ (E.3) The corresponding function of _p_ is often referred to as the _binary entropy function_ . Figure E.1 shows a plot of that function when using base 2 for the logarithm. As can be seen from the figure, the entropy is a concave function. [26] It reaches its maximum at _p_ = [1] 2 [, which corresponds to the] most uncertain case, and its minima at _p_ = 0 or _p_ = 1 which correspond to the fully certain cases. More generally, assume that the input space X has a finite cardinality _N \u2265_ 1. Then, by",
    "chunk_id": "foundations_machine_learning_437"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Jensen\u2019s 26 We will see later that the entropy function is always concave. **450** **Appendix E** **Notions of Information Theory** **Figure E.1** **Image:** [No caption returned] A plot of the binary entropy as a function of the bias _p_ . inequality, in view of the concavity of logarithm, the following inequality holds: _p_ ( _x_ ) _p_ ( _x_ ) 1 H( _X_ ) = E log \ufffd _p_ ( _X_ ) 1 _\u2264_ log E \ufffd \ufffd _p_ ( _X_ ) = log \ufffd \ufffd\ufffd _x\u2208_ X = log \ufffd \ufffd\ufffd = log _N._ (E.4) \ufffd Thus, more generally, the maximum value of the entropy is log _N_, that is the entropy of the uniform distribution. The entropy is a lower bound on lossless data compression and is therefore a critical quantity to consider in information theory. It is also closely related to the notions of entropy in thermodynamics and quantum physics. **E.2** **Relative entropy** Here, we introduce a measure of divergence between two distributions _p_ and _q_, _relative entropy_, which is related to the notion of entropy. The following is its definition in the discrete case. **Definition E.2 (Relative entropy)** _The_ relative entropy _(or Kullback-Leibler divergence) of two_ _distributions p and q is denoted by_ D( _p\u2225q_ ) _and defined by_ _p_ ( _x_ ) _x_ \ufffd _\u2208_ X _p_ ( _x_ ) log \ufffd _q_ ( _x_ ) _q_ ( _x_ ) D( _p\u2225q_ ) = E _p_ log _[p]_ [(] _[X]_ [)] \ufffd _q_ ( _X_ ) = \ufffd \ufffd _,_ (E.5) \ufffd _with the conventions_ 0 log 0 = 0 _,_ 0 log [0] _[a]_ 0 [= +] _[\u221e]_ _[for][ a >]_ [ 0] _[.]_ 0 [= 0] _[, and][ a]_ [ log] _[a]_ 0 Note that, in view of these conventions, whenever _q_ ( _x_ ) = 0 for some _x_ in the support of _p_ ( _p_ ( _x_ ) _>_ 0), the relative entropy is infinite: D( _p\u2225q_ ) = _\u221e_ . Thus, the relative entropy does not provide an informative measure of the divergence of _p_ and _q_ in such cases. As for the entropy, the base of the logarithm is not critical in the definition of the relative entropy and we will consider the natural logarithm unless otherwise specified. If we use base 2, the relative entropy can be interpreted in terms of coding length. Ideally, one could design for _p_ an optimal code with average length the entropy H( _p_ ). The relative entropy is the average number of additional bits needed to encode _p_ when using an optimal code for _q_ instead of one for _p_ since it can be expressed as the difference D( _p\u2225q_ ) = E _p_ [log _q_ (1 _X_ ) []] _[ \u2212]_ [H][(] _[p]_ [), which, as shown by the following] proposition, is always non-negative. **Proposition E.3 (Non-negativity of relative entropy)** _For any two distributions p and q, the_ _following inequality holds:_ D( _p\u2225q_ ) _\u2265_ 0 _._ (E.6) _Furthermore,_ D( _p\u2225q_ ) = 0 _iff p_ = _q._ **E.2** **Relative entropy** **451** Proof:",
    "chunk_id": "foundations_machine_learning_438"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "By the concavity of logarithm and Jensen\u2019s inequality, the following holds: _q_ ( _x_ ) _x_ : _p_ \ufffd ( _x_ ) _>_ 0 _p_ ( _x_ ) log \ufffd _p_ ( _x_ ) _p_ ( _x_ ) _\u2212_ D( _p\u2225q_ ) = \ufffd _p_ ( _x_ ) _\u2264_ log \ufffd \ufffd \ufffd _x_ : _p_ \ufffd ( _x_ ) _>_ 0 _p_ ( _x_ ) _p_ _[q]_ [(] ( _[x]_ _x_ [)] ) \ufffd = log \ufffd _q_ ( _x_ ) _\u2264_ log(1) = 0 _._ \ufffd _x_ : _p_ ( _x_ ) _>_ 0 \ufffd Thus, the relative entropy is always non-negative for all distributions _p_ and _q_ . The equality D( _p\u2225q_ ) = 0 can hold only if both of the inequalities above are equalities. The last one implies that [\ufffd] _x_ : _p_ ( _x_ ) _>_ 0 _[q]_ [(] _[x]_ [) = 1. Since the log function is strictly concave, the first inequality can be] an equality only if _pq_ (( _xx_ )) [is some constant] _[ \u03b1]_ [ over] _[ {][x]_ [:] _[ p]_ [(] _[x]_ [)] _[ >]_ [ 0] _[}]_ [.] Since _p_ ( _x_ ) sums to one over that set, we must have [\ufffd] _x_ : _p_ ( _x_ ) _>_ 0 _[q]_ [(] _[x]_ [) =] _[ \u03b1]_ [. Thus,] _[ \u03b1]_ [ = 1, which implies] _[ q]_ [(] _[x]_ [) =] _[ p]_ [(] _[x]_ [) for all] _x \u2208{x_ : _p_ ( _x_ ) _>_ 0 _}_ and thus for all _x_ . Finally, by definition, for any distribution _p_, D( _p\u2225p_ ) = 0, which completes the proof. The relative entropy is not a distance. It is asymmetric: in general, D( _p\u2225q_ ) _\u0338_ = D( _q\u2225p_ ) for two distributions _p_ and _q_ . Furthermore, in general, the relative entropy does not verify the triangle inequality. **Corollary E.4 (Log-sum inequality)** _For any set of non-negative real numbers a_ 1 _, . . ., a_ _n_ _and_ _b_ 1 _, . . ., b_ _n_ _, the following inequality holds:_ _n_ \ufffd _b_ _i_ _a_ _i_ \ufffd _i_ =1 _a_ _i_ log \ufffd _b_ _i_ _n_ _n_ \ufffd _i_ =1 _[a]_ _[i]_ \ufffd _\u2265_ \ufffd \ufffd _i_ =1 _a_ _i_ \ufffd log \ufffd \ufffd _ni_ =1 _[b]_ _[i]_ _,_ (E.7) \ufffd _with the conventions_ 0 log 0 = 0 _,_ 0 log [0] _[a]_ 0 [= +] _[\u221e]_ _[for][ a >]_ [ 0] _[.]_ 0 [= 0] _[, and][ a]_ [ log] _[a]_ 0 Furthermore, equality holds in (E.7) iff _[a]_ _b_ _i_ _[i]_ [is a constant (does not depend on] _[ i]_ [).] Proof: With the conventions adopted, it is clear that the equality holds if [\ufffd] _[n]_ _i_ =1 _[a]_ _[i]_ [ = 0, that is] _a_ _i_ = 0 for all _i \u2208_ [ _n_ ], or [\ufffd] _[n]_ _i_ =1 _[b]_ _[i]_ [ = 0, that is] _[ b]_ _[i]_ [ = 0 for all] _[ i][ \u2208]_ [[] _[n]_ []. Thus, we can assume that] \ufffd _ni_ =1 _[a]_ _[i]_ _[",
    "chunk_id": "foundations_machine_learning_439"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "\u0338]_ [= 0 and][ \ufffd] _[n]_ _i_ =1 _[b]_ _[i]_ _[ \u0338]_ [= 0. Since the inequality is invariant by scaling of the] _[ a]_ _[i]_ [s or] _[ b]_ _[i]_ [s, we] can multiply them by positive constants such that [\ufffd] _[n]_ _i_ =1 _[a]_ _[i]_ [ =][ \ufffd] _[n]_ _i_ =1 _[b]_ _[i]_ [ = 1. The inequality then] coincides with the non-negativity of the relative entropy of the distributions thereby defined by _a_ _i_ s and _b_ _i_ s and the result holds by Proposition E.3. **Corollary E.5 (Joint convexity of relative entropy)** _The relative entropy function_ ( _p, q_ ) _\ufffd\u2192_ D( _p\u2225q_ ) _is convex._ Proof: For any _\u03b1 \u2208_ [0 _,_ 1] and any four probability distributions _p_ 1 _, p_ 2 _, q_ 1 _, q_ 2, by the Log-sum inequality (Corollary E.4), the following holds for any fixed _x_ : _\u03b1p_ 1 ( _x_ ) + (1 _\u2212_ _\u03b1_ ) _p_ 2 ( _x_ ) ( _\u03b1p_ 1 ( _x_ ) + (1 _\u2212_ _\u03b1_ ) _p_ 2 ( _x_ )) log \ufffd _\u03b1q_ 1 ( _x_ ) + (1 _\u2212_ _\u03b1_ ) _q_ 2 ( _x_ ) \ufffd _\u03b1p_ 1 ( _x_ ) _\u2264_ _\u03b1p_ 1 ( _x_ ) log \ufffd _\u03b1q_ 1 ( _x_ ) (1 _\u2212_ _\u03b1_ ) _p_ 1 ( _x_ ) + (1 _\u2212_ _\u03b1_ ) _p_ 2 ( _x_ ) log \ufffd \ufffd (1 _\u2212_ _\u03b1_ ) _q_ 2 ( _x_ ) _._ (E.8) \ufffd Summing up these inequalities over all _x_ yields: D _\u03b1p_ 1 + (1 _\u2212_ _\u03b1_ ) _p_ 2 _\u2225\u03b1q_ 1 + (1 _\u2212_ _\u03b1_ ) _q_ 2 _\u2264_ _\u03b1_ D( _p_ 1 _\u2225q_ 1 ) + (1 _\u2212_ _\u03b1_ )D( _p_ 2 _\u2225q_ 2 ) _,_ (E.9) \ufffd \ufffd which concludes the proof. **Corollary E.6 (Concavity of the entropy)** _The entropy function p \ufffd\u2192_ H( _p_ ) _is concave._ Proof: Observe that for any fixed distribution _p_ 0 over X, by definition of the relative entropy, we can write D( _p\u2225p_ 0 ) = \ufffd _p_ ( _x_ ) log( _p_ ( _x_ )) _\u2212_ \ufffd _p_ ( _x_ ) log( _p_ 0 ( _x_ )) _._ (E.10) \ufffd _p_ ( _x_ ) log( _p_ ( _x_ )) _\u2212_ \ufffd _x\u2208_ X _x\u2208_ \ufffd _p_ ( _x_ ) log( _p_ 0 ( _x_ )) _._ (E.10) _x\u2208_ X **452** **Appendix E** **Notions of Information Theory** Thus, H( _p_ ) = _\u2212_ D( _p\u2225p_ 0 ) _\u2212_ [\ufffd] _x\u2208_ X _[p]_ [(] _[x]_ [) log(] _[p]_ [0] [(] _[x]_ [)). By Corollary E.5, the first term is a concave] function of _p_ . The second term is linear in _p_ and therefore is concave. Thus, H is concave as a sum of two concave functions. **Proposition E.7 (Pinsker\u2019s inequality)** _For any two distributions p and q, the following inequal-_ _ity holds:_ D( _p\u2225q_ ) _\u2265_ [1] 1 _[.]_ (E.11) 2 _[\u2225][p][ \u2212]_ _[q][\u2225]_ [2] Proof: We first show that the inequality holds for distributions over a set A = _{a_ 0 _, a_ 1",
    "chunk_id": "foundations_machine_learning_440"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_}_ of cardinality 2. Let _p_ 0 = _p_ ( _a_ 0 ) and _q_ 0 = _q_ ( _a_ 0 ). Fix _p_ 0 _\u2208_ [0 _,_ 1] and consider the function _f_ : _q_ 0 _\ufffd\u2192_ _f_ ( _q_ 0 ) defined by _[p]_ [0] + (1 _\u2212_ _p_ 0 ) log [1] _[ \u2212]_ _[p]_ [0] _q_ 0 1 _\u2212_ _q_ 0 _f_ ( _q_ 0 ) = _p_ 0 log _[p]_ [0] _\u2212_ 2( _p_ 0 _\u2212_ _q_ 0 ) [2] _._ (E.12) 1 _\u2212_ _q_ 0 Observe that _f_ ( _p_ 0 ) = 0 and that for _q_ 0 _\u2208_ (0 _,_ 1), _[p]_ [0] + [1] _[ \u2212]_ _[p]_ [0] _q_ 0 1 _\u2212_ _q_ 0 _f_ _[\u2032]_ ( _q_ 0 ) = _\u2212_ _[p]_ [0] 1 [1] _[ \u2212]_ _[p]_ [0] + 4( _p_ 0 _\u2212_ _q_ 0 ) = ( _q_ 0 _\u2212_ _p_ 0 ) _\u2212_ 4 _._ (E.13) 1 _\u2212_ _q_ 0 \ufffd (1 _\u2212_ _q_ 0 ) _q_ 0 \ufffd 1 Since (1 _\u2212_ _q_ 0 ) _q_ 0 _\u2264_ [1] 4 [, [] (1 _\u2212q_ 0 ) _q_ 0 _[\u2212]_ [4] is non-negative. Thus,] _[ f]_ _[\u2032]_ [(] _[q]_ [0] [)] _[ \u2264]_ [0 for] _[ q]_ [0] _[ \u2264]_ _[p]_ [0] [ and] _[ f]_ _[\u2032]_ [(] _[q]_ [0] [)] _[ \u2265]_ [0] for _q_ 0 _\u2265_ _p_ 0 . Thus, _f_ reaches its minimum at _q_ 0 = _p_ 0, which implies _f_ ( _q_ 0 ) _\u2265_ _f_ ( _p_ 0 ) = 0 for all _q_ 0 . Since _f_ ( _q_ 0 ) can be expressed as follows: _f_ ( _q_ 0 ) = D( _p\u2225q_ ) _\u2212_ 2( _p_ 0 _\u2212_ _q_ 0 ) [2] (E.14) = D( _p\u2225q_ ) _\u2212_ [1] 2 2 _|p_ 0 _\u2212_ _q_ 0 _|_ + _|_ (1 _\u2212_ _p_ 0 ) _\u2212_ (1 _\u2212_ _q_ 0 ) _|_ (E.15) \ufffd \ufffd = D( _p\u2225q_ ) _\u2212_ [1] 2 _[\u2225][p][ \u2212]_ _[q][\u2225]_ 1 [2] _[\u2265]_ [0] _[,]_ (E.16) this proves the inequality for a set A = _{a_ 0 _, a_ 1 _}_ of cardinality 2. Now, consider the distributions _p_ _[\u2032]_ and _q_ _[\u2032]_ defined over A = _{a_ 0 _, a_ 1 _}_ with _p_ _[\u2032]_ ( _a_ 0 ) = [\ufffd] _x\u2208a_ 0 _[p]_ [(] _[x]_ [),] and _q_ _[\u2032]_ ( _a_ 0 ) = [\ufffd] _x\u2208a_ 0 _[q]_ [(] _[x]_ [) where] _[ a]_ [0] [ =] _[ {][x][ \u2208]_ [X] [:] _[ p]_ [(] _[x]_ [)] _[ \u2265]_ _[q]_ [(] _[x]_ [)] _[}]_ [ and] _[ a]_ [1] [ =] _[ {][x][ \u2208]_ [X] [:] _[ p]_ [(] _[x]_ [)] _[ < q]_ [(] _[x]_ [)] _[}]_ [. By] the Log-sum inequality (Corollary E.4), _p_ ( _x_ ) _x_ \ufffd _\u2208a_ 0 _p_ ( _x_ ) log \ufffd _q_ ( _x_ ) _q_ ( _x_ ) D( _p\u2225q_ ) = \ufffd _q_ ( _x_ ) + \ufffd \ufffd _p_ ( _x_ ) _x_ \ufffd _\u2208a_ 1 _p_ ( _x_ ) log \ufffd _q_",
    "chunk_id": "foundations_machine_learning_441"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "( _x_ ) (E.17) \ufffd (E.18) \ufffd _p_ ( _a_ 0 ) _\u2265_ _p_ ( _a_ 0 ) log \ufffd _q_ ( _a_ 0 ) _p_ ( _a_ 1 ) + _p_ ( _a_ 1 ) log \ufffd \ufffd _q_ ( _a_ 1 ) = D( _p_ _[\u2032]_ _\u2225q_ _[\u2032]_ ) _._ (E.19) Combining this inequality with the observation that _\u2225p_ _[\u2032]_ _\u2212_ _q_ _[\u2032]_ _\u2225_ 1 = ( _p_ ( _a_ 0 ) _\u2212_ _q_ ( _a_ 0 )) _\u2212_ ( _p_ ( _a_ 1 ) _\u2212_ _q_ ( _a_ 1 )) (E.20) = \ufffd \ufffd ( _p_ ( _x_ ) _\u2212_ _q_ ( _x_ )) (E.21) _x\u2208a_ 1 \ufffd ( _p_ ( _x_ ) _\u2212_ _q_ ( _x_ )) _\u2212_ \ufffd _x\u2208a_ 0 _x\u2208a_ = \ufffd _|p_ ( _x_ ) _\u2212_ _q_ ( _x_ ) _|_ (E.22) _x\u2208_ X = _\u2225p \u2212_ _q\u2225_ 1 _,_ (E.23) shows that D( _p\u2225q_ ) _\u2265_ D( _p_ _[\u2032]_ _\u2225q_ _[\u2032]_ ) _\u2265_ [1] 2 _[\u2225][p][ \u2212]_ _[q][\u2225]_ 1 [2] [and concludes the proof.] **Definition E.8 (Conditional relative entropy)** _Let p and q be two probability distributions de-_ _fined over_ X _\u00d7_ Y _and r a distribution over_ X _. Then, the_ conditional relative entropy _of p and q_ _with respect to the marginal r is defined as the expectation of the relative entropy of p_ ( _\u00b7|X_ ) _and_ _q_ ( _\u00b7|X_ ) _with respect to r:_ _y_ \ufffd _\u2208_ Y _p_ ( _y|x_ ) log _[p]_ _q_ ( [(] _y_ _[y]_ _|_ _[|]_ _x_ _[x]_ [)] (E.24) _q_ ( _y|x_ ) [=][ D][(] _[p]_ [\ufffd] _[\u2225][q]_ [\ufffd][)] _[,]_ E _X\u223cr_ \ufffdD\ufffd _p_ ( _\u00b7|X_ ) _\u2225q_ ( _\u00b7|X_ )\ufffd [\ufffd] = \ufffd \ufffd _r_ ( _x_ ) \ufffd _x\u2208_ X _y\u2208_ Y **E.3** **Mutual information** **453** **Figure E.2** **Image:** [No caption returned] Illustration of the quantity measured by the Bregman divergence defined based on a convex and differentiable function _F_ . The divergence measures the distance between _F_ ( _x_ ) and the hyperplane tangent to the curve at point _y_ . _where_ \ufffd _p_ ( _x, y_ ) = _r_ ( _x_ ) _p_ ( _y|x_ ) _and_ \ufffd _q_ ( _x, y_ ) = _r_ ( _x_ ) _q_ ( _y|x_ ) _, with the conventions_ 0 log 0 = 0 _,_ 0 log [0] _where_ \ufffd _p_ ( _x, y_ ) = _r_ ( _x_ ) _p_ ( _y|x_ ) _and_ \ufffd _q_ ( _x, y_ ) = _r_ ( _x_ ) _q_ ( _y|x_ ) _, with the conventions_ 0 log 0 = 0 _,_ 0 log 0 [= 0] _[,]_ _and a_ log _[a]_ [= +] _[\u221e]_ _[for][ a >]_ [ 0] _[.]_ _[a]_ 0 [= +] _[\u221e]_ _[for][ a >]_ [ 0] _[.]_ **E.3** **Mutual information** **Definition E.9 (Mutual information)** _Let X and Y be two random variables with joint probability_ _distribution function p_ ( _\u00b7, \u00b7_ ) _and marginal probability distribution functions p_ ( _x_ ) _and p_ ( _y_ ) _. Then,_ _the_ mutual information _of X and Y is denoted by I_ (",
    "chunk_id": "foundations_machine_learning_442"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_X, Y_ ) _and defined as follows:_ _I_ ( _X, Y_ ) = D( _p_ ( _x, y_ ) _\u2225p_ ( _x_ ) _p_ ( _y_ )) (E.25) = \ufffd \ufffd _p_ ( _x, y_ ) _x\u2208_ \ufffd X _,y\u2208_ Y _p_ ( _x, y_ ) log \ufffd _p_ ( _x_ ) _p_ ( _y_ _p_ ( _x_ ) _p_ ( _y_ ) = E _p_ ( _x,y_ ) _p_ ( _X, Y_ ) log \ufffd _p_ ( _X_ ) _p_ ( _Y_ ) _,_ (E.26) \ufffd _with the conventions_ 0 log 0 = 0 _,_ 0 log [0] _[a]_ 0 [= +] _[\u221e]_ _[for][ a >]_ [ 0] _[.]_ 0 [= 0] _[, and][ a]_ [ log] _[a]_ 0 When the random variables _X_ and _Y_ are independent, their joint distributions is the product of the marginals _p_ ( _x_ ) and _p_ ( _y_ ). Thus, the mutual information is a measure of the closeness of the joint distribution _p_ ( _x, y_ ) to its value when _X_ and _Y_ are independent, where closeness is measured via the relative entropy divergence. As such, it can be viewed as a measure of the amount of information that each random variable can provide about the other. Note that by Proposition E.3, the equality _I_ ( _X, Y_ ) = 0 holds iff _p_ ( _x, y_ ) = _p_ ( _x_ ) _p_ ( _y_ ) for all _x, y_, that is iff _X_ and _Y_ are independent. **E.4** **Bregman divergences** Here we introduce the so-called _unnormalized relative entropy_ D [\ufffd] defined for all non-negative functions _p, q_ in R [X] by _p_ ( _x_ ) _x_ \ufffd _\u2208_ X _p_ ( _x_ ) log \ufffd _q_ ( _x_ ) D\ufffd( _p\u2225q_ ) = \ufffd _q_ ( _x_ ) + \ufffd _q_ ( _x_ ) _\u2212_ _p_ ( _x_ )\ufffd _,_ (E.27) \ufffd **454** **Appendix E** **Notions of Information Theory** **Table E.1** Examples of Bregman divergences and corresponding convex functions. **B** _**F**_ **(** _**x**_ _**\u2225**_ _**y**_ **)** _**F**_ **(** _**x**_ **)** Squared _L_ 2 -distance _\u2225x \u2212_ _y\u2225_ [2] _\u2225x\u2225_ [2] Mahalanobis distanceUnnormalized relative entropy ( _x \u2212_ D _y_ \ufffd)( _[\u22a4]_ _x \u2225Q_ ( _yx_ ) _\u2212_ _y_ ) \ufffd _i\u2208I_ _[x]_ [(] _[i]_ [) log(] _x_ _[\u22a4]_ _Qx_ _[x]_ [(] _[i]_ [))] _[ \u2212]_ _[x]_ [(] _[i]_ [)] with the conventions 0 log 0 = 0, 0 log [0] 0 [= 0, and] _[ a]_ [ log] _[a]_ 0 with the conventions 0 log 0 = 0, 0 log 0 [= 0, and] _[ a]_ [ log] _[a]_ 0 [= +] _[\u221e]_ [for] _[ a >]_ [ 0. The relative entropy] coincides with the unnormalized relative entropy when restricted to \u2206 _\u00d7_ \u2206, where \u2206is the family of distributions defined over X. The relative entropy inherits several properties of the unnormalized relative entropy, in particular, it can be shown that D [\ufffd] ( _p\u2225q_ ) _\u2265_ 0. Many of these properties are in fact shared by a broader family of divergences known as _Bregman divergences_ . **Definition E.10 (Bregman",
    "chunk_id": "foundations_machine_learning_443"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "divergences)** _Let F be a convex and differentiable function defined_ _over a convex (open) set_ C _in a Hilbert space_ H _. Then, the_ Bregman divergence B _F_ _associated_ _to F is defined for all x, y \u2208_ C _by_ B _F_ ( _x\u2225y_ ) = _F_ ( _x_ ) _\u2212_ _F_ ( _y_ ) _\u2212\u27e8\u2207F_ ( _y_ ) _, x \u2212_ _y\u27e9_ _._ (E.28) Thus, B _F_ ( _x \u2225_ _y_ ) measures the difference of _F_ ( _x_ ) and its linear approximation. Figure E.2 illustrates this definition. Table E.1 provides several examples of Bregman divergences along with their corresponding convex functions _F_ ( _x_ ). Note that, although the unnormalized relative entropy is a Bregman divergence, the relative entropy is not a Bregman divergence since it is defined over the simplex which is not an open set and has an empty interior. The following proposition presents several general properties of Bregman divergences. **Proposition E.11** _Let F be a convex and differentiable function defined over a convex set_ C _in a_ _Hilbert space_ H _. Then, the following properties hold:_ _1. \u2200x, y \u2208_ C _,_ B _F_ ( _x \u2225_ _y_ ) _\u2265_ 0 _._ _2. \u2200x, y, z \u2208_ C _, \u27e8\u2207F_ ( _x_ ) _\u2212\u2207F_ ( _y_ ) _, x \u2212_ _z\u27e9_ = B _F_ ( _x \u2225_ _y_ ) + B _F_ ( _z \u2225_ _x_ ) _\u2212_ B _F_ ( _z \u2225_ _y_ ) _._ _3._ B _F_ _is convex in its first argument. If additionally F is strictly convex, then_ B _F_ _is strictly_ _convex in its first argument._ _4. Linearity: let G be a convex and differentiable function over_ C _, then, for any \u03b1, \u03b2 \u2208_ R _,_ B _\u03b1F_ + _\u03b2G_ = _\u03b1_ B _F_ + _\u03b2_ B _G_ _._ _For the following properties, we will assume additionally that F is strictly convex._ _5. Projection: for any y \u2208_ C _and any closed convex set_ K _\u2286_ C _, the_ B _F_ -projection of _y_ over _K,_ _P_ K ( _y_ ) = argmin _x\u2208_ K B _F_ ( _x \u2225_ _y_ ) _, is unique._ _6. Pythagorean theorem: for y \u2208_ C _and any closed convex set_ K _\u2286_ C _, the following holds for all_ _x \u2208_ K _:_ B _F_ ( _x \u2225_ _y_ ) _\u2265_ B _F_ ( _x \u2225_ _P_ K ( _y_ )) + B _F_ ( _P_ K ( _y_ ) _\u2225_ _y_ ) _._ (E.29) _7. Conjugate divergence: assume that F is closed proper strictly convex, and that the norm of_ _its gradient tends to infinity near the boundary of_ C _:_ lim _x\u2192\u2202_ C _\u2225\u2207F_ ( _x_ ) _\u2225_ = + _\u221e. The pair_ (C _, F_ ) _is then said to be a_ convex function of Legendre type _. Then, the conjugate of F_ _, F_ _[\u2217]_ _,_ _is differentiable and the following holds for all x, y \u2208_ C _:_ B _F_ ( _x \u2225_ _y_ ) = B _F_ _\u2217_ ( _\u2207F_ ( _y_ ) _\u2225\u2207F_ ( _x_ ))",
    "chunk_id": "foundations_machine_learning_444"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_._ (E.30) **E.4** **Bregman divergences** **455** _y_ **Image:** [No caption returned] **Figure E.3** A depiction of the Pythagorea ~~n~~ ~~theorem~~ ~~stated~~ ~~in~~ ~~proposition~~ ~~E.11,~~ ~~where~~ ~~the~~ ~~squared~~ ~~length~~ ~~of~~ each line illustrates the magnitude of the Bregman divergence between the points it connects. Proof: Property (1) holds by convexity of the function _F_ (the graph of _F_ is above its tangent, see equation (B.3)). Property (2) follows directly from the definition of the Bregman divergence: B _F_ ( _x \u2225_ _y_ ) + B _F_ ( _z \u2225_ _x_ ) _\u2212_ B _F_ ( _z \u2225_ _y_ ) = _\u2212\u27e8\u2207F_ ( _y_ ) _, x \u2212_ _y\u27e9\u2212\u27e8\u2207F_ ( _x_ ) _, z \u2212_ _x\u27e9_ + _\u27e8\u2207F_ ( _y_ ) _, z \u2212_ _y\u27e9_ = _\u27e8\u2207F_ ( _x_ ) _\u2212\u2207F_ ( _y_ ) _, x \u2212_ _z\u27e9_ _._ Property (3) holds since _x \ufffd\u2192_ _F_ ( _x_ ) _\u2212_ _F_ ( _y_ ) _\u2212\u27e8\u2207F_ ( _y_ ) _, x \u2212_ _y\u27e9_ is convex as a sum of the convex function _x \ufffd\u2192_ _F_ ( _x_ ) and the affine and thus convex function _x \ufffd\u2192\u2212F_ ( _y_ ) _\u2212\u27e8\u2207F_ ( _y_ ) _, x \u2212_ _y\u27e9_ . Similarly, B _F_ is strictly convex with respect to its first argument if _F_ is strictly convex, as a sum of a strictly convex function and an affine function. Property (4) follows from a series of equalities: B _\u03b1F_ + _\u03b2G_ = _\u03b1F_ ( _x_ ) + _\u03b2G_ ( _x_ ) _\u2212_ _\u03b1F_ ( _y_ ) _\u2212_ _\u03b2G_ ( _y_ ) _\u2212_ \ufffd _\u2207_ \ufffd _\u03b1F_ ( _y_ ) + _\u03b2G_ ( _y_ )\ufffd _, x \u2212_ _y_ \ufffd = _\u03b1_ \ufffd _F_ ( _x_ ) _\u2212_ _F_ ( _y_ ) _\u2212\u27e8\u2207F_ ( _y_ ) _, x \u2212_ _y\u27e9_ \ufffd + _\u03b2_ \ufffd _G_ ( _x_ ) _\u2212_ _G_ ( _y_ ) _\u2212\u27e8\u2207G_ ( _y_ ) _, x \u2212_ _y\u27e9_ \ufffd = _\u03b1_ B _F_ + _\u03b2_ B _G_ _,_ where we have used the fact that both the gradient and inner-product are linear functions. Property (5) holds since, by Property (3), min _x\u2208_ K B _F_ ( _x \u2225_ _y_ ) is a convex optimization problem with a strictly convex objective function. For property (6), fix _y \u2208_ C and let _J_ be the function defined for all _\u03b1 \u2208_ [0 _,_ 1] by _J_ ( _\u03b1_ ) = B _F_ ( _\u03b1x_ + (1 _\u2212_ _\u03b1_ ) _P_ K ( _y_ ) _\u2225_ _y_ ) _._ Since C is convex, for any _\u03b1 \u2208_ [0 _,_ 1], _\u03b1x_ + (1 _\u2212_ _\u03b1_ ) _P_ K ( _y_ ) is in C. _F_ is differentiable over C therefore _J_ is also differentiable as a composition of _F_ with _\u03b1 \ufffd\u2192_ _\u03b1x_ +(1 _\u2212_ _\u03b1_ ) _P_ K ( _y_ ). By definition of _P_ K ( _y_ ), for any _\u03b1 \u2208_ (0 _,_ 1], _J_ ( _\u03b1_ ) _\u2212_ _J_ (0) [B] _[F]_ [(] _[\u03b1x]_ [ + ][(][1] _[ \u2212]_ _[\u03b1]_ [)] _[P]_ [K] [(] _[y]_ [)] _[\u2225]_ _[y]_ [)] _[ \u2212]_ [B] _[F]_",
    "chunk_id": "foundations_machine_learning_445"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "[(] _[P]_ [K] [(] _[y]_ [)] _[\u2225]_ _[y]_ [)] _J_ (0) = [B] _[F]_ [(] _[\u03b1x]_ [ + ][(][1] _[ \u2212]_ _[\u03b1]_ [)] _[P]_ [K] [(] _[y]_ [)] _[\u2225]_ _[y]_ [)] _[ \u2212]_ [B] _[F]_ [(] _[P]_ [K] [(] _[y]_ [)] _[\u2225]_ _[y]_ [)] _\u03b1_ _\u03b1_ _\u2265_ 0 _._ (E.31) _\u03b1_ This implies that _J_ _[\u2032]_ (0) _\u2265_ 0. From the following expression of _J_ ( _\u03b1_ ): _J_ ( _\u03b1_ ) = _F_ ( _\u03b1x_ + (1 _\u2212_ _\u03b1_ ) _P_ K ( _y_ )) _\u2212_ _F_ ( _y_ ) _\u2212\u27e8\u2207F_ ( _y_ ) _, \u03b1x_ + (1 _\u2212_ _\u03b1_ ) _P_ K ( _y_ ) _\u2212_ _y\u27e9_ _,_ (E.32) we can compute its derivative at 0: _J_ _[\u2032]_ (0) = _\u27e8x \u2212_ _P_ K ( _y_ ) _, \u2207F_ ( _P_ K ( _y_ )) _\u27e9\u2212\u27e8\u2207F_ ( _y_ ) _, x \u2212_ _P_ K ( _y_ ) _\u27e9_ = _\u2212_ B _F_ ( _x \u2225_ _P_ K ( _y_ )) + _F_ ( _x_ ) _\u2212_ _F_ ( _P_ K ( _y_ )) _\u2212\u27e8\u2207F_ ( _y_ ) _, x \u2212_ _P_ K ( _y_ ) _\u27e9_ = _\u2212_ B _F_ ( _x \u2225_ _P_ K ( _y_ )) + _F_ ( _x_ ) _\u2212_ _F_ ( _P_ K ( _y_ )) _\u2212\u27e8\u2207F_ ( _y_ ) _, x \u2212_ _y\u27e9\u2212\u27e8\u2207F_ ( _y_ ) _, y \u2212_ _P_ K ( _y_ ) _\u27e9_ = _\u2212_ B _F_ ( _x \u2225_ _P_ K ( _y_ )) + B _F_ ( _x \u2225_ _y_ ) + _F_ ( _y_ ) _\u2212_ _F_ ( _P_ K ( _y_ )) _\u2212\u27e8\u2207F_ ( _y_ ) _, y \u2212_ _P_ K ( _y_ ) _\u27e9_ = _\u2212_ B _F_ ( _x \u2225_ _P_ K ( _y_ )) + B _F_ ( _x \u2225_ _y_ ) _\u2212_ B _F_ ( _P_ K ( _y_ ) _\u2225_ _y_ ) _\u2265_ 0 _,_ which concludes the proof of Property (6). **456** **Appendix E** **Notions of Information Theory** For property (7), note that, by definition, for any _y_, _F_ _[\u2217]_ is defined by _F_ _[\u2217]_ ( _y_ ) = sup _x\u2208_ C \ufffd _\u27e8x, y\u27e9\u2212_ _F_ ( _x_ )\ufffd _._ (E.33) _F_ _[\u2217]_ is convex and admits a sub-differential at any _y_ . By the strict convexity of _F_, the function _x \ufffd\u2192\u27e8x, y\u27e9\u2212_ _F_ ( _x_ ) is strictly concave and differentiable over C and the norm of its gradient, _y \u2212\u2207F_ ( _x_ ), tends to infinity near the boundary of C (by the corresponding property assumed for _F_ ). Thus, its supremum is reached at a unique point _x_ _y_ _\u2208_ C where its gradient is zero, that is at _x_ _y_ with _\u2207F_ ( _x_ _y_ ) = _y_ . This implies that for any _y_, _\u2202F_ _[\u2217]_ ( _y_ ), the subdifferential of _F_ _[\u2217]_, is reduced to a singleton. Thus, _F_ _[\u2217]_ is differentiable and its gradient at _y_ is _\u2207F_ _[\u2217]_ ( _y_ ) = _x_ _y_ = _\u2207_ _[\u2212]_ [1] _F_ ( _y_ ). Since _F_ _[\u2217]_ is convex and differentiable, its Bregman divergence",
    "chunk_id": "foundations_machine_learning_446"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "is well defined. Furthermore, _F_ _[\u2217]_ ( _y_ ) = \ufffd _\u2207F_ _[\u2212]_ [1] ( _y_ ) _, y_ \ufffd _\u2212_ _F_ ( _\u2207F_ _[\u2212]_ [1] ( _y_ )) since _x_ _y_ = _\u2207_ _[\u2212]_ [1] _F_ ( _y_ ). For any _x, y \u2208_ C, using the definition of _B_ _F_ _\u2217_ and the expression of _\u2207F_ _[\u2217]_ ( _y_ ) and _F_ _[\u2217]_ ( _y_ ) we can write B _F_ _\u2217_ ( _\u2207F_ ( _y_ ) _\u2225\u2207F_ ( _x_ )) = _F_ _[\u2217]_ ( _\u2207F_ ( _y_ )) _\u2212_ _F_ _[\u2217]_ ( _\u2207F_ ( _x_ )) _\u2212_ \ufffd _\u2207_ _[\u2212]_ [1] _F_ ( _\u2207F_ ( _x_ )) _, \u2207F_ ( _y_ ) _\u2212\u2207F_ ( _x_ )\ufffd = _F_ _[\u2217]_ ( _\u2207F_ ( _y_ )) _\u2212_ _F_ _[\u2217]_ ( _\u2207F_ ( _x_ )) _\u2212\u27e8x, \u2207F_ ( _y_ ) _\u2212\u2207F_ ( _x_ ) _\u27e9_ = \ufffd _\u2207_ _[\u2212]_ [1] _F_ ( _\u2207F_ ( _y_ )) _, \u2207F_ ( _y_ )\ufffd _\u2212_ _F_ ( _\u2207_ _[\u2212]_ [1] _F_ ( _\u2207F_ ( _y_ ))) _\u2212_ \ufffd _\u2207_ _[\u2212]_ [1] _F_ ( _\u2207F_ ( _x_ )) _, \u2207F_ ( _x_ )\ufffd + _F_ ( _\u2207_ _[\u2212]_ [1] _F_ ( _\u2207F_ ( _x_ ))) _\u2212\u27e8x, \u2207F_ ( _y_ ) _\u2212\u2207F_ ( _x_ ) _\u27e9_ = _\u27e8y, \u2207F_ ( _y_ ) _\u27e9\u2212_ _F_ ( _y_ ) _\u2212\u27e8x, \u2207F_ ( _x_ ) _\u27e9_ + _F_ ( _x_ ) _\u2212\u27e8x, \u2207F_ ( _y_ ) _\u2212\u2207F_ ( _x_ ) _\u27e9_ = _\u27e8y, \u2207F_ ( _y_ ) _\u27e9\u2212_ _F_ ( _y_ ) + _F_ ( _x_ ) _\u2212\u27e8x, \u2207F_ ( _y_ ) _\u27e9_ = _F_ ( _x_ ) _\u2212_ _F_ ( _y_ ) _\u2212\u27e8x \u2212_ _y, \u2207F_ ( _y_ ) _\u27e9_ = B _F_ ( _x \u2225_ _y_ ) _,_ which completes the proof. Notice that while the unnormalized relative entropy (and thus the relative entropy) are convex functions of the pair of their arguments, this in general does not hold for all Bregman divergences, only convexity with respect to the first argument is guaranteed. The notion of Bregman divergence can be extended to the case of non-differentiable functions (see section 14.3). **E.5** **Chapter notes** The notion of entropy presented in this chapter is due to Shannon [1948] who, more generally, within the same article, set the foundation of information theory. More general definitions of entropy ( _R\u00b4enyi entropy_ ) and relative entropy ( _R\u00b4enyi divergence_ ) were later introduced by R\u00b4enyi [1961]. The Kullback-Leibler divergence was introduced in [Kullback and Leibler, 1951]. Pinsker\u2019s inequality is due to Pinsker [1964]. Finer inequalities relating the relative entropy and the _L_ 1 -norm were later given by Csisz\u00b4ar [1967] and Kullback [1967]. See [Reid and Williamson, 2009] for a generalization of such inequalities to the case of _f_ -divergences. The notion of Bregman divergence is due to Bregman [1967]. For a more extensive material on information theory, we strongly recommend the book of Cover and Thomas [2006]. **E.6** **Exercises** **457** _p_ _\u03b8_ _r_ _q_ **Figure E.4** An illustration of the parallelogram identity. **E.6** **Exercises** _p_ + _q_ _p_ + _q_ 2 E.1",
    "chunk_id": "foundations_machine_learning_447"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Parallelogram identity. Prove the following _parallelogram identity_ for any three distributions _p_, _q_, and _r_ on X: _p_ + _q_ D( _p \u2225_ _r_ ) + D( _q \u2225_ _r_ ) = 2D \ufffd 2 2 + _q_ 2 _\u2225_ _r_ \ufffd + D\ufffd _p \u2225_ _[p]_ [ +] 2 _[ q]_ \ufffd + D\ufffd _q \u2225_ _[p]_ [ +] 2 _[ q]_ _._ (E.34) \ufffd Does the equality hold if we replace the relative entropy by the norm-2 squared? Figure E.4 illustrates a particular example of this identity. Note, in the example we have _\u2225p \u2212_ _r\u2225_ [2] = \ufffd\ufffd\ufffd _p \u2212_ _[p]_ [ +] 2 _[ q]_ \ufffd + \ufffd _p_ +2 _q_ _\u2212_ _r_ \ufffd\ufffd\ufffd 2 _p_ + _q_ \ufffd + \ufffd _p_ + _q_ = \ufffd\ufffd _p \u2212_ 2 2 _q_ _\u2212_ _r_ \ufffd\ufffd 2 _\u2212_ 2 cos( _\u03c0 \u2212_ _\u03b8_ )\ufffd\ufffd _p \u2212_ _p_ +2 _q_ 2 _p_ + _q_ \ufffd\ufffd + \ufffd\ufffd 2 \ufffd\ufffd\ufffd\ufffd _p_ +2 _q_ _\u2212_ _r_ \ufffd\ufffd and _\u2225q \u2212_ _r\u2225_ [2] = \ufffd\ufffd\ufffd _q \u2212_ _[p]_ [ +] 2 _[ q]_ \ufffd + \ufffd _p_ +2 _q_ _\u2212_ _r_ \ufffd\ufffd\ufffd 2 _p_ + _q_ = \ufffd\ufffd _q \u2212_ 2 _p_ + _q_ = \ufffd\ufffd _q \u2212_ 2 _p_ + _q_ \ufffd\ufffd + \ufffd\ufffd 2 _q_ _\u2212_ _r_ \ufffd\ufffd 2 _\u2212_ 2 cos( _\u03b8_ )\ufffd\ufffd _q \u2212_ _p_ +2 _q_ 2 \ufffd\ufffd\ufffd\ufffd _p_ +2 _q_ _\u2212_ _r_ \ufffd\ufffd _._ \ufffd\ufffd\ufffd\ufffd _p_ + _q_ Summing these two quantities shows the identity holds for the example. # F Notation **Table F.1** Summary of notation. R Set of real numbers R + Set of non-negative real numbers R _[n]_ Set of _n_ -dimensional real-valued vectors R _[n][\u00d7][m]_ Set of _n \u00d7 m_ real-valued matrices [ _a, b_ ] Closed interval between _a_ and _b_ ( _a, b_ ) Open interval between _a_ and _b_ _{a, b, c}_ Set containing elements _a_, _b_ and _c_ [ _n_ ] The set _{_ 1 _,_ 2 _, . . ., n}_ N Set of natural numbers, i.e., _{_ 0 _,_ 1 _, . . .}_ log Logarithm with base _e_ log _a_ Logarithm with base _a_ S An arbitrary set _|_ S _|_ Number of elements in S _s \u2208_ S An element in set S X Input space Y Target space H Feature space _\u27e8\u00b7, \u00b7\u27e9_ Inner product in feature space **v** An arbitrary vector **1** Vector of all ones _v_ _i_ _i_ th component of **v** _\u2225_ **v** _\u2225_ _L_ 2 norm of **v** _\u2225_ **v** _\u2225_ _p_ _L_ _p_ norm of **v** **u** _\u25e6_ **v** Hadamard or entry-wise product of vectors **u** and **v** **460** **Appendix F** **Notation** _f \u25e6_ _g_ Composition of functions _f_ and _g_ _T_ 1 _\u25e6_ _T_ 2 Composition of weighted transducers _T_ 1 and _T_ 2 **M** An arbitrary matrix _\u2225_ **M** _\u2225_ 2 Spectral norm of **M** _\u2225_ **M** _\u2225_ _F_ Frobenius norm of **M** **M** _[\u22a4]_ Transpose of **M** **M** _[\u2020]_ Pseudo-inverse of **M** Tr[ **M** ] Trace of **M** **I** Identity matrix _K_ : X _\u00d7_ X _\u2192_",
    "chunk_id": "foundations_machine_learning_448"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "R Kernel function over X **K** Kernel matrix 1 _A_ Indicator function indicating membership in subset _A_ _h_ _S_ The hypothesis function returned when training with sample _S_ _R_ \ufffd( _\u00b7_ ) Generalization error or risk _R_ \ufffd _S_ ( _\u00b7_ ) Empirical error or risk with respect to sample _S_ _R_ _S,\u03c1_ ( _\u00b7_ ) Empirical margin error with margin _\u03c1_ and with respect to sample _S_ R\ufffd _m_ ( _\u00b7_ ) Rademacher complexity over all samples of size _m_ R _S_ ( _\u00b7_ ) Empirical Rademacher complexity with respect to sample _S_ _N_ (0 _,_ 1) Standard normal distribution E Expectation over _x_ drawn from distribution D _x\u223c_ D [[] _[\u00b7]_ []] \u03a3 _[\u2217]_ Kleene closure over a set of characters \u03a3 **Bibliography** Shivani Agarwal and Partha Niyogi. Stability and generalization of bipartite ranking algorithms. In _Conference On Learning Theory_, pages 32\u201347, 2005. Shivani Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, and Dan Roth. Generalization bounds for the area under the ROC curve. _Journal of Machine Learning Research_, 6:393\u2013425, 2005. Nir Ailon and Mehryar Mohri. An efficient reduction of ranking to classification. In _Conference_ _On Learning Theory_, pages 87\u201398, 2008. Mark A. Aizerman, E. M. Braverman, and Lev I. Rozono`er. Theoretical foundations of the potential function method in pattern recognition learning. _Automation and Remote Control_, 25: 821\u2013837, 1964. Cyril Allauzen and Mehryar Mohri. N-way composition of weighted finite-state transducers. _In-_ _ternational Journal of Foundations of Computer Science_, 20(4):613\u2013627, 2009. Cyril Allauzen, Corinna Cortes, and Mehryar Mohri. Large-scale training of SVMs with automata kernels. In _International Conference on Implementation and Application of Automata_, pages 17\u2013 27, 2010. Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying approach for margin classifiers. _Journal of Machine Learning Research_, 1:113\u2013141, 2000. Noga Alon and Joel Spencer. _The Probabilistic Method_ . John Wiley, 1992. Noga Alon, Shai Ben-David, Nicol`o Cesa-Bianchi, and David Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. _Journal of ACM_, 44:615\u2013631, July 1997. Yasemin Altun and Alexander J. Smola. Unifying divergence minimization and statistical inference via convex duality. In _Conference On Learning Theory_, pages 139\u2013153, 2006. Galen Andrew and Jianfeng Gao. Scalable training of _l_ 1 -regularized log-linear models. In _Pro-_ _ceedings of ICML_, pages 33\u201340, 2007. Dana Angluin. On the complexity of minimum inference of regular sets. _Information and Control_, 39(3):337\u2013350, 1978. Dana Angluin. Inference of reversible languages. _Journal of the ACM_, 29(3):741\u2013765, 1982. Dana Angluin and Leslie G. Valiant. Fast probabilistic algorithms for hamiltonian circuits and matchings. _J. Comput. Syst. Sci._, 18(2):155\u2013193, 1979. Martin Anthony and Peter L. Bartlett. _Neural Network Learning:_ _Theoretical Foundations_ . Cambridge University Press, 1999. Nachman Aronszajn. Theory of reproducing kernels. _Transactions of the American Mathematical_ _Society_, 68(3):337\u2013404, 1950. Patrick Assouad. Densit\u00b4e et dimension. _Annales de l\u2019institut Fourier_, 33(3):233\u2013282, 1983. **462** **Bibliography** Kazuoki Azuma. Weighted sums of certain dependent random variables. _Tohoku Mathematical_ _Journal_, 19(3):357\u2013367, 1967. Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer, Don Coppersmith, John Langford, and Gregory B. Sorkin. Robust reductions from ranking to classification. _Machine Learning_, 72(1-2): 139\u2013153, 2008. Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk",
    "chunk_id": "foundations_machine_learning_449"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "bounds and structural results. _Journal of Machine Learning Research_, 3, 2002. Peter L. Bartlett, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Model selection and error estimation. _Machine Learning_, 48:85\u2013113, September 2002a. Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized Rademacher complexities. In _Conference on Computational Learning Theory_, volume 2375, pages 79\u201397. Springer-Verlag, 2002b. Amos Beimel, Francesco Bergadano, Nader H. Bshouty, Eyal Kushilevitz, and Stefano Varricchio. Learning functions represented as multiplicity automata. _Journal of the ACM_, 47:2000, 2000. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In _Neural Information Processing Systems_, 2001. George Bennett. Probability inequalities for the sum of independent random variables. _Journal_ _of the American Statistical Association_, 57:33\u201345, 1962. Christian Berg, Jens P.R. Christensen, and Paul Ressel. _Harmonic Analysis on Semigroups:_ _Theory of Positive Definite and Related Functions_, volume 100. Springer, 1984. Francesco Bergadano and Stefano Varricchio. Learning behaviors of automata from shortest counterexamples. In _European Conference on Computational Learning Theory_, pages 380\u2013391, 1995. Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. A maximum entropy approach to natural language processing. _Comp. Linguistics_, 22(1), 1996. Joseph Berkson. Application of the logistic function to bio-assay. _Journal of the American_ _Statistical Association_, 39:357\u2014-365, 1944. Sergei Natanovich Bernstein. Sur l\u2019extension du th\u00b4eor`eme limite du calcul des probabilit\u00b4es aux sommes de quantit\u00b4es d\u00b4ependantes. _Mathematische Annalen_, 97:1\u201359, 1927. Dimitri P. Bertsekas. _Dynamic Programming: Deterministic and Stochastic Models_ . PrenticeHall, 1987. Dmitri P. Bertsekas, Angelica Nedi\u00b4c, and Asuman E. Ozdaglar. _Convex Analysis and Optimiza-_ _tion_ . Athena Scientific, 2003. Laurence Bisht, Nader H. Bshouty, and Hanna Mazzawi. On optimal learning algorithms for multiplicity automata. In _Conference On Learning Theory_, pages 184\u2013198, 2006. Avrim Blum and Yishay Mansour. From external to internal regret. In _Conference On Learning_ _Theory_, pages 621\u2013636, 2005. Avrim Blum and Yishay Mansour. Learning, regret minimization, and equilibria. In Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay Vazirani, editors, [\u00b4] _Algorithmic Game Theory_, chapter 4, pages 4\u201330. Cambridge University Press, 2007. Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. _Journal of the ACM_, 36(4):929\u2013965, 1989. Jonathan Borwein and Qiji Zhu. _Techniques of Variational Analysis_ . Springer, New York, 2005. Jonathan M. Borwein and Adrian S. Lewis. _Convex Analysis and Nonlinear Optimization, Theory_ _and Examples_ . Springer, 2000. Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classifiers. In _Conference On Learning Theory_, pages 144\u2013152, 1992. **Bibliography** **463** Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. _Journal of Machine Learning_ _Research_, 2:499\u2013526, 2002. Stephen P. Boyd and Lieven Vandenberghe. _Convex Optimization_ . Cambridge University Press, 2004. Lev M. Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. _USSR Computational Mathematics_ _and Mathematical Physics_, 7:200\u2013217, 1967. Leo Breiman. Prediction games and arcing algorithms. _Neural Computation_, 11:1493\u20131517, October 1999. Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. _Classification and Regression Trees_ . Wadsworth, 1984. Nicol`o Cesa-Bianchi. Analysis of two gradient-based algorithms for on-line regression. _Journal of_ _Computer System Sciences_,",
    "chunk_id": "foundations_machine_learning_450"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "59(3):392\u2013411, 1999. Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Potential-based algorithms in online prediction and game theory. In _Conference On Learning Theory_, pages 48\u201364, 2001. Nicol`o Cesa-Bianchi and Gabor Lugosi. _Prediction, Learning, and Games_ . Cambridge University Press, 2006. Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. _Journal of the ACM_, 44(3):427\u2013485, 1997. Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. In _Neural Information Processing Systems_, pages 359\u2013366, 2001. Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. _IEEE Transactions on Information Theory_, 50(9):2050\u20132057, 2004. Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for prediction with expert advice. In _Conference On Learning Theory_, pages 217\u2013232, 2005. Parinya Chalermsook, Bundit Laekhanukit, and Danupon Nanongkai. Pre-reduction graph products: Hardnesses of properly learning dfas and approximating edp on dags. In _Symposium on_ _Foundations of Computer Science_, pages 444\u2013453. IEEE, 2014. Bernard Chazelle. _The Discrepancy Method: Randomness and Complexity_ . Cambridge University Press, New York, NY, USA, 2000. Stanley F. Chen and Ronald Rosenfeld. A survey of smoothing techniques for ME models. _IEEE_ _Transactions on Speech and Audio Processing_, 8(1), 2000. Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. _The Annals of Mathematical Statistics_, 23(4):493\u2013507, 12 1952. Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, Adaboost and Bregman distances. _Machine Learning_, 48:253\u2013285, September 2002. Corinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization. In _Neural_ _Information Processing Systems_, 2003. Corinna Cortes and Mehryar Mohri. Confidence intervals for the area under the ROC curve. In _Neural Information Processing Systems_, volume 17, Vancouver, Canada, 2005. MIT Press. Corinna Cortes and Vladimir Vapnik. Support-vector networks. _Machine Learning_, 20(3):273\u2013297, 1995. Corinna Cortes, Patrick Haffner, and Mehryar Mohri. Rational kernels: Theory and algorithms. _Journal of Machine Learning Research_, 5:1035\u20131062, 2004. Corinna Cortes, Leonid Kontorovich, and Mehryar Mohri. Learning languages with rational kernels. In _Conference On Learning Theory_, volume 4539 of _Lecture Notes in Computer Science_, pages 349\u2013364. Springer, Heidelberg, Germany, June 2007a. **464** **Bibliography** Corinna Cortes, Mehryar Mohri, and Ashish Rastogi. An alternative ranking problem for search engines. In _Workshop on Experimental Algorithms_, pages 1\u201322, 2007b. Corinna Cortes, Mehryar Mohri, and Jason Weston. A general regression framework for learning string-to-string mappings. In _Predicted Structured Data_ . MIT Press, 2007c. Corinna Cortes, Mehryar Mohri, Dmitry Pechyony, and Ashish Rastogi. Stability of transductive regression algorithms. In _International Conference on Machine Learning_, Helsinki, Finland, July 2008a. Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Learning sequence kernels. In _Pro-_ _ceedings of IEEE International Workshop on Machine Learning for Signal Processing_, Canc\u00b4un, Mexico, October 2008b. Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In _Neural Information Processing Systems_, Vancouver, Canada, 2010a. MIT Press. Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the impact of kernel approximation on learning accuracy. In _Conference on Artificial Intelligence and Statistics_, 2010b. Corinna Cortes, Spencer Greenberg, and Mehryar Mohri. Relative deviation learning bounds and generalization with unbounded",
    "chunk_id": "foundations_machine_learning_451"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "loss functions. _ArXiv 1310.5796_, October 2013. URL `http:` `//arxiv.org/pdf/1310.5796v4.pdf` . Corinna Cortes, Mehryar Mohri, and Umar Syed. Deep boosting. In _International Conference on_ _Machine Learning_, pages 1179\u20131187, 2014. Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Structural Maxent models. In _International Conference on Machine Learning_, pages 391\u2013399, 2015. David Cossock and Tong Zhang. Statistical analysis of Bayes optimal subset ranking. _IEEE_ _Transactions on Information Theory_, 54(11):5140\u20135154, 2008. Thomas M. Cover and Joy M. Thomas. _Elements of Information Theory_ . Wiley-Interscience, 2006. Trevor F. Cox and Michael A. A. Cox. _Multidimensional Scaling_ . Chapman & Hall/CRC, 2nd edition, 2000. Koby Crammer and Yoram Singer. Improved output coding for classification using continuous relaxation. In _Neural Information Processing Systems_, 2001. Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines. _Journal of Machine Learning Research_, 2, 2002. Robert Crites and Andrew Barto. Improving elevator performance using reinforcement learning. In _Neural Information Processing Systems_, pages 1017\u20131023. MIT Press, 1996. Imre Csisz\u00b4ar. Information-type measures of difference of probability distributions and indirect observations. _Studia Scientiarum Mathematicarum Hungarica_, 2:299\u2013318, 1967. Felipe Cucker and Steve Smale. On the mathematical foundations of learning. _Bulletin of the_ _American Mathematical Society_, 39(1):1\u201349, 2001. J. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. _Annals of_ _Mathematical Statistics_, pages 1470\u20131480, 1972. Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss. _Random Structures and Algorithms_, 22(1):60\u201365, 2003. Colin De la Higuera. _Grammatical inference: learning automata and grammars_ . Cambridge University Press, 2010. Giulia DeSalvo, Mehryar Mohri, and Umar Syed. Learning with deep cascades. In _Conference on_ _Algorithmic Learning Theory_, pages 254\u2013269, 2015. **Bibliography** **465** Luc Devroye and G\u00b4abor Lugosi. Lower bounds in pattern recognition and learning. _Pattern_ _Recognition_, 28(7):1011\u20131018, 1995. Luc Devroye and T. J. Wagner. Distribution-free inequalities for the deleted and holdout error estimates. _IEEE Transactions on Information Theory_, 25(2):202\u2013207, 1979a. Luc Devroye and T. J. Wagner. Distribution-free performance bounds for potential function rules. _IEEE Transactions on Information Theory_, 25(5):601\u2013604, 1979b. Thomas G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. _Machine Learning_, 40(2):139\u2013157, 2000. Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via errorcorrecting output codes. _Journal of Artificial Intelligence Research_, 2:263\u2013286, 1995. Harris Drucker and Corinna Cortes. Boosting decision trees. In _Neural Information Processing_ _Systems_, pages 479\u2013485, 1995. Harris Drucker, Robert E. Schapire, and Patrice Simard. Boosting performance in neural networks. _International Journal of Pattern Recognition and Artificial Intelligence_, 7(4):705\u2013719, 1993. Miroslav Dud\u00b4\u0131k, Steven J. Phillips, and Robert E. Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. _Journal of_ _Machine Learning Research_, 8, 2007. Richard M. Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian processes. _Journal of Functional Analysis_, 1(3):290\u2013330, 1967. Richard M. Dudley. A course on empirical processes. _Lecture Notes in Mathematics_, 1097:2 \u2013 142, 1984. Richard M. Dudley. Universal Donsker classes and metric entropy. _Annals of Probability_, 14(4): 1306\u20131326, 1987. Richard M. Dudley. _Uniform Central Limit Theorems_ . Cambridge University Press, 1999. Nigel Duffy",
    "chunk_id": "foundations_machine_learning_452"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "and David P. Helmbold. Potential boosters? In _Neural Information Processing_ _Systems_, pages 258\u2013264, 1999. Aryeh Dvoretzky. On stochastic approximation. In _Proceedings of the Third Berkeley Symposium_ _on Mathematical Statistics and Probability_, pages 39\u201355, 1956. Cynthia Dwork, Ravi Kumar, Moni Naor, and D. Sivakumar. Rank aggregation methods for the web. In _International World Wide Web Conference_, pages 613\u2013622, 2001. Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. _Annals of Statistics_, 32(2):407\u2013499, 2004. James P. Egan. _Signal Detection Theory and ROC Analysis_ . Academic Press, 1975. Andrzej Ehrenfeucht, David Haussler, Michael J. Kearns, and Leslie G. Valiant. A general lower bound on the number of examples needed for learning. In _Conference On Learning Theory_, pages 139\u2013154, 1988. Jane Elith, Steven J. Phillips, Trevor Hastie, Miroslav Dud\u00b4\u0131k, Yung En Chee, and Colin J. Yates. A statistical explanation of MaxEnt for ecologists. _Diversity and Distributions_, 1, 2011. Eyal Even-Dar and Yishay Mansour. Learning rates for q-learning. _Machine Learning_, 5:1\u201325, 2003. Dean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilibrium. _Games and_ _Economic Behavior_, 21:40\u201355, 1997. Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration. _Biometrika_, pages 379\u2013390, 1998. Dean P. Foster and Rakesh V. Vohra. Regret in the on-line decision problem. _Games and Economic_ _Behavior_, 29(1-2):7\u201335, 1999. **466** **Bibliography** Yoav Freund. Boosting a weak learning algorithm by majority. In _Information and Computation_, pages 202\u2013216. Morgan Kaufmann Publishers Inc., 1990. Yoav Freund. Boosting a weak learning algorithm by majority. _Information and Computation_, 121:256\u2013285, September 1995. Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In _Conference_ _On Learning Theory_, pages 325\u2013332, 1996. Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of Computer System Sciences_, 55(1):119\u2013139, 1997. Yoav Freund and Robert E. Schapire. Large margin classification using the perceptron algorithm. _Machine Learning_, 37:277\u2013296, 1999a. Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. _Games_ _and Economic Behavior_, 29(1-2):79\u2013103, October 1999b. Yoav Freund, Michael J. Kearns, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda Sellie. Efficient learning of typical finite automata from random walks. In _Proceedings the ACM_ _Symposium on Theory of Computing_, pages 315\u2013324, 1993. Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. An efficient boosting algorithm for combining preferences. _Journal of Machine Learning Research_, 4, 2003. Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. _Annals of_ _Statistics_, 29:1189\u20131232, 2000. Jerome H. Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: A statistical view of boosting. _Annals of Statistics_, 38(2), 2000. E. Mark Gold. Language identification in the limit. _Information and Control_, 10(5):447\u2013474, 1967. E. Mark Gold. Complexity of automaton identification from given data. _Information and Control_, 37(3):302\u2013320, 1978. Joshua Goodman. Exponential priors for maximum entropy models. In _Proceedings of HLT-_ _NAACL_, pages 305\u2013312, 2004. David M. Green and John A Swets. _Signal Detection Theory and Psychophysics_ . Wiley, 1966. Michelangelo Grigni, Vincent Mirelli, and Christos H Papadimitriou. On the difficulty of designing good classifiers. _SIAM Journal on Computing_, 30(1):318\u2013323, 2000. Adam J. Grove and Dale Schuurmans. Boosting in",
    "chunk_id": "foundations_machine_learning_453"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "the limit: Maximizing the margin of learned ensembles. In _Proceedings of the Fifteenth National Conference on Artificial Intelligence_, pages 692\u2013699, 1998. Uffe Haagerup. The best constants in the Khintchine inequality. _Studia Math_, 70(3):231\u2013283, 1982. Torben Hagerup and Christine R\u00a8ub. A guided tour of chernoff bounds. _Information Processing_ _Letters_, 33(6):305\u2013308, 1990. Jihun Ham, Daniel D. Lee, Sebastian Mika, and Bernhard Sch\u00a8olkopf. A kernel view of the dimensionality reduction of manifolds. In _International Conference on Machine Learning_, 2004. James A. Hanley and Barbara J. McNeil. The meaning and use of the area under a receiver operating characteristic (ROC) curve. _Radiology_, 143:29\u201336, 1982. James Hannan. Approximation to Bayes risk in repeated plays. _Contributions to the Theory of_ _Games_, 3:97\u2013139, 1957. Sergiu Hart and Andreu M. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. _Econometrica_, 68(5):1127\u20131150, 2000. **Bibliography** **467** David Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. _Information and Computation_, 100(1):78\u2013150, 1992. David Haussler. Sphere packing numbers for subsets of the boolean n-cube with bounded VapnikChervonenkis dimension. _Journal of Combinatorial Theory, Series A_, 69(2):217 \u2013 232, 1995. David Haussler. Convolution Kernels on Discrete Structures. Technical Report UCSC-CRL-99-10, University of California at Santa Cruz, 1999. David Haussler, Nick Littlestone, and Manfred K. Warmuth. Predicting _{_ 0,1 _}_ -functions on randomly drawn points (extended abstract). In _Symposium on Foundations of Computer Science_, pages 100\u2013109, 1988. Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Large margin rank boundaries for ordinal regression. In _Advances in Large Margin Classifiers_, pages 115\u2013132. MIT Press, Cambridge, MA, 2000. Wassily Hoeffding. Probability inequalities for sums of bounded random variables. _Journal of the_ _American Statistical Association_, 58(301):13\u201330, 1963. Arthur E. Hoerl and Robert W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. _Technometrics_, 12(1):55\u201367, 1970. Klaus-Uwe H\u00a8offgen, Hans-Ulrich Simon, and Kevin S. Van Horn. Robust trainability of single neurons. _Journal of Computer and Systems Sciences_, 50(1):114\u2013125, 1995. John E. Hopcroft and Jeffrey D. Ullman. _Introduction to Automata Theory, Languages and_ _Computation_ . Addison-Wesley, 1979. Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear SVM. In _International Conference on Machine_ _Learning_, pages 408\u2013415, 2008. Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. Convergence of stochastic iterative dynamic programming algorithms. _Neural Computation_, 6:1185\u20131201, 1994. Kalervo J\u00a8arvelin and Jaana Kek\u00a8al\u00a8ainen. IR evaluation methods for retrieving highly relevant documents. In _ACM Special Interest Group on Information Retrieval_, pages 41\u201348, 2000. E. T. Jaynes. Information theory and statistical mechanics. _Physical Review_, 106(4):620\u2013630, 1957. E. T. Jaynes. _Papers on probability, statistics, and statistical physics_ . Synthese library. D. Reidel Pub. Co., 1983. Thorsten Joachims. Optimizing search engines using clickthrough data. In _Knowledge and Dis-_ _covery and Data Mining_, pages 133\u2013142, 2002. William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. _Contemporary Mathematics_, 26:189\u2013\u2013206, 1984. Jean-Pierre Kahane. Sur les sommes vectorielles [\ufffd] _\u00b1u_ _n_ . _Comptes Rendus Hebdomadaires des_ _S\u2019eances de l\u2019Acad\u00b4emie des Sciences, Paris_, 259:2577\u20132580, 1964. Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. In _Conference_ _On Learning Theory_, pages 26\u201340, 2003. William",
    "chunk_id": "foundations_machine_learning_454"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Karush. _Minima of Functions of Several Variables with Inequalities as Side Constraints_ . Master\u2019s thesis, Department of Mathematics, University of Chicago, 1939. Jun\u2019ichi Kazama and Jun\u2019ichi Tsujii. Evaluation and extension of maximum entropy models with inequality constraints. In _Proceedings of EMNLP_, pages 137\u2013144, 2003. Michael J. Kearns and Yishay Mansour. A fast, bottom-up decision tree pruning algorithm with near-optimal generalization. In _International Conference on Machine Learning_, pages 269\u2013277, 1998. **468** **Bibliography** Michael J. Kearns and Yishay Mansour. On the boosting ability of top-down decision tree learning algorithms. _Journal of Computer and System Sciences_, 58(1):109\u2013128, 1999. Michael J. Kearns and Dana Ron. Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. _Neural Computation_, 11(6):1427\u20131453, 1999. Michael J. Kearns and Robert E. Schapire. Efficient distribution-free learning of probabilistic concepts (extended abstract). In _Symposium on Foundations of Computer Science_, pages 382\u2013 391, 1990. Michael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning boolean formulae and finite automata. Technical Report 14, Harvard University, 1988. Michael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning boolean formulae and finite automata. _Journal of ACM_, 41(1):67\u201395, 1994. Michael J. Kearns and Umesh V. Vazirani. _An Introduction to Computational Learning Theory_ . MIT Press, 1994. Aleksandr Khintchine. Uber dyadische br\u00a8uche. [\u00a8] _Mathematische Zeitschrift_, 18(1):109\u2013116, 1923. Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of the maximum of a regression function. _Annals of Mathematical Statistics_, 23(1):462\u2013466, 1952. George Kimeldorf and Grace Wahba. Some results on tchebycheffian spline functions. _Journal of_ _Mathematical Analysis and Applications_, 33(1):82\u201395, 1971. Jyrki Kivinen and Manfred K. Warmuth. Boosting as entropy projection. In _Conference On_ _Learning Theory_, pages 134\u2013144, 1999. Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. _IEEE Transactions_ _on Information Theory_, 47(5):1902\u20131914, 2001. Vladimir Koltchinskii and Dmitry Panchenko. Rademacher processes and bounding the risk of function learning. In _High Dimensional Probability II_, pages 443\u2013459. Birkh\u00a8auser, 2000. Vladmir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. _Annals of Statistics_, 30, 2002. Leonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Learning linearly separable languages. In _Algorithmic Learning Theory_, pages 288\u2013303, 2006. Leonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Kernel methods for learning languages. _Theoretical Computer Science_, 405:223\u2013236, 2008. Harold W. Kuhn and Albert W. Tucker. Nonlinear programming. In _2nd Berkeley Symposium_, pages 481\u2013492, Berkeley, 1951. University of California Press. Solomon Kullback. A lower bound for discrimination information in terms of variation. _IEEE_ _Transactions on Information Theory_, 13(1):126\u2013127, 1967. Solomon Kullback and Richard A. Leibler. On information and sufficiency. _Ann. Math. Statist._, 22(1):79\u201386, 1951. Harold Kushner. Stochastic approximation: a survey. _Wiley Interdisciplinary Reviews Computa-_ _tional Statistics_, 2(1):87\u201396, 2010. Harold J. Kushner and D. S. Clark. _Stochastic Approximation Methods for Constrained and_ _Unconstrained Systems_, volume 26 of _Applied Mathematical Sciences_ . Springer-Verlag, 1978. Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Multi-class deep boosting. In _Neural Infor-_ _mation Processing Systems_, 2014. John Lafferty. Additive models, boosting, and inference for generalized divergences. In _Conference_ _On Learning Theory_, pages 125\u2013133, 1999. **Bibliography** **469** John D. Lafferty, Stephen Della Pietra, and Vincent J. Della Pietra. Statistical learning algorithms based on bregman distances. In _Proceedings of the",
    "chunk_id": "foundations_machine_learning_455"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Canadian Workshop on Information Theory_, 1997. John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In _International Conference on_ _Machine Learning_, pages 282\u2013289, 2001. Rafa\ufffdl Lata\ufffdla and Krzysztof Oleszkiewicz. On the best constant in the khintchine-kahane inequality. _Studia Math_, 109(1):101\u2013104, 1994. Guy Lebanon and John D. Lafferty. Boosting and maximum likelihood for exponential models. In _Neural Information Processing Systems_, pages 447\u2013454, 2001. Michel Ledoux and Michel Talagrand. _Probability in Banach Spaces: Isoperimetry and Processes_ . Springer, New York, 1991. Ehud Lehrer. A wide range no-regret theorem. _Games and Economic Behavior_, 42(1):101\u2013115, 2003. Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. _Machine Learning_, 2(4):285\u2013318, 1987. Nick Littlestone. From on-line to batch learning. In _Conference On Learning Theory_, pages 269\u2013284, 1989. Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. In _Symposium on_ _Foundations of Computer Science_, pages 256\u2013261, 1989. Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. _Information and_ _Computation_, 108(2):212\u2013261, 1994. Michael L. Littman. _Algorithms for Sequential Decision Making_ . PhD thesis, Brown University, 1996. Philip M. Long and Rocco A. Servedio. Random classification noise defeats all convex potential boosters. _Machine Learning_, 78:287\u2013304, March 2010. M. Lothaire. _Combinatorics on Words_ . Cambridge University Press, 1982. M. Lothaire. _Mots_ . Herm`es, 1990. M. Lothaire. _Applied Combinatorics on Words_ . Cambridge University Press, 2005. Robert Malouf. A comparison of algorithms for maximum entropy parameter estimation. In _Proceedings of CoNLL-2002_, pages 49\u201355, 2002. Christopher D. Manning and Dan Klein. Optimization, maxent models, and conditional estimation without magic. In _Proceedings of HLT-NAACL_, 2003. Yishay Mansour and David A. McAllester. Boosting with multi-way branching in decision trees. In _Neural Information Processing Systems_, pages 300\u2013306, 1999. Yishay Mansour and David A. McAllester. Generalization bounds for decision trees. In _Conference_ _On Learning Theory_, pages 69\u201374, 2000. Llew Mason, Jonathan Baxter, Peter L. Bartlett, and Marcus R. Frean. Boosting algorithms as gradient descent. In _Neural Information Processing Systems_, pages 512\u2013518, 1999. Pascal Massart. Some applications of concentration inequalities to statistics. _Annales de la Facult\u00b4e_ _des Sciences de Toulouse_, IX:245\u2013303, 2000. Peter McCullagh. Regression models for ordinal data. _Journal of the Royal Statistical Society B_, 42(2), 1980. Peter McCullagh and John A. Nelder. _Generalized Linear Models_ . Chapman & Hall, 1983. **470** **Bibliography** Colin McDiarmid. On the method of bounded differences. _Surveys in Combinatorics_, 141(1): 148\u2013188, 1989. Ron Meir and Gunnar R\u00a8atsch. Advanced lectures on machine learning, machine learning summer school, canberra, australia. In _Machine Learning Summer School_, pages 118\u2013183, 2002. Ron Meir and Gunnar R\u00a8atsch. _An Introduction to Boosting and Leveraging_, pages 118\u2013183. Springer, 2003. James Mercer. Functions of positive and negative type, and their connection with the theory of integral equations. _Philosophical Transactions of the Royal Society of London. Series A, Containing_ _Papers of a Mathematical or Physical Character_, 209(441-458):415, 1909. Sebastian Mika, Bernhard Scholkopf, Alex J. Smola, Klaus-Robert Muller, Matthias Scholz, and Gunnar Ratsch. Kernel PCA and de-noising in feature spaces. In _Neural Information Processing_ _Systems_, pages 536\u2013542, 1999. Marvin Minsky and Seymour Papert. _Perceptrons: An Introduction to Computational",
    "chunk_id": "foundations_machine_learning_456"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Geometry_ . MIT Press, 1969. Mehryar Mohri. Semiring frameworks and algorithms for shortest-distance problems. _Journal of_ _Automata, Languages and Combinatorics_, 7(3):321\u2013350, 2002. Mehryar Mohri. Weighted automata algorithms. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, _Handbook of Weighted Automata_, pages 213\u2013254. Springer, 2009. Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary _\u03d5_ -mixing and _\u03b2_ -mixing processes. _Journal of Machine Learning Research_, 11:789\u2013814, 2010. Mehryar Mohri and Afshin Rostamizadeh. Perceptron mistake bounds. _ArXiv 1305.0208_, March 2013. Mehryar Mohri, Fernando Pereira, and Michael D. Riley. Weighted automata in text and speech processing. _European Conference on Artificial Intelligence, Workshop on Extended Finite State_ _Models of Language_, 2005. Jorge Nocedal. Updating quasi-newton matrices with limited storage. _Mathematics of Computa-_ _tion_, 35(151):773\u2013782, 1980. Albert B.J. Novikoff. On convergence proofs on perceptrons. In _Proceedings of the Symposium_ _on the Mathematical Theory of Automata_, volume 12, pages 615\u2013622, 1962. Jos\u00b4e Oncina, Pedro Garc\u00b4\u0131a, and Enrique Vidal. Learning subsequential transducers for pattern recognition interpretation tasks. _IEEE Transactions on Pattern Analysis and Machine Intelli-_ _gence_, 15(5):448\u2013458, 1993. Karl Pearson. On lines and planes of closest fit to systems of points in space. _Philosophical_ _Magazine_, 2(6):559\u2013572, 1901. Fernando C. N. Pereira and Michael D. Riley. Speech recognition by composition of weighted finite automata. In _Finite-State Language Processing_, pages 431\u2013453. MIT Press, 1997. Dominique Perrin. Finite automata. In J. Van Leuwen, editor, _Handbook of Theoretical Computer_ _Science, Volume B: Formal Models and Semantics_, pages 1\u201357. Elsevier, 1990. Steven J. Phillips, Miroslav Dud\u00b4\u0131k, and Robert E. Schapire. A maximum entropy approach to species distribution modeling. In _Proceedings of ICML_, 2004. Steven J. Phillips, Robet P. Anderson, and Robert E. Schapire. Maximum entropy modeling of species geographic distributions. _Ecological Modelling_, 190:231\u2013259, 2006. Stephen Della Pietra, Vincent J. Della Pietra, and John D. Lafferty. Inducing features of random fields. _IEEE Trans. Pattern Anal. Mach. Intell._, 19(4), 1997. Mark Semenovich Pinsker. _Information and Information Stability of Random Variables and Pro-_ _cesses_ . Holden-Day, 1964. **Bibliography** **471** Leonard Pitt and Manfred K. Warmuth. The minimum consistent DFA problem cannot be approximated within any polynomial. _Journal of the ACM_, 40(1):95\u2013142, 1993. John C. Platt. Fast training of support vector machines using sequential minimal optimization. In _Advances in Kernel Methods_, pages 185\u2013208. MIT Press, 1999. David Pollard. _Convergence of Stochastic Processess_ . Springer, 1984. David Pollard. Asymptotics via empirical processes. _Statistical Science_, 4(4):341 \u2013 366, 1989. Martin L. Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_ . John Wiley & Sons, Inc., 1994. J. Ross Quinlan. Induction of decision trees. _Machine Learning_, 1(1):81\u2013106, 1986. J. Ross Quinlan. _C4.5: Programs for Machine Learning_ . Morgan Kaufmann, 1993. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In _Neural_ _Information Processing Systems_, pages 1177\u20131184, 2007. Adwait Ratnaparkhi. Maximum entropy models for natural language processing. In _Encyclopedia_ _of Machine Learning_, pages 647\u2013651. Springer, 2010. Gunnar R\u00a8atsch and Manfred K. Warmuth. Maximizing the margin with boosting. In _Conference_ _On Learning Theory_, pages 334\u2013350, 2002. Gunnar R\u00a8atsch, Sebastian Mika, and Manfred K. Warmuth. On the convergence of leveraging. In _NIPS_, pages 487\u2013494, 2001. Gunnar R\u00a8atsch, Takashi Onoda, and Klaus-Robert M\u00a8uller.",
    "chunk_id": "foundations_machine_learning_457"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Soft margins for AdaBoost. _Machine_ _Learning_, 42:287\u2013320, March 2001. Mark D. Reid and Robert C. Williamson. Generalised pinsker inequalities. In _22nd Conference_ _on Learning Theory (COLT 2009)_, 2009. Alfr\u00b4ed R\u00b4enyi. On measures of entropy and information. In _Proceedings of the Fourth Berkeley_ _Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory_ _of Statistics_, pages 547\u2013561. University of California Press, 1961. Ryan Rifkin and Aldebaro Klautau. In defense of one-vs-all classification. _Journal of Machine_ _Learning Research_, 5:101\u2013141, 2004. Ryan M. Rifkin. _Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine_ _Learning_ . PhD thesis, Massachusetts Institute of Technology, 2002. H. Robbins and S. Monro. A stochastic approximation method. _Annals of Mathematical Statistics_, 22(3):400\u2013407, 1951. R. Tyrrell Rockafellar. _Convex analysis_ . Princeton University Press, 1997. W.H. Rogers and T. J. Wagner. A finite sample distribution-free performance bound for local discrimination rules. _Annals of Statistics_, 6(3):506\u2013514, 1978. Dana Ron, Yoram Singer, and Naftali Tishby. On the learnability and usage of acyclic probabilistic finite automata. In _Journal of Computer and System Sciences_, pages 31\u201340, 1995. Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. _Psychological Review_, 65(6):386, 1958. Ronald Rosenfeld. A maximum entropy approach to adaptive statistical language modelling. _Computer Speech & Language_, 10(3):187\u2013228, 1996. Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. _Science_, 290(5500):2323, 2000. Cynthia Rudin, Ingrid Daubechies, and Robert E. Schapire. The dynamics of AdaBoost: Cyclic behavior and convergence of margins. _Journal of Machine Learning Research_, 5:1557\u20131595, 2004. **472** **Bibliography** Cynthia Rudin, Corinna Cortes, Mehryar Mohri, and Robert E. Schapire. Margin-based ranking meets boosting in the middle. In _Conference On Learning Theory_, 2005. Walter Rudin. _Fourier analysis on groups_ . Number 12 in Interscience tracts in pure and applied mathematics. John Wiley & Sons, 1990. I. N. Sanov. On the probability of large deviations of random variables. _Matematicheskii Sbornik_, 42(84):11\u201344, 1957. Norbert Sauer. On the density of families of sets. _Journal of Combinatorial Theory, Series A_, 13 (1):145\u2013147, 1972. Craig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge regression learning algorithm in dual variables. In _International Conference on Machine Learning_, volume 521, 1998. Robert E. Schapire. The strength of weak learnability. _Machine Learning_, 5:197\u2013227, July 1990. Robert E. Schapire. The boosting approach to machine learning: An overview. In _Nonlinear_ _Estimation and Classification_, pages 149\u2013172. Springer, 2003. Robert E. Schapire and Yoav Freund. _Boosting: Foundations and Algorithms_ . The MIT Press, 2012. Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions. _Machine Learning_, 37(3):297\u2013336, 1999. Robert E. Schapire and Yoram Singer. Boostexter: A boosting-based system for text categorization. _Machine Learning_, 39(2-3):135\u2013168, 2000. Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In _International Conference on Machine_ _Learning_, pages 322\u2013330, 1997. Leopold Schmetterer. Stochastic approximation. In _Proceedings of the Fourth Berkeley Symposium_ _on Mathematical Statistics and Probability_, pages 587\u2013609, 1960. Isaac J. Schoenberg. Metric spaces and positive definite functions. _Transactions of the American_ _Mathematical Society_, 44(3):522\u2013536, 1938. Bernhard Sch\u00a8olkopf and",
    "chunk_id": "foundations_machine_learning_458"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "Alex Smola. _Learning with Kernels_ . MIT Press, 2002. Bernhard Sch\u00a8olkopf, Ralf Herbrich, Alex J. Smola, and Robert Williamson. A generalized representer theorem. Technical Report 2000-81, Neuro-COLT, 2000. Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability and stability in the general learning setting. In _Conference On Learning Theory_, 2009. Claude E. Shannon. A mathematical theory of communication. _Bell System Technical Journal_, 27:379\u2013423, 1948. John Shawe-Taylor and Nello Cristianini. _Kernel Methods for Pattern Analysis_ . Cambridge University Press, 2004. John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural risk minimization over data-dependent hierarchies. _IEEE Transactions on Information Theory_, 44(5):1926\u20131940, 1998. Saharon Shelah. A combinatorial problem; stability and order for models and theories in infinitary languages. _Pacific Journal of Mathematics_, 41(1), 1972. Satinder P. Singh. _Learning to Solve Markovian Decision Processes_ . PhD thesis, University of Massachusetts, 1993. Satinder P. Singh and Dimitri Bertsekas. Reinforcement learning for dynamic channel allocation in cellular telephone systems. In _Neural Information Processing Systems_, pages 974\u2013980. MIT Press, 1997. **Bibliography** **473** Maurice Sion. On general minimax theorems. _Pacific Journal of Mathematics_, 8(1):171\u2013176, 1958. Eric V. Slud. Distribution inequalities for the binomial law. _Annals of Probability_, 5(3):404\u2013412, 1977. Bharath Sriperumbudur and Zolt\u00b4an Szab\u00b4o. Optimal rates for random fourier features. In _Neural_ _Information Processing Systems_, pages 1144\u20131152, 2015. Gilles Stoltz and G\u00b4abor Lugosi. Internal regret in on-line portfolio selection. In _Conference On_ _Learning Theory_, pages 403\u2013417, 2003. Rich Sutton. _Temporal Credit Assignment in Reinforcement Learning_ . PhD thesis, University of Massachusetts, 1984. Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning : An Introduction_ . MIT Press, 1998. S.J. Szarek. On the best constants in the Khintchin inequality. _Studia Math_, 58(2):197\u2013208, 1976. Csaba Szepesv\u00b4ari. _Algorithms for Reinforcement Learning_ . Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool, 2010. Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. In _Conference_ _On Learning Theory_, pages 74\u201389, 2002. Benjamin Taskar, Carlos Guestrin, and Daphne Koller. Max-margin Markov networks. In _Neural_ _Information Processing Systems_, 2003. Robert F. Tate. On a double inequality of the normal distribution. _The Annals of Mathematical_ _Statistics_, 1:132\u2013134, 1953. Joshua Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. _Science_, 290(5500):2319\u20132323, 2000. Gerald Tesauro. Temporal difference learning and TD-gammon. _Communications of the ACM_, 38:58\u201368, March 1995. Robert Tibshirani. Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical_ _Society. Series B_, 58(1):267\u2013288, 1996. B. Tomaszewski. Two remarks on the Khintchine-Kahane inequality. In _Colloquium Mathe-_ _maticum_, volume 46, 1982. Boris Trakhtenbrot and Janis M. Barzdin. _Finite Automata: Behavior and Synthesis_ . NorthHolland, 1973. John N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. In _Machine Learning_, volume 16, pages 185\u2013202, 1994. Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large margin methods for structured and interdependent output variables. _Journal of Machine Learning_ _Research_, 6:1453\u20131484, 2005. Leslie G. Valiant. A theory of the learnable. _Communications of the ACM_, 27(11):1134\u20131142, 1984. Vladimir N. Vapnik. _Statistical Learning Theory_ . Wiley-Interscience, 1998. Vladimir N. Vapnik. _The Nature of Statistical Learning Theory_ . Springer-Verlag, 2000. Vladimir N. Vapnik. _Estimation",
    "chunk_id": "foundations_machine_learning_459"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "of Dependences Based on Empirical Data_ . Springer-Verlag, 2006. Vladimir N. Vapnik and Alexey Chervonenkis. A note on one class of perceptrons. _Automation_ _and Remote Control_, 25, 1964. Vladimir N. Vapnik and Alexey Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Theory of Probability and Its Applications_, 16:264, 1971. **474** **Bibliography** Vladimir N. Vapnik and Alexey Chervonenkis. _Theory of Pattern Recognition_ . Nauka, 1974. Santosh S. Vempala. The random projection method. In _DIMACS Series in Discrete Mathematics_ _and Theoretical Computer Science_, volume 65. American Mathematical Society, 2004. Pierre Fran\u00b8cois Verhulst. Notice sur la loi que la population suit dans son accroissement. _Corre-_ _spondance math\u00b4ematique et physique_, 10:113\u2014-121, 1838. Pierre Fran\u00b8cois Verhulst. Recherches math\u00b4ematiques sur la loi d\u2019accroissement de la population. _Nouveaux M\u00b4emoires de l\u2019Acad\u00b4emie Royale des Sciences et Belles-Lettres de Bruxelles_, 18:1\u2014-42, 1845. Mathukumalli Vidyasagar. _A Theory of Learning and Generalization:_ _With Applications to_ _Neural Networks and Control Systems_ . Springer-Verlag, 1997. Sethu Vijayakumar and Si Wu. Sequential support vector classifiers and regression. _International_ _Conference on Soft Computing_, 1999. John von Neumann. Zur Theorie der Gesellschaftsspiele. _Mathematische Annalen_, 100(1):295\u2013320, 1928. Vladimir G. Vovk. Aggregating strategies. In _Conference On Learning Theory_, pages 371\u2013386, 1990. Grace Wahba. _Spline Models for Observational Data_, volume 59 of _CBMS-NSF Regional Con-_ _ference Series in Applied Mathematics_ . Society for Industrial and Applied Mathematics, 1990. Christopher J. C. H. Watkins. _Learning from Delayed Rewards_ . PhD thesis, Cambridge University, 1989. Christopher J. C. H. Watkins. Dynamic alignment kernels. Technical Report CSD-TR-98-11, Royal Holloway, University of London, 1999. Christopher J. C. H. Watkins and Peter Dayan. Q-learning. _Machine Learning_, 8(3-4):279\u2013292, 1992. Andr\u00b4e Weil. _L\u2019int\u00b4egration dans les groupes topologiques et ses applications_, volume 1145. Hermann Paris, 1965. Kilian Q. Weinberger and Lawrence K. Saul. An introduction to nonlinear dimensionality reduction by maximum variance unfolding. In _Conference on Artificial Intelligence_, 2006. Jason Weston and Chris Watkins. Support vector machines for multi-class pattern recognition. _European Symposium on Artificial Neural Networks_, 4(6), 1999. Bernard Widrow and Marcian E. Hoff. Adaptive switching circuits. _Neurocomputing: Foundations_ _of Research_, 1988. Peter M. Williams. Bayesian regularisation and pruning using a Laplace prior. _Neural Computa-_ _tion_, 7:117\u2013143, 1994. Huan Xu, Shie Mannor, and Constantine Caramanis. Sparse algorithms are not stable: A no-freelunch theorem. In _Conference on Communication, Control, and Computing_, pages 1299\u20131303, 2008. Yinyu Ye. The simplex and policy-iteration methods are strongly polynomial for the markov decision problem with a fixed discount rate. _Mathematics of Operations Research_, 36(4):593\u2013603, 2011. Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. _Annals of Statistics_, 32:56\u2013134, 2003a. Tong Zhang. Sequential greedy approximation for certain convex optimization problems. _IEEE_ _Trans. Inf. Theor._, 49(3):682\u2013691, 2003b. Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _International Conference on Machine Learning_, pages 928\u2013936, 2003. **Index** _L_ 1 -geometric margin, _see_ margin _L_ 1 -margin, _see_ margin _L_ 1 -regularized AdaBoost, 165 logistic regression, 325 _\u03b2_ -contracting, 388 _\u03b2_ -stable, 334, 338, 340\u2013342 uniformly, 334 _\u03f5_ -greedy policy, 401 _\u03f5_ -insensitive loss, 282 _\u03f5_ -transition, 361 _\u03b3_ -fat-dimension, 274, _see_ fat-shattering dimension _\u03b3_ -shattered, 274,",
    "chunk_id": "foundations_machine_learning_460"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "_see_ fat-shattered log-linear model, 321, 326 _\u03c1_ -margin loss function, _see_ margin _\u03c3_ -admissible, 337, 338, 340\u2013342 _\u03c3_ -algebra, 429 _k_ -CNF formula, 18, 19 _k_ -deterministic, 377 _k_ -reversible, 377 _k_ -term DNF formula, 18 _n_ -fold cross-validation, 71 _n_ -way composition, 128, 136 _p_ th-moment, 434 absolutely continuous, 429 accepted, 28, 361 accepting path, 123, 361 access string, 364\u2013368 accuracy, 8, 11, 17, 23, 46, 148\u2013150, 154, 167, 169, 172, 244, 283 pairwise ranking, 245, 255, 256 action, 7, 163, 164, 183, 205, 240, 379\u2013383, 387\u2013390, 393, 398, 399, 401\u2013404 greedy, 401, 402 policy, _see_ policy random, 401 active learning, 7, 362 acyclic, 361 AdaBoost, 145 _L_ 1 -regularized, 165 AdaBoost\u2019s weak learning condition, 162 AdaBoost.MH, 222, 223, 236, 237 AdaBoost.MR, 222, 236, 238 adaptive boosting, 150 adversarial, 177, 178, 180, 204, 260 argument, 180 assumption, 178 choice, 260 scenario, 177 advice, 178 affine, 421 agent, 379 aggregated algorithms, 213, 221 algebraic transductions, 127 algorithm dependent, 333 deterministic, 183, 258\u2013260, 264 learning, 1, 4\u20136, 9, 19, 20, 23, 24, 27, 43, 46, 47, 55, 57, 71, 80, 85, 98, 146, 148, 150, 168, 173, 179, 186, 202, 252, 257, 333, 334, 343, 362 off-policy, 401, 402 on-policy, 402 randomized, 186, 209, 239, 259, 260, 264 algorithmic stability, _see_ stability **476** **Index** approximately correct, 11 approximation error, 61\u201364 area under the curve, _see_ AUC AUC, 239, 255, 256, 264, 265 automaton _k_ -reversible, 377 deterministic, 360, _see also_ DFA, 361, 362 finite, 125, 129, 130, 360, 361, 370, 375, 377 learning, 359 prefix-tree, 370, 371, 373 reverse deterministic, 374 reversible, 370, 371, 373, 374 average noise, 23 precision, 263 regret, 186 Azuma\u2019s inequality, 202, 442, 445 base classifier set, 146 classifiers, 146 rankers, 244 Bayes classifier, 22, 47, 61, 74, 75, 78, 140 error, 22, 23, 28, 61, 67, 259 formula, 431 hypothesis, 22 scoring function, 74 Bellman equations, 385\u2013387, 389, 390, 392 Bennett\u2019s inequality, 447 Bernstein\u2019s inequality, 438, 440, 447 bias, 46, 71, 296, 446, 450 bigram, 128, 129 gappy, 128, 129 kernel, 128, 129 binary classification, 9 classifier, 79 decision tree, 224 entropy, 450 entropy function, 449 space partition (BSP) trees, 225 binomial distribution, 430, 440, 448 bipartite ranking, 251 Boltzmann exploration, 401 boosted, 168 boosting, 145\u2013149, 152, 154, 155, 159, 160, 163, 165, 167\u2013172, 174, 175, 221\u2013224, 236, 237, 239, 244\u2013246, 251, 291, 298, 320, 330 by filtering, 168 by majority, 168 multi-class, 213, _see also_ AdaBoost.MH, _see also_ AdaBoost.MR, 237 round, 147, 148, 171 trees, 291 Bregman divergence, 169, 295, 307, 313, 331, 337, 453\u2013456 generalized, 337, 338 calibration problem, 229, 230 categorical question, 224 Cauchy-Schwarz inequality, 53, 97, 98, 112, 118, 197, 309, 310, 339, 341, 409, 410, 433 chain rule, 431 Chebyshev\u2019s inequality, 433, 447 Chernoff bound, 28, 45 bounding technique, 437, 438 multiplicative bounds, 439, 445 chi-squared distribution, 430 Cholesky decomposition, 115, 413 partial, 280, 285 classification, 3, 259 binary, 4, 33, 34, 61, 74, 79, 102, 159, 173, 213, 228\u2013231, 239, 244, 252, 257, 264, 271, 281, 325, 330, 331 document, 2, 3, 106, 215 image, 3, 140 linear, 79, 105, 177, 190,",
    "chunk_id": "foundations_machine_learning_461"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "198 multi-class, xiii, 168, 213\u2013215, 217, 219\u2013222, 224, 225, 228, 229, 232, 233, 235\u2013237, 264, 315, 321, 331 text, 3 classifier, 38, 67, 146\u2013149, 153, 159, 164, 169, 172, 173, 214, 223, 230\u2013232, 239, 255, 273 base, 146\u2013148, 151, 160, 162, 163, 167, 169, 170, 175, 224 **Index** **477** binary, 74, 229\u2013232, 259, 265 linear, 80 clique, 234 closed, 422 clustering, 3, 117, 224 co-accessible, 125 code binary, 232 word, 231, 232 complementarity conditions, 83, 90, 282, 421 complete metric space, 388 composition, 123 concave, 84, 91, 118, 226, 278, 304, 416\u2013418, 420, 449, 451, 452, 456 function, 84, 118, 416, 449, 452 problem, 420 concentration inequality, xiv, 437, 445 concept, 1, 3, 9\u201319, 24\u201327, 29, 36, 41, 54\u201357, 145, 179, 180, 347, 348, 361\u2013364, 369, 377 class, 10\u201312, 14, 16\u201319, 24\u201327, 29, 54, 55, 57, 145, 179, 362, 363, 377 class universal, 17 conditional maximum entropy models, 315 probability, 431 relative entropy, 316, 319, 331, 452 Conditional Random Fields, _see_ CRFs confidence, 17, 27, 28, 57, 92, 93, 95, 97, 148, 155, 157, 159, 215, 229, 231, 232, 241, 264, 315 margin, 92, _see_ margin, 95, 97, 155, 157, 159 conjugate, 157, 201, 300, 304, 307, 308, 313, 328, 329, 410, 423, 424, 426, 427, 454 function, 300, 304, 307, 308, 328, 329, 423, 424, 427 consistent, 7\u20139, 15\u201317, 19\u201321, 24, 27, 55, 57, 163, 172, 179, 362\u2013364, 375 algorithm, 16, 17 case, 20, 21 DFA, 362, 363, 375 hypothesis, 15\u201317, 172 pairwise, 257 constrained optimization problem, 420 constraint equality, 102, 420 qualification, 420 qualification strong, 421 qualification weak, 421 context-free grammars, 363 convex, 415, 417 combination, 157\u2013159, 222 differentiable functions, 421 function, 73, 151, 152, 165, 235, 237, 255, 275, 307, 337, 415\u2013419, 422, 423, 455 function of Legendre type, 454 functions, 415, 421 hull, 37, 38, 157, 191, 250, 416 loss, 77, 153, 250, 284 optimization, xiii, xiv, 75, 82, 84, 88, 99, 110, 151, 162, 166, 221, 278, 286, 299, 304, 307, 317, 320, 323, 325, 331, 415, 420, 422, 426, 455 set, 299, 300, 317, 318, 416\u2013419, 454 sets, 415 strictly, 82, 299, 307, 317, 417, 454, 455 convexity, 47, 75, 77, 88, 107, 188, 191, 203, 210, 211, 237, 277, 310, 416, 418\u2013420, 422, 423, 437, 439, 443, 444, 451, 455, 456 core, 423 covariance, 348, 349, 353, 356, 433 matrix, 348, 349, 353, 356, 434 covering, 49, 58, 59, 133, 134, 263 number, 49, 58, 134, 263 CRFs, 235, 237 cross-validation, 68\u201373, 99, 102, 103, 166, 167, 285, 323, 324 _n_ -fold, 70\u201373, 88, 228 error, 71, 102 data set, 102, 109, 139, 161, 255, 347, 348, 357, 379 test, 5, 6 training, 5, 6, 21, 24, 25, 71, 72, 87, 88, 93, 101, 102, 170, 176, 228, 229 unseen, 8 **478** **Index** validation, 4 DCG, 263, 264 normalized, 263 decision epochs, 381 stump, 154 tree, 154, 155, 168, 169, 213, 221, 224, 225, 227, 228, 236, 291, 298, 365, 366, 368, 376 tree binary, 238, 365 DeepBoost, 169 degrees of freedom, 430 deterministic, 22, 361, 381 determinization, 361 DFA, 361, 362, 364\u2013366, 368\u2013370, 375\u2013377",
    "chunk_id": "foundations_machine_learning_462"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "consistent, 362, 363, 375 equivalent, 361 minimal, 361, 362, 364, 376 dichotomy, 34, 36, 37, 50 differentiable function, 170, 337, 415, 417, 453, 454 dimensionality reduction, 3, 6, 117, 347, 348, 351, 354, 356 discounted cumulative gain, _see_ DCG distinguishing strings, 364 distribution, 430 -free model, 11 binomial, 430, 440, 448 chi-squared, 430 Gaussian, 303 Gibbs, 295, 299, 300, 306, 312, 317, 430 Laplace, 430 normal, 330, 355, 358, 430, 434, 443, 460 Poisson, 430 probability, 22, 131, 140, 397, 403, 429, 431, 453 divergence Bregman, 169, 295, 307, 313, 331, 337, 453\u2013456 Bregman generalized, 337, 338 Kullback-Leibler, 344, 450, 456 R\u00b4enyi, 456 DNF formula, 17 doubling trick, 185, 189, 204, 205 dual, 420 Langrange function, 420 norm, 158, 287, 301, 308, 324, 327, 410 optimization, 83\u201385, 89\u201391, 99, 103, 116, 142, 194, 222, 237, 278, 279, 284, 292, 295, 302, 315, 318, 319 optimization problem, 420 problem, 84, 91, 278, 279, 282, 284, 299, 300, 302, 307, 313, 317, 318, 320, 331, 420, 425 variables, 83, 86, 90, 292, 293 duality Fenchel, 300, 307, 328, 426 gap, 420 strong, 84, 420, 425, 426 weak, 425 early stopping, 165, 167, 170 edge, 149, 246 eigenvalue, 411, 413, 418 emphasis function, 262 empirical error, 10 kernel map, 113, 114 kernel maps, 112 Rademacher complexity, 30 risk, 10 risk minimization, _see_ risk minimization empty string, 122, 360 ensemble algorithms, 145 hypotheses, 155 methods, 145, 155, 165, 250, 251 entropy, 168, 169, 199, 201, 226, 227, 295, 296, 298, 299, 302, 306\u2013308, 312, 317, 330, 331, 344, 345, 438, 439, 449\u2013454, 456, 457, 477, 482 binary, 450 binary function, 449 conditional relative, 319, 331 maximum, 295 maximum conditional, 315 R\u00b4enyi, 456 regularized, 344 relative, 450 relative conditional, 316, 452 unnormalized relative, 453 envelope, 290 **Index** **479** environment, 1, 7, 379\u2013381, 387, 393, 397, 403, 404 model, 379, 380, 387, 393, 397 unknown, 403 epigraph, 416 equivalence queries, 363 equivalent, 409 Erd\u00a8os, 43 ERM, _see_ risk minimization error, 10 empirical, 10, 19\u201321, 57, 59, 65, 67, 88, 145, 148\u2013151, 154, 168, 171, 172, 175, 211, 214, 227, 236, 241, 246\u2013248, 252, 269, 270, 273, 275, 276, 286, 294, 334, 336 estimation, 61\u201364, 67, 73 excess, 64, 65, 67, 74, 76\u201378 generalization, 10, 11, 16, 19\u201322, 24, 26, 43, 59, 65, 66, 69, 70, 79, 86, 87, 95, 97\u201399, 140, 155, 161, 166, 172, 178, 201, 202, 204, 212, 214, 217, 230, 238, 241\u2013243, 252, 268, 276, 323, 334, 336, 342 leave-one-out, 72, 85, 86, 193, 194, 293, 294, 342 mean squared, 228, 275, 277, 289 reconstruction, 353 test, 6, 155 training, 65, 155, 174 true, 20 error-correcting output codes (ECOC), 231 estimation error, 61 events, 140 set, 429 examples, 4 labeled, 5\u20137, 59, 71, 170, 223, 364 misclassified, 171 negative, 23 positive, 16, 17, 227, 369, 376 excess error, 61 expectation, 431 linearity, 86, 133, 336, 432 expected loss, 163 experience, 1 expert advice, 27, 177\u2013179 algorithm, 205 best, 7, 178, 181\u2013183, 205 exploration versus exploitation, 7 exponential inequality, 448 false negative, 12 positive, 12, 103, 143, 256 positive rate, 256 fat-shattered, 274 fat-shattering, 290 dimension,",
    "chunk_id": "foundations_machine_learning_463"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "274 feature, 4 extraction, 347 function, 430 mapping, 112\u2013114, 117\u2013119, 137, 139, 219, 220, 243, 244, 275, 276, 281, 283, 284, 297, 315, 347, 350 missing, 228 space, 92, 97\u201399, 104, 106, 107, 112, 130, 131, 139, 140, 167, 224, 243, 276, 280, 289, 376, 459 vector, 156, 225, 234, 279, 280, 300, 320, 321, 327, 350 Fenchel conjugate, 423 duality theorem, 300, 307, 328, 426 problems, 425 Fermat\u2019s theorem, 415 final state, 123, 125, 360, 361, 364, 366, 370\u2013372, 374, 398 weight, 122\u2013124 finite, 381 horizon, 381 query subset, 257 fixed point, 230, 388, 394, 397, 401 Frobenius norm, 412 product, 412 Fubini\u2019s theorem, 44, 432 function affine, 151, 275, 455 measurable, 22, 28, 273 symmetric, 107, 262 **480** **Index** game zero-sum, 163, 164, 204 gap penalty, 128 Gaussian, 430 distribution, 303 kernel, 110 generalization bound, 14, 15, 21, 35, 43, 58, 59, 93, 94, 97, 159, 217, 220, 235, 236, 243, 268, 272, 273, 280, 283, 284, 287, 288, 293, 331, 333, 341, 343 error, 10 geometric margin _L_ 1 -, 157, 161 Gibbs distribution, 295, 299, 300, 306, 312, 317, 430 gradient, 415 descent, 191, 192, 207, 289, 291, 294, 304, 313, 320, 327, 404 Gram matrix, 84, 108, 139 graph acyclic, 55 Laplacian, 352, 357 neighborhood, 352, 353 structure, 235 graphical model, 234 group Lasso, _see_ Lasso norm, 412 growth function, 29, 34\u201336, 40\u201342, 50, 56, 63, 333 H\u00a8older\u2019s inequality, 166, 210, 301, 303, 322, 329, 411 Halving algorithm, 179, 181, 183 Hamming distance, 214, 231, 232, 234, 444 Hessian, 82, 84, 210, 313, 415, 417 Hilbert space, 73, 105, 107, 108, 110, 112, 113, 121, 138, 139, 141, 142, 410, 422, 423, 425, 444, 454 pre-, 112 reproducing kernel, 110\u2013112, 117, 336, 350 hinge loss, 88, 89, 99, 174, 207, 341\u2013343 quadratic, 89, 343 Hoeffding\u2019s inequality, 19, 27, 59, 69, 134, 203, 268, 269, 437\u2013439, 441, 443, 445, 447 lemma, 188, 201, 306, 441, 445 horizon finite, 382 infinite, 382, 385 Huber loss, 284 hyperparameters, 4, 5 hyperplane marginal, 81, 83, 87, 90, 91 maximum-margin, 80, 81, 83, 197 optimal, 100 hypothesis base, 151, 152, 155\u2013157, 173, 174, 211, 255 linear, 64, 97, 98, 155, 265, 277 set, 5, 7, 10 set finite, 15, 20, 21, 25, 27, 53, 180, 268 single, 19 i.i.d., 10, 11, 15, 20, 32, 193, 194, 252, 296, 334\u2013336, 404, 431, 480 impurity Gini index, 226 misclassification, 226 inconsistent, 9 case, 19, 27, 269 independent and identically distributed (i.i.d.), 431 inequality Azuma\u2019s, 202, 442, 445 Bennett\u2019s, 447 Bernstein\u2019s, 438, 440, 447 Cauchy-Schwarz, 53, 97, 98, 112, 118, 197, 309, 310, 339, 341, 409, 410, 433 Chebyshev\u2019s, 433, 447 concentration, xiv, 437, 445 exponential, 448 H\u00a8older\u2019s, 166, 210, 301, 303, 322, 329, 411 Hoeffding\u2019s, 19, 27, 59, 69, 134, 203, 268, 269, 437\u2013439, 441, 443, 445, 447 Jensen\u2019s, 51\u201353, 77, 97, 104, 118, 133, 134, 188, 311, 318, 327, 443, 444, 450, 451 **Index** **481** Khintchine-Kahane, 118, 186, 445 Log-sum, 451, 452 Markov\u2019s, 134, 354, 432, 433, 437 maximal, 324 McDiarmid\u2019s, 29, 31, 32, 139, 310, 335, 442, 443, 445 Pinsker\u2019s, 302, 318,",
    "chunk_id": "foundations_machine_learning_464"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "344, 439, 456 Slud\u2019s, 440 Young\u2019s, 410 information theory, xiv, 23, 407, 449, 450, 456 initial state, 381 input space, 9, 22, 30, 33, 52, 56, 79, 105\u2013109, 112, 121, 130, 139, 173, 197, 213, 240, 267, 275, 285, 351, 449 instances, 9 inverse generalized, 412 Isomap, 351, 352, 356 iterative scaling generalized, 313 Jensen\u2019s inequality, 51\u201353, 77, 97, 104, 118, 133, 134, 188, 311, 318, 327, 443, 444, 450, 451 Johnson-Lindenstrauss lemma, 348, 354, 356 joint probability mass function, 429 kernel, 105, 106 approximate feature maps, 131 bigram, 128, 129 bigram sequence, 129 continuous, 131, 142 convolution, 136 difference, 138 empirical map, 112, 113 functions, 107, 130\u2013132, 135, 137, 138, 222, 343, 350 gappy bigram, 129 Gaussian, 110, 113, 116 map empirical, 114 matrix, 108, 113\u2013116, 118, 128, 143, 244, 270, 278, 280, 282, 284, 285, 293, 343, 344, 350, 352\u2013354, 357 methods, xiv, 85, 105, 106, 130, 136, 351 negative definite symmetric, 105, 119, 121, 141 normalized, 112, 113, 116, 137, 285 PCA, 347, 349\u2013354, 356, 357 polynomial, 108, 109, 131, 139 positive definite, 108 positive definite symmetric, 110\u2013119, 121, 137\u2013140, 197, 199, 219, 220, 233, 243, 276, 281\u2013284, 289, 293, 336, 338, 350 positive semidefinite, 108 rational, 105, 122, 127, 142 ridge regression, 267, 275\u2013277, 292, 294, 333, 343 sequence, 121, 129 shift-invariant, 131 sigmoid, 110 Khintchine-Kahane inequality, 118, 186, 445 KKT conditions, 83, 89, 221, 278, 282, 284, 421 KPCA, 347, 349\u2013354, 356, 357 Kullback-Leibler divergence, 344, 450, 456 labels, 4, 9 Lagrange, 83, 89, 90, 99, 101, 102, 166, 304, 313, 324, 420 dual function, _see_ dual function, 420 multipliers, 101, 102 variables, 83, 89, 90 Lagrangian, 83, 89, 90, 221, 278, 282, 284, 420\u2013422 Laplace distribution, 430 Laplacian eigenmaps, 351, 352, 357 Lasso, 267, 275, 285\u2013288, 290, 291, 293, 294, 343 group, 289 on-line, 294 law of large numbers, 394 learner strong, 146 weak, 145, 146 learning active, 7, 362 algorithm, 1, 4\u20136, 9, 19, 20, 23, 24, 27, 43, 46, 47, 55, 57, 71, 80, 85, 98, 146, **482** **Index** 148, 150, 168, 173, 179, 186, 202, 252, 257, 333, 334, 343, 362 algorithm PAC, 12, 16, 26\u201328, 146 algorithm weak, 146, 244 on-line, 7, 177 passive, 7 policy, 401 problem, 380 reinforcement, 7, 379 with queries, 363 leave-one-out cross-validation, 71 error, 85 lemma Hoeffding\u2019s, 188, 201, 306, 441, 445 Johnson-Lindenstrauss, 348, 354, 356 Massart\u2019s, 35, 51, 287 Sauer\u2019s, 40\u201343, 49, 50, 55 Talagrand\u2019s, 52, 216, 217, 242 linear -ly separable labeling, 50 algebra, 409 classification problem, 79 classifiers, 79 Lipschitz function, 52, 93 LLE, 353, 354, 356, 357 locally linear embedding, _see_ LLE Log-sum inequality, 451, 452 logistic, 330 form, 326 loss, 153 multinomial regression, 315 regression, 153, 315, 325 logistic regression _L_ 1 -regularized, 325 loss _\u03f5_ -insensitive, 282 convex, 77, 153, 250, 284 function, 268 hinge, 88, 89, 99, 174, 207, 341\u2013343 Huber, 284 logistic, 153 margin, 92 matrix, 163 quadratic _\u03f5_ -insensitive, 283 quadratic hinge, 88, 89, 343 squared, 268 loss function, 4 manifold learning, 3 margin _L_ 1 -, 156 _L_ 1 -geometric, 156 confidence, 92 geometric, 80 hard, 88",
    "chunk_id": "foundations_machine_learning_465"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "loss function, 92 soft, 88 Markov decision process, _see_ MDP Markov\u2019s inequality, 134, 354, 432, 433, 437 martingale differences, 441 Massart\u2019s lemma, 35, 51, 287 Maxent conditional, 316 conditional models, 315 conditional principle, 316 conditional structural models, 330 models, 295, 298, 299, 306, 307, 312, 315\u2013317, 319\u2013321, 325\u2013327, 330\u2013332 principle, 298, 299, 302, 306, 312, 315, 316, 319, 330 structural models, 312 unregularized, 298 unregularized conditional, 316 maximal inequality, 324 maximum a posteriori, 297 maximum entropy models, 295 maximum likelihood principle, 296 McDiarmid\u2019s inequality, 29, 31, 32, 139, 310, 335, 442, 443, 445 MDP, 380, 381, 383, 385, 389\u2013391, 393 finite, 382, 385\u2013387, 399 partially observable, 403 mean squared error, 268 measurable, 429 membership queries, 363 Mercer\u2019s condition, 107, 142 minimization, 361 mistake bound, 179 **Index** **483** bound model, 179 model, 178 mixed strategy, 163 model -based, 393 -free approach, 393 selection, 61, 71 model selection, 61 moment-generating function, 354, 434, 445 mono-label case, 213 Moore-Penrose pseudo-inverse, 412 multi-class classification, xiii, 168, 213\u2013215, 217, 219\u2013222, 224, 225, 228, 229, 232, 233, 235\u2013237, 264, 315, 321, 331 multi-label case, 213 mutual information, 453 NFA, 361 node impurity, 226 noise, 23 non-realizable, 29 non-stationary policy, 382 norm, 409 dual, 158, 287, 301, 308, 324, 327, 410 Frobenius, 412 matrix, 411 operator induced, 411 spectral, 411 normal, 430 distribution, 330, 355, 358, 430, 434, 443, 460 normalization, 110 normalized discounted cumulative gain, _see_ DCG normalized Occam\u2019s razor principle, 23, 43, 79, 269, 362 off-policy algorithm, 401 on-line learning, 7, 177 on-policy algorithm, 402 one-versus-all, 229\u2013232, 236 one-versus-one, 229\u2013232, 238 orthogonal projection, 412 outlier, 87, 168 OVA, _see_ one-versus-all OVO, _see_ one-versus-one PAC, 11 agnostic learning, 22, 48, 61 learnable, 11 learnable with membership queries, 363 learning, 12, 14, 16\u201319, 22, 23, 26\u201328, 45, 57, 145, 146, 343, 361, 363, 364, 376 learning algorithm, 11, 12, 16, 26\u201328, 146 learning framework, 9 weakly learnable, 145 packing numbers, 49 pairwise consistent, 257 independence, 258 parallelogram identity, 457 part-of-speech tagging, 233 partition function, 300, 430 passive, 362 learning, 7 PCA, 347\u2013351, 356\u2013358 penalized risk estimate, 211 Perceptron algorithm, 100, 190\u2013193, 196\u2013199, 201, 206\u2013208, 265 dual, 197 kernel, 197, 206, 212 update, 207 voted, 193, 197 Pinsker\u2019s inequality, 302, 318, 344, 439, 456 pivot, 261 planning, 379, 387 pointwise maximum, 418 supremum, 418 Poisson distribution, 430 policy, 379\u2013381, 390, 393 iteration algorithm, 390 value, 379 POMDP, _see_ MDP partially observable positive, 12 definite, 105, 108, 412 definite symmetric, 107, 108 semidefinite, 108, 412 **484** **Index** precision, 263 preference -based setting, 240, 257 function, 240, 257 prefix-tree automaton, 370 primal problem, 420 principal component analysis, _see_ PCA prior knowledge, 5 probability, 27 probabilistic method, 43 probability, 11 density function, 429 distribution, 429 mass function, 429 space, 429 probably approximately correct, _see_ PAC probit model, 330 proper, 422 pseudo-dimension, 267, 272 pure strategies, 163 Q-learning algorithm, 393, 394, 398, 399 QP, 82, 84, 100, 101, 230, 235, 282, 288 convex, 82, 91, 282\u2013284 quadratic _\u03f5_ -insensitive loss, 283 hinge loss, 88 programming, 82 SVR, 283 QuickSort algorithm, 261 randomized, 260 R\u00b4enyi divergence, 456 entropy, 456 Rademacher complexity, 29\u201336, 43, 48, 50\u201353,",
    "chunk_id": "foundations_machine_learning_466"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "63, 68, 79, 97, 100, 118, 157\u2013159, 169, 213, 219, 220, 236, 239, 241, 243, 251, 263, 267, 269, 270, 274, 275, 287, 298, 316, 333, 460 bound, 43, 287, 298, 316 empirical, 29\u201331, 33, 34, 49, 51\u201353, 93, 97, 117, 118, 143, 157, 216, 250, 271, 277, 283, 287, 323, 324, 327, 443 generalization bound, 158 local, 48 Rademacher variables, 30, 32, 51, 118, 186, 219, 443 radial basis function, 110 Radon\u2019s theorem, 38 random variable, 429 Randomized-Weighted-Majority algorithm, 184, 186, 209 RankBoost, 236\u2013239, 244\u2013255, 264, 265 ranking, 3, 239, 259 RankPerceptron, 265 rational, 126 Rayleigh quotient, 349, 413 RBF, _see_ radial basis function realizable, 29 recall, 263 reconstruction error, 348 regression, 3, 228, 267 kernel, 267, 275, 276 linear, 267, 275 ordinal, 264 support vector, 267, 275, 281 regret, 7, 178, 258 external, 178, 205 internal, 205 swap, 205 regular expressions, 361 languages, 361 regularization -based algorithm, 72 parameter, 73 path, 288 term, 73 reinforcement learning, 7, 379 relative entropy, 450 representer theorem, 117 reproducing property, 111 reversible, 370 languages, 370 reward, 379 probability, 381 risk, 10 risk minimization empirical, 34, 62\u201365, 67, 68, 70, 73, 236 **Index** **485** structural, 64 voted, 78 RKHS, _see_ Hilbert space ROC curve, 239, _see also_ AUC, 255, 256, 264 RWM algorithm, _see_ Randomized-Weighted-Majority algorithm sample complexity, 1, 9 space, 429 Sanov\u2019s theorem, 439 SARSA algorithm, 402, 403 Sauer\u2019s lemma, 40\u201343, 49, 50, 55 scoring function, 240 semi-supervised learning, 6 shattering, 36, 271 shortest-distance, 123 algorithm, 136 singular value, 412 decomposition, 350, 412, 413 singular vector, 412 slack variables, 87 Slater\u2019s condition, 421 weak, 421 Slud\u2019s inequality, 440 SMO algorithm, 84, 100\u2013102 society, 213 soft-max, 235 sphere trees, 225 SPSD, _see_ symmetric positive semidefinite squared loss, 268 SRM, 64\u201370, 72, 73, 77 stability, 334 algorithmic, 333 coefficient, 334 standard deviation, 432 start state, 381 state, 379, 380 state-action value function, 385, 394, 398, 402 stationary point, 415 policy, 382 stochastic approximation, 394 assumption, 361, 362 scenario, 21 subgradient descent, 191 structural risk minimization, _see_ risk minimization structured output, 233 stumps, 154 subdifferential, 337, 422 subgradient, 337, 422 submultiplicative property, 411 supervised learning, 6 support vector, 83, 90, 193 support vector machines, _see_ SVM support vector networks, _see_ SVM SVD, _see_ singular value decomposition SVM, 79\u201384, 86, 90, 91, 100, 101, 103, 105, 116, 130, 143, 156, 162, 170, 194, 196, 207, 222, 243, 281 multi-class, 221, 236 SVMStruct, 235 SVR, 275, 281\u2013285, 289, 291\u2013293, 333, 337, 339\u2013341 dual, 291 on-line, 291 quadratic, 284, 294 symmetric, 411 positive semidefinite, 108, 110, 114, 115, 128, 279, 357, 412, 413 Talagrand\u2019s lemma, 52, 216, 217, 242 tensor product, 114 test sample, 4 theorem Fenchel duality, 300, 307, 328, 426 Fermat\u2019s, 415 Fubini\u2019s, 44, 432 Radon\u2019s, 38 representer, 117 Sanov\u2019s, 439 trace, 411 training sample, 4 transducer weighted, 122 transductive inference, 6 **486** **Index** transition, 360 probability, 381 transpose, 411 trigrams, 106 true positive rate, 256 uncentered, 356 uncombined algorithms, 213, 221 uncorrelated, 433 uniform convergence bound, 15 unnormalized relative entropy, 453 unstable, 228 unsupervised learning, 6 update rule, 401 validation sample, 4 validation set, 68 value, 382",
    "chunk_id": "foundations_machine_learning_467"
  },
  {
    "doc": "foundations_machine_learning.pdf",
    "chunk": "iteration algorithm, 387 variance, 432 VC-dimension, 29, 36\u201340, 42, 43, 45, 46, 48\u201351, 53\u201357, 63, 68, 77, 79, 91, 98, 100, 103, 104, 145, 154, 155, 158, 167, 168, 170, 172, 179, 238, 243, 263, 267, 271\u2013273, 290, 333, 376, 377 generalization bound, 57 visualization, 347 voted risk minimization, _see_ risk minimization weight, 181 function, 262 Weighted-Majority algorithm, 181, 183, 184, 186, 198, 205 Widrow-Hoff algorithm, 289, 290 on-line, 291 Winnow algorithm, 198, 199, 206 update, 198 witness, 271 WM algorithm, _see_ Weighted-Majority algorithm Young\u2019s inequality, 410 zero-sum game, 163 Adaptive Computation and Machine Learning Francis Bach, Editor _Bioinformatics: The Machine Learning Approach_, Pierre Baldi and S\u00f8ren Brunak _Reinforcement Learning: An Introduction_, Richard S. Sutton and Andrew G. Barto _Graphical Models for Machine Learning and Digital Communication_, Brendan J. Frey _Learning in Graphical Models_, Michael I. Jordan _Causation, Prediction, and Search_, second edition, Peter Spirtes, Clark Glymour, and Richard Scheines _Principles of Data Mining_, David Hand, Heikki Mannila, and Padhraic Smyth _Bioinformatics: The Machine Learning Approach_, second edition, Pierre Baldi and Soren Brunak _Learning Kernel Classifiers: Theory and Algorithms_, Ralf Herbrich _Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond_, Bernhard Sch\u00a8olkopf and Alexander J. Smola _Introduction to Machine Learning_, Ethem Alpaydin _Gaussian Processes for Machine Learning_, Carl Edward Rasmussen and Christopher K.I. Williams _Semi-Supervised Learning_, Olivier Chapelle, Bernhard Sch\u00a8olkopf, and Alexander Zien, Eds. _The Minimum Description Length Principle_, Peter D. Gr\u00a8unwald _Introduction to Statistical Relational Learning_, Lise Getoor and Ben Taskar, Eds. _Probabilistic Graphical Models: Principles and Techniques_, Daphne Koller and Nir Friedman _Introduction to Machine Learning_, second edition, Ethem Alpaydin _Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adapta-_ _tion_, Masashi Sugiyama and Motoaki Kawanabe _Boosting: Foundations and Algorithms_, Robert E. Schapire and Yoav Freund _Machine Learning: A Probabilistic Perspective_, Kevin P. Murphy _Foundations of Machine Learning_, Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar _Introduction to Machine Learning_, third edition, Ethem Alpaydin _Deep Learning_, Ian Goodfellow, Yoshua Bengio, and Aaron Courville _Elements of Causal Inference_, Jonas Peters, Dominik Janzing, and Bernhard Sch\u00a8olkopf _Machine Learning for Data Streams, with Practical Examples in MOA_, Albert Bifet, Ricard Gavald`a, Geoffrey Holmes, Bernhard Pfahringer _Foundations of Machine Learning_, second edition, Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar",
    "chunk_id": "foundations_machine_learning_468"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/0.0_Welcome.md",
    "chunk": "<img src=\"images/cuda-python.jpg\" style=\"float: right;\" /> # GPU Development in Python 101 _Written by [Jacob Tomlinson](https://jacobtomlinson.dev)_ **Welcome to the GPU Development in Python 101 tutorial.** I joined NVIDIA in 2019 and since then I\u2019ve gotten to grips with the fundamentals of writing accelerated code in Python. I was amazed to discover that I didn\u2019t need to learn C++ and I didn\u2019t need new development tools. Writing GPU code in Python is easier today than ever, and in this tutorial, I will share what I\u2019ve learned and how you can get started with accelerating your code. In this tutorial we will cover: - What is a GPU and why is it different to a CPU? - An overview of the CUDA development model. - Numba: A high performance compiler for Python. - Writing your first GPU code in Python. - Managing memory. - Understanding what your GPU is doing with pyNVML (memory usage, utilization, etc). - RAPIDS: A suite of GPU accelerated data science libraries. - Working with NumPy style arrays on the GPU. - Working with Pandas style dataframes on the GPU. - Performing some scikit-learn style machine learning on the GPU. Attendees will be expected to have a general knowledge of Python and programming concepts, but no GPU experience will be necessary. The key takeaway for attendees will be the knowledge that they don\u2019t have to do much differently to get their code running on a GPU. ### Outline - **Chapter 1** - Intro to GPUs (30 mins) - **Chapter 2** - Writing low level GPU code in Python with Numba (30 mins) - **Chapter 3** - More Numba (30 mins) - **Chapter 4** - Observability and interoperability (30 mins) - **Chapter 5** - Working with NumPy style arrays in Cupy (30 mins) - **Chapter 6** - Accelerating DataFrames with cuDF (30 mins) - **Chapter 7** - High performance machine learning with cuML (30 mins) - **Chapter 8** - Distributing GPU Python code with Dask (30 mins) ### Requirements #### At a conference Generally when this tutorial is run as part of a conference a compute platform will be provided for attendees. Usually this is a custom [Binder](https://mybinder.org/) platform with GPUs available and dependencies such as drivers preinstalled. If you are following along with this tutorial at a conference right now then refer to your instructor for platform details. #### Running it locally Alternatively you can use your own environment. All workshop material is compatible with systems that meet the [RAPIDS system requirements](https://docs.rapids.ai/install/). If you have a system that meets these requirements, you can install RAPIDS with conda, pip, or docker using the [RAPIDS installation instructions](https://docs.rapids.ai/install/).",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/0.0_Welcome.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/7.0_cuML.md",
    "chunk": "# cuML Another package we are going to explore is [cuML](https://github.com/rapidsai/cuml). Similar to `scikit-learn`, you can use `cuml` to train machine learning models on your data to make predictions. As with other packages in the RAPIDS suite of tools the API of `cuml` is the same as `scikit-learn` but the underlying code has been implemented to run on the GPU. ```python !git clone https://github.com/rapidsai/rapidsai-csp-utils.git !python rapidsai-csp-utils/colab/pip-install.py ``` ```python import cudf ``` Let's look at training a K Nearest Neighbors model to predict whether someone has diabetes based on some other attributes such as their blood pressure, glucose levels, BMI, etc. We start by loading in our data to a GPU dataframe with `cudf`. ```python df = cudf.read_csv(\"https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-python-tutorial/data/diabetes.csv\") df.head() ``` Next we need to create two separate tables. One containing the attributes of the patient except the diabetes column, and one with just the diabetes column. ```python X = df.drop(columns=[\"Outcome\"]) X.head() ``` ```python y = df[\"Outcome\"].values y[0:5] ``` Next we need to use the `train_test_split` method from `cuml` to split our data into two sets. The first larger set will be used to train our model. We will take 80% of the data from each table and call them `X_train` and `y_train`. When the model is trained it will be able to see both sets of data in order to perform clustering. The other 20% of the data will be called `X_test` and `y_test`. Once our model is trained we will feed our `X_test` data through our model to predict whether those people have diabetes. We can then compare those pridictions with the actual `y_test` data to see how accurate our model is. We also set `random_state` to `1` to make the random selection consistent, just for the purposes of this tutorial. We also set `stratify` which means that if 75% of people in our initial data have diabetes then 75% of people in our training set will be guaranteed to have diabetes. ```python from cuml.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y) ``` Now that we have our training data we can import our `KNeighborsClassifier` from `cuml` and fit our model. ```python from cuml.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors = 3) knn.fit(X_train,y_train) ``` Fitting our model happened on our GPU and now we can make some predictions. Let's predict the first five people from our test set. ```python knn.predict(X_test)[0:5] ``` We can see here that our new model thinks that the first patient has diabetes but the rest do not. Let's run the whole test set through the scoring function along with the actual answers and see how well our model performs. ```python knn.score(X_test, y_test) ``` Congratulations! You just trained a machine learning model on the GPU in Python and achieved a score of 69% accuracy. There are a bunch of things we could do here to improve this score, but that is beyond the scope of this tutorial.",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/7.0_cuML.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/3.1_Numba_lab_2.md",
    "chunk": "# Black and white Now that we've had a look at multi-dimensional indexing, why don't you try and use two-dimensional indexing to make our image black and white? Instead of operating over all pixels channel by channel we want to just operate over all pixels and average the channels out. Before you begin, please turn off Google Colab's autocompletion by going to the settings gear in the top right -> Editor -> Uncheck \"Automatically trigger code completions\". ```python import matplotlib as mpl import matplotlib.pyplot as plt from numba import cuda from numba import config as numba_config import numpy as np import math numba_config.CUDA_ENABLE_PYNVJITLINK = True plt.rcParams[\"figure.figsize\"] = (30, 4) ``` **1. Load our image with matplotlib.** ```python !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/main/gpu-python-tutorial/images/numba.png im = ... plt.imshow(im) ``` **2. Move our image to the GPU and create an output array of the same size.** ```python gpu_im = ... gpu_output = ... ``` **3. Set our two-dimensional thead size and block size.** _Hint: Our `threadsperblock` should still multiply to `128`._ ```python threadsperblock = ... blockspergrid_x = ... blockspergrid_y = ... blockspergrid = ... ``` **4. Write our kernel.** ```python @cuda.jit def black_white(im, output): # With our two-dimensional grid we can get our index position in two dimensions x, y = ... # Because our grid is slightly larger than our image anything outside the image should be ignored if x < im.shape[0] and y < im.shape[1]: # Calculate the average across the RGB channels ... # Set all output RGB channels to the average ... # Pass the alpha channel through ... ``` **5. Run the kernel.** ```python ... ``` **6. Move the data back from the GPU and plot it.** ```python plt.imshow(...) ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/3.1_Numba_lab_2.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/4.1_CUDA_Array_Interface.md",
    "chunk": "# CUDA Array Interface Because moving data from the CPU to GPU is expensive, we want to keep as much data located on the GPU as possible at all times. Sometimes in our workflow we want to change which tool we are using too. Perhaps we load an array of data with `cupy` but we want to write a custom CUDA kernel with `numba`. Or perhaps we want to switch to using a Deep Learning framework like `pytorch`. When any of these libraries load data onto the GPU, the array in memory is pretty much the same. The differences between a cupy `ndarray` and a numba `DeviceNDArray` just boil down to how that array is wrapped and hooked into Python. Thankfully with utilities like [DLPack](https://github.com/dmlc/dlpack) and [`__cuda_array__interface__`](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html), we can convert from one type to another without modifying the data on the GPU. We just create a new Python wrapper object and transfer all the device pointers across. Ensuring compatibility between popular GPU Python libraries is one of the core goals of the RAPIDS community. ![](images/array-interface.png) Let's see this in action! We start off by creating an array with CuPy. ```python import cupy as cp cp_arr = cp.random.random((1, 100_000, 10_000)) cp_arr ``` ```python type(cp_arr) ``` Now let's convert this to a Numba array. ```python from numba import cuda from numba import config as numba_config numba_config.CUDA_ENABLE_PYNVJITLINK = True numba_arr = cuda.to_device(cp_arr) numba_arr ``` _Notice that the GPU memory usage stays the same. This is because both `cp_arr` and `numba_arr` reference the same underlying data array, but are different types._ We can also convert our array to a PyTorch `Tensor` object. ```python import torch # Requires pytorch ``` ```python torch_arr = torch.as_tensor(numba_arr, device='cuda') torch_arr ``` ```python type(torch_arr) ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/4.1_CUDA_Array_Interface.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/6.0_cuDF.md",
    "chunk": "# cuDF Now let's move onto some more high level APIs, starting with [cuDF](https://github.com/rapidsai/cudf). Similar to `pandas`, the `cudf` library is a dataframe package for working with tabular datasets. Data is loaded onto the GPU and all operations are performed with GPU compute, but the API of `cudf` should feel very familiar to `pandas` users. ```python !git clone https://github.com/rapidsai/rapidsai-csp-utils.git !python rapidsai-csp-utils/colab/pip-install.py ``` ```python import cudf ``` ## Data loading In this tutorial we have some data stored in `data/`. Most of this data is too small to really benefit from GPU acceleration, but let's explore it anyway. ```python import pandas as pd ``` ```python df = pd.read_csv(\"https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-python-tutorial/data/pageviews_small.csv\", sep=\" \") df.head() ``` ```python pageviews = cudf.read_csv(\"https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/refs/heads/main/gpu-python-tutorial/data/pageviews_small.csv\", sep=\" \") pageviews.head() ``` This `pageviews.csv` file contains just over `1M` records of pageview counts from Wikipedia in various languages. Let's rename the columns and drop the unused `x` column. ```python pageviews.columns = ['project', 'page', 'requests', 'x'] pageviews = pageviews.drop('x', axis=1) pageviews ``` Next, let's count how many English records are in this dataset. ```python print(pageviews[pageviews.project == 'en'].count()) ``` Then let's perform a groupby where we count all of the pages by language. ```python grouped_pageviews = pageviews.groupby('project').count().reset_index() grouped_pageviews ``` And finally, let's have a look at the results for English, French, Chinese, and Polish. ```python print(grouped_pageviews[grouped_pageviews.project.isin(['en', 'fr', 'zh', 'pl'])]) ``` If you have used `pandas` before, then all of this syntax should be very familiar to you. In the same way that `cupy` implements a large portion of the `numpy` API, `cudf` implements a large portion of the `pandas` API. The only difference is that all of our filtering and groupby operations happen on the GPU instead of the CPU giving much better performance. ### Strings GPUs historically are well known for numerical work and have not been used for working with more complex objects. With cuDF, string operations are also accelerated with specialized kernels. This means operations like capitalizing strings can be parallelised on the GPU. ```python pageviews[pageviews.project == 'en'].page.str.upper() ``` ```python pageviews_en = pageviews[pageviews.project == 'en'] print(pageviews_en.page.str.upper().head()) ``` ### UDFs cuDF also has support for user defined functions (UDFs) that can be mapped over a Series or DataFrame in parallel on the GPU. UDFs can be defined as pure Python functions that take a single value. These will be compiled down by Numba at runtime into something that can run on the GPU when we call `.apply()`. ```python def udf(x): if x < 5: return 0 return x ``` ```python pageviews.requests = pageviews.requests.apply(udf) ``` ```python pageviews.requests ``` It is also possible to use Numba directly to write kernels that take pointers to an input column and an output column along with additional arguments. The kernel can then use `cuda.grid` the same way we did in chapters 2/3 to get an index to operate on. We then use `.forall()` to map our kernel over a column. ```python pageviews['mul_requests'] = 0.0 ``` ```python from numba import cuda from numba import config as numba_config numba_config.CUDA_ENABLE_PYNVJITLINK = True ``` ```python @cuda.jit def multiply(in_col, out_col, multiplier): i = cuda.grid(1) if i < in_col.size: # boundary guard out_col[i] = in_col[i] *",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/6.0_cuDF.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/6.0_cuDF.md",
    "chunk": "multiplier ``` ```python multiply.forall(len(pageviews))(pageviews['requests'], pageviews['mul_requests'], 10.0) ``` ```python print(pageviews.head()) ``` ## Rolling windows In cuDF there is also support for applying kernels over rolling windows. This is effectively a 1D stencil and can allow us to perform operations based on our neigbors. ![](images/rolling-windows.png) ```python def neigborhood_mean(window): c = 0 for val in window: c += val return c / len(window) ``` ```python pageviews.requests.rolling(3, 1, True).apply(neigborhood_mean) ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/6.0_cuDF.md_1"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/5.1_Cupy_Lab.md",
    "chunk": "# CuPy Lab It's your turn again. In this lab we will work through some fundamental `cupy` operations. Before you begin, please turn off Google Colab's autocompletion by going to the settings gear in the top right -> Editor -> Uncheck \"Automatically trigger code completions\". ```python import cupy as cp ``` **1. Create the input data array with the numbers `1` to `500_000_000`.** ```python arr = ... arr ``` **2. Calculate how large the array is in GB with `nbytes`** _Hint: GB is `1e9`_ ```python arr... ``` **3. How many dimensions does the array have?** ```python arr... ``` **4. How many elements does the array have?** ```python arr... ``` **5. What is the shape of the array?** ```python arr... ``` **6. Create a new array with `5_000_000` elements representing the linear space of `0` to `1000`.** ```python arr = ... arr ``` **7. Create a random array that is `10_000` by `5_000`.** ```python arr = ... arr ``` **8. Sort that array.** ```python arr = ... arr ``` **Extra Credit 9. Reshape the array to have one dimension of length `5`** ```python arr = ... arr ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/5.1_Cupy_Lab.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/3.0_Numba_gauss.md",
    "chunk": "# Gaussian blur with Numba Let's try doing something a little more complex. Let's take an image and use our GPU to apply a Gaussian blur. ```python import matplotlib as mpl import matplotlib.pyplot as plt from numba import cuda from numba import config as numba_config import numpy as np import math numba_config.CUDA_ENABLE_PYNVJITLINK = True plt.rcParams[\"figure.figsize\"] = (30, 4) ``` ## Data loading We can read in an image file as a NumPy array. Let's use the Numba logo. ```python !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/main/gpu-python-tutorial/images/numba.png im = plt.imread(\"numba.png\") ``` ```python type(im) ``` ```python plt.imshow(im) ``` Let's move the image to the GPU and also create an output array for us to blur our image into. ```python gpu_im = cuda.to_device(im) gpu_output = cuda.to_device(np.zeros_like(gpu_im)) ``` ## Multi-dimensional indexing Before we write our blur function, let's talk about multi-dimensional indexing. In our previous example, we used `cuda.grid(1)` to get our `i` value. The `1` in this call refers to the number of dimensions this index should have. We were working with 1-dimensional array so it made sense to have a 1-dimensional index. Now we are working with an image which has three dimensions: two for `x` and `y`, and one for `channel` (red, green, blue, alpha). ```python im.shape ``` We want our CUDA kernel to operate on every pixel in this image. We could continue to use `i`, ensure `i` is `116 * 434 * 4` and unwrap our index ourselves to determine the pixel we want. But instead we can use three-dimensional indexing so that instead of `i`, we can have `x`, `y`, and `channel` indices. First, we need to specify a three dimensional thread block size. Let's continue using a thread block size of `128` but we can specify this as three numbers which multiply to be `128`. ```python # threadsperblock = 128 threadsperblock = (2, 16, 4) ``` Next we need to calculate our grid size. We will use the dimensions of our image to calculate how many threads we would need to cover our image. ```python blockspergrid_x = math.ceil(gpu_im.shape[0] / threadsperblock[0]) blockspergrid_y = math.ceil(gpu_im.shape[1] / threadsperblock[1]) blockspergrid_z = math.ceil(gpu_im.shape[2] / threadsperblock[2]) blockspergrid = (blockspergrid_x, blockspergrid_y, blockspergrid_z) ``` ```python blockspergrid ``` If we multiply our threads and blocks together, we can see that we have a grid that is slightly larger than our image. ```python [t * b for t, b in zip(threadsperblock, blockspergrid)] ``` ## Writing our blur kernel Our kernel needs to take in the image and the output array. It needs to get the grid position to operate on and then perform the blur on the image in that position. We only care about blurring across the `x` and `y` dimensions, not across the colour channels. So we will just pass the `c` value straight through. ```python @cuda.jit def blur(im, output): # With our three-dimensional grid, we can get our index position in three dimensions x, y, c = cuda.grid(3) # Because our grid is slightly larger than our image, anything outside the image should be ignored if x < im.shape[0] and y < im.shape[1] and c < im.shape[2]: # Set the output",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/3.0_Numba_gauss.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/3.0_Numba_gauss.md",
    "chunk": "array pixel to the average of the nine pixels around that point on the input array output[x, y, c] = (im[x-1, y-1, c] + im[x, y-1, c] + im[x+1, y-1, c] + / im[x-1, y, c] + im[x, y, c] + im[x+1, y, c] + / im[x-1, y+1, c] + im[x+1, y, c] + im[x+1, y+1, c]) / 9 ``` ## Running our kernel Now let's run our kernel a number of times to get our desired level of blur. We need each pass to complete before starting the next one. We also need our output to become our input, and we need a new output to work into. We can reuse the old input as our new output array. This is efficient because we can just reuse both existing arrays on the GPU. This is where manual memory management comes in handy. We can call our kernel many times but leave all the data on the GPU. We just swap pointers around between each pass. ```python for i in range(5): blur[blockspergrid, threadsperblock](gpu_im, gpu_output) gpu_im, gpu_output = gpu_output, gpu_im ``` Now if we look at our image it should be sufficiently blurry. ```python plt.imshow(gpu_output.copy_to_host()) ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/3.0_Numba_gauss.md_1"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/2.0_Numba.md",
    "chunk": "# Numba [Numba](https://numba.pydata.org/) is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code. Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels written in Numba appear to have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically. ## What is a kernel? A kernel is similar to a function, it is a block of code which takes some inputs and is executed by a processor. The difference between a function and a kernel is: - A kernel cannot return anything, it must instead modify memory - A kernel must specify its thread hierarchy (threads and blocks) ## What are grids, threads and blocks (and warps)? [Threads and blocks](https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming) ) are how you instruct you GPU to process some code in parallel. Our GPU is a parallel processor, so we need to specify how many times we want our kernel to be executed. Threads have the benefit of having some shared cache memory between them, but there are a limited number of cores on each GPU so we need to break our work down into blocks which will be scheduled and run in parallel on the GPU. <figure> ![CPU GPU Comparison](images/threads-blocks-warps.png) <figcaption style=\"text-align: center;\"> Image source <a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a> </figcaption> </figure> ### What?? Don't worry too much about this now. Just take away the idea that **we need to specify the number of times we want our kernel to be called**, and that is given as two numbers which are multiplied together to give your overall grid size. Rules of thumb for choosing the number of threads per block: - It should be a multiple of the warp size (32) - A good place to start is 128-512 threads per block, but benchmarking is required to determine the optimal value. ## Hello world Let's dig in with some code and hopefully things will become more clear. To start off, let's write a simple CPU based Python function which we will call repeatedly within a [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions). From a Python perspective, list comprehensions can be a good jumping off point for parallel computing because they feel somewhat parallel already. ```python data = range(10) def foo(i): return i [foo(i) for i in data] ``` Here we have our `foo` function return its index value and use a `for` loop to iterate over our data which is generated by `range`. Next, we will convert this to a CUDA kernel and run it on our GPU with Numba CUDA. First we need to remember that our kernel cannot return anything. Instead, we will use an output list to store the values we would return. ```python data = range(10) output = [] def foo(i): output.append(i) [foo(i) for i in data] output ``` Our next challenge is that our output array on our GPU must have a fixed length. We can't start off with an empty array and keep appending things. So let's use NumPy to create",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/2.0_Numba.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/2.0_Numba.md",
    "chunk": "an array with the same length as our input data. We will also convert our input list to a NumPy array, as that's what we can move to our GPU. ```python import numpy as np ``` ```python data = np.asarray(range(10)) output = np.zeros(len(data)) def foo(i): output[i] = i [foo(i) for i in data] output ``` Now that our pure Python function behaves like a kernel, let's use Numba to convert it into one. ```python from numba import cuda from numba import config as numba_config numba_config.CUDA_ENABLE_PYNVJITLINK = True ``` ```python data = np.asarray(range(10)) output = np.zeros(len(data)) @cuda.jit def foo(input_array, output_array): i = cuda.grid(1) output_array[i] = input_array[i] foo[1, len(data)](data, output) output ``` **W00t, the above code ran on our GPU!** Now let's unpack this a bit. To convert our CPU function into a GPU kernel, we need to add the `@cuda.jit` decorator. This tells Numba to compile our code down to CUDA compatible byte code at runtime. Next, we changed our kernel's inputs to `input_array` and `output_array`. This is because our kernel needs a reference to both arrays in order to interact with them. (More on this later.) But what about `i`? Instead of passing our function the index each time we call it, we can rely on a nice CUDA function called `cuda.grid` which allows our kernel to get its own thread index while it is running. Lastly we make a funny looking function call `foo[blocks, threads](input, output)`. In order to run our kernel on the GPU in parallel, we need to specify how many times we want it to run. Kernel functions are configured using square brackets and passing the block size and thread size. With our array only being `10` elements long, we specify a block size of `1` and a thread size of `10` which means our kernel will be executed `10` times. Then we pass our arguments as normal. ## Something a little bigger Now that we've run our first CUDA kernel with Numba let's try something a little bigger. This time we are going to take a large array and double every number in it. We will do it first in pure Python on the CPU and then in a CUDA kernel on the GPU. Let's start with a large array of 30 million random numbers and an output array of equal length. ```python random_array = np.random.random((30_000_000)) random_array ``` ```python output = np.zeros_like(random_array) output ``` Then in Python, let's iterate over this array and double each item into the output array. We can time the cell to see how long this takes. ```python %%time def foo(i): output[i] = random_array[i] * 2 [foo(i) for i in range(len(random_array))] output ``` For me this takes around 10 seconds for the CPU to do this calculation. Next, let's write a CUDA kernel which does exactly the same thing. The only difference from the previous example is that we set our thread size to a fixed value of `128` and then calculate how many blocks we need to cover the whole array. ```python import math ``` ```python %%time output = np.zeros_like(random_array) threads =",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/2.0_Numba.md_1"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/2.0_Numba.md",
    "chunk": "128 blocks = math.ceil(random_array.shape[0] / threads) @cuda.jit def foo(input_array, output_array): i = cuda.grid(1) output_array[i] = input_array[i] * 2 foo[blocks, threads](random_array, output) output ``` Hooray! This is now orders of magnitude faster, only taking a few hundred milliseconds. The savvy among you may be wondering though, NumPy is already a C based optimised library and we are comparing our GPU kernel with some pure Python code. What if we did it in NumPy? Well you would be right, for in this example NumPy is still faster than our GPU. ```python %%time random_array * 2 ``` But the reason this is the case is because of memory management. ## Memory management Earlier we discussed that the CPU and GPU are effectively two separate computers. Each of these computers has its own memory. All of the data we've worked with so far has been created with `numpy` on the CPU. So in order for `numba` to work with this data on the GPU it has been quietly copying data back and forth for us. This data movement comes with a performance penalty. We also have the option of being in control of the data ourselves. We can explicitly move our arrays to the GPU ahead of time with `cuda.to_device`. ```python gpu_random_array = cuda.to_device(random_array) gpu_output = cuda.to_device(np.zeros_like(random_array)) ``` ```python gpu_random_array ``` Now if we run our kernel again and pass it our GPU memory arrays, we see it does in fact outperform NumPy. ```python %%timeit -n 100 foo[blocks, threads](gpu_random_array, gpu_output) ``` However our output result is also still on the GPU. We explicitly copied it there, so we need to explicitly copy it back with `copy_to_host()`. ```python gpu_output.copy_to_host() ``` Both of these data movement operations take time. But the calculation we are performing here is trivial. As the calculation within our kernel gets more complex, the percentage of time spent copying data becomes smaller and smaller. Memory management is useful in other places too. We may wish to write code where we write many kernels and chain them together. It would be inefficient to copy that data to the GPU and back again between each kernel call. ```python # move array to GPU foo[blocks, threads](data, output) # move data back to CPU # move array to GPU bar[blocks, threads](data, output) # move data back to CPU # move array to GPU baz[blocks, threads](data, output) # move data back to CPU ``` So by explicitly putting our data there we can cut down on this time and be more in control of our computation. ```python # move array to GPU data = cuda.to_device(data) output = cuda.to_device(output) foo[blocks, threads](data, output) bar[blocks, threads](data, output) baz[blocks, threads](data, output) # move data back to CPU data = data.copy_to_host() output = output.copy_to_host() ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/2.0_Numba.md_2"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/2.2_Numba_lab_solution.md",
    "chunk": "# Numba Lab Now it's your turn! Your challenge is to implement Fizz Buzz in CUDA Python with Numba and calculate all instances of `Fizz`, `Buzz`, and `Fizz Buzz` for the numbers between `1` and `50_000_000`. ```python import numpy as np from numba import cuda from numba import config as numba_config numba_config.CUDA_ENABLE_PYNVJITLINK = True ``` **1. Create the input data array.** Try using `np.arange`. ```python data = np.arange(1, 50_000_000) data ``` **2. Create the output data array.** ```python output = np.zeros_like(data) output ``` **3. Calculate the number of threads and blocks.** ```python import math threads = 128 blocks = math.ceil(data.shape[0] / threads) ``` **4. Create the kernel.** _Tip: Our output array must be numeric so try using the values `1`, `2`, and `3` to represent `Fizz`, `Buzz`, and `Fizz Buzz`._ ```python @cuda.jit def fizz_buzz(input_array, output_array): i = cuda.grid(1) n = input_array[i] if n % 3 == 0: output_array[i] = 1 # Fizz if n % 5 == 0: output_array[i] = 2 # Buzz if n % 15 == 0: output_array[i] = 3 # Fizz Buzz ``` **5. Run our kernel.** ```python %%time fizz_buzz[blocks, threads](data, output) output[:12] ``` **Extra Credit 6. Calculate the highest instance of `Fizz`.** ```python idx = np.where(output == 1)[0][-1] data[idx] ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/2.2_Numba_lab_solution.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/5.3_Cupy_nvmath.md",
    "chunk": "# CuPy with nvmath-python The **nvmath-python** (Beta) library brings the power of the NVIDIA math libraries to the Python ecosystem. The package aims to provide intuitive pythonic APIs that provide users full access to all the features offered by NVIDIA\u2019s libraries in a variety of execution spaces. nvmath-python works seamlessly with existing Python array/tensor frameworks and focuses on providing functionality that is missing from those frameworks. This library seeks to meet the needs of:\u200b - Researchers seeking productivity, interoperability with other libraries and frameworks, and performance\u200b - Library/Framework developers seeking out-of-the-box performance and better maintainability through Python\u200b - Kernel developers seeking for highest performance without the need to switch to CUDA\u200b Nvmath-python features:\u200b - Low-level bindings to CUDA math libraries\u200b - Pythonic high-level APIs (host and device): \u200bAt this point limited to extended matmul and FFTs\u200b - Device functions callable in Numba kernels\u200b - Interoperability with NumPy, CuPy, and PyTorch tensors\u200b ## Getting Started 1. Prototype Your Problem ```python # Using NumPy array on CPU for first version import numpy as np import nvmath # NumPy array residing on CPU: a = np.random.rand(128, 1024, 1024) # Execution space is CPU (by default): r = nvmath.fft.fft(a) ``` Note: nvmath-python runs not only on GPU but also supports CPU execution space via NVPL (aarch64) and MKL (x86) 2. Move Prototype to GPU ```python # Using CuPy array to move to the GPU import cupy as cp import nvmath # NumPy array residing on CPU: a = cp.random.rand(128, 1024, 1024) # Execution space is CPU (by default): r = nvmath.fft.fft(a) ``` Note: nvmath-python interoperates with existing tensor libraries (numpy, cupy, pytorch) allowing easy integration with existing CPU and GPU workflows 3. Scale ```python # Use nvmath.distributed in order to scale to multi-GPU import numpy as np import nvmath # NumPy local array residing on CPU a = np.random.rand(128, 1024, 1024) # Distributed result as a local array r = nvmath.distributed.fft.fft(a) ``` Note: nvmath-python scales beyond single GPU at peak library performance # High-Level Modules Provide common out-of-box performant operations without leaving Python. This includes: - Linear Algebra - Fast Fourier Transform The nvmath-python library enables the fusion of epilog operations, offering enhanced performance. Available epilog operations include: - RELU: Applies the Rectified Linear Unit activation function. - GELU: Applies the Gaussian Error Linear Unit activation function. - BIAS: Adds a bias vector. - SIGMOID: Applies the sigmoid function. - TANH: Applies the hyperbolic tangent function. These epilogs can be combined, for example, RELU and BIAS can be fused. Custom epilogs can also be defined as Python functions and compiled using LTO-IR. ## Linear Algebra The nvmath-python library offers a specialized matrix multiplication interface to perform scaled matrix-matrix multiplication with predefined epilog operations as a single fused kernel. This kernel fusion can potentially lead to significantly better efficiency. In addition, nvmath-python\u2019s stateful APIs decompose such operations into planning, autotuning, and execution phases, which enables amortization of one-time preparatory costs across multiple executions. ### Matmul with CuPy Arrays (Stateless) This example demonstrates basic matrix multiplication of CuPy arrays. nvmath-python supports multiple frameworks. The result of each operation",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/5.3_Cupy_nvmath.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/5.3_Cupy_nvmath.md",
    "chunk": "is a tensor of the same framework that was used to pass the inputs. It is also located on the same device as the inputs. ```python import cupy as cp import nvmath # Prepare sample input data. n, m, k = 123, 456, 789 a = cp.random.rand(n, k) b = cp.random.rand(k, m) # Perform the multiplication. result = nvmath.linalg.advanced.matmul(a, b) # Synchronize the default stream, since by default the execution is non-blocking for GPU # operands. cp.cuda.get_current_stream().synchronize() # Check if the result is cupy array as well. print(f\"Inputs were of types {type(a)} and {type(b)} and the result is of type {type(result)}.\") assert isinstance(result, cp.ndarray) ``` ### Matmul with CuPy Arrays (Stateful) This example illustrates the use of stateful matrix multiplication objects. Stateful objects amortize the cost of preparation across multiple executions. The inputs as well as the result are CuPy ndarrays. ```python import cupy as cp import nvmath # Prepare sample input data. m, n, k = 123, 456, 789 a = cp.random.rand(m, k) b = cp.random.rand(k, n) # Use the stateful object as a context manager to automatically release resources. with nvmath.linalg.advanced.Matmul(a, b) as mm: # Plan the matrix multiplication. Planning returns a sequence of algorithms that can be # configured as we'll see in a later example. mm.plan() # Execute the matrix multiplication. result = mm.execute() # Synchronize the default stream, since by default the execution is non-blocking for GPU # operands. cp.cuda.get_current_stream().synchronize() print(f\"Input types = {type(a), type(b)}, device = {a.device, b.device}\") print(f\"Result type = {type(result)}, device = {result.device}\") ``` ### Matmul with CuPy Arrays (Stateless with Epilog) This example demonstrates usage of epilogs. Epilogs allow you to execute extra computations after the matrix multiplication in a single fused kernel. In this example we'll use the BIAS epilog, which adds bias to the result. ```python import cupy as cp import nvmath # Prepare sample input data. m, n, k = 64, 128, 256 a = cp.random.rand(m, k) b = cp.random.rand(k, n) bias = cp.random.rand(m, 1) # Perform the multiplication with BIAS epilog. epilog = nvmath.linalg.advanced.MatmulEpilog.BIAS result = nvmath.linalg.advanced.matmul(a, b, epilog=epilog, epilog_inputs={\"bias\": bias}) # Synchronize the default stream, since by default the execution is non-blocking for GPU # operands. cp.cuda.get_current_stream().synchronize() print(f\"Inputs were of types {type(a)} and {type(b)}, the bias type is {type(bias)}, and the result is of type {type(result)}.\") ``` ## Fast Fourier Transform Backed by the NVIDIA cuFFT library, nvmath-python provides a powerful set of APIs to perform N-dimensional discrete Fourier Transformations. These include forward and inverse transformations for complex-to-complex, complex-to-real, and real-to-complex cases. The operations are available in a variety of precisions, both as host and device APIs. The user can provide callback functions written in Python to selected nvmath-python operations like FFT, which results in a fused kernel and can lead to significantly better performance. Advanced users may benefit from nvmath-python device APIs that enable fusing core mathematical operations like FFT and matrix multiplication into a single kernel, bringing performance close to the theoretical maximum. ### FFT's with CuPy Arrays The input as well as the result from the FFT operations are CuPy ndarrays, resulting in effortless interoperability",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/5.3_Cupy_nvmath.md_1"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/5.3_Cupy_nvmath.md",
    "chunk": "between nvmath-python and CuPy. ```python import cupy as cp import nvmath shape = 512, 256, 512 axes = 0, 1 a = cp.random.rand(*shape, dtype=cp.float64) + 1j * cp.random.rand(*shape, dtype=cp.float64) # Forward FFT along the specified axes, batched along the complement. b = nvmath.fft.fft(a, axes=axes) # Inverse FFT along the specified axes, batched along the complement. c = nvmath.fft.ifft(b, axes=axes) # Synchronize the default stream cp.cuda.get_current_stream().synchronize() print(f\"Input type = {type(a)}, device = {a.device}\") print(f\"FFT output type = {type(b)}, device = {b.device}\") print(f\"IFFT output type = {type(c)}, device = {c.device}\") ``` ### FFT with Callback User-defined functions can be compiled to the LTO-IR format and provided as epilog or prolog to the FFT operation, allowing for Link-Time Optimization and fusing. This example shows how to perform a convolution by providing a Python callback function as prolog to the IFFT operation. ```python import cupy as cp import nvmath # Create the data for the batched 1-D FFT. B, N = 256, 1024 a = cp.random.rand(B, N, dtype=cp.float64) + 1j * cp.random.rand(B, N, dtype=cp.float64) # Create the data to use as filter. filter_data = cp.sin(a) # Define the prolog function for the inverse FFT. # A convolution corresponds to pointwise multiplication in the frequency domain. def convolve(data_in, offset, filter_data, unused): # Note we are accessing `data_out` and `filter_data` with a single `offset` integer, # even though the input and `filter_data` are 2D tensors (batches of samples). # Care must be taken to assure that both arrays accessed here have the same memory # layout. return data_in[offset] * filter_data[offset] / N # Compile the prolog to LTO-IR. with cp.cuda.Device(): prolog = nvmath.fft.compile_prolog(convolve, \"complex128\", \"complex128\") # Perform the forward FFT, followed by the inverse FFT, applying the filter as a prolog. r = nvmath.fft.fft(a, axes=[-1]) r = nvmath.fft.ifft(r, axes=[-1], prolog={ \"ltoir\": prolog, \"data\": filter_data.data.ptr }) ``` # Low-level Modules Provides direct access to CUDA internals and CUDA C math libraries. This includes: - Device API's - Math Library Bindings There is also access to Host API's (and Host API's with callbacks), but we will focus on the Device side here. ## Device API's The device module of nvmath-python `nvmath.device` offers integration with NVIDIA\u2019s high-performance computing libraries through device APIs for cuFFTDx, cuBLASDx, and cuRAND. Detailed documentation for these libraries can be found at [cuFFTDx](https://docs.nvidia.com/cuda/cufftdx/1.2.0/), [cuBLASDx](https://docs.nvidia.com/cuda/cublasdx/0.1.1/), and [cuRAND](https://docs.nvidia.com/cuda/curand/group__DEVICE.html#group__DEVICE) device APIs respectively. Users may take advantage of the device module via the two approaches below: - Numba Extensions: Users can access these device APIs via Numba by utilizing specific extensions that simplify the process of defining functions, querying device traits, and calling device functions. - Third-party JIT Compilers: The APIs are also available through low-level interfaces in other JIT compilers, allowing advanced users to work directly with the raw device code. This example shows how to use the cuRAND to sample a single-precision value from a normal distribution. ```python from numba import cuda from numba import config as numba_config numba_config.CUDA_ENABLE_PYNVJITLINK = True from nvmath.device import random compiled_apis = random.Compile() threads, blocks = 64, 64 nthreads = blocks * threads states = random.StatesPhilox4_32_10(nthreads) # Next, define and launch a setup kernel,",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/5.3_Cupy_nvmath.md_2"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/5.3_Cupy_nvmath.md",
    "chunk": "which will initialize the states using # nvmath.device.random.init function. @cuda.jit(link=compiled_apis.files, extensions=compiled_apis.extension) def setup(states): i = cuda.grid(1) random.init(1234, i, 0, states[i]) setup[blocks, threads](states) # With your states array ready, you can use samplers such as # nvmath.device.random.normal2 to sample random values in your kernels. @cuda.jit(link=compiled_apis.files, extensions=compiled_apis.extension) def kernel(states): i = cuda.grid(1) random_values = random.normal2(states[i]) ``` ## Math Library Bindings Low-level Python bindings for C APIs from NVIDIA Math Libraries are exposed under the corresponding modules in nvmath.bindings. To access the Python bindings, use the modules for the corresponding libraries. Under the hood, nvmath-python handles the run-time linking to the libraries for you lazily. The currently supported libraries along with the corresponding module names are listed as follows: - [cuBLAS](https://docs.nvidia.com/cuda/cublas/) (`nvmath.bindings.cublas`) - [cuBLASLt](https://docs.nvidia.com/cuda/cublas/#using-the-cublaslt-api) (`nvmath.bindings.cublasLt`) - [cuFFT](https://docs.nvidia.com/cuda/cufft/) (`nvmath.bindings.cufft`) - [cuRAND](https://docs.nvidia.com/cuda/curand/index.html) (`nvmath.bindings.curand`) - [cuSOLVER](https://docs.nvidia.com/cuda/cusolver/index.html) (`nvmath.bindings.cusolver`) - [cuSOLVERDn](https://docs.nvidia.com/cuda/cusolver/index.html#cusolverdn-dense-lapack) (`nvmath.bindings.cusolverDn`) - [cuSPARSE](https://docs.nvidia.com/cuda/cusparse/) (`nvmath.bindings.cusparse`) Guidance to translate library function names from C to Python are documented here: https://docs.nvidia.com/cuda/nvmath-python/latest/bindings/index.html ## Links to References nvmath-python home: https://developer.nvidia.com/nvmath-python nvmath-python documentation: https://docs.nvidia.com/cuda/nvmath-python/latest/index.html nvmath-python GitHub repository: https://developer.nvidia.com/nvmath-python Fusting Epilog Operations with Matrix Multiplication Using nvmath-python blog post: https://developer.nvidia.com/blog/fusing-epilog-operations-with-matrix-multiplication-using-nvmath-python/ # Examples A complete set of examples are available in the nvmath-python Github repository: https://github.com/NVIDIA/nvmath-python/tree/main/examples",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/5.3_Cupy_nvmath.md_3"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/2.1_Numba_lab.md",
    "chunk": "# Numba Lab Now it's your turn! Your challenge is to implement Fizz Buzz in CUDA Python with Numba and calculate all instances of `Fizz`, `Buzz`, and `Fizz Buzz` for the numbers between `1` and `50_000_000`. Before you begin, if you're running this notebook in Google Colab, please turn off autocompletion by going to the settings gear in the top right -> Editor -> Uncheck \"Automatically trigger code completions\". ```python import numpy as np from numba import cuda from numba import config as numba_config numba_config.CUDA_ENABLE_PYNVJITLINK = True ``` **1. Create the input data array.** Try using `np.arange`. ```python data = ... data ``` **2. Create the output data array.** ```python output = ... output ``` **3. Calculate the number of threads and blocks.** ```python threads = 128 blocks = ... ``` **4. Create the kernel.** _Tip: Our output array must be numeric so try using the values `1`, `2`, and `3` to represent `Fizz`, `Buzz`, and `Fizz Buzz`._ ```python @cuda.jit def fizz_buzz(input_array, output_array): ... ``` **5. Run our kernel.** ```python %%time fizz_buzz... output[:12] ``` **Extra Credit 6. Calculate the highest instance of `Fizz`.** ```python idx = ... data[idx] ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/2.1_Numba_lab.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/8.0_Multi-GPU_with_Dask.md",
    "chunk": "# Multi-GPU with Dask Leveraging GPUs to accelerate your workloads can result in orders of magnitude increase in performance, but once your workload is fully utilizing the device you will start to reach a new ceiling of performance. This is where multi-GPU and multi-node workloads come in. It is possible to use many GPUs together and see another step change in performance. Before we dive into multi-GPU workloads I do want to caution that distributed computation can increase the complexity of your code. The tools discussed in this chapter do everything they can to ease the burden of distributed computing but we should be sure to check that we have squeezed out every last drop of performance on a single GPU before we start scaling out. ```python !git clone https://github.com/rapidsai/rapidsai-csp-utils.git !python rapidsai-csp-utils/colab/pip-install.py ``` ## Dask [Dask](https://dask.org) is a Python library for scaling out your Python code. At its core, Dask takes your Python code and converts it into a computation graph of function calls, inputs and outputs. It then has a selection of schedulers that it can use to execute through this graph in parallel. Here we are going to focus on Dask's distributed scheduler. ```python from dask.distributed import Client client = Client() client ``` ```python # Submit a function to be executed on the Dask cluster f = client.submit(lambda: 10 + 1) f.result() ``` ```python # Use a high level collection API to distribute familiar work on the cluster import dask.array as da arr = da.random.random((1000, 1000), chunks=(100, 100)) arr.mean().compute() ``` Dask doesn't hugely care about what your code it doing, it just attempts to run through the graph as quickly as possible with its pool of workers. Because we've done all of our GPU computation in Python Dask can also distribute our GPU code too. ```python client.close() ``` ### Distributed clusters In order for Dask to distribute a graph onto many machines it needs a scheduler process and a number of worker processes. We can start these manually, either via the CLI commands `dask-scheduler` and `dask-worker` or using any number of Dask's cluster managers. #### Cluster managers Dask can be run in any number of compute environments as long as you have a Python environment, network connectivity and can start the scheduler and worker processes. To make creating a Dask cluster a consistent experience there are a number of cluster manager classes that you can import and instantiate which will construct a cluster for you. The first that most folks interact with is `LocalCluster`. When you create an instance of this class it will inspect the CPU and memory resources available on the local computer and create subprocesses for the scheduler and an appropriate number of workers automatically. ```python from dask.distributed import LocalCluster cluster = LocalCluster() cluster ``` This is great for trying Dask out and using it to leverage all of the CPU cores available on your local machine. Once you are ready to go beyond the confines of your computer, there are cluster managers for HPC platforms like SLURM, PBS, and SGE. There are also cluster managers for",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/8.0_Multi-GPU_with_Dask.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/8.0_Multi-GPU_with_Dask.md",
    "chunk": "Kubernetes, Hadoop, and public cloud providers including Amazon Web Services, Microsoft Azure, and Google Cloud Platform. See the [Dask deployment documentation](https://docs.dask.org/en/stable/deploying.html) for more details. ```python # You can swap out LocalCluster for other cluster types # from dask.distributed import LocalCluster from dask_kubernetes import KubeCluster # cluster = LocalCluster() cluster = KubeCluster() # swap out for Kubernetes, for example client = cluster.get_client() ``` ```python cluster.close() ``` #### Dask CUDA When it comes to using GPUs with Dask, there are a few things we need to bear in mind. Each Dask worker needs to have exactly one GPU, so if your machine has multiple GPUs, you'll need one worker per device. There are also a few other things that need to be done in order for a Dask worker to be able to successfully leverage a GPU. To simplify this for the user, you can use the tools found in the Python package `dask-cuda`. The Dask CUDA package has a cluster manager called `LocalCUDACluster` and an alternative worker CLI command called `dask-cuda-worker`. Both of these inspect your hardware and start one worker per GPU and correctly configure each worker to only use their assigned device. ```python from dask_cuda import LocalCUDACluster cluster = LocalCUDACluster() cluster ``` ```python client = Client(cluster) ``` It is also possible to configure the other cluster managers that leverage HPC and Cloud capabilities to use the Dask CUDA worker instead of the regular worker. Once we have a Dask cluster with GPU workers we could manually submit some CUDA kernels written with Numba to be executed on those GPUs. ```python from numba import cuda from numba import config as numba_config numba_config.CUDA_ENABLE_PYNVJITLINK = True @cuda.jit def some_kernel(): i = 0 while i < 1_000_000: i += 1 f = client.submit(some_kernel[1024*1024, 1024]) f ``` ### High level collections Thankfully we don't have to do everything manually with Dask. Dask has a concept of high-level collections which implement the API of a popular Python package but chunk/partition up data structures and tasks so that they can be run on a Dask cluster. Commonly folks use `dask.array` which follows the NumPy API, `dask.dataframe` which follows the Pandas API and `dask.ml` which follows the Scikit-Learn API. This approach may sound familiar, we've already seen RAPIDS libraries that mimic the APIs of these libraries to provide accelerated computation. Dask does the same but for distributed computation. One of the benefits of this approach is that we can combine them to get distributed and accelerated computation of the tools we already know and love. When `dask.dataframe` create a DataFrame it constructs a task graph that consists of many smaller Pandas DataFrames. Then operations like taking the mean of a series will first be performed on each Pandas DataFrame before the results are aggregated to get the overall mean. But Dask is not limited to using Pandas in its DataFrame collection, it can also leverage other libraries that follow the Pandas API like cuDF. cuDF comes with a useful helper library for constructing Dask DataFrames made up of cuDF DataFrames and we can load our data and perform operations",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/8.0_Multi-GPU_with_Dask.md_1"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/8.0_Multi-GPU_with_Dask.md",
    "chunk": "just as we have seen before. ```python import dask import cudf import dask_cudf ``` ```python def gen_partition(): return cudf.datasets.timeseries() gddf = dask_cudf.from_map(gen_partition, list(range(30))) gddf ``` ```python gddf.head() ``` ```python len(gddf) ``` ```python gddf.groupby(\"name\").x.mean().compute() ``` But now our DataFrame is distributed across all of our GPUs and computations can leverage the performance of all of our hardware. ### Communication In Chapter 1 when we were exploring Numba CUDA, we saw that there were penalties when it comes to moving data from CPU memory into the GPU memory. The same applies with moving data between GPU memories and between GPUs on different machines. By default Dask uses a custom TCP protocol for communication between workers. This means that any memory transfer from one GPU to another has to make its way back up the PCI-e lanes to the CPU, into the operating system's network stack to be routed to its destination. If the destination GPU is in the same machine it will head back down the PCI-e lanes and into the GPU. If it is located on another machine it will make its way our via the IP network, most likely via an Ethernet connection. In the case where our two GPUs are next to each other on the motherboard, this is very wasteful. They could even be connected to each other directly via NVLINK, or at least connected to the same PCI-e switch on the motherboard. Routing every transfer via the CPU is wasteful, and that's where UCX comes in. #### UCX [UCX](https://openucx.org/) is a network protocol which can inspect the topology of the systems and find an optimal route via accelerated hardware. If two GPUs are connected via NVLINK then UCX will use that to transfer data, if they are connected to the same PCI-e switch that is the next best bet. If the GPUs are on two separate machines but those machines have Infiniband network cards then UCX can leverage RDMA over Infiniband to also transfer data directly between the GPUs. UCX will do everything in its power to transfer data as directly and performantly as possible between two locations before ultimately falling back to TCP. #### Dask communication protocols Dask supports alternative communication protocols which can be configured by the user. This includes UCX which we can leverage for more performance but also other protocols like websockets which may be more flexible in modern system architectures due to being easier to proxy. If we use UCX with our GPU workers and have accelerated networking hardware like NVLINK or Infiniband then we can see much reduced memory transfer times between our GPU workers. ### Resource annotations The last topic I wanted to cover with Dask and GPUs is annotations. Dask has a capability where each task in the task graph can be annotated with requirements that a worker needs to have in order to be able to run it. When we start up a worker we can also add our resource labels so that the scheduler can place appropriate tasks on appropriate workers. This feature is most powerful when our workers",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/8.0_Multi-GPU_with_Dask.md_2"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/8.0_Multi-GPU_with_Dask.md",
    "chunk": "are a mixed bag of configurations. ```console $ dask-cuda-worker scheduler:8786 --resources \"GPU=2\" ``` There may be steps in your task graph where memory usage increases heavily during an intermediate calculation. It can be helpful to steer these tasks towards workers that have more memory than the rest. We can also use this for GPU work if not all of our workers have GPUs. It would be reasonable to have a number of regular Dask workers that take on most of your tasks but also have a couple of GPU workers to run steps that have been optimised to run on the GPU. This would likely be most useful if you have an existing workload that leverages Dask and you want to experiment with GPUs. You could add another worker that has a GPU, choose some tasks in your workflow to optimize with Numba, and annotate those tasks to only run on your GPU worker. ```python foo = client.submit(some_non_gpu_function) with dask.annotate(resources={'GPU': 1}): bar = client.submit(a_gpu_function, foo) baz = client.submit(another_non_gpu_function, bar) ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/8.0_Multi-GPU_with_Dask.md_3"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/1.0_CPU_GPU_Comparison.md",
    "chunk": "# CPU and GPU Comparison The CPU is the most common type of processor for executing your code. CPUs have one or more serial processors which each take single instructions from a stack and **execute them sequentially**. GPUs are a form of coprocessor which are commonly used for video and image rendering, but are extremely popular in machine learning and data science fields too. GPUs have one or more streaming multiprocessors which take in arrays of instructions and **execute them in parallel**. <figure> ![CPU GPU Comparison](images/cpu-gpu.png) <figcaption style=\"text-align: center;\"> Image source <a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a> </figcaption> </figure> ## Mythbusters explanation This video may help explain the concept visually. ```python from IPython.display import YouTubeVideo YouTubeVideo(id='-P28LKWTzrI',width=1000,height=600) ``` ## So how do you control the GPU? Executing code on your GPU feels a lot like executing code on a second computer over a network. If I wanted to send a Python program to another machine to be executed I would need a few things: - A way to copy data and code to the remote machine (SCP, SFTP, SMB, NFS, etc) - A way to log in and execute programs on that remote machine (SSH, VNC, Remote Desktop, etc) ![CPU GPU Comparison](images/two-computers-network.png) To achieve the same things with the GPU we need to use CUDA over PCI. But the idea is still the same &mdash; we need to move data and code to the device and execute that code. ![CPU GPU Comparison](images/computer-gpu-cuda.png) ## What is CUDA? [CUDA](https://developer.nvidia.com/cuda-zone) is an extension to C++ which allows us to compile GPU code and interact with the GPU. ### But I write Python, not C++! Over the last few years NVIDIA has invested in bringing CUDA functionality to Python. Today there are packages like [Numba](https://numba.pydata.org/) which allows us to Just In Time (JIT) compile Python code into something that is compatible with CUDA and provides bindings to transfer data and execute that code. There are also many high level packages such as [CuPy](https://cupy.dev/), [cuDF](https://github.com/rapidsai/cudf), [cuML](https://github.com/rapidsai/cuml), [cuGraph](https://github.com/rapidsai/cugraph), and more which implement functionality in CUDA C++ and then package that with Python bindings so that it can be used directly from Python. These packages are collectively known as [RAPIDS](https://rapids.ai/). Lastly, there is also the [CUDA Python](https://developer.nvidia.com/cuda-python) library which provides Cython/Python wrappers for the CUDA driver and runtime APIs. This tutorial will focus on Numba and RAPIDS.",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/1.0_CPU_GPU_Comparison.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/5.2_Cupy_Lab_solution.md",
    "chunk": "# CuPy Lab It's your turn again. In this lab we will work through some fundamental `cupy` operations. ```python import cupy as cp ``` **1. Create the input data array with the numbers `1` to `500_000_000`.** ```python arr = cp.arange(1, 500_000_000) arr ``` **2. Calculate how large the array is in GB with `nbytes`** _Hint: GB is `1e9`_ ```python arr.nbytes / 1e9 ``` **3. How many dimensions does the array have?** ```python arr.ndim ``` **4. How many elements does the array have?** ```python arr.size ``` **5. What is the shape of the array?** ```python arr.shape ``` **6. Create a new array with `5_000_000` elements representing the linear space of `0` to `1000`.** ```python arr = cp.linspace(0, 1000, num=5_000_000) arr ``` **7. Create a random array that is `10_000` by `5_000`.** ```python arr = cp.random.random((10_000, 5_000)) arr ``` **8. Sort that array.** ```python arr = cp.sort(arr) arr ``` **Extra Credit 9. Reshape the array to have one dimension of length `5`** ```python arr = arr.reshape(10_000, 1_000, 5) arr ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/5.2_Cupy_Lab_solution.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/4.0_pyNVML.md",
    "chunk": "# What is my GPU doing? Now that we are executing code on our GPUs, we may want to understand more about how well we are utilising our hardware. This tutorial is designed to be used with [jupyterlab-nvdashboard](https://github.com/rapidsai/jupyterlab-nvdashboard) which is a Jupyter Lab extension that shows graphs of your GPU utilization, memory and transfer speeds. ![](images/nvdashboard.gif) All of the APIs used to create this dashboard are available in Python, so let's dig into pyNVML ourselves. NVML stands for the NVIDIA Management Library and is a package which is included with NVIDIA drivers to report information about GPU status. We can import and use the [`pyNVML` package](https://pypi.org/project/pynvml/) to explore this data. ```python !pip install nvidia-ml-py ``` ```python import pynvml pynvml.nvmlInit() ``` This package provices direct bindings to the C NVML library, so pretty much anything mentioned in their documentation is possible. We can see how many GPUs we have. ```python pynvml.nvmlDeviceGetCount() ``` What our driver version is. ```python pynvml.nvmlSystemGetDriverVersion() ``` We can then grab a handle for each GPU to query specific metrics about it. ```python gpus = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in range(pynvml.nvmlDeviceGetCount())] gpus ``` We can get the model. ```python [pynvml.nvmlDeviceGetName(gpu) for gpu in gpus] ``` We can get memory info (let's convert it to GB) ```python [pynvml.nvmlDeviceGetMemoryInfo(gpu).used / 1e9 for gpu in gpus] ``` ```python [pynvml.nvmlDeviceGetMemoryInfo(gpu).free / 1e9 for gpu in gpus] ``` By accessing rich metrics from our GPUs we can make more informed decisions on how our code is performing and how we can divide up our work.",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/4.0_pyNVML.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/5.0_Cupy.md",
    "chunk": "# CuPy Now that we've explored some low level GPU APIs with Numba, let's shift gears and work with some high level array functionality in [CuPy](https://cupy.dev/). CuPy has maintainers from many organisations including NVIDIA. CuPy implements the familiar NumPy API but with the backend written in CUDA C++. This allows folks who are already familiar with NumPy to get GPU acceleration out of the box quickly by just switching out an import. ```python import numpy as np import cupy as cp cp.cuda.Stream.null.synchronize() ``` Let's walk through some simple examples from this blog post: https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56 ## Creating arrays First let's create ourselves an `2GB` array both on the CPU and GPU and compare how long this takes. ```python %%timeit -r 1 -n 10 global x_cpu x_cpu = np.ones((1000, 500, 500)) ``` ```python %%timeit -n 10 global x_gpu x_gpu = cp.ones((1000, 500, 500)) cp.cuda.Stream.null.synchronize() ``` _Note we need to call `cp.cuda.Stream.null.synchronize()` explicitly here for our timings to be fair. By default cupy will run GPU code concurrently and the function will exit before the GPU has finished. Calling `synchronize()` makes us wait for the GPU to finish before returning._ We can see here that creating this array on the GPU is much faster than doing so on the CPU, but this time our code looks exactly the same. We haven't had to worry about kernels, theads, blocks or any of that stuff. ## Basic operations Next let's have a look at doing some math on our arrays. We can start by multiplying every value in our arrays by `5`. ```python %%time x_cpu *= 5 ``` ```python %%time x_gpu *= 5 cp.cuda.Stream.null.synchronize() ``` Again the GPU completes this much faster, but the code stays the same. Now let's do a couple of operations sequentially, something which would've suffered from memory transfer times in our Numba examples without explicit memory management. ```python %%time x_cpu *= 5 x_cpu *= x_cpu x_cpu += x_cpu ``` ```python %%time x_gpu *= 5 x_gpu *= x_gpu x_gpu += x_gpu cp.cuda.Stream.null.synchronize() ``` Again we can see the GPU ran that much faster even without us explicitly managing memory. This is because CuPy is handling all of this for us transparently. ## More complex operations Now that we've tried out some operators, let's dive into some NumPy functions. Let's compare running a singular value decomposition on a slightly smaller array of data. ```python %%time x_cpu = np.random.random((1000, 1000)) u, s, v = np.linalg.svd(x_cpu) ``` ```python %%time x_gpu = cp.random.random((1000, 1000)) u, s, v = cp.linalg.svd(x_gpu) ``` As we can see the GPU outperforms the CPU again with exactly the same API. It is also interesting to note here that NumPy can intelligently dispatch function calls like this. In the above example we called `cp.linalg.svd`, but we could also call `np.linalg.svd` and pass it our GPU array. NumPy would inspect the input and call `cp.linalg.svd` on our behalf. This makes it even easier to introduce `cupy` into your code with minimal changes. ```python %%time x_gpu = cp.random.random((1000, 1000)) u, s, v = np.linalg.svd(x_gpu) # Note the `np` used here cp.cuda.Stream.null.synchronize() ``` ## Devices CuPy",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/5.0_Cupy.md_0"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/5.0_Cupy.md",
    "chunk": "has a concept of a current device, which is the default GPU device on which the allocation, manipulation, calculation, etc., of arrays take place. Suppose ID of the current device is `0`. In such a case, the following code would create an array `x_on_gpu0` on GPU 0. ```python with cp.cuda.Device(0): x_on_gpu0 = cp.random.random((100000, 1000)) x_on_gpu0.device ``` In general, CuPy functions expect that the array is on the same device as the current one. Passing an array stored on a non-current device may work depending on the hardware configuration but is generally discouraged as it may not be performant.",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/5.0_Cupy.md_1"
  },
  {
    "doc": "/home/asrin101/workspace/documents/converted_markdown/3.2_Numba_lab_2_solution.md",
    "chunk": "# Black and white Now that we've had a look at multi-dimensional indexing, why don't you try and use two-dimensional indexing to make our image black and white? Instead of operating over all pixels channel by channel we want to just operate over all pixels and average the channels out. ```python import matplotlib as mpl import matplotlib.pyplot as plt from numba import cuda from numba import config as numba_config import numpy as np import math numba_config.CUDA_ENABLE_PYNVJITLINK = True plt.rcParams[\"figure.figsize\"] = (30, 4) ``` **1. Load our image with matplotlib.** ```python !wget https://raw.githubusercontent.com/NVIDIA/accelerated-computing-hub/main/gpu-python-tutorial/images/numba.png im = plt.imread(\"numba.png\") plt.imshow(im) ``` **2. Move our image to the GPU and create an output array of the same size.** ```python gpu_im = cuda.to_device(im) gpu_output = cuda.to_device(np.zeros_like(gpu_im)) ``` **3. Set our two-dimensional thead size and block size.** _Hint: Our `threadsperblock` should still multiply to `128`._ ```python threadsperblock = (16, 16) blockspergrid_x = math.ceil(gpu_im.shape[0] / threadsperblock[0]) blockspergrid_y = math.ceil(gpu_im.shape[1] / threadsperblock[1]) blockspergrid = (blockspergrid_x, blockspergrid_y) ``` **4. Write our kernel.** ```python @cuda.jit def black_white(im, output): # With our two-dimensional grid we can get our index position in two dimensions x, y = cuda.grid(2) # Because our grid is slightly larger than our image anything outside the image should be ignored if x < im.shape[0] and y < im.shape[1]: # Calculate the average across the RGB channels average = (im[x, y, 0] + im[x, y, 1] + im[x, y, 2]) / 3 # Set all output RGB channels to the average output[x, y, 0] = average output[x, y, 1] = average output[x, y, 2] = average # Pass the alpha channel through output[x, y, 3] = im[x, y, 3] ``` **5. Run the kernel.** ```python black_white[blockspergrid, threadsperblock](gpu_im, gpu_output) ``` **6. Move the data back from the GPU and plot it.** ```python plt.imshow(gpu_output.copy_to_host()) ```",
    "chunk_id": "/home/asrin101/workspace/documents/converted_markdown/3.2_Numba_lab_2_solution.md_0"
  }
]